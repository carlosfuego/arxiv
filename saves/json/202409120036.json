[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.06691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06691v1",
                "updated": "2024-09-10T17:54:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:54:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric-Averaged Preference Optimization for Soft Preference Labels"
                },
                "summary": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority."
                },
                "authors": [
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "name": "Shixiang Shane Gu"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Heiga Zen"
                    },
                    {
                        "name": "Izzeddin Gur"
                    }
                ],
                "author_detail": {
                    "name": "Izzeddin Gur"
                },
                "author": "Izzeddin Gur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06680v1",
                "updated": "2024-09-10T17:44:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    38,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:44:38Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    38,
                    1,
                    254,
                    0
                ],
                "title": "Sequential stratified inference for the mean",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential stratified inference for the mean"
                },
                "summary": "We develop conservative tests for the mean of a bounded population using data\nfrom a stratified sample. The sample may be drawn sequentially, with or without\nreplacement. The tests are \"anytime valid,\" allowing optional stopping and\ncontinuation in each stratum. We call this combination of properties\nsequential, finite-sample, nonparametric validity. The methods express a\nhypothesis about the population mean as a union of intersection hypotheses\ndescribing within-stratum means. They test each intersection hypothesis using\nindependent test supermartingales (TSMs) combined across strata by\nmultiplication. The $P$-value of the global null hypothesis is then the maximum\n$P$-value of any intersection hypothesis in the union. This approach has three\nprimary moving parts: (i) the rule for deciding which stratum to draw from next\nto test each intersection null, given the sample so far; (ii) the form of the\nTSM for each null in each stratum; and (iii) the method of combining evidence\nacross strata. These choices interact. We examine the performance of a variety\nof rules with differing computational complexity. Approximately optimal methods\nhave a prohibitive computational cost, while naive rules may be inconsistent --\nthey will never reject for some alternative populations, no matter how large\nthe sample. We present a method that is statistically comparable to optimal\nmethods in examples where optimal methods are computable, but computationally\ntractable for arbitrarily many strata. In numerical examples its expected\nsample size is substantially smaller than that of previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop conservative tests for the mean of a bounded population using data\nfrom a stratified sample. The sample may be drawn sequentially, with or without\nreplacement. The tests are \"anytime valid,\" allowing optional stopping and\ncontinuation in each stratum. We call this combination of properties\nsequential, finite-sample, nonparametric validity. The methods express a\nhypothesis about the population mean as a union of intersection hypotheses\ndescribing within-stratum means. They test each intersection hypothesis using\nindependent test supermartingales (TSMs) combined across strata by\nmultiplication. The $P$-value of the global null hypothesis is then the maximum\n$P$-value of any intersection hypothesis in the union. This approach has three\nprimary moving parts: (i) the rule for deciding which stratum to draw from next\nto test each intersection null, given the sample so far; (ii) the form of the\nTSM for each null in each stratum; and (iii) the method of combining evidence\nacross strata. These choices interact. We examine the performance of a variety\nof rules with differing computational complexity. Approximately optimal methods\nhave a prohibitive computational cost, while naive rules may be inconsistent --\nthey will never reject for some alternative populations, no matter how large\nthe sample. We present a method that is statistically comparable to optimal\nmethods in examples where optimal methods are computable, but computationally\ntractable for arbitrarily many strata. In numerical examples its expected\nsample size is substantially smaller than that of previous methods."
                },
                "authors": [
                    {
                        "name": "Jacob V. Spertus"
                    },
                    {
                        "name": "Mayuri Sridhar"
                    },
                    {
                        "name": "Philip B. Stark"
                    }
                ],
                "author_detail": {
                    "name": "Philip B. Stark"
                },
                "author": "Philip B. Stark",
                "arxiv_comment": "43 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06679v1",
                "updated": "2024-09-10T17:44:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    35,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:44:35Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    35,
                    1,
                    254,
                    0
                ],
                "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning"
                },
                "summary": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling."
                },
                "authors": [
                    {
                        "name": "Zihan Liao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17692v2",
                "updated": "2024-09-10T17:39:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    39,
                    41,
                    1,
                    254,
                    0
                ],
                "published": "2024-01-31T09:28:06Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    9,
                    28,
                    6,
                    2,
                    31,
                    0
                ],
                "title": "Mitigating the Influence of Distractor Tasks in LMs with Prior-Aware\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the Influence of Distractor Tasks in LMs with Prior-Aware\n  Decoding"
                },
                "summary": "The broad capabilities of Language Models (LMs) can be limited by their\nsensitivity to distractor tasks: LMs can infer secondary tasks from the prompt\nin addition to the intended one, leading to unwanted outputs. For example,\nprompt injection attacks can cause models to deviate from explicit directives.\nIn some 'inverse scaling' cases, this unwanted behaviour actually worsens as\nmodels scale up to at least 540B parameters. We present a theoretical framework\nthat interprets LMs as a product of experts that combine multiple data\ngeneration processes. Based on this framework, we demonstrate prior-aware\ndecoding (PAD) - a simple contrastive inference method to reduce the influence\nof distractor tasks. We apply PAD to eleven models, across four datasets, and\nfind improvements in 41 out of 44 task-model combinations, with a median\nincrease in task completion proportion of 40%. The results suggest a promising\ndirection for further development towards more reliable language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The broad capabilities of Language Models (LMs) can be limited by their\nsensitivity to distractor tasks: LMs can infer secondary tasks from the prompt\nin addition to the intended one, leading to unwanted outputs. For example,\nprompt injection attacks can cause models to deviate from explicit directives.\nIn some 'inverse scaling' cases, this unwanted behaviour actually worsens as\nmodels scale up to at least 540B parameters. We present a theoretical framework\nthat interprets LMs as a product of experts that combine multiple data\ngeneration processes. Based on this framework, we demonstrate prior-aware\ndecoding (PAD) - a simple contrastive inference method to reduce the influence\nof distractor tasks. We apply PAD to eleven models, across four datasets, and\nfind improvements in 41 out of 44 task-model combinations, with a median\nincrease in task completion proportion of 40%. The results suggest a promising\ndirection for further development towards more reliable language models."
                },
                "authors": [
                    {
                        "name": "Raymond Douglas"
                    },
                    {
                        "name": "Andis Draguns"
                    },
                    {
                        "name": "Tomáš Gavenčiak"
                    }
                ],
                "author_detail": {
                    "name": "Tomáš Gavenčiak"
                },
                "author": "Tomáš Gavenčiak",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06666v1",
                "updated": "2024-09-10T17:34:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    34,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:34:34Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    34,
                    34,
                    1,
                    254,
                    0
                ],
                "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models"
                },
                "summary": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future."
                },
                "authors": [
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Zhengrui Ma"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00055v2",
                "updated": "2024-09-10T17:26:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    26,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-21T04:47:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    47,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models"
                },
                "summary": "The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA."
                },
                "authors": [
                    {
                        "name": "Yang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Cao"
                },
                "author": "Yang Cao",
                "arxiv_comment": "12 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06654v1",
                "updated": "2024-09-10T17:17:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    17,
                    53,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:17:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    17,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "Estimation and Inference for Causal Functions with Multiway Clustered\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and Inference for Causal Functions with Multiway Clustered\n  Data"
                },
                "summary": "This paper proposes methods of estimation and uniform inference for a general\nclass of causal functions, such as the conditional average treatment effects\nand the continuous treatment effects, under multiway clustering. The causal\nfunction is identified as a conditional expectation of an adjusted\n(Neyman-orthogonal) signal that depends on high-dimensional nuisance\nparameters. We propose a two-step procedure where the first step uses machine\nlearning to estimate the high-dimensional nuisance parameters. The second step\nprojects the estimated Neyman-orthogonal signal onto a dictionary of basis\nfunctions whose dimension grows with the sample size. For this two-step\nprocedure, we propose both the full-sample and the multiway cross-fitting\nestimation approaches. A functional limit theory is derived for these\nestimators. To construct the uniform confidence bands, we develop a novel\nresampling procedure, called the multiway cluster-robust sieve score bootstrap,\nthat extends the sieve score bootstrap (Chen and Christensen, 2018) to the\nnovel setting with multiway clustering. Extensive numerical simulations\nshowcase that our methods achieve desirable finite-sample behaviors. We apply\nthe proposed methods to analyze the causal relationship between mistrust levels\nin Africa and the historical slave trade. Our analysis rejects the null\nhypothesis of uniformly zero effects and reveals heterogeneous treatment\neffects, with significant impacts at higher levels of trade volumes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes methods of estimation and uniform inference for a general\nclass of causal functions, such as the conditional average treatment effects\nand the continuous treatment effects, under multiway clustering. The causal\nfunction is identified as a conditional expectation of an adjusted\n(Neyman-orthogonal) signal that depends on high-dimensional nuisance\nparameters. We propose a two-step procedure where the first step uses machine\nlearning to estimate the high-dimensional nuisance parameters. The second step\nprojects the estimated Neyman-orthogonal signal onto a dictionary of basis\nfunctions whose dimension grows with the sample size. For this two-step\nprocedure, we propose both the full-sample and the multiway cross-fitting\nestimation approaches. A functional limit theory is derived for these\nestimators. To construct the uniform confidence bands, we develop a novel\nresampling procedure, called the multiway cluster-robust sieve score bootstrap,\nthat extends the sieve score bootstrap (Chen and Christensen, 2018) to the\nnovel setting with multiway clustering. Extensive numerical simulations\nshowcase that our methods achieve desirable finite-sample behaviors. We apply\nthe proposed methods to analyze the causal relationship between mistrust levels\nin Africa and the historical slave trade. Our analysis rejects the null\nhypothesis of uniformly zero effects and reveals heterogeneous treatment\neffects, with significant impacts at higher levels of trade volumes."
                },
                "authors": [
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Yanbo Liu"
                    },
                    {
                        "name": "Yuya Sasaki"
                    }
                ],
                "author_detail": {
                    "name": "Yuya Sasaki"
                },
                "author": "Yuya Sasaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06653v1",
                "updated": "2024-09-10T17:16:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    16,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:16:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    16,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "Human Perception of LLM-generated Text Content in Social Media\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Perception of LLM-generated Text Content in Social Media\n  Environments"
                },
                "summary": "Emerging technologies, particularly artificial intelligence (AI), and more\nspecifically Large Language Models (LLMs) have provided malicious actors with\npowerful tools for manipulating digital discourse. LLMs have the potential to\naffect traditional forms of democratic engagements, such as voter choice,\ngovernment surveys, or even online communication with regulators; since bots\nare capable of producing large quantities of credible text. To investigate the\nhuman perception of LLM-generated content, we recruited over 1,000 participants\nwho then tried to differentiate bot from human posts in social media discussion\nthreads. We found that humans perform poorly at identifying the true nature of\nuser posts on social media. We also found patterns in how humans identify\nLLM-generated text content in social media discourse. Finally, we observed the\nUncanny Valley effect in text dialogue in both user perception and\nidentification. This indicates that despite humans being poor at the\nidentification process, they can still sense discomfort when reading\nLLM-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging technologies, particularly artificial intelligence (AI), and more\nspecifically Large Language Models (LLMs) have provided malicious actors with\npowerful tools for manipulating digital discourse. LLMs have the potential to\naffect traditional forms of democratic engagements, such as voter choice,\ngovernment surveys, or even online communication with regulators; since bots\nare capable of producing large quantities of credible text. To investigate the\nhuman perception of LLM-generated content, we recruited over 1,000 participants\nwho then tried to differentiate bot from human posts in social media discussion\nthreads. We found that humans perform poorly at identifying the true nature of\nuser posts on social media. We also found patterns in how humans identify\nLLM-generated text content in social media discourse. Finally, we observed the\nUncanny Valley effect in text dialogue in both user perception and\nidentification. This indicates that despite humans being poor at the\nidentification process, they can still sense discomfort when reading\nLLM-generated content."
                },
                "authors": [
                    {
                        "name": "Kristina Radivojevic"
                    },
                    {
                        "name": "Matthew Chou"
                    },
                    {
                        "name": "Karla Badillo-Urquiola"
                    },
                    {
                        "name": "Paul Brenner"
                    }
                ],
                "author_detail": {
                    "name": "Paul Brenner"
                },
                "author": "Paul Brenner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07807v2",
                "updated": "2024-09-10T17:13:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    13,
                    49,
                    1,
                    254,
                    0
                ],
                "published": "2023-12-13T00:04:19Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    0,
                    4,
                    19,
                    2,
                    347,
                    0
                ],
                "title": "IPA: Class 0 Protostars Viewed in CO Emission Using JWST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPA: Class 0 Protostars Viewed in CO Emission Using JWST"
                },
                "summary": "We investigate the bright CO fundamental emission in the central regions of\nfive protostars in their primary mass assembly phase using new observations\nfrom JWST's Near-Infrared Spectrograph (NIRSpec) and Mid-Infrared Instrument\n(MIRI). CO line emission images and fluxes are extracted for a forest of\n$\\sim$150 ro-vibrational transitions from two vibrational bands, $v=1-0$ and\n$v=2-1$. However, ${}^{13}$CO is undetected, indicating that ${}^{12}$CO\nemission is optically thin. We use H$_2$ emission lines to correct fluxes for\nextinction and then construct rotation diagrams for the CO lines with the\nhighest spectral resolution and sensitivity to estimate rotational temperatures\nand numbers of CO molecules. Two distinct rotational temperature components are\nrequired for $v=1$ ($\\sim600$ to 1000 K and 2000 to $\\sim 10^4$ K), while one\nhotter component is required for $v=2$ ($\\gtrsim 3500$ K). ${}^{13}$CO is\ndepleted compared to the abundances found in the ISM, indicating selective UV\nphotodissociation of ${}^{13}$CO; therefore, UV radiative pumping may explain\nthe higher rotational temperatures in $v=2$. The average vibrational\ntemperature is $\\sim 1000$ K for our sources and is similar to the lowest\nrotational temperature components. Using the measured rotational and\nvibrational temperatures to infer a total number of CO molecules, we find that\nthe total gas masses range from lower limits of $\\sim10^{22}$ g for the lowest\nmass protostars to $\\sim 10^{26}$ g for the highest mass protostars. Our gas\nmass lower limits are compatible with those in more evolved systems, which\nsuggest the lowest rotational temperature component comes from the inner disk,\nscattered into our line of sight, but we also cannot exclude the contribution\nto the CO emission from disk winds for higher mass targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the bright CO fundamental emission in the central regions of\nfive protostars in their primary mass assembly phase using new observations\nfrom JWST's Near-Infrared Spectrograph (NIRSpec) and Mid-Infrared Instrument\n(MIRI). CO line emission images and fluxes are extracted for a forest of\n$\\sim$150 ro-vibrational transitions from two vibrational bands, $v=1-0$ and\n$v=2-1$. However, ${}^{13}$CO is undetected, indicating that ${}^{12}$CO\nemission is optically thin. We use H$_2$ emission lines to correct fluxes for\nextinction and then construct rotation diagrams for the CO lines with the\nhighest spectral resolution and sensitivity to estimate rotational temperatures\nand numbers of CO molecules. Two distinct rotational temperature components are\nrequired for $v=1$ ($\\sim600$ to 1000 K and 2000 to $\\sim 10^4$ K), while one\nhotter component is required for $v=2$ ($\\gtrsim 3500$ K). ${}^{13}$CO is\ndepleted compared to the abundances found in the ISM, indicating selective UV\nphotodissociation of ${}^{13}$CO; therefore, UV radiative pumping may explain\nthe higher rotational temperatures in $v=2$. The average vibrational\ntemperature is $\\sim 1000$ K for our sources and is similar to the lowest\nrotational temperature components. Using the measured rotational and\nvibrational temperatures to infer a total number of CO molecules, we find that\nthe total gas masses range from lower limits of $\\sim10^{22}$ g for the lowest\nmass protostars to $\\sim 10^{26}$ g for the highest mass protostars. Our gas\nmass lower limits are compatible with those in more evolved systems, which\nsuggest the lowest rotational temperature component comes from the inner disk,\nscattered into our line of sight, but we also cannot exclude the contribution\nto the CO emission from disk winds for higher mass targets."
                },
                "authors": [
                    {
                        "name": "Adam E. Rubinstein Neal J. Evans II"
                    },
                    {
                        "name": "Himanshu Tyagi"
                    },
                    {
                        "name": "Mayank Narang"
                    },
                    {
                        "name": "Pooneh Nazari"
                    },
                    {
                        "name": "Robert Gutermuth"
                    },
                    {
                        "name": "Samuel Federman"
                    },
                    {
                        "name": "P. Manoj"
                    },
                    {
                        "name": "Joel D. Green"
                    },
                    {
                        "name": "Dan M. Watson"
                    },
                    {
                        "name": "S. Thomas Megeath"
                    },
                    {
                        "name": "Will R. M. Rocha"
                    },
                    {
                        "name": "Nashanty G. C. Brunken"
                    },
                    {
                        "name": "Katerina Slavicinska"
                    },
                    {
                        "name": "Ewine F. van Dishoeck"
                    },
                    {
                        "name": "Henrik Beuther"
                    },
                    {
                        "name": "Tyler L. Bourke"
                    },
                    {
                        "name": "Alessio Caratti o Garatti"
                    },
                    {
                        "name": "Lee Hartmann"
                    },
                    {
                        "name": "Pamela Klaassen"
                    },
                    {
                        "name": "Hendrik Linz"
                    },
                    {
                        "name": "Leslie W. Looney"
                    },
                    {
                        "name": "James Muzerolle"
                    },
                    {
                        "name": "Thomas Stanke"
                    },
                    {
                        "name": "John J. Tobin"
                    },
                    {
                        "name": "Scott J. Wolk"
                    },
                    {
                        "name": "Yao-Lun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yao-Lun Yang"
                },
                "author": "Yao-Lun Yang",
                "arxiv_comment": "31 pages, 7 figures, 4 tables, received to ApJ December 10 2023,\n  accepted to ApJ August 4 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06646v1",
                "updated": "2024-09-10T17:05:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    5,
                    11,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:05:11Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    5,
                    11,
                    1,
                    254,
                    0
                ],
                "title": "Optimal Workload Placement on Multi-Instance GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Workload Placement on Multi-Instance GPUs"
                },
                "summary": "There is an urgent and pressing need to optimize usage of Graphical\nProcessing Units (GPUs), which have arguably become one of the most expensive\nand sought after IT resources. To help with this goal, several of the current\ngeneration of GPUs support a partitioning feature, called Multi-Instance GPU\n(MIG) to allow multiple workloads to share a GPU, albeit with some constraints.\nIn this paper we investigate how to optimize the placement of Large Language\nModel (LLM)-based AI Inferencing workloads on GPUs. We first identify and\npresent several use cases that are encountered in practice that require\nworkloads to be efficiently placed or migrated to other GPUs to make room for\nincoming workloads. The overarching goal is to use as few GPUs as possible and\nto further minimize memory and compute wastage on GPUs that are utilized. We\nhave developed two approaches to address this problem: an optimization method\nand a heuristic method. We benchmark these with two workload scheduling\nheuristics for multiple use cases. Our results show up to 2.85x improvement in\nthe number of GPUs used and up to 70% reduction in GPU wastage over baseline\nheuristics. We plan to enable the SRE community to leverage our proposed method\nin production environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an urgent and pressing need to optimize usage of Graphical\nProcessing Units (GPUs), which have arguably become one of the most expensive\nand sought after IT resources. To help with this goal, several of the current\ngeneration of GPUs support a partitioning feature, called Multi-Instance GPU\n(MIG) to allow multiple workloads to share a GPU, albeit with some constraints.\nIn this paper we investigate how to optimize the placement of Large Language\nModel (LLM)-based AI Inferencing workloads on GPUs. We first identify and\npresent several use cases that are encountered in practice that require\nworkloads to be efficiently placed or migrated to other GPUs to make room for\nincoming workloads. The overarching goal is to use as few GPUs as possible and\nto further minimize memory and compute wastage on GPUs that are utilized. We\nhave developed two approaches to address this problem: an optimization method\nand a heuristic method. We benchmark these with two workload scheduling\nheuristics for multiple use cases. Our results show up to 2.85x improvement in\nthe number of GPUs used and up to 70% reduction in GPU wastage over baseline\nheuristics. We plan to enable the SRE community to leverage our proposed method\nin production environments."
                },
                "authors": [
                    {
                        "name": "Bekir Turkkan"
                    },
                    {
                        "name": "Pavankumar Murali"
                    },
                    {
                        "name": "Pavithra Harsha"
                    },
                    {
                        "name": "Rohan Arora"
                    },
                    {
                        "name": "Gerard Vanloo"
                    },
                    {
                        "name": "Chandra Narayanaswami"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Narayanaswami"
                },
                "author": "Chandra Narayanaswami",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.09947v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.09947v3",
                "updated": "2024-09-10T17:03:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    3,
                    26,
                    1,
                    254,
                    0
                ],
                "published": "2023-09-18T17:12:43Z",
                "published_parsed": [
                    2023,
                    9,
                    18,
                    17,
                    12,
                    43,
                    0,
                    261,
                    0
                ],
                "title": "Deep Visual Odometry with Events and Frames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Visual Odometry with Events and Frames"
                },
                "summary": "Visual Odometry (VO) is crucial for autonomous robotic navigation, especially\nin GPS-denied environments like planetary terrains. To improve robustness,\nrecent model-based VO systems have begun combining standard and event-based\ncameras. While event cameras excel in low-light and high-speed motion, standard\ncameras provide dense and easier-to-track features. However, the field of\nimage- and event-based VO still predominantly relies on model-based methods and\nis yet to fully integrate recent image-only advancements leveraging end-to-end\nlearning-based architectures. Seamlessly integrating the two modalities remains\nchallenging due to their different nature, one asynchronous, the other not,\nlimiting the potential for a more effective image- and event-based VO. We\nintroduce RAMP-VO, the first end-to-end learned image- and event-based VO\nsystem. It leverages novel Recurrent, Asynchronous, and Massively Parallel\n(RAMP) encoders capable of fusing asynchronous events with image data,\nproviding 8x faster inference and 33% more accurate predictions than existing\nsolutions. Despite being trained only in simulation, RAMP-VO outperforms\nprevious methods on the newly introduced Apollo and Malapert datasets, and on\nexisting benchmarks, where it improves image- and event-based methods by 58.8%\nand 30.6%, paving the way for robust and asynchronous VO in space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Odometry (VO) is crucial for autonomous robotic navigation, especially\nin GPS-denied environments like planetary terrains. To improve robustness,\nrecent model-based VO systems have begun combining standard and event-based\ncameras. While event cameras excel in low-light and high-speed motion, standard\ncameras provide dense and easier-to-track features. However, the field of\nimage- and event-based VO still predominantly relies on model-based methods and\nis yet to fully integrate recent image-only advancements leveraging end-to-end\nlearning-based architectures. Seamlessly integrating the two modalities remains\nchallenging due to their different nature, one asynchronous, the other not,\nlimiting the potential for a more effective image- and event-based VO. We\nintroduce RAMP-VO, the first end-to-end learned image- and event-based VO\nsystem. It leverages novel Recurrent, Asynchronous, and Massively Parallel\n(RAMP) encoders capable of fusing asynchronous events with image data,\nproviding 8x faster inference and 33% more accurate predictions than existing\nsolutions. Despite being trained only in simulation, RAMP-VO outperforms\nprevious methods on the newly introduced Apollo and Malapert datasets, and on\nexisting benchmarks, where it improves image- and event-based methods by 58.8%\nand 30.6%, paving the way for robust and asynchronous VO in space."
                },
                "authors": [
                    {
                        "name": "Roberto Pellerito"
                    },
                    {
                        "name": "Marco Cannici"
                    },
                    {
                        "name": "Daniel Gehrig"
                    },
                    {
                        "name": "Joris Belhadj"
                    },
                    {
                        "name": "Olivier Dubois-Matra"
                    },
                    {
                        "name": "Massimo Casasco"
                    },
                    {
                        "name": "Davide Scaramuzza"
                    }
                ],
                "author_detail": {
                    "name": "Davide Scaramuzza"
                },
                "author": "Davide Scaramuzza",
                "arxiv_comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.09947v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.09947v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06643v1",
                "updated": "2024-09-10T16:59:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    59,
                    33,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:59:33Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    59,
                    33,
                    1,
                    254,
                    0
                ],
                "title": "Strategic management analysis: from data to strategy diagram by LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic management analysis: from data to strategy diagram by LLM"
                },
                "summary": "Strategy management analyses are created by business consultants with common\nanalysis frameworks (i.e. comparative analyses) and associated diagrams. We\nshow these can be largely constructed using LLMs, starting with the extraction\nof insights from data, organization of those insights according to a strategy\nmanagement framework, and then depiction in the typical strategy management\ndiagram for that framework (static textual visualizations). We discuss caveats\nand future directions to generalize for broader uses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategy management analyses are created by business consultants with common\nanalysis frameworks (i.e. comparative analyses) and associated diagrams. We\nshow these can be largely constructed using LLMs, starting with the extraction\nof insights from data, organization of those insights according to a strategy\nmanagement framework, and then depiction in the typical strategy management\ndiagram for that framework (static textual visualizations). We discuss caveats\nand future directions to generalize for broader uses."
                },
                "authors": [
                    {
                        "name": "Richard Brath"
                    },
                    {
                        "name": "Adam Bradley"
                    },
                    {
                        "name": "David Jonker"
                    }
                ],
                "author_detail": {
                    "name": "David Jonker"
                },
                "author": "David Jonker",
                "arxiv_comment": "NLVIZ Workshop at IEEE VIZ 2024. 7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14334v3",
                "updated": "2024-09-10T16:55:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    55,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-01-25T17:32:33Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    17,
                    32,
                    33,
                    3,
                    25,
                    0
                ],
                "title": "Momentum, energy and vorticity balances in deep-water surface gravity\n  waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Momentum, energy and vorticity balances in deep-water surface gravity\n  waves"
                },
                "summary": "The particle trajectories in irrotational, incompressible and inviscid\ndeep-water surface gravity waves are open, leading to a net drift in the\ndirection of wave propagation commonly referred to as the Stokes Drift, which\nis responsible for catalysing surface wave-induced mixing in the ocean and\ntransporting marine debris. A balance between phase-averaged momentum density,\nkinetic energy density and vorticity for irrotational, monochromatic and\nperiodic two-dimensional water waves is derived by working directly within the\nLagrangian reference frame, which tracks particle trajectories as a function of\ntheir labels and time. This balance should be expected as all three of these\nquantities are conserved following particles in this system. Vorticity in\nparticular is always conserved along particles in two-dimensional inviscid\nflow, and as such even in its absence it is the value of the vorticity which\nfundamentally sets the drift, which in the Lagrangian frame is identified as\nthe phase-averaged momentum density of the system. A relationship between the\ndrift and the geometric mean water level of particles is found at the surface\nand applications for potential new ways of inferring drift are discussed.\nFinally, an example of an initially quiescent fluid driven by a wavelike\npressure disturbance is considered, showing how the net momentum and energy\nfrom the surface disturbance transfer to the wave field, recognizing the source\nof the mean Lagrangian drift as the net momentum required to generate an\nirrotational surface wave by any conservative force.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The particle trajectories in irrotational, incompressible and inviscid\ndeep-water surface gravity waves are open, leading to a net drift in the\ndirection of wave propagation commonly referred to as the Stokes Drift, which\nis responsible for catalysing surface wave-induced mixing in the ocean and\ntransporting marine debris. A balance between phase-averaged momentum density,\nkinetic energy density and vorticity for irrotational, monochromatic and\nperiodic two-dimensional water waves is derived by working directly within the\nLagrangian reference frame, which tracks particle trajectories as a function of\ntheir labels and time. This balance should be expected as all three of these\nquantities are conserved following particles in this system. Vorticity in\nparticular is always conserved along particles in two-dimensional inviscid\nflow, and as such even in its absence it is the value of the vorticity which\nfundamentally sets the drift, which in the Lagrangian frame is identified as\nthe phase-averaged momentum density of the system. A relationship between the\ndrift and the geometric mean water level of particles is found at the surface\nand applications for potential new ways of inferring drift are discussed.\nFinally, an example of an initially quiescent fluid driven by a wavelike\npressure disturbance is considered, showing how the net momentum and energy\nfrom the surface disturbance transfer to the wave field, recognizing the source\nof the mean Lagrangian drift as the net momentum required to generate an\nirrotational surface wave by any conservative force."
                },
                "authors": [
                    {
                        "name": "Aidan Blaser"
                    },
                    {
                        "name": "Raphaël Benamran"
                    },
                    {
                        "name": "A. Bia Villas Bôas"
                    },
                    {
                        "name": "Luc Lenain"
                    },
                    {
                        "name": "Nick Pizzo"
                    }
                ],
                "author_detail": {
                    "name": "Nick Pizzo"
                },
                "author": "Nick Pizzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06635v1",
                "updated": "2024-09-10T16:46:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders"
                },
                "summary": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks."
                },
                "authors": [
                    {
                        "name": "Wenyu Zhang"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Xunlong Zou"
                    },
                    {
                        "name": "Zhuohan Liu"
                    },
                    {
                        "name": "Yingxu He"
                    },
                    {
                        "name": "Geyu Lin"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06624v1",
                "updated": "2024-09-10T16:26:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    26,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:26:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    26,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio"
                },
                "summary": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance."
                },
                "authors": [
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Kun Fan"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Jinxian Qu"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06622v1",
                "updated": "2024-09-10T16:22:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:22:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "Exploring Italian sentence embeddings properties through multi-tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Italian sentence embeddings properties through multi-tasking"
                },
                "summary": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings."
                },
                "authors": [
                    {
                        "name": "Vivi Nastase"
                    },
                    {
                        "name": "Giuseppe Samo"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Paola Merlo"
                    }
                ],
                "author_detail": {
                    "name": "Paola Merlo"
                },
                "author": "Paola Merlo",
                "arxiv_comment": "9 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.12340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.12340v2",
                "updated": "2024-09-10T16:19:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    19,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2023-05-21T04:22:38Z",
                "published_parsed": [
                    2023,
                    5,
                    21,
                    4,
                    22,
                    38,
                    6,
                    141,
                    0
                ],
                "title": "On the Identifiablility of Nonlocal Interaction Kernels in First-Order\n  Systems of Interacting Particles on Riemannian Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Identifiablility of Nonlocal Interaction Kernels in First-Order\n  Systems of Interacting Particles on Riemannian Manifolds"
                },
                "summary": "In this paper, we tackle a critical issue in nonparametric inference for\nsystems of interacting particles on Riemannian manifolds: the identifiability\nof the interaction functions. Specifically, we define the function spaces on\nwhich the interaction kernels can be identified given infinite i.i.d\nobservational derivative data sampled from a distribution. Our methodology\ninvolves casting the learning problem as a linear statistical inverse problem\nusing a operator theoretical framework. We prove the well-posedness of inverse\nproblem by establishing the strict positivity of a related integral operator\nand our analysis allows us to refine the results on specific manifolds such as\nthe sphere and Hyperbolic space. Our findings indicate that a numerically\nstable procedure exists to recover the interaction kernel from finite (noisy)\ndata, and the estimator will be convergent to the ground truth. This also\nanswers an open question in [MMQZ21] and demonstrate that least square\nestimators can be statistically optimal in certain scenarios. Finally, our\ntheoretical analysis could be extended to the mean-field case, revealing that\nthe corresponding nonparametric inverse problem is ill-posed in general and\nnecessitates effective regularization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we tackle a critical issue in nonparametric inference for\nsystems of interacting particles on Riemannian manifolds: the identifiability\nof the interaction functions. Specifically, we define the function spaces on\nwhich the interaction kernels can be identified given infinite i.i.d\nobservational derivative data sampled from a distribution. Our methodology\ninvolves casting the learning problem as a linear statistical inverse problem\nusing a operator theoretical framework. We prove the well-posedness of inverse\nproblem by establishing the strict positivity of a related integral operator\nand our analysis allows us to refine the results on specific manifolds such as\nthe sphere and Hyperbolic space. Our findings indicate that a numerically\nstable procedure exists to recover the interaction kernel from finite (noisy)\ndata, and the estimator will be convergent to the ground truth. This also\nanswers an open question in [MMQZ21] and demonstrate that least square\nestimators can be statistically optimal in certain scenarios. Finally, our\ntheoretical analysis could be extended to the mean-field case, revealing that\nthe corresponding nonparametric inverse problem is ill-posed in general and\nnecessitates effective regularization techniques."
                },
                "authors": [
                    {
                        "name": "Sui Tang"
                    },
                    {
                        "name": "Malik Tuerkoen"
                    },
                    {
                        "name": "Hanming Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Hanming Zhou"
                },
                "author": "Hanming Zhou",
                "arxiv_comment": "21 pages, 2 figures",
                "arxiv_journal_ref": "Siam Journal on Applied Math 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.12340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.12340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "70F17, 62G05, 62J05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.08034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.08034v2",
                "updated": "2024-09-10T16:03:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    3,
                    3,
                    1,
                    254,
                    0
                ],
                "published": "2023-05-14T00:30:58Z",
                "published_parsed": [
                    2023,
                    5,
                    14,
                    0,
                    30,
                    58,
                    6,
                    134,
                    0
                ],
                "title": "DNN-Defender: A Victim-Focused In-DRAM Defense Mechanism for Taming\n  Adversarial Weight Attack on DNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNN-Defender: A Victim-Focused In-DRAM Defense Mechanism for Taming\n  Adversarial Weight Attack on DNNs"
                },
                "summary": "With deep learning deployed in many security-sensitive areas, machine\nlearning security is becoming progressively important. Recent studies\ndemonstrate attackers can exploit system-level techniques exploiting the\nRowHammer vulnerability of DRAM to deterministically and precisely flip bits in\nDeep Neural Networks (DNN) model weights to affect inference accuracy. The\nexisting defense mechanisms are software-based, such as weight reconstruction\nrequiring expensive training overhead or performance degradation. On the other\nhand, generic hardware-based victim-/aggressor-focused mechanisms impose\nexpensive hardware overheads and preserve the spatial connection between victim\nand aggressor rows. In this paper, we present the first DRAM-based\nvictim-focused defense mechanism tailored for quantized DNNs, named\nDNN-Defender that leverages the potential of in-DRAM swapping to withstand the\ntargeted bit-flip attacks with a priority protection mechanism. Our results\nindicate that DNN-Defender can deliver a high level of protection downgrading\nthe performance of targeted RowHammer attacks to a random attack level. In\naddition, the proposed defense has no accuracy drop on CIFAR-10 and ImageNet\ndatasets without requiring any software training or incurring hardware\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With deep learning deployed in many security-sensitive areas, machine\nlearning security is becoming progressively important. Recent studies\ndemonstrate attackers can exploit system-level techniques exploiting the\nRowHammer vulnerability of DRAM to deterministically and precisely flip bits in\nDeep Neural Networks (DNN) model weights to affect inference accuracy. The\nexisting defense mechanisms are software-based, such as weight reconstruction\nrequiring expensive training overhead or performance degradation. On the other\nhand, generic hardware-based victim-/aggressor-focused mechanisms impose\nexpensive hardware overheads and preserve the spatial connection between victim\nand aggressor rows. In this paper, we present the first DRAM-based\nvictim-focused defense mechanism tailored for quantized DNNs, named\nDNN-Defender that leverages the potential of in-DRAM swapping to withstand the\ntargeted bit-flip attacks with a priority protection mechanism. Our results\nindicate that DNN-Defender can deliver a high level of protection downgrading\nthe performance of targeted RowHammer attacks to a random attack level. In\naddition, the proposed defense has no accuracy drop on CIFAR-10 and ImageNet\ndatasets without requiring any software training or incurring hardware\noverhead."
                },
                "authors": [
                    {
                        "name": "Ranyang Zhou"
                    },
                    {
                        "name": "Sabbir Ahmed"
                    },
                    {
                        "name": "Adnan Siraj Rakin"
                    },
                    {
                        "name": "Shaahin Angizi"
                    }
                ],
                "author_detail": {
                    "name": "Shaahin Angizi"
                },
                "author": "Shaahin Angizi",
                "arxiv_comment": "6 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.08034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.08034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09906v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09906v3",
                "updated": "2024-09-10T15:53:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    53,
                    17,
                    1,
                    254,
                    0
                ],
                "published": "2024-05-16T08:54:24Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    8,
                    54,
                    24,
                    3,
                    137,
                    0
                ],
                "title": "Process-based Inference for Spatial Energetics Using Bayesian Predictive\n  Stacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-based Inference for Spatial Energetics Using Bayesian Predictive\n  Stacking"
                },
                "summary": "Rapid developments in streaming data technologies have enabled real-time\nmonitoring of human activity that can deliver high-resolution data on health\nvariables over trajectories or paths carved out by subjects as they conduct\ntheir daily physical activities. Wearable devices, such as wrist-worn sensors\nthat monitor gross motor activity, have become prevalent and have kindled the\nemerging field of \"spatial energetics\" in environmental health sciences. We\ndevise a Bayesian inferential framework for analyzing such data while\naccounting for information available on specific spatial coordinates comprising\na trajectory or path using a Global Positioning System (GPS) device embedded\nwithin the wearable device. We offer full probabilistic inference with\nuncertainty quantification using spatial-temporal process models adapted for\ndata generated from \"actigraph\" units as the subject traverses a path or\ntrajectory in their daily routine. Anticipating the need for fast inference for\nmobile health data, we pursue exact inference using conjugate Bayesian models\nand employ predictive stacking to assimilate inference across these individual\nmodels. This circumvents issues with iterative estimation algorithms such as\nMarkov chain Monte Carlo. We devise Bayesian predictive stacking in this\ncontext for models that treat time as discrete epochs and that treat time as\ncontinuous. We illustrate our methods with simulation experiments and analysis\nof data from the Physical Activity through Sustainable Transport Approaches\n(PASTA-LA) study conducted by the Fielding School of Public Health at the\nUniversity of California, Los Angeles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid developments in streaming data technologies have enabled real-time\nmonitoring of human activity that can deliver high-resolution data on health\nvariables over trajectories or paths carved out by subjects as they conduct\ntheir daily physical activities. Wearable devices, such as wrist-worn sensors\nthat monitor gross motor activity, have become prevalent and have kindled the\nemerging field of \"spatial energetics\" in environmental health sciences. We\ndevise a Bayesian inferential framework for analyzing such data while\naccounting for information available on specific spatial coordinates comprising\na trajectory or path using a Global Positioning System (GPS) device embedded\nwithin the wearable device. We offer full probabilistic inference with\nuncertainty quantification using spatial-temporal process models adapted for\ndata generated from \"actigraph\" units as the subject traverses a path or\ntrajectory in their daily routine. Anticipating the need for fast inference for\nmobile health data, we pursue exact inference using conjugate Bayesian models\nand employ predictive stacking to assimilate inference across these individual\nmodels. This circumvents issues with iterative estimation algorithms such as\nMarkov chain Monte Carlo. We devise Bayesian predictive stacking in this\ncontext for models that treat time as discrete epochs and that treat time as\ncontinuous. We illustrate our methods with simulation experiments and analysis\nof data from the Physical Activity through Sustainable Transport Approaches\n(PASTA-LA) study conducted by the Fielding School of Public Health at the\nUniversity of California, Los Angeles."
                },
                "authors": [
                    {
                        "name": "Tomoya Wakayama"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Sudipto Banerjee"
                },
                "author": "Sudipto Banerjee",
                "arxiv_comment": "37 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09906v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09906v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06601v1",
                "updated": "2024-09-10T15:51:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "title": "Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling"
                },
                "summary": "Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments."
                },
                "authors": [
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06595v1",
                "updated": "2024-09-10T15:39:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:39:32Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations."
                },
                "authors": [
                    {
                        "name": "Sacha Muller"
                    },
                    {
                        "name": "António Loison"
                    },
                    {
                        "name": "Bilel Omrani"
                    },
                    {
                        "name": "Gautier Viaud"
                    }
                ],
                "author_detail": {
                    "name": "Gautier Viaud"
                },
                "author": "Gautier Viaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08921v2",
                "updated": "2024-09-10T15:38:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    38,
                    56,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-15T12:20:24Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    20,
                    24,
                    3,
                    228,
                    0
                ],
                "title": "Graph Retrieval-Augmented Generation: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation: A Survey"
                },
                "summary": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}."
                },
                "authors": [
                    {
                        "name": "Boci Peng"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Xiaohe Bo"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Ongoing work. Compared to the first version, several references have\n  been added and a GitHub repository link has been provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06593v1",
                "updated": "2024-09-10T15:34:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    34,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:34:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    34,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "Advancing Causal Inference: A Nonparametric Approach to ATE and CATE\n  Estimation with Continuous Treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Causal Inference: A Nonparametric Approach to ATE and CATE\n  Estimation with Continuous Treatments"
                },
                "summary": "This paper introduces a generalized ps-BART model for the estimation of\nAverage Treatment Effect (ATE) and Conditional Average Treatment Effect (CATE)\nin continuous treatments, addressing limitations of the Bayesian Causal Forest\n(BCF) model. The ps-BART model's nonparametric nature allows for flexibility in\ncapturing nonlinear relationships between treatment and outcome variables.\nAcross three distinct sets of Data Generating Processes (DGPs), the ps-BART\nmodel consistently outperforms the BCF model, particularly in highly nonlinear\nsettings. The ps-BART model's robustness in uncertainty estimation and accuracy\nin both point-wise and probabilistic estimation demonstrate its utility for\nreal-world applications. This research fills a crucial gap in causal inference\nliterature, providing a tool better suited for nonlinear treatment-outcome\nrelationships and opening avenues for further exploration in the domain of\ncontinuous treatment effect estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a generalized ps-BART model for the estimation of\nAverage Treatment Effect (ATE) and Conditional Average Treatment Effect (CATE)\nin continuous treatments, addressing limitations of the Bayesian Causal Forest\n(BCF) model. The ps-BART model's nonparametric nature allows for flexibility in\ncapturing nonlinear relationships between treatment and outcome variables.\nAcross three distinct sets of Data Generating Processes (DGPs), the ps-BART\nmodel consistently outperforms the BCF model, particularly in highly nonlinear\nsettings. The ps-BART model's robustness in uncertainty estimation and accuracy\nin both point-wise and probabilistic estimation demonstrate its utility for\nreal-world applications. This research fills a crucial gap in causal inference\nliterature, providing a tool better suited for nonlinear treatment-outcome\nrelationships and opening avenues for further exploration in the domain of\ncontinuous treatment effect estimation."
                },
                "authors": [
                    {
                        "name": "Hugo Gobato Souto"
                    },
                    {
                        "name": "Francisco Louzada Neto"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Louzada Neto"
                },
                "author": "Francisco Louzada Neto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06588v1",
                "updated": "2024-09-10T15:29:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    29,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:29:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    29,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "On Epistemic Properties in Discrete-Event Systems: A Uniform Framework\n  and Its Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Epistemic Properties in Discrete-Event Systems: A Uniform Framework\n  and Its Applications"
                },
                "summary": "In this paper, we investigate the property verification problem for\npartially-observed DES from a new perspective. Specifically, we consider the\nproblem setting where the system is observed by two agents independently, each\nwith its own observation. The purpose of the first agent, referred to as the\nlow-level observer, is to infer the actual behavior of the system, while the\nsecond, referred to as the high-level observer, aims to infer the knowledge of\nAgent 1 regarding the system. We present a general notion called the epistemic\nproperty capturing the inference from the high-level observer to the low-level\nobserver. A typical instance of this definition is the notion of high-order\nopacity, which specifies that the intruder does not know that the system knows\nsome critical information. This formalization is very general and supports any\nuser-defined information-state-based knowledge between the two observers. We\ndemonstrate how the general definition of epistemic properties can be applied\nin different problem settings such as information leakage diagnosis or tactical\ncooperation without explicit communications. Finally, we provide a systematic\napproach for the verification of epistemic properties. Particularly, we\nidentify some fragments of epistemic properties that can be verified more\nefficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the property verification problem for\npartially-observed DES from a new perspective. Specifically, we consider the\nproblem setting where the system is observed by two agents independently, each\nwith its own observation. The purpose of the first agent, referred to as the\nlow-level observer, is to infer the actual behavior of the system, while the\nsecond, referred to as the high-level observer, aims to infer the knowledge of\nAgent 1 regarding the system. We present a general notion called the epistemic\nproperty capturing the inference from the high-level observer to the low-level\nobserver. A typical instance of this definition is the notion of high-order\nopacity, which specifies that the intruder does not know that the system knows\nsome critical information. This formalization is very general and supports any\nuser-defined information-state-based knowledge between the two observers. We\ndemonstrate how the general definition of epistemic properties can be applied\nin different problem settings such as information leakage diagnosis or tactical\ncooperation without explicit communications. Finally, we provide a systematic\napproach for the verification of epistemic properties. Particularly, we\nidentify some fragments of epistemic properties that can be verified more\nefficiently."
                },
                "authors": [
                    {
                        "name": "Bohan Cui"
                    },
                    {
                        "name": "Ziyue Ma"
                    },
                    {
                        "name": "Shaoyuan Li"
                    },
                    {
                        "name": "Xiang Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yin"
                },
                "author": "Xiang Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06586v1",
                "updated": "2024-09-10T15:28:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    28,
                    38,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:28:38Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    28,
                    38,
                    1,
                    254,
                    0
                ],
                "title": "Universal End-to-End Neural Network for Lossy Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal End-to-End Neural Network for Lossy Image Compression"
                },
                "summary": "This paper presents variable bitrate lossy image compression using a\nVAE-based neural network. An adaptable image quality adjustment strategy is\nproposed. The key innovation involves adeptly adjusting the input scale\nexclusively during the inference process, resulting in an exceptionally\nefficient rate-distortion mechanism. Through extensive experimentation, across\ndiverse VAE-based compression architectures (CNN, ViT) and training\nmethodologies (MSE, SSIM), our approach exhibits remarkable universality. This\nsuccess is attributed to the inherent generalization capacity of neural\nnetworks. Unlike methods that adjust model architecture or loss functions, our\napproach emphasizes simplicity, reducing computational complexity and memory\nrequirements. The experiments not only highlight the effectiveness of our\napproach but also indicate its potential to drive advancements in variable-rate\nneural network lossy image compression methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents variable bitrate lossy image compression using a\nVAE-based neural network. An adaptable image quality adjustment strategy is\nproposed. The key innovation involves adeptly adjusting the input scale\nexclusively during the inference process, resulting in an exceptionally\nefficient rate-distortion mechanism. Through extensive experimentation, across\ndiverse VAE-based compression architectures (CNN, ViT) and training\nmethodologies (MSE, SSIM), our approach exhibits remarkable universality. This\nsuccess is attributed to the inherent generalization capacity of neural\nnetworks. Unlike methods that adjust model architecture or loss functions, our\napproach emphasizes simplicity, reducing computational complexity and memory\nrequirements. The experiments not only highlight the effectiveness of our\napproach but also indicate its potential to drive advancements in variable-rate\nneural network lossy image compression methodologies."
                },
                "authors": [
                    {
                        "name": "Bouzid Arezki"
                    },
                    {
                        "name": "Fangchen Feng"
                    },
                    {
                        "name": "Anissa Mokraoui"
                    }
                ],
                "author_detail": {
                    "name": "Anissa Mokraoui"
                },
                "author": "Anissa Mokraoui",
                "arxiv_comment": "Accepted at EUSIPCO European conference on signal processing August\n  26-30 2024 in Lyon France",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06580v1",
                "updated": "2024-09-10T15:19:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    19,
                    50,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:19:50Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    19,
                    50,
                    1,
                    254,
                    0
                ],
                "title": "Exploring Differences between Human Perception and Model Inference in\n  Audio Event Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Differences between Human Perception and Model Inference in\n  Audio Event Recognition"
                },
                "summary": "Audio Event Recognition (AER) traditionally focuses on detecting and\nidentifying audio events. Most existing AER models tend to detect all potential\nevents without considering their varying significance across different\ncontexts. This makes the AER results detected by existing models often have a\nlarge discrepancy with human auditory perception. Although this is a critical\nand significant issue, it has not been extensively studied by the Detection and\nClassification of Sound Scenes and Events (DCASE) community because solving it\nis time-consuming and labour-intensive. To address this issue, this paper\nintroduces the concept of semantic importance in AER, focusing on exploring the\ndifferences between human perception and model inference. This paper constructs\na Multi-Annotated Foreground Audio Event Recognition (MAFAR) dataset, which\ncomprises audio recordings labelled by 10 professional annotators. Through\nlabelling frequency and variance, the MAFAR dataset facilitates the\nquantification of semantic importance and analysis of human perception. By\ncomparing human annotations with the predictions of ensemble pre-trained\nmodels, this paper uncovers a significant gap between human perception and\nmodel inference in both semantic identification and existence detection of\naudio events. Experimental results reveal that human perception tends to ignore\nsubtle or trivial events in the event semantic identification, while model\ninference is easily affected by events with noises. Meanwhile, in event\nexistence detection, models are usually more sensitive than humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Event Recognition (AER) traditionally focuses on detecting and\nidentifying audio events. Most existing AER models tend to detect all potential\nevents without considering their varying significance across different\ncontexts. This makes the AER results detected by existing models often have a\nlarge discrepancy with human auditory perception. Although this is a critical\nand significant issue, it has not been extensively studied by the Detection and\nClassification of Sound Scenes and Events (DCASE) community because solving it\nis time-consuming and labour-intensive. To address this issue, this paper\nintroduces the concept of semantic importance in AER, focusing on exploring the\ndifferences between human perception and model inference. This paper constructs\na Multi-Annotated Foreground Audio Event Recognition (MAFAR) dataset, which\ncomprises audio recordings labelled by 10 professional annotators. Through\nlabelling frequency and variance, the MAFAR dataset facilitates the\nquantification of semantic importance and analysis of human perception. By\ncomparing human annotations with the predictions of ensemble pre-trained\nmodels, this paper uncovers a significant gap between human perception and\nmodel inference in both semantic identification and existence detection of\naudio events. Experimental results reveal that human perception tends to ignore\nsubtle or trivial events in the event semantic identification, while model\ninference is easily affected by events with noises. Meanwhile, in event\nexistence detection, models are usually more sensitive than humans."
                },
                "authors": [
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Yanru Wu"
                    },
                    {
                        "name": "Yuanbo Hou"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Hui Bu"
                    },
                    {
                        "name": "Shengchen Li"
                    },
                    {
                        "name": "Dick Botteldooren"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    }
                ],
                "author_detail": {
                    "name": "Mark D. Plumbley"
                },
                "author": "Mark D. Plumbley",
                "arxiv_comment": "Dataset homepage: https://github.com/Voltmeter00/MAFAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06568v1",
                "updated": "2024-09-10T15:02:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    2,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:02:34Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    2,
                    34,
                    1,
                    254,
                    0
                ],
                "title": "Think-on-Process: Dynamic Process Generation for Collaborative\n  Development of Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-on-Process: Dynamic Process Generation for Collaborative\n  Development of Multi-Agent System"
                },
                "summary": "Software development is a collaborative endeavor that requires individuals\nfrom different departments to work together in order to collectively develop a\nhigh-quality software system. In this context, people have begun to explore a\nmethod that leverages multi-agent systems based on LLMs to carry out software\ndevelopment. However, existing research tends to rigidly fix the software\ndevelopment process in a framework in code form, thus failing to dynamically\nadjust the software development process in real-time to meet the more flexible\nand variable software environment. In this paper, we propose a dynamic process\ngeneration framework, named ToP (Think-on-Process). The core idea of ToP is to\nleverage experiential knowledge (i.e., process models) to guide LLMs in\ngenerating software development processes (i.e., instances). These instances\nwill guide multi-agent in software development and employ a compiler to provide\nfeedback on the development outcomes. Subsequently, we utilize heuristic\nalgorithms to filter the instances and apply process mining algorithms to\nderive process model. Finally, the process model will be converted into text,\nformatted as prompts, to enhance the ability of LLMs to generate other\ninstances. Experiments demonstrate that our framework ToP significantly\nenhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for\nfive categories of software development tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development is a collaborative endeavor that requires individuals\nfrom different departments to work together in order to collectively develop a\nhigh-quality software system. In this context, people have begun to explore a\nmethod that leverages multi-agent systems based on LLMs to carry out software\ndevelopment. However, existing research tends to rigidly fix the software\ndevelopment process in a framework in code form, thus failing to dynamically\nadjust the software development process in real-time to meet the more flexible\nand variable software environment. In this paper, we propose a dynamic process\ngeneration framework, named ToP (Think-on-Process). The core idea of ToP is to\nleverage experiential knowledge (i.e., process models) to guide LLMs in\ngenerating software development processes (i.e., instances). These instances\nwill guide multi-agent in software development and employ a compiler to provide\nfeedback on the development outcomes. Subsequently, we utilize heuristic\nalgorithms to filter the instances and apply process mining algorithms to\nderive process model. Finally, the process model will be converted into text,\nformatted as prompts, to enhance the ability of LLMs to generate other\ninstances. Experiments demonstrate that our framework ToP significantly\nenhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for\nfive categories of software development tasks."
                },
                "authors": [
                    {
                        "name": "Leilei Lin"
                    },
                    {
                        "name": "Yingming Zhou"
                    },
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06565v1",
                "updated": "2024-09-10T14:57:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    57,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:57:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    57,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "Enzyme kinetic reactions as interacting particle systems: Stochastic\n  averaging and parameter inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enzyme kinetic reactions as interacting particle systems: Stochastic\n  averaging and parameter inference"
                },
                "summary": "We consider a stochastic model of multistage Michaelis--Menten (MM) type\nenzyme kinetic reactions describing the conversion of substrate molecules to a\nproduct through several intermediate species. The high-dimensional, multiscale\nnature of these reaction networks presents significant computational\nchallenges, especially in statistical estimation of reaction rates. This\ndifficulty is amplified when direct data on system states are unavailable, and\none only has access to a random sample of product formation times. To address\nthis, we proceed in two stages. First, under certain technical assumptions akin\nto those made in the Quasi-steady-state approximation (QSSA) literature, we\nprove two asymptotic results: a stochastic averaging principle that yields a\nlower-dimensional model, and a functional central limit theorem that quantifies\nthe associated fluctuations. Next, for statistical inference of the parameters\nof the original MM reaction network, we develop a mathematical framework\ninvolving an interacting particle system (IPS) and prove a propagation of chaos\nresult that allows us to write a product-form likelihood function. The novelty\nof the IPS-based inference method is that it does not require information about\nthe state of the system and works with only a random sample of product\nformation times. We provide numerical examples to illustrate the efficacy of\nthe theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a stochastic model of multistage Michaelis--Menten (MM) type\nenzyme kinetic reactions describing the conversion of substrate molecules to a\nproduct through several intermediate species. The high-dimensional, multiscale\nnature of these reaction networks presents significant computational\nchallenges, especially in statistical estimation of reaction rates. This\ndifficulty is amplified when direct data on system states are unavailable, and\none only has access to a random sample of product formation times. To address\nthis, we proceed in two stages. First, under certain technical assumptions akin\nto those made in the Quasi-steady-state approximation (QSSA) literature, we\nprove two asymptotic results: a stochastic averaging principle that yields a\nlower-dimensional model, and a functional central limit theorem that quantifies\nthe associated fluctuations. Next, for statistical inference of the parameters\nof the original MM reaction network, we develop a mathematical framework\ninvolving an interacting particle system (IPS) and prove a propagation of chaos\nresult that allows us to write a product-form likelihood function. The novelty\nof the IPS-based inference method is that it does not require information about\nthe state of the system and works with only a random sample of product\nformation times. We provide numerical examples to illustrate the efficacy of\nthe theoretical results."
                },
                "authors": [
                    {
                        "name": "Arnab Ganguly"
                    },
                    {
                        "name": "Wasiur R. KhudaBukhsh"
                    }
                ],
                "author_detail": {
                    "name": "Wasiur R. KhudaBukhsh"
                },
                "author": "Wasiur R. KhudaBukhsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60F17, 60F05, 62F99, 62M99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13342v2",
                "updated": "2024-09-10T14:56:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    56,
                    38,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-18T09:40:24Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    40,
                    24,
                    3,
                    200,
                    0
                ],
                "title": "Implicit Filtering for Learning Neural Signed Distance Functions from 3D\n  Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Filtering for Learning Neural Signed Distance Functions from 3D\n  Point Clouds"
                },
                "summary": "Neural signed distance functions (SDFs) have shown powerful ability in\nfitting the shape geometry. However, inferring continuous signed distance\nfields from discrete unoriented point clouds still remains a challenge. The\nneural network typically fits the shape with a rough surface and omits\nfine-grained geometric details such as shape edges and corners. In this paper,\nwe propose a novel non-linear implicit filter to smooth the implicit field\nwhile preserving high-frequency geometry details. Our novelty lies in that we\ncan filter the surface (zero level set) by the neighbor input points with\ngradients of the signed distance field. By moving the input raw point clouds\nalong the gradient, our proposed implicit filtering can be extended to non-zero\nlevel sets to keep the promise consistency between different level sets, which\nconsequently results in a better regularization of the zero level set. We\nconduct comprehensive experiments in surface reconstruction from objects and\ncomplex scene point clouds, the numerical and visual comparisons demonstrate\nour improvements over the state-of-the-art methods under the widely used\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural signed distance functions (SDFs) have shown powerful ability in\nfitting the shape geometry. However, inferring continuous signed distance\nfields from discrete unoriented point clouds still remains a challenge. The\nneural network typically fits the shape with a rough surface and omits\nfine-grained geometric details such as shape edges and corners. In this paper,\nwe propose a novel non-linear implicit filter to smooth the implicit field\nwhile preserving high-frequency geometry details. Our novelty lies in that we\ncan filter the surface (zero level set) by the neighbor input points with\ngradients of the signed distance field. By moving the input raw point clouds\nalong the gradient, our proposed implicit filtering can be extended to non-zero\nlevel sets to keep the promise consistency between different level sets, which\nconsequently results in a better regularization of the zero level set. We\nconduct comprehensive experiments in surface reconstruction from objects and\ncomplex scene point clouds, the numerical and visual comparisons demonstrate\nour improvements over the state-of-the-art methods under the widely used\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shengtao Li"
                    },
                    {
                        "name": "Ge Gao"
                    },
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Ming Gu"
                    },
                    {
                        "name": "Yu-Shen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Shen Liu"
                },
                "author": "Yu-Shen Liu",
                "arxiv_comment": "Accepted by ECCV 2024. Project page:\n  https://list17.github.io/ImplicitFilter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12347v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12347v5",
                "updated": "2024-09-10T14:51:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    51,
                    38,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-22T12:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed"
                },
                "summary": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed."
                },
                "authors": [
                    {
                        "name": "Mark Rubin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Rubin"
                },
                "author": "Mark Rubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12347v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12347v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06560v1",
                "updated": "2024-09-10T14:43:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    43,
                    3,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:43:03Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    43,
                    3,
                    1,
                    254,
                    0
                ],
                "title": "A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling"
                },
                "summary": "Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. Deriving the central learning objective for VI must often be tailored\nto new learning tasks where the nature of the problems dictates the conditional\ndependence between variables of interest, such as arising in physics problems.\nIn this paper, we provide an accessible and thorough technical introduction to\nVI for forward and inverse problems, guiding the reader through standard\nderivations of the VI framework and how it can best be realized through deep\nlearning. We then review and unify recent literature exemplifying the creative\nflexibility allowed by VI. This paper is designed for a general scientific\naudience looking to solve physics-based problems with an emphasis on\nuncertainty quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. Deriving the central learning objective for VI must often be tailored\nto new learning tasks where the nature of the problems dictates the conditional\ndependence between variables of interest, such as arising in physics problems.\nIn this paper, we provide an accessible and thorough technical introduction to\nVI for forward and inverse problems, guiding the reader through standard\nderivations of the VI framework and how it can best be realized through deep\nlearning. We then review and unify recent literature exemplifying the creative\nflexibility allowed by VI. This paper is designed for a general scientific\naudience looking to solve physics-based problems with an emphasis on\nuncertainty quantification."
                },
                "authors": [
                    {
                        "name": "Alex Glyn-Davies"
                    },
                    {
                        "name": "Arnaud Vadeboncoeur"
                    },
                    {
                        "name": "O. Deniz Akyildiz"
                    },
                    {
                        "name": "Ieva Kazlauskaite"
                    },
                    {
                        "name": "Mark Girolami"
                    }
                ],
                "author_detail": {
                    "name": "Mark Girolami"
                },
                "author": "Mark Girolami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06558v1",
                "updated": "2024-09-10T14:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    39,
                    4,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:39:04Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    39,
                    4,
                    1,
                    254,
                    0
                ],
                "title": "MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles\n  Through LLMs Penetrated Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles\n  Through LLMs Penetrated Science"
                },
                "summary": "As autonomous vehicles become more prevalent, highly accurate and efficient\nsystems are increasingly critical to improve safety, performance, and energy\nconsumption. Efficient management of energy-reliability tradeoffs in these\nsystems demands the ability to predict various conditions during vehicle\noperations. With the promising improvement of Large Language Models (LLMs) and\nthe emergence of well-known models like ChatGPT, unique opportunities for\nautonomous vehicle-related predictions have been provided in recent years. This\npaper proposed MAPS using LLMs as map reader co-drivers to predict the vital\nparameters to set during the autonomous vehicle operation to balance the\nenergy-reliability tradeoff. The MAPS method demonstrates a 20% improvement in\nnavigation accuracy compared to the best baseline method. MAPS also shows 11%\nenergy savings in computational units and up to 54% in both mechanical and\ncomputational units.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous vehicles become more prevalent, highly accurate and efficient\nsystems are increasingly critical to improve safety, performance, and energy\nconsumption. Efficient management of energy-reliability tradeoffs in these\nsystems demands the ability to predict various conditions during vehicle\noperations. With the promising improvement of Large Language Models (LLMs) and\nthe emergence of well-known models like ChatGPT, unique opportunities for\nautonomous vehicle-related predictions have been provided in recent years. This\npaper proposed MAPS using LLMs as map reader co-drivers to predict the vital\nparameters to set during the autonomous vehicle operation to balance the\nenergy-reliability tradeoff. The MAPS method demonstrates a 20% improvement in\nnavigation accuracy compared to the best baseline method. MAPS also shows 11%\nenergy savings in computational units and up to 54% in both mechanical and\ncomputational units."
                },
                "authors": [
                    {
                        "name": "Mahdieh Aliazam"
                    },
                    {
                        "name": "Ali Javadi"
                    },
                    {
                        "name": "Amir Mahdi Hosseini Monazzah"
                    },
                    {
                        "name": "Ahmad Akbari Azirani"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Akbari Azirani"
                },
                "author": "Ahmad Akbari Azirani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11346v2",
                "updated": "2024-09-10T14:26:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    26,
                    30,
                    1,
                    254,
                    0
                ],
                "published": "2024-06-17T09:08:30Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    8,
                    30,
                    0,
                    169,
                    0
                ],
                "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaDec: Decompiling WebAssembly Using Large Language Model"
                },
                "summary": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm.\n  In this paper, we introduce a novel approach, WaDec, which is the first use\nof a fine-tuned LLM to interpret and decompile Wasm binary code into a\nhigher-level, more comprehensible source code representation. The LLM was\nmeticulously fine-tuned using a specialized dataset of wat-c code snippets,\nemploying self-supervised learning techniques. This enables WaDec to\neffectively decompile not only complete wat functions but also finer-grained\nwat code snippets. Our experiments demonstrate that WaDec markedly outperforms\ncurrent state-of-the-art tools, offering substantial improvements across\nseveral metrics. It achieves a code inflation rate of only 3.34%, a dramatic\n97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines'\noutput that cannot be directly compiled or executed, WaDec maintains a\nrecompilability rate of 52.11%, a re-execution rate of 43.55%, and an output\nconsistency of 27.15%. Additionally, it significantly exceeds state-of-the-art\nperformance in AST edit distance by 185%, cyclomatic complexity by 8%, and\ncosine similarity by 41%, achieving an average code similarity above 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm.\n  In this paper, we introduce a novel approach, WaDec, which is the first use\nof a fine-tuned LLM to interpret and decompile Wasm binary code into a\nhigher-level, more comprehensible source code representation. The LLM was\nmeticulously fine-tuned using a specialized dataset of wat-c code snippets,\nemploying self-supervised learning techniques. This enables WaDec to\neffectively decompile not only complete wat functions but also finer-grained\nwat code snippets. Our experiments demonstrate that WaDec markedly outperforms\ncurrent state-of-the-art tools, offering substantial improvements across\nseveral metrics. It achieves a code inflation rate of only 3.34%, a dramatic\n97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines'\noutput that cannot be directly compiled or executed, WaDec maintains a\nrecompilability rate of 52.11%, a re-execution rate of 43.55%, and an output\nconsistency of 27.15%. Additionally, it significantly exceeds state-of-the-art\nperformance in AST edit distance by 185%, cyclomatic complexity by 8%, and\ncosine similarity by 41%, achieving an average code similarity above 50%."
                },
                "authors": [
                    {
                        "name": "Xinyu She"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "This paper was accepted by ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16623v2",
                "updated": "2024-09-10T14:26:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    26,
                    3,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-23T16:32:38Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    16,
                    32,
                    38,
                    1,
                    205,
                    0
                ],
                "title": "Inverse Particle Filter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Particle Filter"
                },
                "summary": "In cognitive systems, recent emphasis has been placed on studying the\ncognitive processes of the subject whose behavior was the primary focus of the\nsystem's cognitive response. This approach, known as inverse cognition, arises\nin counter-adversarial applications and has motivated the development of\ninverse Bayesian filters. In this context, a cognitive adversary, such as a\nradar, uses a forward Bayesian filter to track its target of interest. An\ninverse filter is then employed to infer the adversary's estimate of the\ntarget's or defender's state. Previous studies have addressed this inverse\nfiltering problem by introducing methods like the inverse Kalman filter (I-KF),\ninverse extended KF (I-EKF), and inverse unscented KF (I-UKF). However, these\nfilters typically assume additive Gaussian noise models and/or rely on local\napproximations of non-linear dynamics at the state estimates, limiting their\npractical application. In contrast, this paper adopts a global filtering\napproach and presents the development of an inverse particle filter (I-PF). The\nparticle filter framework employs Monte Carlo (MC) methods to approximate\narbitrary posterior distributions. Moreover, under mild system-level\nconditions, the proposed I-PF demonstrates convergence to the optimal inverse\nfilter. Additionally, we propose the differentiable I-PF to address scenarios\nwhere system information is unknown to the defender. Using the recursive\nCramer-Rao lower bound and non-credibility index (NCI), our numerical\nexperiments for different systems demonstrate the estimation performance and\ntime complexity of the proposed filter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cognitive systems, recent emphasis has been placed on studying the\ncognitive processes of the subject whose behavior was the primary focus of the\nsystem's cognitive response. This approach, known as inverse cognition, arises\nin counter-adversarial applications and has motivated the development of\ninverse Bayesian filters. In this context, a cognitive adversary, such as a\nradar, uses a forward Bayesian filter to track its target of interest. An\ninverse filter is then employed to infer the adversary's estimate of the\ntarget's or defender's state. Previous studies have addressed this inverse\nfiltering problem by introducing methods like the inverse Kalman filter (I-KF),\ninverse extended KF (I-EKF), and inverse unscented KF (I-UKF). However, these\nfilters typically assume additive Gaussian noise models and/or rely on local\napproximations of non-linear dynamics at the state estimates, limiting their\npractical application. In contrast, this paper adopts a global filtering\napproach and presents the development of an inverse particle filter (I-PF). The\nparticle filter framework employs Monte Carlo (MC) methods to approximate\narbitrary posterior distributions. Moreover, under mild system-level\nconditions, the proposed I-PF demonstrates convergence to the optimal inverse\nfilter. Additionally, we propose the differentiable I-PF to address scenarios\nwhere system information is unknown to the defender. Using the recursive\nCramer-Rao lower bound and non-credibility index (NCI), our numerical\nexperiments for different systems demonstrate the estimation performance and\ntime complexity of the proposed filter."
                },
                "authors": [
                    {
                        "name": "Himali Singh"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Kumar Vijay Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Vijay Mishra"
                },
                "author": "Kumar Vijay Mishra",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06540v1",
                "updated": "2024-09-10T14:15:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    15,
                    30,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:15:30Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    15,
                    30,
                    1,
                    254,
                    0
                ],
                "title": "Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings"
                },
                "summary": "Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure."
                },
                "authors": [
                    {
                        "name": "Jan Elfes"
                    }
                ],
                "author_detail": {
                    "name": "Jan Elfes"
                },
                "author": "Jan Elfes",
                "arxiv_comment": "19 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14467v2",
                "updated": "2024-09-10T14:08:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    8,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-19T17:14:16Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    14,
                    16,
                    4,
                    201,
                    0
                ],
                "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality"
                },
                "summary": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}"
                },
                "authors": [
                    {
                        "name": "Jayr Pereira"
                    },
                    {
                        "name": "Andre Assumpcao"
                    },
                    {
                        "name": "Roberto Lotufo"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Lotufo"
                },
                "author": "Roberto Lotufo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06527v1",
                "updated": "2024-09-10T14:03:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    3,
                    5,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:03:05Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    3,
                    5,
                    1,
                    254,
                    0
                ],
                "title": "Hierarchical Bayesian inference on an analytical model of the LISA\n  massive black hole binary population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Bayesian inference on an analytical model of the LISA\n  massive black hole binary population"
                },
                "summary": "Massive black hole binary (MBHB) mergers will be detectable in large numbers\nby the Lisa Interferometer Space Antenna (LISA), which will thus provide new\ninsights on how they form via repeated dark matter (DM) halo and galaxy\nmergers. Here we present a simple analytical model to generate a population of\nMBHB mergers based on a theoretical prescription that connects them to DM halo\nmergers. The high flexibility of our approach allows us to explore the broad\nand uncertain range of MBH seeding and growth mechanisms, as well as the\ndifferent effects behind the interplay between MBH and galactic astrophysics.\nSuch a flexibility is fundamental for the successful implementation and\noptimisation of the hierarchical Bayesian parameter estimation approach that\nhere we apply to the MBHB population of LISA for the first time. Our inferred\npopulation hyper-parameters are chosen as proxies to characterise the MBH--DM\nhalo mass scaling relation, the occupation fraction of MBHs in DM halos and the\ndelay between halo and MBHB mergers. We find that LISA will provide tight\nconstraints at the lower-end of the MBH-halo scaling relation, well\ncomplementing EM observations which are biased towards large masses.\nFurthermore, our results suggest that LISA will constrain some features of the\nMBH occupation fraction at high redshift, as well as merger time delays of the\norder of a few hundreds of Myr, opening the possibility to constrain dynamical\nevolution time scales such as the dynamical friction. The analysis presented\nhere constitutes a first attempt at developing a hierarchical Bayesian\ninference approach to the LISA MBHB population, opening the way for several\nfurther improvements and investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive black hole binary (MBHB) mergers will be detectable in large numbers\nby the Lisa Interferometer Space Antenna (LISA), which will thus provide new\ninsights on how they form via repeated dark matter (DM) halo and galaxy\nmergers. Here we present a simple analytical model to generate a population of\nMBHB mergers based on a theoretical prescription that connects them to DM halo\nmergers. The high flexibility of our approach allows us to explore the broad\nand uncertain range of MBH seeding and growth mechanisms, as well as the\ndifferent effects behind the interplay between MBH and galactic astrophysics.\nSuch a flexibility is fundamental for the successful implementation and\noptimisation of the hierarchical Bayesian parameter estimation approach that\nhere we apply to the MBHB population of LISA for the first time. Our inferred\npopulation hyper-parameters are chosen as proxies to characterise the MBH--DM\nhalo mass scaling relation, the occupation fraction of MBHs in DM halos and the\ndelay between halo and MBHB mergers. We find that LISA will provide tight\nconstraints at the lower-end of the MBH-halo scaling relation, well\ncomplementing EM observations which are biased towards large masses.\nFurthermore, our results suggest that LISA will constrain some features of the\nMBH occupation fraction at high redshift, as well as merger time delays of the\norder of a few hundreds of Myr, opening the possibility to constrain dynamical\nevolution time scales such as the dynamical friction. The analysis presented\nhere constitutes a first attempt at developing a hierarchical Bayesian\ninference approach to the LISA MBHB population, opening the way for several\nfurther improvements and investigations."
                },
                "authors": [
                    {
                        "name": "Vivienne Langen"
                    },
                    {
                        "name": "Nicola Tamanini"
                    },
                    {
                        "name": "Sylvain Marsat"
                    },
                    {
                        "name": "Elisa Bortolas"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Bortolas"
                },
                "author": "Elisa Bortolas",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02063v2",
                "updated": "2024-09-10T14:02:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    2,
                    58,
                    1,
                    254,
                    0
                ],
                "published": "2024-04-02T16:04:31Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    16,
                    4,
                    31,
                    1,
                    93,
                    0
                ],
                "title": "SPMamba: State-space model is all you need in speech separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPMamba: State-space model is all you need in speech separation"
                },
                "summary": "Existing CNN-based speech separation models face local receptive field\nlimitations and cannot effectively capture long time dependencies. Although\nLSTM and Transformer-based speech separation models can avoid this problem,\ntheir high complexity makes them face the challenge of computational resources\nand inference efficiency when dealing with long audio. To address this\nchallenge, we introduce an innovative speech separation method called SPMamba.\nThis model builds upon the robust TF-GridNet architecture, replacing its\ntraditional BLSTM modules with bidirectional Mamba modules. These modules\neffectively model the spatiotemporal relationships between the time and\nfrequency dimensions, allowing SPMamba to capture long-range dependencies with\nlinear computational complexity. Specifically, the bidirectional processing\nwithin the Mamba modules enables the model to utilize both past and future\ncontextual information, thereby enhancing separation performance. Extensive\nexperiments conducted on public datasets, including WSJ0-2Mix, WHAM!, and\nLibri2Mix, as well as the newly constructed Echo2Mix dataset, demonstrated that\nSPMamba significantly outperformed existing state-of-the-art models, achieving\nsuperior results while also reducing computational complexity. These findings\nhighlighted the effectiveness of SPMamba in tackling the intricate challenges\nof speech separation in complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing CNN-based speech separation models face local receptive field\nlimitations and cannot effectively capture long time dependencies. Although\nLSTM and Transformer-based speech separation models can avoid this problem,\ntheir high complexity makes them face the challenge of computational resources\nand inference efficiency when dealing with long audio. To address this\nchallenge, we introduce an innovative speech separation method called SPMamba.\nThis model builds upon the robust TF-GridNet architecture, replacing its\ntraditional BLSTM modules with bidirectional Mamba modules. These modules\neffectively model the spatiotemporal relationships between the time and\nfrequency dimensions, allowing SPMamba to capture long-range dependencies with\nlinear computational complexity. Specifically, the bidirectional processing\nwithin the Mamba modules enables the model to utilize both past and future\ncontextual information, thereby enhancing separation performance. Extensive\nexperiments conducted on public datasets, including WSJ0-2Mix, WHAM!, and\nLibri2Mix, as well as the newly constructed Echo2Mix dataset, demonstrated that\nSPMamba significantly outperformed existing state-of-the-art models, achieving\nsuperior results while also reducing computational complexity. These findings\nhighlighted the effectiveness of SPMamba in tackling the intricate challenges\nof speech separation in complex environments."
                },
                "authors": [
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Runxuan Yang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/JusperLee/SPMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06518v1",
                "updated": "2024-09-10T13:54:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    54,
                    4,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T13:54:04Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    54,
                    4,
                    1,
                    254,
                    0
                ],
                "title": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games"
                },
                "summary": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs."
                },
                "authors": [
                    {
                        "name": "Juhwan Choi"
                    },
                    {
                        "name": "YoungBin Kim"
                    }
                ],
                "author_detail": {
                    "name": "YoungBin Kim"
                },
                "author": "YoungBin Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05610v2",
                "updated": "2024-09-10T13:51:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    51,
                    0,
                    1,
                    254,
                    0
                ],
                "published": "2024-02-08T12:08:52Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    12,
                    8,
                    52,
                    3,
                    39,
                    0
                ],
                "title": "Extending 6D Object Pose Estimators for Stereo Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending 6D Object Pose Estimators for Stereo Vision"
                },
                "summary": "Estimating the 6D pose of objects accurately, quickly, and robustly remains a\ndifficult task. However, recent methods for directly regressing poses from RGB\nimages using dense features have achieved state-of-the-art results. Stereo\nvision, which provides an additional perspective on the object, can help reduce\npose ambiguity and occlusion. Moreover, stereo can directly infer the distance\nof an object, while mono-vision requires internalized knowledge of the object's\nsize. To extend the state-of-the-art in 6D object pose estimation to stereo, we\ncreated a BOP compatible stereo version of the YCB-V dataset. Our method\noutperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo\nvision and can easily be adopted for other dense feature-based algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the 6D pose of objects accurately, quickly, and robustly remains a\ndifficult task. However, recent methods for directly regressing poses from RGB\nimages using dense features have achieved state-of-the-art results. Stereo\nvision, which provides an additional perspective on the object, can help reduce\npose ambiguity and occlusion. Moreover, stereo can directly infer the distance\nof an object, while mono-vision requires internalized knowledge of the object's\nsize. To extend the state-of-the-art in 6D object pose estimation to stereo, we\ncreated a BOP compatible stereo version of the YCB-V dataset. Our method\noutperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo\nvision and can easily be adopted for other dense feature-based algorithms."
                },
                "authors": [
                    {
                        "name": "Thomas Pöllabauer"
                    },
                    {
                        "name": "Jan Emrich"
                    },
                    {
                        "name": "Volker Knauthe"
                    },
                    {
                        "name": "Arjan Kuijper"
                    }
                ],
                "author_detail": {
                    "name": "Arjan Kuijper"
                },
                "author": "Arjan Kuijper",
                "arxiv_comment": "4th International Conference on Pattern Recognition and Artificial\n  Intelligence (ICPRAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07606v2",
                "updated": "2024-09-10T13:39:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    39,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2023-09-14T11:13:36Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    11,
                    13,
                    36,
                    3,
                    257,
                    0
                ],
                "title": "Zero-shot Audio Topic Reranking using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Audio Topic Reranking using Large Language Models"
                },
                "summary": "Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them."
                },
                "authors": [
                    {
                        "name": "Mengjie Qian"
                    },
                    {
                        "name": "Rao Ma"
                    },
                    {
                        "name": "Adian Liusie"
                    },
                    {
                        "name": "Erfan Loweimi"
                    },
                    {
                        "name": "Kate M. Knill"
                    },
                    {
                        "name": "Mark J. F. Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark J. F. Gales"
                },
                "author": "Mark J. F. Gales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05211v2",
                "updated": "2024-09-10T13:21:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    21,
                    8,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-09T17:59:49Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    59,
                    49,
                    4,
                    222,
                    0
                ],
                "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
                },
                "summary": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Shaoqi Dong"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Yunsheng Wu"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Project Page: https://vita-home.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06393v3",
                "updated": "2024-09-10T12:58:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    58,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-04-09T15:35:52Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    15,
                    35,
                    52,
                    1,
                    100,
                    0
                ],
                "title": "MuPT: A Generative Symbolic Music Pretrained Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuPT: A Generative Symbolic Music Pretrained Transformer"
                },
                "summary": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions."
                },
                "authors": [
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Ziya Zhou"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Lejun Min"
                    },
                    {
                        "name": "Xueling Liu"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Fengze Han"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00222v2",
                "updated": "2024-09-10T12:57:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    57,
                    19,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-30T19:26:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    26,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Can Large Language Models Address Open-Target Stance Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Address Open-Target Stance Detection?"
                },
                "summary": "Stance detection (SD) assesses a text's position towards a target, typically\nlabeled as \"favor,\" \"against,\" or \"neutral.\" We introduce Open-Target Stance\nDetection (OTSD), where targets are neither seen during training nor provided\nas input. Evaluating Large Language Models (LLMs) like GPT-3.5, GPT-4o, Llama\n3, and Mistral, we compare their performance with the Target-Stance Extraction\n(TSE) approach, which has the advantage of using predefined targets. LLMs\nperform better than TSE in target generation when the real target is explicitly\nand not explicitly mentioned in the text. For stance detection, LLMs perform\nbetter in explicit scenarios but fail in non-explicit ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection (SD) assesses a text's position towards a target, typically\nlabeled as \"favor,\" \"against,\" or \"neutral.\" We introduce Open-Target Stance\nDetection (OTSD), where targets are neither seen during training nor provided\nas input. Evaluating Large Language Models (LLMs) like GPT-3.5, GPT-4o, Llama\n3, and Mistral, we compare their performance with the Target-Stance Extraction\n(TSE) approach, which has the advantage of using predefined targets. LLMs\nperform better than TSE in target generation when the real target is explicitly\nand not explicitly mentioned in the text. For stance detection, LLMs perform\nbetter in explicit scenarios but fail in non-explicit ones."
                },
                "authors": [
                    {
                        "name": "Abu Ubaida Akash"
                    },
                    {
                        "name": "Ahmed Fahmy"
                    },
                    {
                        "name": "Amine Trabelsi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Trabelsi"
                },
                "author": "Amine Trabelsi",
                "arxiv_comment": "12 pages; currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06452v1",
                "updated": "2024-09-10T12:17:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    17,
                    23,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T12:17:23Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    17,
                    23,
                    1,
                    254,
                    0
                ],
                "title": "Ransomware Detection Using Machine Learning in the Linux Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ransomware Detection Using Machine Learning in the Linux Kernel"
                },
                "summary": "Linux-based cloud environments have become lucrative targets for ransomware\nattacks, employing various encryption schemes at unprecedented speeds.\nAddressing the urgency for real-time ransomware protection, we propose\nleveraging the extended Berkeley Packet Filter (eBPF) to collect system call\ninformation regarding active processes and infer about the data directly at the\nkernel level. In this study, we implement two Machine Learning (ML) models in\neBPF - a decision tree and a multilayer perceptron. Benchmarking latency and\naccuracy against their user space counterparts, our findings underscore the\nefficacy of this approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linux-based cloud environments have become lucrative targets for ransomware\nattacks, employing various encryption schemes at unprecedented speeds.\nAddressing the urgency for real-time ransomware protection, we propose\nleveraging the extended Berkeley Packet Filter (eBPF) to collect system call\ninformation regarding active processes and infer about the data directly at the\nkernel level. In this study, we implement two Machine Learning (ML) models in\neBPF - a decision tree and a multilayer perceptron. Benchmarking latency and\naccuracy against their user space counterparts, our findings underscore the\nefficacy of this approach."
                },
                "authors": [
                    {
                        "name": "Adrian Brodzik"
                    },
                    {
                        "name": "Tomasz Malec-Kruszyński"
                    },
                    {
                        "name": "Wojciech Niewolski"
                    },
                    {
                        "name": "Mikołaj Tkaczyk"
                    },
                    {
                        "name": "Krzysztof Bocianiak"
                    },
                    {
                        "name": "Sok-Yen Loui"
                    }
                ],
                "author_detail": {
                    "name": "Sok-Yen Loui"
                },
                "author": "Sok-Yen Loui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14887v3",
                "updated": "2024-09-10T12:14:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    14,
                    56,
                    1,
                    254,
                    0
                ],
                "published": "2024-02-22T08:32:31Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    8,
                    32,
                    31,
                    3,
                    53,
                    0
                ],
                "title": "Infer metabolic fluxes from moment differences of mass-weighted\n  concentration distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infer metabolic fluxes from moment differences of mass-weighted\n  concentration distributions"
                },
                "summary": "Metabolic pathways are fundamental maps in biochemistry that detail how\nmolecules are transformed through various reactions. The complexity of\nmetabolic network, where a single compound can play a part in multiple\npathways, poses a challenge in inferring metabolic direction changes over time\nor after different treatments. Isotopic labeling experiment is the standard\nmethod to infer metabolic flux, which is currently defined as the flow of a\nmetabolite through a given pathway over time. However, there is still no way to\naccurately infer the metabolic direction changes after different treatments in\nan experiment. This study introduces a different concept: mass-weighted\nconcentration distribution, which is the empirical distribution of the\nconcentrations of metabolites times their associated molecular weights. By\nestimating the differences of the location and scale estimates of these\ndistributions, it becomes possible to infer the metabolic direction and\nmagnitude changes without requiring knowledge of the exact chemical structures\nof these compounds and their related pathways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metabolic pathways are fundamental maps in biochemistry that detail how\nmolecules are transformed through various reactions. The complexity of\nmetabolic network, where a single compound can play a part in multiple\npathways, poses a challenge in inferring metabolic direction changes over time\nor after different treatments. Isotopic labeling experiment is the standard\nmethod to infer metabolic flux, which is currently defined as the flow of a\nmetabolite through a given pathway over time. However, there is still no way to\naccurately infer the metabolic direction changes after different treatments in\nan experiment. This study introduces a different concept: mass-weighted\nconcentration distribution, which is the empirical distribution of the\nconcentrations of metabolites times their associated molecular weights. By\nestimating the differences of the location and scale estimates of these\ndistributions, it becomes possible to infer the metabolic direction and\nmagnitude changes without requiring knowledge of the exact chemical structures\nof these compounds and their related pathways."
                },
                "authors": [
                    {
                        "name": "Li Tuobang"
                    }
                ],
                "author_detail": {
                    "name": "Li Tuobang"
                },
                "author": "Li Tuobang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06451v1",
                "updated": "2024-09-10T12:13:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    13,
                    24,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T12:13:24Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    13,
                    24,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Emotional Text-to-Speech Controllability with Natural Language\n  Guidance through Contrastive Learning and Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Emotional Text-to-Speech Controllability with Natural Language\n  Guidance through Contrastive Learning and Diffusion Models"
                },
                "summary": "While current emotional text-to-speech (TTS) systems can generate highly\nintelligible emotional speech, achieving fine control over emotion rendering of\nthe output speech still remains a significant challenge. In this paper, we\nintroduce ParaEVITS, a novel emotional TTS framework that leverages the\ncompositionality of natural language to enhance control over emotional\nrendering. By incorporating a text-audio encoder inspired by ParaCLAP, a\ncontrastive language-audio pretraining (CLAP) model for computational\nparalinguistics, the diffusion model is trained to generate emotional\nembeddings based on textual emotional style descriptions. Our framework first\ntrains on reference audio using the audio encoder, then fine-tunes a diffusion\nmodel to process textual inputs from ParaCLAP's text encoder. During inference,\nspeech attributes such as pitch, jitter, and loudness are manipulated using\nonly textual conditioning. Our experiments demonstrate that ParaEVITS\neffectively control emotion rendering without compromising speech quality.\nSpeech demos are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current emotional text-to-speech (TTS) systems can generate highly\nintelligible emotional speech, achieving fine control over emotion rendering of\nthe output speech still remains a significant challenge. In this paper, we\nintroduce ParaEVITS, a novel emotional TTS framework that leverages the\ncompositionality of natural language to enhance control over emotional\nrendering. By incorporating a text-audio encoder inspired by ParaCLAP, a\ncontrastive language-audio pretraining (CLAP) model for computational\nparalinguistics, the diffusion model is trained to generate emotional\nembeddings based on textual emotional style descriptions. Our framework first\ntrains on reference audio using the audio encoder, then fine-tunes a diffusion\nmodel to process textual inputs from ParaCLAP's text encoder. During inference,\nspeech attributes such as pitch, jitter, and loudness are manipulated using\nonly textual conditioning. Our experiments demonstrate that ParaEVITS\neffectively control emotion rendering without compromising speech quality.\nSpeech demos are publicly available."
                },
                "authors": [
                    {
                        "name": "Xin Jing"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Andreas Triantafyllopoulos"
                    },
                    {
                        "name": "Björn W. Schuller"
                    }
                ],
                "author_detail": {
                    "name": "Björn W. Schuller"
                },
                "author": "Björn W. Schuller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06450v1",
                "updated": "2024-09-10T12:12:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    12,
                    9,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T12:12:09Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    12,
                    9,
                    1,
                    254,
                    0
                ],
                "title": "Multimodal Large Language Model Driven Scenario Testing for Autonomous\n  Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Model Driven Scenario Testing for Autonomous\n  Vehicles"
                },
                "summary": "The generation of corner cases has become increasingly crucial for\nefficiently testing autonomous vehicles prior to road deployment. However,\nexisting methods struggle to accommodate diverse testing requirements and often\nlack the ability to generalize to unseen situations, thereby reducing the\nconvenience and usability of the generated scenarios. A method that facilitates\neasily controllable scenario generation for efficient autonomous vehicles (AV)\ntesting with realistic and challenging situations is greatly needed. To address\nthis, we proposed OmniTester: a multimodal Large Language Model (LLM) based\nframework that fully leverages the extensive world knowledge and reasoning\ncapabilities of LLMs. OmniTester is designed to generate realistic and diverse\nscenarios within a simulation environment, offering a robust solution for\ntesting and evaluating AVs. In addition to prompt engineering, we employ tools\nfrom Simulation of Urban Mobility to simplify the complexity of codes generated\nby LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a\nself-improvement mechanism to enhance the LLM's understanding of scenarios,\nthereby increasing its ability to produce more realistic scenes. In the\nexperiments, we demonstrated the controllability and realism of our approaches\nin generating three types of challenging and complex scenarios. Additionally,\nwe showcased its effectiveness in reconstructing new scenarios described in\ncrash report, driven by the generalization capability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation of corner cases has become increasingly crucial for\nefficiently testing autonomous vehicles prior to road deployment. However,\nexisting methods struggle to accommodate diverse testing requirements and often\nlack the ability to generalize to unseen situations, thereby reducing the\nconvenience and usability of the generated scenarios. A method that facilitates\neasily controllable scenario generation for efficient autonomous vehicles (AV)\ntesting with realistic and challenging situations is greatly needed. To address\nthis, we proposed OmniTester: a multimodal Large Language Model (LLM) based\nframework that fully leverages the extensive world knowledge and reasoning\ncapabilities of LLMs. OmniTester is designed to generate realistic and diverse\nscenarios within a simulation environment, offering a robust solution for\ntesting and evaluating AVs. In addition to prompt engineering, we employ tools\nfrom Simulation of Urban Mobility to simplify the complexity of codes generated\nby LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a\nself-improvement mechanism to enhance the LLM's understanding of scenarios,\nthereby increasing its ability to produce more realistic scenes. In the\nexperiments, we demonstrated the controllability and realism of our approaches\nin generating three types of challenging and complex scenarios. Additionally,\nwe showcased its effectiveness in reconstructing new scenarios described in\ncrash report, driven by the generalization capability of LLMs."
                },
                "authors": [
                    {
                        "name": "Qiujing Lu"
                    },
                    {
                        "name": "Xuanhan Wang"
                    },
                    {
                        "name": "Yiwei Jiang"
                    },
                    {
                        "name": "Guangming Zhao"
                    },
                    {
                        "name": "Mingyue Ma"
                    },
                    {
                        "name": "Shuo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Feng"
                },
                "author": "Shuo Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06446v1",
                "updated": "2024-09-10T12:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    1,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T12:01:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    1,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data"
                },
                "summary": "Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness."
                },
                "authors": [
                    {
                        "name": "Hossein Hajipour"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Thorsten Holz"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "24 pages, 16 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09654v2",
                "updated": "2024-09-10T11:58:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    58,
                    23,
                    1,
                    254,
                    0
                ],
                "published": "2024-04-15T10:42:22Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    10,
                    42,
                    22,
                    0,
                    106,
                    0
                ],
                "title": "Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection"
                },
                "summary": "Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Shaofeng Cai"
                    },
                    {
                        "name": "Fang Deng"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    },
                    {
                        "name": "Junran Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junran Wu"
                },
                "author": "Junran Wu",
                "arxiv_comment": "Accepted by MM'24 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06442v1",
                "updated": "2024-09-10T11:48:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T11:48:05Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "title": "Prompt2Fashion: An automatically generated fashion dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt2Fashion: An automatically generated fashion dataset"
                },
                "summary": "Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion."
                },
                "authors": [
                    {
                        "name": "Georgia Argyro"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04775v2",
                "updated": "2024-09-10T11:43:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    43,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-07T09:30:26Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    9,
                    30,
                    26,
                    5,
                    251,
                    0
                ],
                "title": "Leveraging LLMs, Graphs and Object Hierarchies for Task Planning in\n  Large-Scale Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs, Graphs and Object Hierarchies for Task Planning in\n  Large-Scale Environments"
                },
                "summary": "Planning methods struggle with computational intractability in solving\ntask-level problems in large-scale environments. This work explores leveraging\nthe commonsense knowledge encoded in LLMs to empower planning techniques to\ndeal with these complex scenarios. We achieve this by efficiently using LLMs to\nprune irrelevant components from the planning problem's state space,\nsubstantially simplifying its complexity. We demonstrate the efficacy of this\nsystem through extensive experiments within a household simulation environment,\nalongside real-world validation using a 7-DoF manipulator (video\nhttps://youtu.be/6ro2UOtOQS4).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning methods struggle with computational intractability in solving\ntask-level problems in large-scale environments. This work explores leveraging\nthe commonsense knowledge encoded in LLMs to empower planning techniques to\ndeal with these complex scenarios. We achieve this by efficiently using LLMs to\nprune irrelevant components from the planning problem's state space,\nsubstantially simplifying its complexity. We demonstrate the efficacy of this\nsystem through extensive experiments within a household simulation environment,\nalongside real-world validation using a 7-DoF manipulator (video\nhttps://youtu.be/6ro2UOtOQS4)."
                },
                "authors": [
                    {
                        "name": "Rodrigo Pérez-Dattari"
                    },
                    {
                        "name": "Zhaoting Li"
                    },
                    {
                        "name": "Robert Babuška"
                    },
                    {
                        "name": "Jens Kober"
                    },
                    {
                        "name": "Cosimo Della Santina"
                    }
                ],
                "author_detail": {
                    "name": "Cosimo Della Santina"
                },
                "author": "Cosimo Della Santina",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06433v1",
                "updated": "2024-09-10T11:31:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    31,
                    2,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T11:31:02Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    31,
                    2,
                    1,
                    254,
                    0
                ],
                "title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for\n  Scholarly Knowledge Organization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for\n  Scholarly Knowledge Organization"
                },
                "summary": "The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public."
                },
                "authors": [
                    {
                        "name": "Gollam Rabby"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Allard Oelen"
                    }
                ],
                "author_detail": {
                    "name": "Allard Oelen"
                },
                "author": "Allard Oelen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06417v1",
                "updated": "2024-09-10T10:56:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    56,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T10:56:34Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    56,
                    34,
                    1,
                    254,
                    0
                ],
                "title": "Fast nonparametric inference of network backbones for graph\n  sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast nonparametric inference of network backbones for graph\n  sparsification"
                },
                "summary": "A network backbone provides a useful sparse representation of a weighted\nnetwork by keeping only its most important links, permitting a range of\ncomputational speedups and simplifying complex network visualizations. There\nare many possible criteria for a link to be considered important, and hence\nmany methods have been developed for the task of network backboning for graph\nsparsification. These methods can be classified as global or local in nature\ndepending on whether they evaluate the importance of an edge in the context of\nthe whole network or an individual node neighborhood. A key limitation of\nexisting network backboning methods is that they either artificially restrict\nthe topology of the backbone to take a specific form (e.g. a tree) or they\nrequire the specification of a free parameter (e.g. a significance level) that\ndetermines the number of edges to keep in the backbone. Here we develop a\ncompletely nonparametric framework for inferring the backbone of a weighted\nnetwork that overcomes these limitations by automatically selecting the optimal\nnumber of edges to retain in the backbone using the Minimum Description Length\n(MDL) principle from information theory. We develop two encoding schemes that\nserve as objective functions for global and local network backbones, as well as\nefficient optimization algorithms to identify the optimal backbones according\nto these objectives with runtime complexity log-linear in the number of edges.\nWe show that the proposed framework is generalizable to any discrete weight\ndistribution on the edges using a maximum a posteriori (MAP) estimation\nprocedure with an asymptotically equivalent Bayesian generative model of the\nbackbone. We compare the proposed method with existing methods in a range of\ntasks on real and synthetic networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A network backbone provides a useful sparse representation of a weighted\nnetwork by keeping only its most important links, permitting a range of\ncomputational speedups and simplifying complex network visualizations. There\nare many possible criteria for a link to be considered important, and hence\nmany methods have been developed for the task of network backboning for graph\nsparsification. These methods can be classified as global or local in nature\ndepending on whether they evaluate the importance of an edge in the context of\nthe whole network or an individual node neighborhood. A key limitation of\nexisting network backboning methods is that they either artificially restrict\nthe topology of the backbone to take a specific form (e.g. a tree) or they\nrequire the specification of a free parameter (e.g. a significance level) that\ndetermines the number of edges to keep in the backbone. Here we develop a\ncompletely nonparametric framework for inferring the backbone of a weighted\nnetwork that overcomes these limitations by automatically selecting the optimal\nnumber of edges to retain in the backbone using the Minimum Description Length\n(MDL) principle from information theory. We develop two encoding schemes that\nserve as objective functions for global and local network backbones, as well as\nefficient optimization algorithms to identify the optimal backbones according\nto these objectives with runtime complexity log-linear in the number of edges.\nWe show that the proposed framework is generalizable to any discrete weight\ndistribution on the edges using a maximum a posteriori (MAP) estimation\nprocedure with an asymptotically equivalent Bayesian generative model of the\nbackbone. We compare the proposed method with existing methods in a range of\ntasks on real and synthetic networks."
                },
                "authors": [
                    {
                        "name": "Alec Kirkley"
                    }
                ],
                "author_detail": {
                    "name": "Alec Kirkley"
                },
                "author": "Alec Kirkley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06416v1",
                "updated": "2024-09-10T10:55:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    55,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T10:55:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    55,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "Exploring the Integration of Large Language Models in Industrial Test\n  Maintenance Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Large Language Models in Industrial Test\n  Maintenance Processes"
                },
                "summary": "Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplored the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also proposed and demonstrated\nimplementations of two multi-agent architectures that can predict which test\ncases require maintenance following a change to the source code. Collectively,\nthese contributions advance our theoretical and practical understanding of how\nLLMs can be deployed to benefit industrial test maintenance processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplored the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also proposed and demonstrated\nimplementations of two multi-agent architectures that can predict which test\ncases require maintenance following a change to the source code. Collectively,\nthese contributions advance our theoretical and practical understanding of how\nLLMs can be deployed to benefit industrial test maintenance processes."
                },
                "authors": [
                    {
                        "name": "Ludvig Lemner"
                    },
                    {
                        "name": "Linnea Wahlgren"
                    },
                    {
                        "name": "Gregory Gay"
                    },
                    {
                        "name": "Nasser Mohammadiha"
                    },
                    {
                        "name": "Jingxiong Liu"
                    },
                    {
                        "name": "Joakim Wennerberg"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Wennerberg"
                },
                "author": "Joakim Wennerberg",
                "arxiv_comment": "Under submission to ACM TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06413v1",
                "updated": "2024-09-10T10:54:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    54,
                    16,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T10:54:16Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    54,
                    16,
                    1,
                    254,
                    0
                ],
                "title": "This is not normal! (Re-) Evaluating the lower $n$ guildelines for\n  regression analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is not normal! (Re-) Evaluating the lower $n$ guildelines for\n  regression analysis"
                },
                "summary": "The commonly cited rule of thumb for regression analysis, which suggests that\na sample size of $n \\geq 30$ is sufficient to ensure valid inferences, is\nfrequently referenced but rarely scrutinized. This research note evaluates the\nlower bound for the number of observations required for regression analysis by\nexploring how different distributional characteristics, such as skewness and\nkurtosis, influence the convergence of t-values to the t-distribution in linear\nregression models. Through an extensive simulation study involving over 22\nbillion regression models, this paper examines a range of symmetric,\nplatykurtic, and skewed distributions, testing sample sizes from 4 to 10,000.\nThe results reveal that it is sufficient that either the dependent or\nindependent variable follow a symmetric distribution for the t-values to\nconverge to the t-distribution at much smaller sample sizes than $n=30$. This\nis contrary to previous guidance which suggests that the error term needs to be\nnormally distributed for this convergence to happen at low $n$. On the other\nhand, if both dependent and independent variables are highly skewed the\nrequired sample size is substantially higher. In cases of extreme skewness,\neven sample sizes of 10,000 do not ensure convergence. These findings suggest\nthat the $n\\geq30$ rule is too permissive in certain cases but overly\nconservative in others, depending on the underlying distributional\ncharacteristics. This study offers revised guidelines for determining the\nminimum sample size necessary for valid regression analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The commonly cited rule of thumb for regression analysis, which suggests that\na sample size of $n \\geq 30$ is sufficient to ensure valid inferences, is\nfrequently referenced but rarely scrutinized. This research note evaluates the\nlower bound for the number of observations required for regression analysis by\nexploring how different distributional characteristics, such as skewness and\nkurtosis, influence the convergence of t-values to the t-distribution in linear\nregression models. Through an extensive simulation study involving over 22\nbillion regression models, this paper examines a range of symmetric,\nplatykurtic, and skewed distributions, testing sample sizes from 4 to 10,000.\nThe results reveal that it is sufficient that either the dependent or\nindependent variable follow a symmetric distribution for the t-values to\nconverge to the t-distribution at much smaller sample sizes than $n=30$. This\nis contrary to previous guidance which suggests that the error term needs to be\nnormally distributed for this convergence to happen at low $n$. On the other\nhand, if both dependent and independent variables are highly skewed the\nrequired sample size is substantially higher. In cases of extreme skewness,\neven sample sizes of 10,000 do not ensure convergence. These findings suggest\nthat the $n\\geq30$ rule is too permissive in certain cases but overly\nconservative in others, depending on the underlying distributional\ncharacteristics. This study offers revised guidelines for determining the\nminimum sample size necessary for valid regression analysis."
                },
                "authors": [
                    {
                        "name": "David Randahl"
                    }
                ],
                "author_detail": {
                    "name": "David Randahl"
                },
                "author": "David Randahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06411v1",
                "updated": "2024-09-10T10:49:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    49,
                    38,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T10:49:38Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    49,
                    38,
                    1,
                    254,
                    0
                ],
                "title": "Length Desensitization in Directed Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length Desensitization in Directed Preference Optimization"
                },
                "summary": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "arxiv_comment": "21 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2204.08901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2204.08901v2",
                "updated": "2024-09-10T10:39:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    39,
                    24,
                    1,
                    254,
                    0
                ],
                "published": "2022-04-19T13:58:20Z",
                "published_parsed": [
                    2022,
                    4,
                    19,
                    13,
                    58,
                    20,
                    1,
                    109,
                    0
                ],
                "title": "Inferring Epidemics from Multiple Dependent Data via Pseudo-Marginal\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Epidemics from Multiple Dependent Data via Pseudo-Marginal\n  Methods"
                },
                "summary": "Health-policy planning requires evidence on the burden that epidemics place\non healthcare systems. Multiple, often dependent, datasets provide a noisy and\nfragmented signal from the unobserved epidemic process including transmission\nand severity dynamics. This paper explores important challenges to the use of\nstate-space models for epidemic inference when multiple dependent datasets are\nanalysed. We propose a new semi-stochastic model that exploits deterministic\napproximations for large-scale transmission dynamics while retaining\nstochasticity in the occurrence and reporting of relatively rare severe events.\nThis model is suitable for many real-time situations including large seasonal\nepidemics and pandemics. Within this context, we develop algorithms to provide\nexact parameter inference and test them via simulation. Finally, we apply our\njoint model and the proposed algorithm to several surveillance data on the\n2017-18 influenza epidemic in England to reconstruct transmission dynamics and\nestimate the daily new influenza infections as well as severity indicators such\nas the case-hospitalisation risk and the hospital-intensive care risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-policy planning requires evidence on the burden that epidemics place\non healthcare systems. Multiple, often dependent, datasets provide a noisy and\nfragmented signal from the unobserved epidemic process including transmission\nand severity dynamics. This paper explores important challenges to the use of\nstate-space models for epidemic inference when multiple dependent datasets are\nanalysed. We propose a new semi-stochastic model that exploits deterministic\napproximations for large-scale transmission dynamics while retaining\nstochasticity in the occurrence and reporting of relatively rare severe events.\nThis model is suitable for many real-time situations including large seasonal\nepidemics and pandemics. Within this context, we develop algorithms to provide\nexact parameter inference and test them via simulation. Finally, we apply our\njoint model and the proposed algorithm to several surveillance data on the\n2017-18 influenza epidemic in England to reconstruct transmission dynamics and\nestimate the daily new influenza infections as well as severity indicators such\nas the case-hospitalisation risk and the hospital-intensive care risk."
                },
                "authors": [
                    {
                        "name": "Alice Corbella"
                    },
                    {
                        "name": "Anne M Presanis"
                    },
                    {
                        "name": "Paul J Birrell"
                    },
                    {
                        "name": "Daniela De Angelis"
                    }
                ],
                "author_detail": {
                    "name": "Daniela De Angelis"
                },
                "author": "Daniela De Angelis",
                "arxiv_comment": "23 pages, 6 figures, 4 tables, 1 algorithm. Submitted to the Annals\n  of Applied Statistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2204.08901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2204.08901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2010.10859v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2010.10859v4",
                "updated": "2024-09-10T10:24:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    24,
                    45,
                    1,
                    254,
                    0
                ],
                "published": "2020-10-21T09:35:50Z",
                "published_parsed": [
                    2020,
                    10,
                    21,
                    9,
                    35,
                    50,
                    2,
                    295,
                    0
                ],
                "title": "On the Semantic Expressiveness of Iso- and Equi-Recursive Types",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Semantic Expressiveness of Iso- and Equi-Recursive Types"
                },
                "summary": "Recursive types extend the simply-typed lambda calculus (STLC) with the\nadditional expressive power to enable diverging computation and to encode\nrecursive data-types (e.g., lists). Two formulations of recursive types exist:\niso-recursive and equi-recursive. The relative advantages of iso- and\nequi-recursion are well-studied when it comes to their impact on\ntype-inference. However, the relative semantic expressiveness of the two\nformulations remains unclear so far. This paper studies the semantic\nexpressiveness of STLC with iso- and equi-recursive types, proving that these\nformulations are equally expressive. In fact, we prove that they are both as\nexpressive as STLC with only term-level recursion. We phrase these\nequi-expressiveness results in terms of full abstraction of three canonical\ncompilers between these three languages (STLC with iso-, with equi-recursive\ntypes and with term-level recursion). Our choice of languages allows us to\nstudy expressiveness when interacting over both a simply-typed and a\nrecursively-typed interface. The three proofs all rely on a typed version of a\nproof technique called approximate backtranslation. Together, our results show\nthat there is no difference in semantic expressiveness between STLCs with iso-\nand equi-recursive types. In this paper, we focus on a simply-typed setting but\nwe believe our results scale to more powerful type systems like System F.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive types extend the simply-typed lambda calculus (STLC) with the\nadditional expressive power to enable diverging computation and to encode\nrecursive data-types (e.g., lists). Two formulations of recursive types exist:\niso-recursive and equi-recursive. The relative advantages of iso- and\nequi-recursion are well-studied when it comes to their impact on\ntype-inference. However, the relative semantic expressiveness of the two\nformulations remains unclear so far. This paper studies the semantic\nexpressiveness of STLC with iso- and equi-recursive types, proving that these\nformulations are equally expressive. In fact, we prove that they are both as\nexpressive as STLC with only term-level recursion. We phrase these\nequi-expressiveness results in terms of full abstraction of three canonical\ncompilers between these three languages (STLC with iso-, with equi-recursive\ntypes and with term-level recursion). Our choice of languages allows us to\nstudy expressiveness when interacting over both a simply-typed and a\nrecursively-typed interface. The three proofs all rely on a typed version of a\nproof technique called approximate backtranslation. Together, our results show\nthat there is no difference in semantic expressiveness between STLCs with iso-\nand equi-recursive types. In this paper, we focus on a simply-typed setting but\nwe believe our results scale to more powerful type systems like System F."
                },
                "authors": [
                    {
                        "name": "Dominique Devriese"
                    },
                    {
                        "name": "Eric Mark Martin"
                    },
                    {
                        "name": "Marco Patrignani"
                    }
                ],
                "author_detail": {
                    "name": "Marco Patrignani"
                },
                "author": "Marco Patrignani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2010.10859v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2010.10859v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13870v4",
                "updated": "2024-09-10T10:12:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    12,
                    56,
                    1,
                    254,
                    0
                ],
                "published": "2023-11-23T09:27:08Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    9,
                    27,
                    8,
                    3,
                    327,
                    0
                ],
                "title": "Multi-intention Inverse Q-learning for Interpretable Behavior\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-intention Inverse Q-learning for Interpretable Behavior\n  Representation"
                },
                "summary": "In advancing the understanding of natural decision-making processes, inverse\nreinforcement learning (IRL) methods have proven instrumental in reconstructing\nanimal's intentions underlying complex behaviors. Given the recent development\nof a continuous-time multi-intention IRL framework, there has been persistent\ninquiry into inferring discrete time-varying rewards with IRL. To address this\nchallenge, we introduce the class of hierarchical inverse Q-learning (HIQL)\nalgorithms. Through an unsupervised learning process, HIQL divides expert\ntrajectories into multiple intention segments, and solves the IRL problem\nindependently for each. Applying HIQL to simulated experiments and several real\nanimal behavior datasets, our approach outperforms current benchmarks in\nbehavior prediction and produces interpretable reward functions. Our results\nsuggest that the intention transition dynamics underlying complex\ndecision-making behavior is better modeled by a step function instead of a\nsmoothly varying function. This advancement holds promise for neuroscience and\ncognitive science, contributing to a deeper understanding of decision-making\nand uncovering underlying brain mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In advancing the understanding of natural decision-making processes, inverse\nreinforcement learning (IRL) methods have proven instrumental in reconstructing\nanimal's intentions underlying complex behaviors. Given the recent development\nof a continuous-time multi-intention IRL framework, there has been persistent\ninquiry into inferring discrete time-varying rewards with IRL. To address this\nchallenge, we introduce the class of hierarchical inverse Q-learning (HIQL)\nalgorithms. Through an unsupervised learning process, HIQL divides expert\ntrajectories into multiple intention segments, and solves the IRL problem\nindependently for each. Applying HIQL to simulated experiments and several real\nanimal behavior datasets, our approach outperforms current benchmarks in\nbehavior prediction and produces interpretable reward functions. Our results\nsuggest that the intention transition dynamics underlying complex\ndecision-making behavior is better modeled by a step function instead of a\nsmoothly varying function. This advancement holds promise for neuroscience and\ncognitive science, contributing to a deeper understanding of decision-making\nand uncovering underlying brain mechanisms."
                },
                "authors": [
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Brice De La Crompe"
                    },
                    {
                        "name": "Gabriel Kalweit"
                    },
                    {
                        "name": "Artur Schneider"
                    },
                    {
                        "name": "Maria Kalweit"
                    },
                    {
                        "name": "Ilka Diester"
                    },
                    {
                        "name": "Joschka Boedecker"
                    }
                ],
                "author_detail": {
                    "name": "Joschka Boedecker"
                },
                "author": "Joschka Boedecker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06377v1",
                "updated": "2024-09-10T09:58:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    58,
                    55,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T09:58:55Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    58,
                    55,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration"
                },
                "summary": "Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec."
                },
                "authors": [
                    {
                        "name": "Weicong Qin"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "arxiv_comment": "First 3 authors contributes equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06364v1",
                "updated": "2024-09-10T09:42:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    42,
                    58,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T09:42:58Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    42,
                    58,
                    1,
                    254,
                    0
                ],
                "title": "What happens to diffusion model likelihood when your model is\n  conditional?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What happens to diffusion model likelihood when your model is\n  conditional?"
                },
                "summary": "Diffusion Models (DMs) iteratively denoise random samples to produce\nhigh-quality data. The iterative sampling process is derived from Stochastic\nDifferential Equations (SDEs), allowing a speed-quality trade-off chosen at\ninference. Another advantage of sampling with differential equations is exact\nlikelihood computation. These likelihoods have been used to rank unconditional\nDMs and for out-of-domain classification. Despite the many existing and\npossible uses of DM likelihoods, the distinct properties captured are unknown,\nespecially in conditional contexts such as Text-To-Image (TTI) or\nText-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods\nare agnostic to the text input. TTI likelihood is more expressive but cannot\ndiscern confounding prompts. Our results show that applying DMs to conditional\ntasks reveals inconsistencies and strengthens claims that the properties of DM\nlikelihood are unknown. This impact sheds light on the previously unknown\nnature of DM likelihoods. Although conditional DMs maximise likelihood, the\nlikelihood in question is not as sensitive to the conditioning input as one\nexpects. This investigation provides a new point-of-view on diffusion\nlikelihoods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models (DMs) iteratively denoise random samples to produce\nhigh-quality data. The iterative sampling process is derived from Stochastic\nDifferential Equations (SDEs), allowing a speed-quality trade-off chosen at\ninference. Another advantage of sampling with differential equations is exact\nlikelihood computation. These likelihoods have been used to rank unconditional\nDMs and for out-of-domain classification. Despite the many existing and\npossible uses of DM likelihoods, the distinct properties captured are unknown,\nespecially in conditional contexts such as Text-To-Image (TTI) or\nText-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods\nare agnostic to the text input. TTI likelihood is more expressive but cannot\ndiscern confounding prompts. Our results show that applying DMs to conditional\ntasks reveals inconsistencies and strengthens claims that the properties of DM\nlikelihood are unknown. This impact sheds light on the previously unknown\nnature of DM likelihoods. Although conditional DMs maximise likelihood, the\nlikelihood in question is not as sensitive to the conditioning input as one\nexpects. This investigation provides a new point-of-view on diffusion\nlikelihoods."
                },
                "authors": [
                    {
                        "name": "Mattias Cross"
                    },
                    {
                        "name": "Anton Ragni"
                    }
                ],
                "author_detail": {
                    "name": "Anton Ragni"
                },
                "author": "Anton Ragni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03454v2",
                "updated": "2024-09-10T09:22:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    22,
                    26,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-05T12:06:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes"
                },
                "summary": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains."
                },
                "authors": [
                    {
                        "name": "Inacio Vieira"
                    },
                    {
                        "name": "Will Allred"
                    },
                    {
                        "name": "Séamus Lankford"
                    },
                    {
                        "name": "Sheila Castilho"
                    },
                    {
                        "name": "Andy Way"
                    }
                ],
                "author_detail": {
                    "name": "Andy Way"
                },
                "author": "Andy Way",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06351v1",
                "updated": "2024-09-10T09:10:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    10,
                    30,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T09:10:30Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    10,
                    30,
                    1,
                    254,
                    0
                ],
                "title": "MAGDA: Multi-agent guideline-driven diagnostic assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGDA: Multi-agent guideline-driven diagnostic assistance"
                },
                "summary": "In emergency departments, rural hospitals, or clinics in less developed\nregions, clinicians often lack fast image analysis by trained radiologists,\nwhich can have a detrimental effect on patients' healthcare. Large Language\nModels (LLMs) have the potential to alleviate some pressure from these\nclinicians by providing insights that can help them in their decision-making.\nWhile these LLMs achieve high test results on medical exams showcasing their\ngreat theoretical medical knowledge, they tend not to follow medical\nguidelines. In this work, we introduce a new approach for zero-shot\nguideline-driven decision support. We model a system of multiple LLM agents\naugmented with a contrastive vision-language model that collaborate to reach a\npatient diagnosis. After providing the agents with simple diagnostic\nguidelines, they will synthesize prompts and screen the image for findings\nfollowing these guidelines. Finally, they provide understandable\nchain-of-thought reasoning for their diagnosis, which is then self-refined to\nconsider inter-dependencies between diseases. As our method is zero-shot, it is\nadaptable to settings with rare diseases, where training data is limited, but\nexpert-crafted disease descriptions are available. We evaluate our method on\ntwo chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing\nperformance improvement over existing zero-shot methods and generalizability to\nrare diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In emergency departments, rural hospitals, or clinics in less developed\nregions, clinicians often lack fast image analysis by trained radiologists,\nwhich can have a detrimental effect on patients' healthcare. Large Language\nModels (LLMs) have the potential to alleviate some pressure from these\nclinicians by providing insights that can help them in their decision-making.\nWhile these LLMs achieve high test results on medical exams showcasing their\ngreat theoretical medical knowledge, they tend not to follow medical\nguidelines. In this work, we introduce a new approach for zero-shot\nguideline-driven decision support. We model a system of multiple LLM agents\naugmented with a contrastive vision-language model that collaborate to reach a\npatient diagnosis. After providing the agents with simple diagnostic\nguidelines, they will synthesize prompts and screen the image for findings\nfollowing these guidelines. Finally, they provide understandable\nchain-of-thought reasoning for their diagnosis, which is then self-refined to\nconsider inter-dependencies between diseases. As our method is zero-shot, it is\nadaptable to settings with rare diseases, where training data is limited, but\nexpert-crafted disease descriptions are available. We evaluate our method on\ntwo chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing\nperformance improvement over existing zero-shot methods and generalizability to\nrare diseases."
                },
                "authors": [
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Matthias Keicher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Keicher"
                },
                "author": "Matthias Keicher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06341v1",
                "updated": "2024-09-10T08:51:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    51,
                    52,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:51:52Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    51,
                    52,
                    1,
                    254,
                    0
                ],
                "title": "A Wearable Multi-Modal Edge-Computing System for Real-Time Kitchen\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Wearable Multi-Modal Edge-Computing System for Real-Time Kitchen\n  Activity Recognition"
                },
                "summary": "In the human activity recognition research area, prior studies predominantly\nconcentrate on leveraging advanced algorithms on public datasets to enhance\nrecognition performance, little attention has been paid to executing real-time\nkitchen activity recognition on energy-efficient, cost-effective edge devices.\nBesides, the prevalent approach of segregating data collection and context\nextraction across different devices escalates power usage, latency, and user\nprivacy risks, impeding widespread adoption. This work presents a multi-modal\nwearable edge computing system for human activity recognition in real-time.\nIntegrating six different sensors, ranging from inertial measurement units\n(IMUs) to thermal cameras, and two different microcontrollers, this system\nachieves end-to-end activity recognition, from data capture to context\nextraction, locally. Evaluation in an unmodified realistic kitchen validates\nits efficacy in recognizing fifteen activities, including a null class.\nEmploying a compact machine learning model (184.5 kbytes) yields an average\naccuracy of 87.83 \\%, with model inference completed in 25.26 ms on the\nmicrocontroller. Comparative analysis with alternative microcontrollers\nshowcases power consumption and inference speed performance, demonstrating the\nproposed system's viability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the human activity recognition research area, prior studies predominantly\nconcentrate on leveraging advanced algorithms on public datasets to enhance\nrecognition performance, little attention has been paid to executing real-time\nkitchen activity recognition on energy-efficient, cost-effective edge devices.\nBesides, the prevalent approach of segregating data collection and context\nextraction across different devices escalates power usage, latency, and user\nprivacy risks, impeding widespread adoption. This work presents a multi-modal\nwearable edge computing system for human activity recognition in real-time.\nIntegrating six different sensors, ranging from inertial measurement units\n(IMUs) to thermal cameras, and two different microcontrollers, this system\nachieves end-to-end activity recognition, from data capture to context\nextraction, locally. Evaluation in an unmodified realistic kitchen validates\nits efficacy in recognizing fifteen activities, including a null class.\nEmploying a compact machine learning model (184.5 kbytes) yields an average\naccuracy of 87.83 \\%, with model inference completed in 25.26 ms on the\nmicrocontroller. Comparative analysis with alternative microcontrollers\nshowcases power consumption and inference speed performance, demonstrating the\nproposed system's viability."
                },
                "authors": [
                    {
                        "name": "Mengxi Liu"
                    },
                    {
                        "name": "Sungho Suh"
                    },
                    {
                        "name": "Juan Felipe Vargas"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Agnes Grünerbl"
                    },
                    {
                        "name": "Paul Lukowicz"
                    }
                ],
                "author_detail": {
                    "name": "Paul Lukowicz"
                },
                "author": "Paul Lukowicz",
                "arxiv_comment": "the paper was accepted by the IJCAI24 workshp (4th International\n  Workshop on Deep Learning for Human Activity Recognition)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06338v1",
                "updated": "2024-09-10T08:48:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:48:05Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "title": "Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long\n  Context Evaluation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long\n  Context Evaluation Tasks"
                },
                "summary": "We argue that there are two major distinct capabilities in long context\nunderstanding: retrieval and holistic understanding. Understanding and further\nimproving LLMs' long context capabilities would not be possible without knowing\nthe tasks' focus categories. We aim to automatically identify retrieval focused\nand holistic understanding focused problems from suites of benchmarks and\nquantitatively measure the difficulty within each focus. In this paper, we\npresent the Dolce framework, which parameterizes each problem by $\\lambda$\n(complexity) and $k$ (redundancy) and assigns to one of five predefined focus\ncategories. We propose to sample short contexts from the full context and\nestimate the probability an LLM solves the problem using the sampled spans. To\nfind the $\\lambda$ and $k$ for each problem, we further propose a mixture model\nof a non-parametric background noise component and a parametric/non-parametric\nhybrid oracle component, where we derive the probability functions\nparameterized by $\\lambda$ and $k$ for both the correct-or-wrong (COW) scenario\nand the partial-point-in-grading (PIG) scenario. Our proposed methods can\nidentify 0% to 67% of the problems are retrieval focused and 0% to 90% of the\nproblems are holistic understanding focused across 44 existing long context\nevaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that there are two major distinct capabilities in long context\nunderstanding: retrieval and holistic understanding. Understanding and further\nimproving LLMs' long context capabilities would not be possible without knowing\nthe tasks' focus categories. We aim to automatically identify retrieval focused\nand holistic understanding focused problems from suites of benchmarks and\nquantitatively measure the difficulty within each focus. In this paper, we\npresent the Dolce framework, which parameterizes each problem by $\\lambda$\n(complexity) and $k$ (redundancy) and assigns to one of five predefined focus\ncategories. We propose to sample short contexts from the full context and\nestimate the probability an LLM solves the problem using the sampled spans. To\nfind the $\\lambda$ and $k$ for each problem, we further propose a mixture model\nof a non-parametric background noise component and a parametric/non-parametric\nhybrid oracle component, where we derive the probability functions\nparameterized by $\\lambda$ and $k$ for both the correct-or-wrong (COW) scenario\nand the partial-point-in-grading (PIG) scenario. Our proposed methods can\nidentify 0% to 67% of the problems are retrieval focused and 0% to 90% of the\nproblems are holistic understanding focused across 44 existing long context\nevaluation tasks."
                },
                "authors": [
                    {
                        "name": "Zi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zi Yang"
                },
                "author": "Zi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06336v1",
                "updated": "2024-09-10T08:47:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    47,
                    23,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:47:23Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    47,
                    23,
                    1,
                    254,
                    0
                ],
                "title": "Towards Agentic AI on Particle Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic AI on Particle Accelerators"
                },
                "summary": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show two examples, where we demonstrate viability of such\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show two examples, where we demonstrate viability of such\narchitecture."
                },
                "authors": [
                    {
                        "name": "Antonin Sulc"
                    },
                    {
                        "name": "Thorsten Hellert"
                    },
                    {
                        "name": "Raimund Kammering"
                    },
                    {
                        "name": "Hayden Houscher"
                    },
                    {
                        "name": "Jason St. John"
                    }
                ],
                "author_detail": {
                    "name": "Jason St. John"
                },
                "author": "Jason St. John",
                "arxiv_comment": "4 pages, 3 figures, Machine Learning and the Physical Sciences at\n  Workshop at the 38th conference on Neural Information Processing Systems\n  (NeurIPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06330v1",
                "updated": "2024-09-10T08:38:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    38,
                    11,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:38:11Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    38,
                    11,
                    1,
                    254,
                    0
                ],
                "title": "InstructSing: High-Fidelity Singing Voice Generation via Instructing\n  Yourself",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructSing: High-Fidelity Singing Voice Generation via Instructing\n  Yourself"
                },
                "summary": "It is challenging to accelerate the training process while ensuring both\nhigh-quality generated voices and acceptable inference speed. In this paper, we\npropose a novel neural vocoder called InstructSing, which can converge much\nfaster compared with other neural vocoders while maintaining good performance\nby integrating differentiable digital signal processing and adversarial\ntraining. It includes one generator and two discriminators. Specifically, the\ngenerator incorporates a harmonic-plus-noise (HN) module to produce 8kHz audio\nas an instructive signal. Subsequently, the HN module is connected with an\nextended WaveNet by an UNet-based module, which transforms the output of the HN\nmodule to a latent variable sequence containing essential periodic and\naperiodic information. In addition to the latent sequence, the extended WaveNet\nalso takes the mel-spectrogram as input to generate 48kHz high-fidelity singing\nvoices. In terms of discriminators, we combine a multi-period discriminator, as\noriginally proposed in HiFiGAN, with a multi-resolution multi-band STFT\ndiscriminator. Notably, InstructSing achieves comparable voice quality to other\nneural vocoders but with only one-tenth of the training steps on a 4 NVIDIA\nV100 GPU machine\\footnote{{Demo page:\n\\href{https://wavelandspeech.github.io/instructsing/}{\\texttt{https://wavelandspeech.github.io/inst\\\\ructsing/}}}}.\nWe plan to open-source our code and pretrained model once the paper get\naccepted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is challenging to accelerate the training process while ensuring both\nhigh-quality generated voices and acceptable inference speed. In this paper, we\npropose a novel neural vocoder called InstructSing, which can converge much\nfaster compared with other neural vocoders while maintaining good performance\nby integrating differentiable digital signal processing and adversarial\ntraining. It includes one generator and two discriminators. Specifically, the\ngenerator incorporates a harmonic-plus-noise (HN) module to produce 8kHz audio\nas an instructive signal. Subsequently, the HN module is connected with an\nextended WaveNet by an UNet-based module, which transforms the output of the HN\nmodule to a latent variable sequence containing essential periodic and\naperiodic information. In addition to the latent sequence, the extended WaveNet\nalso takes the mel-spectrogram as input to generate 48kHz high-fidelity singing\nvoices. In terms of discriminators, we combine a multi-period discriminator, as\noriginally proposed in HiFiGAN, with a multi-resolution multi-band STFT\ndiscriminator. Notably, InstructSing achieves comparable voice quality to other\nneural vocoders but with only one-tenth of the training steps on a 4 NVIDIA\nV100 GPU machine\\footnote{{Demo page:\n\\href{https://wavelandspeech.github.io/instructsing/}{\\texttt{https://wavelandspeech.github.io/inst\\\\ructsing/}}}}.\nWe plan to open-source our code and pretrained model once the paper get\naccepted."
                },
                "authors": [
                    {
                        "name": "Chang Zeng"
                    },
                    {
                        "name": "Chunhui Wang"
                    },
                    {
                        "name": "Xiaoxiao Miao"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yong Chen"
                },
                "author": "Yong Chen",
                "arxiv_comment": "To appear in 2024 IEEE Spoken Language Technology Workshop, Dec\n  02-05, 2024, Macao, China",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06328v1",
                "updated": "2024-09-10T08:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    33,
                    31,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    33,
                    31,
                    1,
                    254,
                    0
                ],
                "title": "Extracting Paragraphs from LLM Token Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Paragraphs from LLM Token Activations"
                },
                "summary": "Generative large language models (LLMs) excel in natural language processing\ntasks, yet their inner workings remain underexplored beyond token-level\npredictions. This study investigates the degree to which these models decide\nthe content of a paragraph at its onset, shedding light on their contextual\nunderstanding. By examining the information encoded in single-token\nactivations, specifically the \"\\textbackslash n\\textbackslash n\" double newline\ntoken, we demonstrate that patching these activations can transfer significant\ninformation about the context of the following paragraph, providing further\ninsights into the model's capacity to plan ahead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) excel in natural language processing\ntasks, yet their inner workings remain underexplored beyond token-level\npredictions. This study investigates the degree to which these models decide\nthe content of a paragraph at its onset, shedding light on their contextual\nunderstanding. By examining the information encoded in single-token\nactivations, specifically the \"\\textbackslash n\\textbackslash n\" double newline\ntoken, we demonstrate that patching these activations can transfer significant\ninformation about the context of the following paragraph, providing further\ninsights into the model's capacity to plan ahead."
                },
                "authors": [
                    {
                        "name": "Nicholas Pochinkov"
                    },
                    {
                        "name": "Angelo Benoit"
                    },
                    {
                        "name": "Lovkush Agarwal"
                    },
                    {
                        "name": "Zainab Ali Majid"
                    },
                    {
                        "name": "Lucile Ter-Minassian"
                    }
                ],
                "author_detail": {
                    "name": "Lucile Ter-Minassian"
                },
                "author": "Lucile Ter-Minassian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20684v2",
                "updated": "2024-09-10T08:19:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    19,
                    8,
                    1,
                    254,
                    0
                ],
                "published": "2024-05-31T08:26:47Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    8,
                    26,
                    47,
                    4,
                    152,
                    0
                ],
                "title": "Joint Embeddings for Graph Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Embeddings for Graph Instruction Tuning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance in text\nunderstanding and have become an essential tool for building smart assistants.\nOriginally focusing on text, they have been enhanced with multimodal\ncapabilities in recent works that successfully built visual instruction\nfollowing assistants. As far as the graph modality goes, however, no such\nassistants have yet been developed. Graph structures are complex in that they\nrepresent relation between different features and are permutation invariant.\nMoreover, representing them in purely textual form does not always lead to good\nLLM performance even for finetuned models. As a result, there is a need to\ndevelop a new method to integrate graphs in LLMs for general graph\nunderstanding. This work explores the integration of the graph modality in LLM\nfor general graph instruction following tasks. It aims at producing a deep\nlearning model that enhances an underlying LLM with graph embeddings and trains\nit to understand them and to produce, given an instruction, an answer grounded\nin the graph representation. The approach performs significantly better than a\ngraph to text approach and remains consistent even for larger graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance in text\nunderstanding and have become an essential tool for building smart assistants.\nOriginally focusing on text, they have been enhanced with multimodal\ncapabilities in recent works that successfully built visual instruction\nfollowing assistants. As far as the graph modality goes, however, no such\nassistants have yet been developed. Graph structures are complex in that they\nrepresent relation between different features and are permutation invariant.\nMoreover, representing them in purely textual form does not always lead to good\nLLM performance even for finetuned models. As a result, there is a need to\ndevelop a new method to integrate graphs in LLMs for general graph\nunderstanding. This work explores the integration of the graph modality in LLM\nfor general graph instruction following tasks. It aims at producing a deep\nlearning model that enhances an underlying LLM with graph embeddings and trains\nit to understand them and to produce, given an instruction, an answer grounded\nin the graph representation. The approach performs significantly better than a\ngraph to text approach and remains consistent even for larger graphs."
                },
                "authors": [
                    {
                        "name": "Aaron Haag"
                    },
                    {
                        "name": "Vlad Argatu"
                    },
                    {
                        "name": "Oliver Lohse"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lohse"
                },
                "author": "Oliver Lohse",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13375v2",
                "updated": "2024-09-10T08:08:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    8,
                    40,
                    1,
                    254,
                    0
                ],
                "published": "2024-06-19T09:16:14Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    9,
                    16,
                    14,
                    2,
                    171,
                    0
                ],
                "title": "ALiiCE: Evaluating Positional Fine-grained Citation Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALiiCE: Evaluating Positional Fine-grained Citation Generation"
                },
                "summary": "Large Language Models (LLMs) can enhance the credibility and verifiability by\ngenerating text with citations. However, existing tasks and evaluation methods\nare predominantly limited to sentence-level statement, neglecting the\nsignificance of positional fine-grained citations that can appear anywhere\nwithin sentences. To facilitate further exploration of the fine-grained\ncitation generation, we propose ALiiCE, the first automatic evaluation\nframework for this task. Our framework first parses the sentence claim into\natomic claims via dependency analysis and then calculates citation quality at\nthe atomic claim level. ALiiCE introduces three novel metrics for positional\nfined-grained citation quality assessment, including positional fine-grained\ncitation recall and precision, and coefficient of variation of citation\npositions. We evaluate the positional fine-grained citation generation\nperformance of several LLMs on two long-form QA datasets. Our experiments and\nanalyses demonstrate the effectiveness and reasonableness of ALiiCE. The\nresults also indicate that existing LLMs still struggle to provide positional\nfine-grained citations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance the credibility and verifiability by\ngenerating text with citations. However, existing tasks and evaluation methods\nare predominantly limited to sentence-level statement, neglecting the\nsignificance of positional fine-grained citations that can appear anywhere\nwithin sentences. To facilitate further exploration of the fine-grained\ncitation generation, we propose ALiiCE, the first automatic evaluation\nframework for this task. Our framework first parses the sentence claim into\natomic claims via dependency analysis and then calculates citation quality at\nthe atomic claim level. ALiiCE introduces three novel metrics for positional\nfined-grained citation quality assessment, including positional fine-grained\ncitation recall and precision, and coefficient of variation of citation\npositions. We evaluate the positional fine-grained citation generation\nperformance of several LLMs on two long-form QA datasets. Our experiments and\nanalyses demonstrate the effectiveness and reasonableness of ALiiCE. The\nresults also indicate that existing LLMs still struggle to provide positional\nfine-grained citations."
                },
                "authors": [
                    {
                        "name": "Yilong Xu"
                    },
                    {
                        "name": "Jinhua Gao"
                    },
                    {
                        "name": "Xiaoming Yu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08248v2",
                "updated": "2024-09-10T07:59:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    59,
                    21,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-11T07:50:25Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    7,
                    50,
                    25,
                    3,
                    193,
                    0
                ],
                "title": "Toward accessible comics for blind and low vision readers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward accessible comics for blind and low vision readers"
                },
                "summary": "This work explores how to fine-tune large language models using prompt\nengineering techniques with contextual information for generating an accurate\ntext description of the full story, ready to be forwarded to off-the-shelve\nspeech synthesis tools. We propose to use existing computer vision and optical\ncharacter recognition techniques to build a grounded context from the comic\nstrip image content, such as panels, characters, text, reading order and the\nassociation of bubbles and characters. Then we infer character identification\nand generate comic book script with context-aware panel description including\ncharacter's appearance, posture, mood, dialogues etc. We believe that such\nenriched content description can be easily used to produce audiobook and eBook\nwith various voices for characters, captions and playing sound effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores how to fine-tune large language models using prompt\nengineering techniques with contextual information for generating an accurate\ntext description of the full story, ready to be forwarded to off-the-shelve\nspeech synthesis tools. We propose to use existing computer vision and optical\ncharacter recognition techniques to build a grounded context from the comic\nstrip image content, such as panels, characters, text, reading order and the\nassociation of bubbles and characters. Then we infer character identification\nand generate comic book script with context-aware panel description including\ncharacter's appearance, posture, mood, dialogues etc. We believe that such\nenriched content description can be easily used to produce audiobook and eBook\nwith various voices for characters, captions and playing sound effects."
                },
                "authors": [
                    {
                        "name": "Christophe Rigaud"
                    },
                    {
                        "name": "Jean-Christophe Burie"
                    },
                    {
                        "name": "Samuel Petit"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Petit"
                },
                "arxiv_affiliation": "Comix AI",
                "author": "Samuel Petit",
                "arxiv_comment": "Accepted to MANPU 2024 (Athens, Greece, August 30, 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06299v1",
                "updated": "2024-09-10T07:53:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    53,
                    10,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:53:10Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    53,
                    10,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory"
                },
                "summary": "Recently, integrating visual foundation models into large language models\n(LLMs) to form video understanding systems has attracted widespread attention.\nMost of the existing models compress diverse semantic information within the\nwhole video and feed it into LLMs for content comprehension. While this method\nexcels in short video understanding, it may result in a blend of multiple event\ninformation in long videos due to coarse compression, which causes information\nredundancy. Consequently, the semantics of key events might be obscured within\nthe vast information that hinders the model's understanding capabilities. To\naddress this issue, we propose a Hierarchical Event-based Memory-enhanced LLM\n(HEM-LLM) for better understanding of long videos. Firstly, we design a novel\nadaptive sequence segmentation scheme to divide multiple events within long\nvideos. In this way, we can perform individual memory modeling for each event\nto establish intra-event contextual connections, thereby reducing information\nredundancy. Secondly, while modeling current event, we compress and inject the\ninformation of the previous event to enhance the long-term inter-event\ndependencies in videos. Finally, we perform extensive experiments on various\nvideo understanding tasks and the results show that our model achieves\nstate-of-the-art performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, integrating visual foundation models into large language models\n(LLMs) to form video understanding systems has attracted widespread attention.\nMost of the existing models compress diverse semantic information within the\nwhole video and feed it into LLMs for content comprehension. While this method\nexcels in short video understanding, it may result in a blend of multiple event\ninformation in long videos due to coarse compression, which causes information\nredundancy. Consequently, the semantics of key events might be obscured within\nthe vast information that hinders the model's understanding capabilities. To\naddress this issue, we propose a Hierarchical Event-based Memory-enhanced LLM\n(HEM-LLM) for better understanding of long videos. Firstly, we design a novel\nadaptive sequence segmentation scheme to divide multiple events within long\nvideos. In this way, we can perform individual memory modeling for each event\nto establish intra-event contextual connections, thereby reducing information\nredundancy. Secondly, while modeling current event, we compress and inject the\ninformation of the previous event to enhance the long-term inter-event\ndependencies in videos. Finally, we perform extensive experiments on various\nvideo understanding tasks and the results show that our model achieves\nstate-of-the-art performances."
                },
                "authors": [
                    {
                        "name": "Dingxin Cheng"
                    },
                    {
                        "name": "Mingda Li"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Yongxin Guo"
                    },
                    {
                        "name": "Bin Jiang"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06297v1",
                "updated": "2024-09-10T07:51:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    51,
                    53,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:51:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    51,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "User Preferences for Large Language Model versus Template-Based\n  Explanations of Movie Recommendations: A Pilot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Preferences for Large Language Model versus Template-Based\n  Explanations of Movie Recommendations: A Pilot Study"
                },
                "summary": "Recommender systems have become integral to our digital experiences, from\nonline shopping to streaming platforms. Still, the rationale behind their\nsuggestions often remains opaque to users. While some systems employ a\ngraph-based approach, offering inherent explainability through paths\nassociating recommended items and seed items, non-experts could not easily\nunderstand these explanations. A popular alternative is to convert graph-based\nexplanations into textual ones using a template and an algorithm, which we\ndenote here as ''template-based'' explanations. Yet, these can sometimes come\nacross as impersonal or uninspiring. A novel method would be to employ large\nlanguage models (LLMs) for this purpose, which we denote as ''LLM-based''. To\nassess the effectiveness of LLMs in generating more resonant explanations, we\nconducted a pilot study with 25 participants. They were presented with three\nexplanations: (1) traditional template-based, (2) LLM-based rephrasing of the\ntemplate output, and (3) purely LLM-based explanations derived from the\ngraph-based explanations. Although subject to high variance, preliminary\nfindings suggest that LLM-based explanations may provide a richer and more\nengaging user experience, further aligning with user expectations. This study\nsheds light on the potential limitations of current explanation methods and\noffers promising directions for leveraging large language models to improve\nuser satisfaction and trust in recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have become integral to our digital experiences, from\nonline shopping to streaming platforms. Still, the rationale behind their\nsuggestions often remains opaque to users. While some systems employ a\ngraph-based approach, offering inherent explainability through paths\nassociating recommended items and seed items, non-experts could not easily\nunderstand these explanations. A popular alternative is to convert graph-based\nexplanations into textual ones using a template and an algorithm, which we\ndenote here as ''template-based'' explanations. Yet, these can sometimes come\nacross as impersonal or uninspiring. A novel method would be to employ large\nlanguage models (LLMs) for this purpose, which we denote as ''LLM-based''. To\nassess the effectiveness of LLMs in generating more resonant explanations, we\nconducted a pilot study with 25 participants. They were presented with three\nexplanations: (1) traditional template-based, (2) LLM-based rephrasing of the\ntemplate output, and (3) purely LLM-based explanations derived from the\ngraph-based explanations. Although subject to high variance, preliminary\nfindings suggest that LLM-based explanations may provide a richer and more\nengaging user experience, further aligning with user expectations. This study\nsheds light on the potential limitations of current explanation methods and\noffers promising directions for leveraging large language models to improve\nuser satisfaction and trust in recommender systems."
                },
                "authors": [
                    {
                        "name": "Julien Albert"
                    },
                    {
                        "name": "Martin Balfroid"
                    },
                    {
                        "name": "Miriam Doh"
                    },
                    {
                        "name": "Jeremie Bogaert"
                    },
                    {
                        "name": "Luca La Fisca"
                    },
                    {
                        "name": "Liesbet De Vos"
                    },
                    {
                        "name": "Bryan Renard"
                    },
                    {
                        "name": "Vincent Stragier"
                    },
                    {
                        "name": "Emmanuel Jean"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Jean"
                },
                "author": "Emmanuel Jean",
                "arxiv_comment": "Presented to the Dutch-Belgian Workshop on Recommender Systems 2023\n  (14-15 December, 2023 - Antwerp, Belgium)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06750v2",
                "updated": "2024-09-10T07:48:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    48,
                    50,
                    1,
                    254,
                    0
                ],
                "published": "2024-05-10T18:08:30Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    18,
                    8,
                    30,
                    4,
                    131,
                    0
                ],
                "title": "Scalar Field Dominated Cosmology with Woods-Saxon Like Potential",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalar Field Dominated Cosmology with Woods-Saxon Like Potential"
                },
                "summary": "Dark energy can be characterized by a canonical scalar field, known as\nquintessence. Quintessence allows for a dynamical equation of state $-1 \\le\n\\omega \\le -\\frac{1}{3}$. A previous study by Oikonomou and Chatzarakis have\nshown that a scalar field model with a Woods-Saxon like potential can\nsuccessfully explain the early inflation. In this work, we consider a\nquintessence model with a potential of similar form to explain the late time\nacceleration. The model is studied at late phase assuming flat cosmology, and\nthe model parameters are constrained using Type Ia supernova data and\nObservational Hubble data. In particular we employ Markov Chain Monte Carlo\nmethods for the Bayesian inference of these parameters. We obtain the value of\nthe Hubble constant $H_0 \\sim 68 \\text{ km s}^{-1} \\text{Mpc}^{-1}$ and the\nmatter energy density parameter $\\Omega_{m_0} \\sim 0.30 $, which are in close\nagreement with the values obtained from the Planck CMB data, assuming the\n$\\Lambda$CDM model. Computation of the $\\chi^2_{min}$, AIC and BIC reveal that\nthis model is slightly preferred according to AIC and $\\chi^2_{min}$ criteria,\nwhile the $\\Lambda$CDM is preferred according to BIC. We demonstrate that the\nmodel possesses a stable attractor in the asymptotic future, which confirms the\ndynamical stability of the model. Thus, this model may be considered as a\npotential alternative to the $\\Lambda$CDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark energy can be characterized by a canonical scalar field, known as\nquintessence. Quintessence allows for a dynamical equation of state $-1 \\le\n\\omega \\le -\\frac{1}{3}$. A previous study by Oikonomou and Chatzarakis have\nshown that a scalar field model with a Woods-Saxon like potential can\nsuccessfully explain the early inflation. In this work, we consider a\nquintessence model with a potential of similar form to explain the late time\nacceleration. The model is studied at late phase assuming flat cosmology, and\nthe model parameters are constrained using Type Ia supernova data and\nObservational Hubble data. In particular we employ Markov Chain Monte Carlo\nmethods for the Bayesian inference of these parameters. We obtain the value of\nthe Hubble constant $H_0 \\sim 68 \\text{ km s}^{-1} \\text{Mpc}^{-1}$ and the\nmatter energy density parameter $\\Omega_{m_0} \\sim 0.30 $, which are in close\nagreement with the values obtained from the Planck CMB data, assuming the\n$\\Lambda$CDM model. Computation of the $\\chi^2_{min}$, AIC and BIC reveal that\nthis model is slightly preferred according to AIC and $\\chi^2_{min}$ criteria,\nwhile the $\\Lambda$CDM is preferred according to BIC. We demonstrate that the\nmodel possesses a stable attractor in the asymptotic future, which confirms the\ndynamical stability of the model. Thus, this model may be considered as a\npotential alternative to the $\\Lambda$CDM."
                },
                "authors": [
                    {
                        "name": "Sreerag Radhakrishnan"
                    },
                    {
                        "name": "Sarath Nelleri"
                    },
                    {
                        "name": "Navaneeth Poonthottathil"
                    }
                ],
                "author_detail": {
                    "name": "Navaneeth Poonthottathil"
                },
                "author": "Navaneeth Poonthottathil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02897v3",
                "updated": "2024-09-10T07:43:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    43,
                    19,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-04T17:41:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    41,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA"
                },
                "summary": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Wanjun Gu"
                    },
                    {
                        "name": "Danqing Liu"
                    },
                    {
                        "name": "Minhao Zou"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06289v1",
                "updated": "2024-09-10T07:42:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    42,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:42:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    42,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Automate Strategy Finding with LLM in Quant investment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automate Strategy Finding with LLM in Quant investment"
                },
                "summary": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets."
                },
                "authors": [
                    {
                        "name": "Zhizhuo Kou"
                    },
                    {
                        "name": "Holam Yu"
                    },
                    {
                        "name": "Jingshu Peng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06288v1",
                "updated": "2024-09-10T07:41:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    41,
                    50,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:41:50Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    41,
                    50,
                    1,
                    254,
                    0
                ],
                "title": "Ensemble Doubly Robust Bayesian Inference via Regression Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Doubly Robust Bayesian Inference via Regression Synthesis"
                },
                "summary": "The doubly robust estimator, which models both the propensity score and\noutcomes, is a popular approach to estimate the average treatment effect in the\npotential outcome setting. The primary appeal of this estimator is its\ntheoretical property, wherein the estimator achieves consistency as long as\neither the propensity score or outcomes is correctly specified. In most\napplications, however, both are misspecified, leading to considerable bias that\ncannot be checked. In this paper, we propose a Bayesian ensemble approach that\nsynthesizes multiple models for both the propensity score and outcomes, which\nwe call doubly robust Bayesian regression synthesis. Our approach applies\nBayesian updating to the ensemble model weights that adapt at the unit level,\nincorporating data heterogeneity, to significantly mitigate misspecification\nbias. Theoretically, we show that our proposed approach is consistent regarding\nthe estimation of both the propensity score and outcomes, ensuring that the\ndoubly robust estimator is consistent, even if no single model is correctly\nspecified. An efficient algorithm for posterior computation facilitates the\ncharacterization of uncertainty regarding the treatment effect. Our proposed\napproach is compared against standard and state-of-the-art methods through two\ncomprehensive simulation studies, where we find that our approach is superior\nin all cases. An empirical study on the impact of maternal smoking on birth\nweight highlights the practical applicability of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The doubly robust estimator, which models both the propensity score and\noutcomes, is a popular approach to estimate the average treatment effect in the\npotential outcome setting. The primary appeal of this estimator is its\ntheoretical property, wherein the estimator achieves consistency as long as\neither the propensity score or outcomes is correctly specified. In most\napplications, however, both are misspecified, leading to considerable bias that\ncannot be checked. In this paper, we propose a Bayesian ensemble approach that\nsynthesizes multiple models for both the propensity score and outcomes, which\nwe call doubly robust Bayesian regression synthesis. Our approach applies\nBayesian updating to the ensemble model weights that adapt at the unit level,\nincorporating data heterogeneity, to significantly mitigate misspecification\nbias. Theoretically, we show that our proposed approach is consistent regarding\nthe estimation of both the propensity score and outcomes, ensuring that the\ndoubly robust estimator is consistent, even if no single model is correctly\nspecified. An efficient algorithm for posterior computation facilitates the\ncharacterization of uncertainty regarding the treatment effect. Our proposed\napproach is compared against standard and state-of-the-art methods through two\ncomprehensive simulation studies, where we find that our approach is superior\nin all cases. An empirical study on the impact of maternal smoking on birth\nweight highlights the practical applicability of our proposed method."
                },
                "authors": [
                    {
                        "name": "Kaoru Babasaki"
                    },
                    {
                        "name": "Shonosuke Sugasawa"
                    },
                    {
                        "name": "Kosaku Takanashi"
                    },
                    {
                        "name": "Kenichiro McAlinn"
                    }
                ],
                "author_detail": {
                    "name": "Kenichiro McAlinn"
                },
                "author": "Kenichiro McAlinn",
                "arxiv_comment": "22 pages (main) + 15 pages (supplement)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11569v2",
                "updated": "2024-09-10T07:34:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    34,
                    50,
                    1,
                    254,
                    0
                ],
                "published": "2024-06-17T14:06:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    6,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs"
                },
                "summary": "For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory."
                },
                "authors": [
                    {
                        "name": "Haifeng Wen"
                    },
                    {
                        "name": "Hong Xing"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "38 pages, 7 figures, submitted for possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06280v1",
                "updated": "2024-09-10T07:31:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    31,
                    56,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:31:56Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    31,
                    56,
                    1,
                    254,
                    0
                ],
                "title": "Catch Me if You Can: Detecting Unauthorized Data Use in Deep Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catch Me if You Can: Detecting Unauthorized Data Use in Deep Learning\n  Models"
                },
                "summary": "The rise of deep learning (DL) has led to a surging demand for training data,\nwhich incentivizes the creators of DL models to trawl through the Internet for\ntraining materials. Meanwhile, users often have limited control over whether\ntheir data (e.g., facial images) are used to train DL models without their\nconsent, which has engendered pressing concerns.\n  This work proposes MembershipTracker, a practical data provenance tool that\ncan empower ordinary users to take agency in detecting the unauthorized use of\ntheir data in training DL models. We view tracing data provenance through the\nlens of membership inference (MI). MembershipTracker consists of a lightweight\ndata marking component to mark the target data with small and targeted changes,\nwhich can be strongly memorized by the model trained on them; and a specialized\nMI-based verification process to audit whether the model exhibits strong\nmemorization on the target samples.\n  Overall, MembershipTracker only requires the users to mark a small fraction\nof data (0.005% to 0.1% in proportion to the training set), and it enables the\nusers to reliably detect the unauthorized use of their data (average 0%\nFPR@100% TPR). We show that MembershipTracker is highly effective across\nvarious settings, including industry-scale training on the full-size\nImageNet-1k dataset. We finally evaluate MembershipTracker under multiple\nclasses of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of deep learning (DL) has led to a surging demand for training data,\nwhich incentivizes the creators of DL models to trawl through the Internet for\ntraining materials. Meanwhile, users often have limited control over whether\ntheir data (e.g., facial images) are used to train DL models without their\nconsent, which has engendered pressing concerns.\n  This work proposes MembershipTracker, a practical data provenance tool that\ncan empower ordinary users to take agency in detecting the unauthorized use of\ntheir data in training DL models. We view tracing data provenance through the\nlens of membership inference (MI). MembershipTracker consists of a lightweight\ndata marking component to mark the target data with small and targeted changes,\nwhich can be strongly memorized by the model trained on them; and a specialized\nMI-based verification process to audit whether the model exhibits strong\nmemorization on the target samples.\n  Overall, MembershipTracker only requires the users to mark a small fraction\nof data (0.005% to 0.1% in proportion to the training set), and it enables the\nusers to reliably detect the unauthorized use of their data (average 0%\nFPR@100% TPR). We show that MembershipTracker is highly effective across\nvarious settings, including industry-scale training on the full-size\nImageNet-1k dataset. We finally evaluate MembershipTracker under multiple\nclasses of countermeasures."
                },
                "authors": [
                    {
                        "name": "Zitao Chen"
                    },
                    {
                        "name": "Karthik Pattabiraman"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Pattabiraman"
                },
                "author": "Karthik Pattabiraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06277v2",
                "updated": "2024-09-11T01:47:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    47,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-10T07:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    28,
                    13,
                    1,
                    254,
                    0
                ],
                "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret."
                },
                "authors": [
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06273v1",
                "updated": "2024-09-10T07:23:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    23,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:23:29Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    23,
                    29,
                    1,
                    254,
                    0
                ],
                "title": "Mechanistic-statistical model for the expansion of ash dieback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic-statistical model for the expansion of ash dieback"
                },
                "summary": "Hymenoscyphus fraxineus is an invasive forest fungal pathogen that induces\nsevere dieback in European ash populations. The spread of the disease has been\nclosely monitored in France by the forest health survey system. We have\ndeveloped a mechanisticstatistical model that describes the spread of the\ndisease. It takes into account climate (summer temperature and spring\nrainfall), pathogen population dynamics (foliar infection, Allee effect induced\nby limited sexual partner encounters) and host density. We fitted this model\nusing available disease reports. We estimated the parameters of our model,\nfirst identifying the appropriate ranges for the parameters, which led to a\nmodel reduction, and then using an adaptive multiple importance sampling\nalgorithm for fitting. The model reproduces well the propagation observed in\nFrance over the last 20 years. In particular, it predicts the absence of\ndisease impact in the south-east of the country and its weak development in the\nGaronne valley in south-west France. Summer temperature is the factor with the\nhighest overall effect on disease spread, and explains the limited impact in\nsouthern France. Among the different temperature indices tested, the number of\nsummer days with temperatures above 28{\\textdegree}C gave the best qualitative\nbehavior and the best fit. In contrast, the Allee effect and the heterogeneity\nof spring precipitation did not strongly affect the overall expansion of H.\nfraxineus in France and could be neglected in the modeling process. The model\ncan be used to infer the average annual dispersal of H. fraxineus in France.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymenoscyphus fraxineus is an invasive forest fungal pathogen that induces\nsevere dieback in European ash populations. The spread of the disease has been\nclosely monitored in France by the forest health survey system. We have\ndeveloped a mechanisticstatistical model that describes the spread of the\ndisease. It takes into account climate (summer temperature and spring\nrainfall), pathogen population dynamics (foliar infection, Allee effect induced\nby limited sexual partner encounters) and host density. We fitted this model\nusing available disease reports. We estimated the parameters of our model,\nfirst identifying the appropriate ranges for the parameters, which led to a\nmodel reduction, and then using an adaptive multiple importance sampling\nalgorithm for fitting. The model reproduces well the propagation observed in\nFrance over the last 20 years. In particular, it predicts the absence of\ndisease impact in the south-east of the country and its weak development in the\nGaronne valley in south-west France. Summer temperature is the factor with the\nhighest overall effect on disease spread, and explains the limited impact in\nsouthern France. Among the different temperature indices tested, the number of\nsummer days with temperatures above 28{\\textdegree}C gave the best qualitative\nbehavior and the best fit. In contrast, the Allee effect and the heterogeneity\nof spring precipitation did not strongly affect the overall expansion of H.\nfraxineus in France and could be neglected in the modeling process. The model\ncan be used to infer the average annual dispersal of H. fraxineus in France."
                },
                "authors": [
                    {
                        "name": "Coralie Fritsch"
                    },
                    {
                        "name": "Marie Grosdidier"
                    },
                    {
                        "name": "Anne Gégout-Petit"
                    },
                    {
                        "name": "Benoit Marçais"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Marçais"
                },
                "arxiv_affiliation": "IAM",
                "author": "Benoit Marçais",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02443v2",
                "updated": "2024-09-10T06:59:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    59,
                    37,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-04T04:41:15Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    41,
                    15,
                    2,
                    248,
                    0
                ],
                "title": "Exploring the applicability of Large Language Models to citation context\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the applicability of Large Language Models to citation context\n  analysis"
                },
                "summary": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses."
                },
                "authors": [
                    {
                        "name": "Kai Nishikawa"
                    },
                    {
                        "name": "Hitoshi Koshiba"
                    }
                ],
                "author_detail": {
                    "name": "Hitoshi Koshiba"
                },
                "author": "Hitoshi Koshiba",
                "arxiv_doi": "10.1007/s11192-024-05142-9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11192-024-05142-9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06243v1",
                "updated": "2024-09-10T06:24:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    24,
                    46,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T06:24:46Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    24,
                    46,
                    1,
                    254,
                    0
                ],
                "title": "Inference is All You Need: Self Example Retriever for Cross-domain\n  Dialogue State Tracking with ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference is All You Need: Self Example Retriever for Cross-domain\n  Dialogue State Tracking with ChatGPT"
                },
                "summary": "Traditional dialogue state tracking approaches heavily rely on extensive\ntraining data and handcrafted features, limiting their scalability and\nadaptability to new domains. In this paper, we propose a novel method that\nleverages inference and in-context learning with ChatGPT for domain transfer in\ndialogue state tracking, without any parameter updates. By guiding ChatGPT's\nchain of thought, we enable it to retrieve relevant examples and generalize\nknowledge to accurately infer dialogue states, solely through inference.\nExperimental results on the MultiWOZ dataset demonstrate competitive\nperformance and promising generalization across domains. Our parameter-free\napproach offers a scalable and adaptable solution, opening new research\ndirections in domain transfer learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional dialogue state tracking approaches heavily rely on extensive\ntraining data and handcrafted features, limiting their scalability and\nadaptability to new domains. In this paper, we propose a novel method that\nleverages inference and in-context learning with ChatGPT for domain transfer in\ndialogue state tracking, without any parameter updates. By guiding ChatGPT's\nchain of thought, we enable it to retrieve relevant examples and generalize\nknowledge to accurately infer dialogue states, solely through inference.\nExperimental results on the MultiWOZ dataset demonstrate competitive\nperformance and promising generalization across domains. Our parameter-free\napproach offers a scalable and adaptable solution, opening new research\ndirections in domain transfer learning."
                },
                "authors": [
                    {
                        "name": "Jihyun Lee"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06241v1",
                "updated": "2024-09-10T06:17:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    17,
                    27,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T06:17:27Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    17,
                    27,
                    1,
                    254,
                    0
                ],
                "title": "DiPT: Enhancing LLM reasoning through diversified perspective-taking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiPT: Enhancing LLM reasoning through diversified perspective-taking"
                },
                "summary": "Existing work on improving language model reasoning typically explores a\nsingle solution path, which can be prone to errors. Inspired by\nperspective-taking in social studies, this paper introduces DiPT, a novel\napproach that complements current reasoning methods by explicitly incorporating\ndiversified viewpoints. This approach allows the model to gain a deeper\nunderstanding of the problem's context and identify the most effective solution\npath during the inference stage. Additionally, it provides a general\ndata-centric AI recipe for augmenting existing data to improve their quality\nfor fine-tuning.\n  Our empirical results demonstrate that DiPT can be flexibly integrated into\nexisting methods that focus on a single reasoning approach, enhancing their\nreasoning performance and stability when presented with paraphrased problems.\nFurthermore, we illustrate improved context understanding by maintaining the\nmodel's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning\nwith data enriched with diverse perspectives can boost the reasoning\ncapabilities of the model compared to fine-tuning with raw data alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on improving language model reasoning typically explores a\nsingle solution path, which can be prone to errors. Inspired by\nperspective-taking in social studies, this paper introduces DiPT, a novel\napproach that complements current reasoning methods by explicitly incorporating\ndiversified viewpoints. This approach allows the model to gain a deeper\nunderstanding of the problem's context and identify the most effective solution\npath during the inference stage. Additionally, it provides a general\ndata-centric AI recipe for augmenting existing data to improve their quality\nfor fine-tuning.\n  Our empirical results demonstrate that DiPT can be flexibly integrated into\nexisting methods that focus on a single reasoning approach, enhancing their\nreasoning performance and stability when presented with paraphrased problems.\nFurthermore, we illustrate improved context understanding by maintaining the\nmodel's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning\nwith data enriched with diverse perspectives can boost the reasoning\ncapabilities of the model compared to fine-tuning with raw data alone."
                },
                "authors": [
                    {
                        "name": "Hoang Anh Just"
                    },
                    {
                        "name": "Mahavir Dabas"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Ming Jin"
                    },
                    {
                        "name": "Ruoxi Jia"
                    }
                ],
                "author_detail": {
                    "name": "Ruoxi Jia"
                },
                "author": "Ruoxi Jia",
                "arxiv_comment": "LLM Reasoning with Perspectives, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05385v2",
                "updated": "2024-09-10T06:11:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    11,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T07:32:30Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    32,
                    30,
                    0,
                    253,
                    0
                ],
                "title": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models"
                },
                "summary": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability."
                },
                "authors": [
                    {
                        "name": "Hong Xingyun Hong"
                    },
                    {
                        "name": "Shao Yan Shao"
                    },
                    {
                        "name": "Wang Zhilin Wang"
                    },
                    {
                        "name": "Duan Manni Duan"
                    },
                    {
                        "name": "Jin Xiongnan"
                    }
                ],
                "author_detail": {
                    "name": "Jin Xiongnan"
                },
                "author": "Jin Xiongnan",
                "arxiv_comment": "This paper has been accepted by NLPCC-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06237v1",
                "updated": "2024-09-10T06:10:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    10,
                    33,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T06:10:33Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    10,
                    33,
                    1,
                    254,
                    0
                ],
                "title": "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for\n  Robust Singing Voice Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for\n  Robust Singing Voice Conversion"
                },
                "summary": "Singing voice conversion (SVC) is hindered by noise sensitivity due to the\nuse of non-robust methods for extracting pitch and energy during the inference.\nAs clean signals are key for the source audio in SVC, music source separation\npreprocessing offers a viable solution for handling noisy audio, like singing\nwith background music (BGM). However, current separating methods struggle to\nfully remove noise or excessively suppress signal components, affecting the\nnaturalness and similarity of the processed audio. To tackle this, our study\nintroduces RobustSVC, a novel any-to-one SVC framework that converts noisy\nvocals into clean vocals sung by the target singer. We replace the non-robust\nfeature with a HuBERT-based melody extractor and use adversarial training\nmechanisms with three discriminators to reduce information leakage in\nself-supervised representations. Experimental results show that RobustSVC is\nnoise-robust and achieves higher similarity and naturalness than baseline\nmethods in both noisy and clean vocal conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Singing voice conversion (SVC) is hindered by noise sensitivity due to the\nuse of non-robust methods for extracting pitch and energy during the inference.\nAs clean signals are key for the source audio in SVC, music source separation\npreprocessing offers a viable solution for handling noisy audio, like singing\nwith background music (BGM). However, current separating methods struggle to\nfully remove noise or excessively suppress signal components, affecting the\nnaturalness and similarity of the processed audio. To tackle this, our study\nintroduces RobustSVC, a novel any-to-one SVC framework that converts noisy\nvocals into clean vocals sung by the target singer. We replace the non-robust\nfeature with a HuBERT-based melody extractor and use adversarial training\nmechanisms with three discriminators to reduce information leakage in\nself-supervised representations. Experimental results show that RobustSVC is\nnoise-robust and achieves higher similarity and naturalness than baseline\nmethods in both noisy and clean vocal conditions."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xintao Zhao"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Binzhu Sha"
                    },
                    {
                        "name": "Zhiwei Lin"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "Accepted by ISCSLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01639v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01639v5",
                "updated": "2024-09-10T06:02:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    2,
                    25,
                    1,
                    254,
                    0
                ],
                "published": "2023-12-04T05:41:02Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    5,
                    41,
                    2,
                    0,
                    338,
                    0
                ],
                "title": "On the Effectiveness of Large Language Models in Domain-Specific Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effectiveness of Large Language Models in Domain-Specific Code\n  Generation"
                },
                "summary": "Large language models (LLMs) such as ChatGPT have shown remarkable\ncapabilities in code generation. Despite significant achievements, they rely on\nenormous training data to acquire a broad spectrum of open-domain knowledge.\nBesides, their evaluation revolves around open-domain benchmarks like\nHumanEval, which primarily consist of programming contests. Therefore, it is\nhard to fully characterize the intricacies and challenges associated with\nparticular domains (e.g., web, game, and math). In this paper, we conduct an\nin-depth study of the LLMs in domain-specific code generation. Our results\ndemonstrate that LLMs exhibit sub-optimal performance in generating\ndomain-specific code, due to their limited proficiency in utilizing\ndomain-specific libraries. We further observe that incorporating API knowledge\nas prompts can empower LLMs to generate more professional code. Based on these\nfindings, we further investigate how to effectively incorporate API knowledge\ninto the code generation process. We experiment with three strategies for\nincorporating domain knowledge, namely, external knowledge inquirer,\nchain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these\nstrategies as a new code generation approach called DomCoder. Experimental\nresults show that all strategies of DomCoder lead to improvement in the\neffectiveness of domain-specific code generation under certain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as ChatGPT have shown remarkable\ncapabilities in code generation. Despite significant achievements, they rely on\nenormous training data to acquire a broad spectrum of open-domain knowledge.\nBesides, their evaluation revolves around open-domain benchmarks like\nHumanEval, which primarily consist of programming contests. Therefore, it is\nhard to fully characterize the intricacies and challenges associated with\nparticular domains (e.g., web, game, and math). In this paper, we conduct an\nin-depth study of the LLMs in domain-specific code generation. Our results\ndemonstrate that LLMs exhibit sub-optimal performance in generating\ndomain-specific code, due to their limited proficiency in utilizing\ndomain-specific libraries. We further observe that incorporating API knowledge\nas prompts can empower LLMs to generate more professional code. Based on these\nfindings, we further investigate how to effectively incorporate API knowledge\ninto the code generation process. We experiment with three strategies for\nincorporating domain knowledge, namely, external knowledge inquirer,\nchain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these\nstrategies as a new code generation approach called DomCoder. Experimental\nresults show that all strategies of DomCoder lead to improvement in the\neffectiveness of domain-specific code generation under certain settings."
                },
                "authors": [
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Yalan Lin"
                    },
                    {
                        "name": "Yuhan Hu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Zhao Wei"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Juhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Juhong Wang"
                },
                "author": "Juhong Wang",
                "arxiv_comment": "Accepted by the ACM Transactions on Software Engineering and\n  Methodology (TOSEM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01639v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01639v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06223v1",
                "updated": "2024-09-10T05:26:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    26,
                    53,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T05:26:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    26,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models"
                },
                "summary": "The Audio Question Answering task includes audio event classification, audio\ncaptioning, and open ended reasoning. Recently, Audio Question Answering has\ngarnered attention due to the advent of Large Audio Language Models. Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext only Large Language Models through a projection module. While Large Audio\nLanguage Models excel in general audio understanding, they are limited in\ntemporal reasoning which may hinder their commercial applications and on device\ndeployment. This paper addresses these challenges and limitations in audio\ntemporal reasoning. First, we introduce a data augmentation technique for\ngenerating reliable audio temporal questions and answers using an LLM. Second,\nwe propose a continued finetuning curriculum learning strategy to specialize in\ntemporal reasoning without compromising performance on finetuned tasks.\nFinally, we develop a reliable and transparent automated metric, assisted by an\nLLM, to measure the correlation between Large Audio Language Model responses\nand ground truth data intelligently. We demonstrate the effectiveness of our\nproposed techniques using SOTA LALMs on public audio benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Audio Question Answering task includes audio event classification, audio\ncaptioning, and open ended reasoning. Recently, Audio Question Answering has\ngarnered attention due to the advent of Large Audio Language Models. Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext only Large Language Models through a projection module. While Large Audio\nLanguage Models excel in general audio understanding, they are limited in\ntemporal reasoning which may hinder their commercial applications and on device\ndeployment. This paper addresses these challenges and limitations in audio\ntemporal reasoning. First, we introduce a data augmentation technique for\ngenerating reliable audio temporal questions and answers using an LLM. Second,\nwe propose a continued finetuning curriculum learning strategy to specialize in\ntemporal reasoning without compromising performance on finetuned tasks.\nFinally, we develop a reliable and transparent automated metric, assisted by an\nLLM, to measure the correlation between Large Audio Language Model responses\nand ground truth data intelligently. We demonstrate the effectiveness of our\nproposed techniques using SOTA LALMs on public audio benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Arvind Krishna Sridhar"
                    },
                    {
                        "name": "Yinyi Guo"
                    },
                    {
                        "name": "Erik Visser"
                    }
                ],
                "author_detail": {
                    "name": "Erik Visser"
                },
                "author": "Erik Visser",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06211v1",
                "updated": "2024-09-10T04:34:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    34,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:34:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    34,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning"
                },
                "summary": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available."
                },
                "authors": [
                    {
                        "name": "Jaeseong Lee"
                    },
                    {
                        "name": "seung-won hwang"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Daniel F Campos"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06205v1",
                "updated": "2024-09-10T04:18:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    18,
                    49,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:18:49Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    18,
                    49,
                    1,
                    254,
                    0
                ],
                "title": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing\n  Behaviors with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing\n  Behaviors with LLMs"
                },
                "summary": "This paper introduces text-to-shape-display, a novel approach to generating\ndynamic shape changes in pin-based shape displays through natural language\ncommands. By leveraging large language models (LLMs) and AI-chaining, our\napproach allows users to author shape-changing behaviors on demand through text\nprompts without programming. We describe the foundational aspects necessary for\nsuch a system, including the identification of key generative elements\n(primitive, animation, and interaction) and design requirements to enhance user\ninteraction, based on formative exploration and iterative design processes.\nBased on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a\n24 x 24 shape display, which translates the user's textual command into\nexecutable code and allows for quick exploration through a web-based control\ninterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\nperformance evaluation and 2) user evaluation (N= 10). The study conclusions\nhighlight the ability to facilitate rapid ideation of a wide range of\nshape-changing behaviors with AI. However, the findings also expose\naccuracy-related challenges and limitations, prompting further exploration into\nrefining the framework for leveraging AI to better suit the unique requirements\nof shape-changing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces text-to-shape-display, a novel approach to generating\ndynamic shape changes in pin-based shape displays through natural language\ncommands. By leveraging large language models (LLMs) and AI-chaining, our\napproach allows users to author shape-changing behaviors on demand through text\nprompts without programming. We describe the foundational aspects necessary for\nsuch a system, including the identification of key generative elements\n(primitive, animation, and interaction) and design requirements to enhance user\ninteraction, based on formative exploration and iterative design processes.\nBased on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a\n24 x 24 shape display, which translates the user's textual command into\nexecutable code and allows for quick exploration through a web-based control\ninterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\nperformance evaluation and 2) user evaluation (N= 10). The study conclusions\nhighlight the ability to facilitate rapid ideation of a wide range of\nshape-changing behaviors with AI. However, the findings also expose\naccuracy-related challenges and limitations, prompting further exploration into\nrefining the framework for leveraging AI to better suit the unique requirements\nof shape-changing systems."
                },
                "authors": [
                    {
                        "name": "Wanli Qian"
                    },
                    {
                        "name": "Chenfeng Gao"
                    },
                    {
                        "name": "Anup Sathya"
                    },
                    {
                        "name": "Ryo Suzuki"
                    },
                    {
                        "name": "Ken Nakagaki"
                    }
                ],
                "author_detail": {
                    "name": "Ken Nakagaki"
                },
                "author": "Ken Nakagaki",
                "arxiv_doi": "10.1145/3654777.3676348",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3654777.3676348",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for ACM UIST 2024",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.12391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.12391v2",
                "updated": "2024-09-10T04:16:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    16,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2023-11-27T13:35:15Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    13,
                    35,
                    15,
                    0,
                    331,
                    0
                ],
                "title": "vTrain: A Simulation Framework for Evaluating Cost-effective and\n  Compute-optimal Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTrain: A Simulation Framework for Evaluating Cost-effective and\n  Compute-optimal Large Language Model Training"
                },
                "summary": "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget."
                },
                "authors": [
                    {
                        "name": "Jehyeon Bang"
                    },
                    {
                        "name": "Yujeong Choi"
                    },
                    {
                        "name": "Myeongwoo Kim"
                    },
                    {
                        "name": "Yongdeok Kim"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.12391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.12391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19997v2",
                "updated": "2024-09-10T04:14:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    14,
                    27,
                    1,
                    254,
                    0
                ],
                "published": "2024-03-29T06:24:16Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    6,
                    24,
                    16,
                    4,
                    89,
                    0
                ],
                "title": "Size-dependent fracture in elastomers: experiments and continuum\n  modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Size-dependent fracture in elastomers: experiments and continuum\n  modeling"
                },
                "summary": "Elastomeric materials display a complicated set of stretchability and\nfracture properties that strongly depend on the flaw size, which has long been\nof interest to engineers and materials scientists. Here, we combine experiments\nand numerical simulations for a comprehensive understanding of the nonlocal,\nsize-dependent features of fracture in elastomers. We show the size-dependent\nfracture behavior is quantitatively described through a nonlocal continuum\nmodel. The key ingredient of the nonlocal model is the use of an intrinsic\nlength scale associated with a finite fracture process zone, which is inferred\nfrom experiments. Of particular importance, our experimental and theoretical\napproach passes the critical set of capturing key aspects of the size-dependent\nfracture in elastomers. Applications to a wide range of synthetic elastomers\nthat exhibit moderate (~100%) to extreme stretchability (~1000%) are presented,\nwhich is also used to demonstrate the applicability of our approach in\nelastomeric specimens with complex geometries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastomeric materials display a complicated set of stretchability and\nfracture properties that strongly depend on the flaw size, which has long been\nof interest to engineers and materials scientists. Here, we combine experiments\nand numerical simulations for a comprehensive understanding of the nonlocal,\nsize-dependent features of fracture in elastomers. We show the size-dependent\nfracture behavior is quantitatively described through a nonlocal continuum\nmodel. The key ingredient of the nonlocal model is the use of an intrinsic\nlength scale associated with a finite fracture process zone, which is inferred\nfrom experiments. Of particular importance, our experimental and theoretical\napproach passes the critical set of capturing key aspects of the size-dependent\nfracture in elastomers. Applications to a wide range of synthetic elastomers\nthat exhibit moderate (~100%) to extreme stretchability (~1000%) are presented,\nwhich is also used to demonstrate the applicability of our approach in\nelastomeric specimens with complex geometries."
                },
                "authors": [
                    {
                        "name": "Jaehee Lee"
                    },
                    {
                        "name": "Jeongun Lee"
                    },
                    {
                        "name": "Seounghee Yun"
                    },
                    {
                        "name": "Sanha Kim"
                    },
                    {
                        "name": "Howon Lee"
                    },
                    {
                        "name": "Shawn A. Chester"
                    },
                    {
                        "name": "Hansohl Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hansohl Cho"
                },
                "author": "Hansohl Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05239v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05239v6",
                "updated": "2024-09-10T03:53:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    53,
                    55,
                    1,
                    254,
                    0
                ],
                "published": "2023-12-08T18:44:09Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    18,
                    44,
                    9,
                    4,
                    342,
                    0
                ],
                "title": "SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational\n  Score Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational\n  Score Distillation"
                },
                "summary": "Despite their ability to generate high-resolution and diverse images from\ntext prompts, text-to-image diffusion models often suffer from slow iterative\nsampling processes. Model distillation is one of the most effective directions\nto accelerate these models. However, previous distillation methods fail to\nretain the generation quality while requiring a significant amount of images\nfor training, either from real data or synthetically generated by the teacher\nmodel. In response to this limitation, we present a novel image-free\ndistillation scheme named $\\textbf{SwiftBrush}$. Drawing inspiration from\ntext-to-3D synthesis, in which a 3D neural radiance field that aligns with the\ninput prompt can be obtained from a 2D text-to-image diffusion prior via a\nspecialized loss without the use of any 3D data ground-truth, our approach\nre-purposes that same loss for distilling a pretrained multi-step text-to-image\nmodel to a student network that can generate high-fidelity images with just a\nsingle inference step. In spite of its simplicity, our model stands as one of\nthe first one-step text-to-image generators that can produce images of\ncomparable quality to Stable Diffusion without reliance on any training image\ndata. Remarkably, SwiftBrush achieves an FID score of $\\textbf{16.67}$ and a\nCLIP score of $\\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive\nresults or even substantially surpassing existing state-of-the-art distillation\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their ability to generate high-resolution and diverse images from\ntext prompts, text-to-image diffusion models often suffer from slow iterative\nsampling processes. Model distillation is one of the most effective directions\nto accelerate these models. However, previous distillation methods fail to\nretain the generation quality while requiring a significant amount of images\nfor training, either from real data or synthetically generated by the teacher\nmodel. In response to this limitation, we present a novel image-free\ndistillation scheme named $\\textbf{SwiftBrush}$. Drawing inspiration from\ntext-to-3D synthesis, in which a 3D neural radiance field that aligns with the\ninput prompt can be obtained from a 2D text-to-image diffusion prior via a\nspecialized loss without the use of any 3D data ground-truth, our approach\nre-purposes that same loss for distilling a pretrained multi-step text-to-image\nmodel to a student network that can generate high-fidelity images with just a\nsingle inference step. In spite of its simplicity, our model stands as one of\nthe first one-step text-to-image generators that can produce images of\ncomparable quality to Stable Diffusion without reliance on any training image\ndata. Remarkably, SwiftBrush achieves an FID score of $\\textbf{16.67}$ and a\nCLIP score of $\\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive\nresults or even substantially surpassing existing state-of-the-art distillation\ntechniques."
                },
                "authors": [
                    {
                        "name": "Thuan Hoang Nguyen"
                    },
                    {
                        "name": "Anh Tran"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tran"
                },
                "author": "Anh Tran",
                "arxiv_comment": "Accepted to CVPR 2024; Github:\n  https://github.com/VinAIResearch/SwiftBrush",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05239v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05239v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06192v1",
                "updated": "2024-09-10T03:43:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    43,
                    26,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:43:26Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    43,
                    26,
                    1,
                    254,
                    0
                ],
                "title": "NOVI : Chatbot System for University Novice with BERT and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOVI : Chatbot System for University Novice with BERT and LLMs"
                },
                "summary": "To mitigate the difficulties of university freshmen in adapting to university\nlife, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes\npost and comment data from SKKU 'Everytime', a university community site.\nDeveloped using LangChain, NOVI's performance has been evaluated with a BLEU\nscore, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR\nscore. This approach is not only limited to help university freshmen but is\nalso expected to help various people adapting to new environments with\ndifferent data. This research explores the development and potential\napplication of new educational technology tools, contributing to easier social\nadaptation for beginners and settling a foundation for future advancement in\nLLM studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the difficulties of university freshmen in adapting to university\nlife, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes\npost and comment data from SKKU 'Everytime', a university community site.\nDeveloped using LangChain, NOVI's performance has been evaluated with a BLEU\nscore, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR\nscore. This approach is not only limited to help university freshmen but is\nalso expected to help various people adapting to new environments with\ndifferent data. This research explores the development and potential\napplication of new educational technology tools, contributing to easier social\nadaptation for beginners and settling a foundation for future advancement in\nLLM studies."
                },
                "authors": [
                    {
                        "name": "Yoonji Nam"
                    },
                    {
                        "name": "TaeWoong Seo"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Sangji Lee"
                    },
                    {
                        "name": "JaeEun Im"
                    }
                ],
                "author_detail": {
                    "name": "JaeEun Im"
                },
                "author": "JaeEun Im",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05273v2",
                "updated": "2024-09-10T03:35:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    35,
                    36,
                    1,
                    254,
                    0
                ],
                "published": "2023-10-08T20:12:34Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    20,
                    12,
                    34,
                    6,
                    281,
                    0
                ],
                "title": "Learning force laws in many-body systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning force laws in many-body systems"
                },
                "summary": "Scientific laws describing natural systems may be more complex than our\nintuition can handle, thus how we discover laws must change. Machine learning\n(ML) models can analyze large quantities of data, but their structure should\nmatch the underlying physical constraints to provide useful insight. While\nprogress has been made using simulated data where the underlying physics is\nknown, training and validating ML models on experimental data requires\nfundamentally new approaches. Here we demonstrate and experimentally validate\nan ML approach that incorporates physical intuition to infer force laws in\ndusty plasma, a complex, many-body system. Trained on 3D particle trajectories,\nthe model accounts for inherent symmetries, non-identical particles, and learns\nthe effective non-reciprocal forces between particles with exquisite accuracy\n(R^2>0.99). We validate the model by inferring particle masses in two\nindependent yet consistent ways. The model's accuracy enables precise\nmeasurements of particle charge and screening length, discovering violations of\ncommon theoretical assumptions. Our ability to identify new physics from\nexperimental data demonstrates how ML-powered approaches can guide new routes\nof scientific discovery in many-body systems. Furthermore, we anticipate our ML\napproach to be a starting point for inferring laws from dynamics in a wide\nrange of many-body systems, from colloids to living organisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific laws describing natural systems may be more complex than our\nintuition can handle, thus how we discover laws must change. Machine learning\n(ML) models can analyze large quantities of data, but their structure should\nmatch the underlying physical constraints to provide useful insight. While\nprogress has been made using simulated data where the underlying physics is\nknown, training and validating ML models on experimental data requires\nfundamentally new approaches. Here we demonstrate and experimentally validate\nan ML approach that incorporates physical intuition to infer force laws in\ndusty plasma, a complex, many-body system. Trained on 3D particle trajectories,\nthe model accounts for inherent symmetries, non-identical particles, and learns\nthe effective non-reciprocal forces between particles with exquisite accuracy\n(R^2>0.99). We validate the model by inferring particle masses in two\nindependent yet consistent ways. The model's accuracy enables precise\nmeasurements of particle charge and screening length, discovering violations of\ncommon theoretical assumptions. Our ability to identify new physics from\nexperimental data demonstrates how ML-powered approaches can guide new routes\nof scientific discovery in many-body systems. Furthermore, we anticipate our ML\napproach to be a starting point for inferring laws from dynamics in a wide\nrange of many-body systems, from colloids to living organisms."
                },
                "authors": [
                    {
                        "name": "Wentao Yu"
                    },
                    {
                        "name": "Eslam Abdelaleem"
                    },
                    {
                        "name": "Ilya Nemenman"
                    },
                    {
                        "name": "Justin C. Burton"
                    }
                ],
                "author_detail": {
                    "name": "Justin C. Burton"
                },
                "author": "Justin C. Burton",
                "arxiv_comment": "14 pages, 4 Figures, 2 Supplemental Figures, 8 Supplemental Videos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06185v1",
                "updated": "2024-09-10T03:26:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    26,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    26,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Unlock Novel Scientific Research Ideas?"
                },
                "summary": "\"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Sandeep Kumar"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Vinayak Goyal"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "arxiv_comment": "24 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10483v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10483v5",
                "updated": "2024-09-10T03:15:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    15,
                    54,
                    1,
                    254,
                    0
                ],
                "published": "2023-10-16T15:03:55Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    15,
                    3,
                    55,
                    0,
                    289,
                    0
                ],
                "title": "Passive Inference Attacks on Split Learning via Adversarial\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive Inference Attacks on Split Learning via Adversarial\n  Regularization"
                },
                "summary": "Split Learning (SL) has emerged as a practical and efficient alternative to\ntraditional federated learning. While previous attempts to attack SL have often\nrelied on overly strong assumptions or targeted easily exploitable models, we\nseek to develop more capable attacks. We introduce SDAR, a novel attack\nframework against SL with an honest-but-curious server. SDAR leverages\nauxiliary data and adversarial regularization to learn a decodable simulator of\nthe client's private model, which can effectively infer the client's private\nfeatures under the vanilla SL, and both features and labels under the U-shaped\nSL. We perform extensive experiments in both configurations to validate the\neffectiveness of our proposed attacks. Notably, in challenging scenarios where\nexisting passive attacks struggle to reconstruct the client's private data\neffectively, SDAR consistently achieves significantly superior attack\nperformance, even comparable to active attacks. On CIFAR-10, at the deep split\nlevel of 7, SDAR achieves private feature reconstruction with less than 0.025\nmean squared error in both the vanilla and the U-shaped SL, and attains a label\ninference accuracy of over 98% in the U-shaped setting, while existing attacks\nfail to produce non-trivial results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning (SL) has emerged as a practical and efficient alternative to\ntraditional federated learning. While previous attempts to attack SL have often\nrelied on overly strong assumptions or targeted easily exploitable models, we\nseek to develop more capable attacks. We introduce SDAR, a novel attack\nframework against SL with an honest-but-curious server. SDAR leverages\nauxiliary data and adversarial regularization to learn a decodable simulator of\nthe client's private model, which can effectively infer the client's private\nfeatures under the vanilla SL, and both features and labels under the U-shaped\nSL. We perform extensive experiments in both configurations to validate the\neffectiveness of our proposed attacks. Notably, in challenging scenarios where\nexisting passive attacks struggle to reconstruct the client's private data\neffectively, SDAR consistently achieves significantly superior attack\nperformance, even comparable to active attacks. On CIFAR-10, at the deep split\nlevel of 7, SDAR achieves private feature reconstruction with less than 0.025\nmean squared error in both the vanilla and the U-shaped SL, and attains a label\ninference accuracy of over 98% in the U-shaped setting, while existing attacks\nfail to produce non-trivial results."
                },
                "authors": [
                    {
                        "name": "Xiaochen Zhu"
                    },
                    {
                        "name": "Xinjian Luo"
                    },
                    {
                        "name": "Yuncheng Wu"
                    },
                    {
                        "name": "Yangfan Jiang"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Beng Chin Ooi"
                },
                "author": "Beng Chin Ooi",
                "arxiv_comment": "To appear at NDSS 2025; 25 pages, 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10483v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10483v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06177v1",
                "updated": "2024-09-10T03:12:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    12,
                    39,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:12:39Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    12,
                    39,
                    1,
                    254,
                    0
                ],
                "title": "HierLLM: Hierarchical Large Language Model for Question Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierLLM: Hierarchical Large Language Model for Question Recommendation"
                },
                "summary": "Question recommendation is a task that sequentially recommends questions for\nstudents to enhance their learning efficiency. That is, given the learning\nhistory and learning target of a student, a question recommender is supposed to\nselect the question that will bring the most improvement for students. Previous\nmethods typically model the question recommendation as a sequential\ndecision-making problem, estimating students' learning state with the learning\nhistory, and feeding the learning state with the learning target to a neural\nnetwork to select the recommended question from a question set. However,\nprevious methods are faced with two challenges: (1) learning history is\nunavailable in the cold start scenario, which makes the recommender generate\ninappropriate recommendations; (2) the size of the question set is much large,\nwhich makes it difficult for the recommender to select the best question\nprecisely. To address the challenges, we propose a method called hierarchical\nlarge language model for question recommendation (HierLLM), which is a\nLLM-based hierarchical structure. The LLM-based structure enables HierLLM to\ntackle the cold start issue with the strong reasoning abilities of LLM. The\nhierarchical structure takes advantage of the fact that the number of concepts\nis significantly smaller than the number of questions, narrowing the range of\nselectable questions by first identifying the relevant concept for the\nto-recommend question, and then selecting the recommended question based on\nthat concept. This hierarchical structure reduces the difficulty of the\nrecommendation.To investigate the performance of HierLLM, we conduct extensive\nexperiments, and the results demonstrate the outstanding performance of\nHierLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question recommendation is a task that sequentially recommends questions for\nstudents to enhance their learning efficiency. That is, given the learning\nhistory and learning target of a student, a question recommender is supposed to\nselect the question that will bring the most improvement for students. Previous\nmethods typically model the question recommendation as a sequential\ndecision-making problem, estimating students' learning state with the learning\nhistory, and feeding the learning state with the learning target to a neural\nnetwork to select the recommended question from a question set. However,\nprevious methods are faced with two challenges: (1) learning history is\nunavailable in the cold start scenario, which makes the recommender generate\ninappropriate recommendations; (2) the size of the question set is much large,\nwhich makes it difficult for the recommender to select the best question\nprecisely. To address the challenges, we propose a method called hierarchical\nlarge language model for question recommendation (HierLLM), which is a\nLLM-based hierarchical structure. The LLM-based structure enables HierLLM to\ntackle the cold start issue with the strong reasoning abilities of LLM. The\nhierarchical structure takes advantage of the fact that the number of concepts\nis significantly smaller than the number of questions, narrowing the range of\nselectable questions by first identifying the relevant concept for the\nto-recommend question, and then selecting the recommended question based on\nthat concept. This hierarchical structure reduces the difficulty of the\nrecommendation.To investigate the performance of HierLLM, we conduct extensive\nexperiments, and the results demonstrate the outstanding performance of\nHierLLM."
                },
                "authors": [
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Haipeng Liu"
                    },
                    {
                        "name": "Ting Long"
                    }
                ],
                "author_detail": {
                    "name": "Ting Long"
                },
                "author": "Ting Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06173v1",
                "updated": "2024-09-10T03:06:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    6,
                    17,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:06:17Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    6,
                    17,
                    1,
                    254,
                    0
                ],
                "title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks"
                },
                "summary": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors."
                },
                "authors": [
                    {
                        "name": "Georgios Chochlakis"
                    },
                    {
                        "name": "Niyantha Maruthu Pandiyan"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Shrikanth Narayanan"
                },
                "author": "Shrikanth Narayanan",
                "arxiv_comment": "5 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.06691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06691v1",
                "updated": "2024-09-10T17:54:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:54:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric-Averaged Preference Optimization for Soft Preference Labels"
                },
                "summary": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority."
                },
                "authors": [
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "name": "Shixiang Shane Gu"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Heiga Zen"
                    },
                    {
                        "name": "Izzeddin Gur"
                    }
                ],
                "author_detail": {
                    "name": "Izzeddin Gur"
                },
                "author": "Izzeddin Gur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06679v1",
                "updated": "2024-09-10T17:44:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    35,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:44:35Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    35,
                    1,
                    254,
                    0
                ],
                "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning"
                },
                "summary": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling."
                },
                "authors": [
                    {
                        "name": "Zihan Liao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06666v1",
                "updated": "2024-09-10T17:34:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    34,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:34:34Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    34,
                    34,
                    1,
                    254,
                    0
                ],
                "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models"
                },
                "summary": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future."
                },
                "authors": [
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Zhengrui Ma"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00055v2",
                "updated": "2024-09-10T17:26:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    26,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-21T04:47:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    47,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models"
                },
                "summary": "The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA."
                },
                "authors": [
                    {
                        "name": "Yang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Cao"
                },
                "author": "Yang Cao",
                "arxiv_comment": "12 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06653v1",
                "updated": "2024-09-10T17:16:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    16,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:16:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    16,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "Human Perception of LLM-generated Text Content in Social Media\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Perception of LLM-generated Text Content in Social Media\n  Environments"
                },
                "summary": "Emerging technologies, particularly artificial intelligence (AI), and more\nspecifically Large Language Models (LLMs) have provided malicious actors with\npowerful tools for manipulating digital discourse. LLMs have the potential to\naffect traditional forms of democratic engagements, such as voter choice,\ngovernment surveys, or even online communication with regulators; since bots\nare capable of producing large quantities of credible text. To investigate the\nhuman perception of LLM-generated content, we recruited over 1,000 participants\nwho then tried to differentiate bot from human posts in social media discussion\nthreads. We found that humans perform poorly at identifying the true nature of\nuser posts on social media. We also found patterns in how humans identify\nLLM-generated text content in social media discourse. Finally, we observed the\nUncanny Valley effect in text dialogue in both user perception and\nidentification. This indicates that despite humans being poor at the\nidentification process, they can still sense discomfort when reading\nLLM-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging technologies, particularly artificial intelligence (AI), and more\nspecifically Large Language Models (LLMs) have provided malicious actors with\npowerful tools for manipulating digital discourse. LLMs have the potential to\naffect traditional forms of democratic engagements, such as voter choice,\ngovernment surveys, or even online communication with regulators; since bots\nare capable of producing large quantities of credible text. To investigate the\nhuman perception of LLM-generated content, we recruited over 1,000 participants\nwho then tried to differentiate bot from human posts in social media discussion\nthreads. We found that humans perform poorly at identifying the true nature of\nuser posts on social media. We also found patterns in how humans identify\nLLM-generated text content in social media discourse. Finally, we observed the\nUncanny Valley effect in text dialogue in both user perception and\nidentification. This indicates that despite humans being poor at the\nidentification process, they can still sense discomfort when reading\nLLM-generated content."
                },
                "authors": [
                    {
                        "name": "Kristina Radivojevic"
                    },
                    {
                        "name": "Matthew Chou"
                    },
                    {
                        "name": "Karla Badillo-Urquiola"
                    },
                    {
                        "name": "Paul Brenner"
                    }
                ],
                "author_detail": {
                    "name": "Paul Brenner"
                },
                "author": "Paul Brenner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06646v1",
                "updated": "2024-09-10T17:05:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    5,
                    11,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:05:11Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    5,
                    11,
                    1,
                    254,
                    0
                ],
                "title": "Optimal Workload Placement on Multi-Instance GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Workload Placement on Multi-Instance GPUs"
                },
                "summary": "There is an urgent and pressing need to optimize usage of Graphical\nProcessing Units (GPUs), which have arguably become one of the most expensive\nand sought after IT resources. To help with this goal, several of the current\ngeneration of GPUs support a partitioning feature, called Multi-Instance GPU\n(MIG) to allow multiple workloads to share a GPU, albeit with some constraints.\nIn this paper we investigate how to optimize the placement of Large Language\nModel (LLM)-based AI Inferencing workloads on GPUs. We first identify and\npresent several use cases that are encountered in practice that require\nworkloads to be efficiently placed or migrated to other GPUs to make room for\nincoming workloads. The overarching goal is to use as few GPUs as possible and\nto further minimize memory and compute wastage on GPUs that are utilized. We\nhave developed two approaches to address this problem: an optimization method\nand a heuristic method. We benchmark these with two workload scheduling\nheuristics for multiple use cases. Our results show up to 2.85x improvement in\nthe number of GPUs used and up to 70% reduction in GPU wastage over baseline\nheuristics. We plan to enable the SRE community to leverage our proposed method\nin production environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an urgent and pressing need to optimize usage of Graphical\nProcessing Units (GPUs), which have arguably become one of the most expensive\nand sought after IT resources. To help with this goal, several of the current\ngeneration of GPUs support a partitioning feature, called Multi-Instance GPU\n(MIG) to allow multiple workloads to share a GPU, albeit with some constraints.\nIn this paper we investigate how to optimize the placement of Large Language\nModel (LLM)-based AI Inferencing workloads on GPUs. We first identify and\npresent several use cases that are encountered in practice that require\nworkloads to be efficiently placed or migrated to other GPUs to make room for\nincoming workloads. The overarching goal is to use as few GPUs as possible and\nto further minimize memory and compute wastage on GPUs that are utilized. We\nhave developed two approaches to address this problem: an optimization method\nand a heuristic method. We benchmark these with two workload scheduling\nheuristics for multiple use cases. Our results show up to 2.85x improvement in\nthe number of GPUs used and up to 70% reduction in GPU wastage over baseline\nheuristics. We plan to enable the SRE community to leverage our proposed method\nin production environments."
                },
                "authors": [
                    {
                        "name": "Bekir Turkkan"
                    },
                    {
                        "name": "Pavankumar Murali"
                    },
                    {
                        "name": "Pavithra Harsha"
                    },
                    {
                        "name": "Rohan Arora"
                    },
                    {
                        "name": "Gerard Vanloo"
                    },
                    {
                        "name": "Chandra Narayanaswami"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Narayanaswami"
                },
                "author": "Chandra Narayanaswami",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06643v1",
                "updated": "2024-09-10T16:59:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    59,
                    33,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:59:33Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    59,
                    33,
                    1,
                    254,
                    0
                ],
                "title": "Strategic management analysis: from data to strategy diagram by LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic management analysis: from data to strategy diagram by LLM"
                },
                "summary": "Strategy management analyses are created by business consultants with common\nanalysis frameworks (i.e. comparative analyses) and associated diagrams. We\nshow these can be largely constructed using LLMs, starting with the extraction\nof insights from data, organization of those insights according to a strategy\nmanagement framework, and then depiction in the typical strategy management\ndiagram for that framework (static textual visualizations). We discuss caveats\nand future directions to generalize for broader uses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategy management analyses are created by business consultants with common\nanalysis frameworks (i.e. comparative analyses) and associated diagrams. We\nshow these can be largely constructed using LLMs, starting with the extraction\nof insights from data, organization of those insights according to a strategy\nmanagement framework, and then depiction in the typical strategy management\ndiagram for that framework (static textual visualizations). We discuss caveats\nand future directions to generalize for broader uses."
                },
                "authors": [
                    {
                        "name": "Richard Brath"
                    },
                    {
                        "name": "Adam Bradley"
                    },
                    {
                        "name": "David Jonker"
                    }
                ],
                "author_detail": {
                    "name": "David Jonker"
                },
                "author": "David Jonker",
                "arxiv_comment": "NLVIZ Workshop at IEEE VIZ 2024. 7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06635v1",
                "updated": "2024-09-10T16:46:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders"
                },
                "summary": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks."
                },
                "authors": [
                    {
                        "name": "Wenyu Zhang"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Xunlong Zou"
                    },
                    {
                        "name": "Zhuohan Liu"
                    },
                    {
                        "name": "Yingxu He"
                    },
                    {
                        "name": "Geyu Lin"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06624v1",
                "updated": "2024-09-10T16:26:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    26,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:26:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    26,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio"
                },
                "summary": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance."
                },
                "authors": [
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Kun Fan"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Jinxian Qu"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05731v2",
                "updated": "2024-09-10T16:25:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    25,
                    58,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T15:41:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    41,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and\n  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and\n  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"
                },
                "summary": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems."
                },
                "authors": [
                    {
                        "name": "Robert Kaufman"
                    },
                    {
                        "name": "Aaron Broukhim"
                    },
                    {
                        "name": "David Kirsh"
                    },
                    {
                        "name": "Nadir Weibel"
                    }
                ],
                "author_detail": {
                    "name": "Nadir Weibel"
                },
                "author": "Nadir Weibel",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06622v1",
                "updated": "2024-09-10T16:22:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:22:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "Exploring Italian sentence embeddings properties through multi-tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Italian sentence embeddings properties through multi-tasking"
                },
                "summary": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings."
                },
                "authors": [
                    {
                        "name": "Vivi Nastase"
                    },
                    {
                        "name": "Giuseppe Samo"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Paola Merlo"
                    }
                ],
                "author_detail": {
                    "name": "Paola Merlo"
                },
                "author": "Paola Merlo",
                "arxiv_comment": "9 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04745v3",
                "updated": "2024-09-10T15:51:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    27,
                    1,
                    254,
                    0
                ],
                "published": "2024-03-07T18:49:36Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    18,
                    49,
                    36,
                    3,
                    67,
                    0
                ],
                "title": "Not All Errors Are Made Equal: A Regret Metric for Detecting\n  System-level Trajectory Prediction Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Errors Are Made Equal: A Regret Metric for Detecting\n  System-level Trajectory Prediction Failures"
                },
                "summary": "Robot decision-making increasingly relies on data-driven human prediction\nmodels when operating around people. While these models are known to mispredict\nin out-of-distribution interactions, only a subset of prediction errors impact\ndownstream robot performance. We propose characterizing such \"system-level\"\nprediction failures via the mathematical notion of regret: high-regret\ninteractions are precisely those in which mispredictions degraded closed-loop\nrobot performance. We further introduce a probabilistic generalization of\nregret that calibrates failure detection across disparate deployment contexts\nand renders regret compatible with reward-based and reward-free (e.g.,\ngenerative) planners. In simulated autonomous driving interactions and social\nnavigation interactions deployed on hardware, we showcase that our system-level\nfailure metric can be used offline to automatically extract closed-loop\nhuman-robot interactions that state-of-the-art generative human predictors and\nrobot planners previously struggled with. We further find that the very\npresence of high-regret data during human predictor fine-tuning is highly\npredictive of robot re-deployment performance improvements. Fine-tuning with\nthe informative but significantly smaller high-regret data (23% of deployment\ndata) is competitive with fine-tuning on the full deployment dataset,\nindicating a promising avenue for efficiently mitigating system-level\nhuman-robot interaction failures. Project website:\nhttps://cmu-intentlab.github.io/not-all-errors/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot decision-making increasingly relies on data-driven human prediction\nmodels when operating around people. While these models are known to mispredict\nin out-of-distribution interactions, only a subset of prediction errors impact\ndownstream robot performance. We propose characterizing such \"system-level\"\nprediction failures via the mathematical notion of regret: high-regret\ninteractions are precisely those in which mispredictions degraded closed-loop\nrobot performance. We further introduce a probabilistic generalization of\nregret that calibrates failure detection across disparate deployment contexts\nand renders regret compatible with reward-based and reward-free (e.g.,\ngenerative) planners. In simulated autonomous driving interactions and social\nnavigation interactions deployed on hardware, we showcase that our system-level\nfailure metric can be used offline to automatically extract closed-loop\nhuman-robot interactions that state-of-the-art generative human predictors and\nrobot planners previously struggled with. We further find that the very\npresence of high-regret data during human predictor fine-tuning is highly\npredictive of robot re-deployment performance improvements. Fine-tuning with\nthe informative but significantly smaller high-regret data (23% of deployment\ndata) is competitive with fine-tuning on the full deployment dataset,\nindicating a promising avenue for efficiently mitigating system-level\nhuman-robot interaction failures. Project website:\nhttps://cmu-intentlab.github.io/not-all-errors/"
                },
                "authors": [
                    {
                        "name": "Kensuke Nakamura"
                    },
                    {
                        "name": "Ran Tian"
                    },
                    {
                        "name": "Andrea Bajcsy"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bajcsy"
                },
                "author": "Andrea Bajcsy",
                "arxiv_comment": "6 figures, 3 tables, Accepted to CoRL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06601v1",
                "updated": "2024-09-10T15:51:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "title": "Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling"
                },
                "summary": "Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments."
                },
                "authors": [
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06595v1",
                "updated": "2024-09-10T15:39:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:39:32Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations."
                },
                "authors": [
                    {
                        "name": "Sacha Muller"
                    },
                    {
                        "name": "António Loison"
                    },
                    {
                        "name": "Bilel Omrani"
                    },
                    {
                        "name": "Gautier Viaud"
                    }
                ],
                "author_detail": {
                    "name": "Gautier Viaud"
                },
                "author": "Gautier Viaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08921v2",
                "updated": "2024-09-10T15:38:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    38,
                    56,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-15T12:20:24Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    20,
                    24,
                    3,
                    228,
                    0
                ],
                "title": "Graph Retrieval-Augmented Generation: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation: A Survey"
                },
                "summary": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}."
                },
                "authors": [
                    {
                        "name": "Boci Peng"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Xiaohe Bo"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Ongoing work. Compared to the first version, several references have\n  been added and a GitHub repository link has been provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06568v1",
                "updated": "2024-09-10T15:02:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    2,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:02:34Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    2,
                    34,
                    1,
                    254,
                    0
                ],
                "title": "Think-on-Process: Dynamic Process Generation for Collaborative\n  Development of Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-on-Process: Dynamic Process Generation for Collaborative\n  Development of Multi-Agent System"
                },
                "summary": "Software development is a collaborative endeavor that requires individuals\nfrom different departments to work together in order to collectively develop a\nhigh-quality software system. In this context, people have begun to explore a\nmethod that leverages multi-agent systems based on LLMs to carry out software\ndevelopment. However, existing research tends to rigidly fix the software\ndevelopment process in a framework in code form, thus failing to dynamically\nadjust the software development process in real-time to meet the more flexible\nand variable software environment. In this paper, we propose a dynamic process\ngeneration framework, named ToP (Think-on-Process). The core idea of ToP is to\nleverage experiential knowledge (i.e., process models) to guide LLMs in\ngenerating software development processes (i.e., instances). These instances\nwill guide multi-agent in software development and employ a compiler to provide\nfeedback on the development outcomes. Subsequently, we utilize heuristic\nalgorithms to filter the instances and apply process mining algorithms to\nderive process model. Finally, the process model will be converted into text,\nformatted as prompts, to enhance the ability of LLMs to generate other\ninstances. Experiments demonstrate that our framework ToP significantly\nenhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for\nfive categories of software development tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development is a collaborative endeavor that requires individuals\nfrom different departments to work together in order to collectively develop a\nhigh-quality software system. In this context, people have begun to explore a\nmethod that leverages multi-agent systems based on LLMs to carry out software\ndevelopment. However, existing research tends to rigidly fix the software\ndevelopment process in a framework in code form, thus failing to dynamically\nadjust the software development process in real-time to meet the more flexible\nand variable software environment. In this paper, we propose a dynamic process\ngeneration framework, named ToP (Think-on-Process). The core idea of ToP is to\nleverage experiential knowledge (i.e., process models) to guide LLMs in\ngenerating software development processes (i.e., instances). These instances\nwill guide multi-agent in software development and employ a compiler to provide\nfeedback on the development outcomes. Subsequently, we utilize heuristic\nalgorithms to filter the instances and apply process mining algorithms to\nderive process model. Finally, the process model will be converted into text,\nformatted as prompts, to enhance the ability of LLMs to generate other\ninstances. Experiments demonstrate that our framework ToP significantly\nenhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for\nfive categories of software development tasks."
                },
                "authors": [
                    {
                        "name": "Leilei Lin"
                    },
                    {
                        "name": "Yingming Zhou"
                    },
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.14999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.14999v2",
                "updated": "2024-09-10T14:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    53,
                    46,
                    1,
                    254,
                    0
                ],
                "published": "2022-10-26T19:40:10Z",
                "published_parsed": [
                    2022,
                    10,
                    26,
                    19,
                    40,
                    10,
                    2,
                    299,
                    0
                ],
                "title": "Secure IP Address Allocation at Cloud Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure IP Address Allocation at Cloud Scale"
                },
                "summary": "Public clouds necessitate dynamic resource allocation and sharing. However,\nthe dynamic allocation of IP addresses can be abused by adversaries to source\nmalicious traffic, bypass rate limiting systems, and even capture traffic\nintended for other cloud tenants. As a result, both the cloud provider and\ntheir customers are put at risk, and defending against these threats requires a\nrigorous analysis of tenant behavior, adversarial strategies, and cloud\nprovider policies. In this paper, we develop a practical defense for IP address\nallocation through such an analysis. We first develop a statistical model of\ncloud tenant deployment behavior based on literature and measurement of\ndeployed systems. Through this, we analyze IP allocation policies under\nexisting and novel threat models. In response to our stronger proposed threat\nmodel, we design IP scan segmentation, an IP allocation policy that protects\nthe address pool against adversarial scanning even when an adversary is not\nlimited by number of cloud tenants. Through empirical evaluation on both\nsynthetic and real-world allocation traces, we show that IP scan segmentation\nreduces adversaries' ability to rapidly allocate addresses, protecting both\naddress space reputation and cloud tenant data. In this way, we show that\nprincipled analysis and implementation of cloud IP address allocation can lead\nto substantial security gains for tenants and their users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public clouds necessitate dynamic resource allocation and sharing. However,\nthe dynamic allocation of IP addresses can be abused by adversaries to source\nmalicious traffic, bypass rate limiting systems, and even capture traffic\nintended for other cloud tenants. As a result, both the cloud provider and\ntheir customers are put at risk, and defending against these threats requires a\nrigorous analysis of tenant behavior, adversarial strategies, and cloud\nprovider policies. In this paper, we develop a practical defense for IP address\nallocation through such an analysis. We first develop a statistical model of\ncloud tenant deployment behavior based on literature and measurement of\ndeployed systems. Through this, we analyze IP allocation policies under\nexisting and novel threat models. In response to our stronger proposed threat\nmodel, we design IP scan segmentation, an IP allocation policy that protects\nthe address pool against adversarial scanning even when an adversary is not\nlimited by number of cloud tenants. Through empirical evaluation on both\nsynthetic and real-world allocation traces, we show that IP scan segmentation\nreduces adversaries' ability to rapidly allocate addresses, protecting both\naddress space reputation and cloud tenant data. In this way, we show that\nprincipled analysis and implementation of cloud IP address allocation can lead\nto substantial security gains for tenants and their users."
                },
                "authors": [
                    {
                        "name": "Eric Pauley"
                    },
                    {
                        "name": "Kyle Domico"
                    },
                    {
                        "name": "Blaine Hoak"
                    },
                    {
                        "name": "Ryan Sheatsley"
                    },
                    {
                        "name": "Quinn Burke"
                    },
                    {
                        "name": "Yohan Beugin"
                    },
                    {
                        "name": "Engin Kirda"
                    },
                    {
                        "name": "Patrick McDaniel"
                    }
                ],
                "author_detail": {
                    "name": "Patrick McDaniel"
                },
                "arxiv_affiliation": "University of Wisconsin-Madison",
                "author": "Patrick McDaniel",
                "arxiv_comment": "Replaced with version to appear in 2025 Network and Distributed\n  Systems Security (NDSS) Symposium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.14999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.14999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06558v1",
                "updated": "2024-09-10T14:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    39,
                    4,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:39:04Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    39,
                    4,
                    1,
                    254,
                    0
                ],
                "title": "MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles\n  Through LLMs Penetrated Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles\n  Through LLMs Penetrated Science"
                },
                "summary": "As autonomous vehicles become more prevalent, highly accurate and efficient\nsystems are increasingly critical to improve safety, performance, and energy\nconsumption. Efficient management of energy-reliability tradeoffs in these\nsystems demands the ability to predict various conditions during vehicle\noperations. With the promising improvement of Large Language Models (LLMs) and\nthe emergence of well-known models like ChatGPT, unique opportunities for\nautonomous vehicle-related predictions have been provided in recent years. This\npaper proposed MAPS using LLMs as map reader co-drivers to predict the vital\nparameters to set during the autonomous vehicle operation to balance the\nenergy-reliability tradeoff. The MAPS method demonstrates a 20% improvement in\nnavigation accuracy compared to the best baseline method. MAPS also shows 11%\nenergy savings in computational units and up to 54% in both mechanical and\ncomputational units.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous vehicles become more prevalent, highly accurate and efficient\nsystems are increasingly critical to improve safety, performance, and energy\nconsumption. Efficient management of energy-reliability tradeoffs in these\nsystems demands the ability to predict various conditions during vehicle\noperations. With the promising improvement of Large Language Models (LLMs) and\nthe emergence of well-known models like ChatGPT, unique opportunities for\nautonomous vehicle-related predictions have been provided in recent years. This\npaper proposed MAPS using LLMs as map reader co-drivers to predict the vital\nparameters to set during the autonomous vehicle operation to balance the\nenergy-reliability tradeoff. The MAPS method demonstrates a 20% improvement in\nnavigation accuracy compared to the best baseline method. MAPS also shows 11%\nenergy savings in computational units and up to 54% in both mechanical and\ncomputational units."
                },
                "authors": [
                    {
                        "name": "Mahdieh Aliazam"
                    },
                    {
                        "name": "Ali Javadi"
                    },
                    {
                        "name": "Amir Mahdi Hosseini Monazzah"
                    },
                    {
                        "name": "Ahmad Akbari Azirani"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Akbari Azirani"
                },
                "author": "Ahmad Akbari Azirani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11346v2",
                "updated": "2024-09-10T14:26:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    26,
                    30,
                    1,
                    254,
                    0
                ],
                "published": "2024-06-17T09:08:30Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    8,
                    30,
                    0,
                    169,
                    0
                ],
                "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaDec: Decompiling WebAssembly Using Large Language Model"
                },
                "summary": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm.\n  In this paper, we introduce a novel approach, WaDec, which is the first use\nof a fine-tuned LLM to interpret and decompile Wasm binary code into a\nhigher-level, more comprehensible source code representation. The LLM was\nmeticulously fine-tuned using a specialized dataset of wat-c code snippets,\nemploying self-supervised learning techniques. This enables WaDec to\neffectively decompile not only complete wat functions but also finer-grained\nwat code snippets. Our experiments demonstrate that WaDec markedly outperforms\ncurrent state-of-the-art tools, offering substantial improvements across\nseveral metrics. It achieves a code inflation rate of only 3.34%, a dramatic\n97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines'\noutput that cannot be directly compiled or executed, WaDec maintains a\nrecompilability rate of 52.11%, a re-execution rate of 43.55%, and an output\nconsistency of 27.15%. Additionally, it significantly exceeds state-of-the-art\nperformance in AST edit distance by 185%, cyclomatic complexity by 8%, and\ncosine similarity by 41%, achieving an average code similarity above 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm.\n  In this paper, we introduce a novel approach, WaDec, which is the first use\nof a fine-tuned LLM to interpret and decompile Wasm binary code into a\nhigher-level, more comprehensible source code representation. The LLM was\nmeticulously fine-tuned using a specialized dataset of wat-c code snippets,\nemploying self-supervised learning techniques. This enables WaDec to\neffectively decompile not only complete wat functions but also finer-grained\nwat code snippets. Our experiments demonstrate that WaDec markedly outperforms\ncurrent state-of-the-art tools, offering substantial improvements across\nseveral metrics. It achieves a code inflation rate of only 3.34%, a dramatic\n97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines'\noutput that cannot be directly compiled or executed, WaDec maintains a\nrecompilability rate of 52.11%, a re-execution rate of 43.55%, and an output\nconsistency of 27.15%. Additionally, it significantly exceeds state-of-the-art\nperformance in AST edit distance by 185%, cyclomatic complexity by 8%, and\ncosine similarity by 41%, achieving an average code similarity above 50%."
                },
                "authors": [
                    {
                        "name": "Xinyu She"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "This paper was accepted by ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06540v1",
                "updated": "2024-09-10T14:15:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    15,
                    30,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:15:30Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    15,
                    30,
                    1,
                    254,
                    0
                ],
                "title": "Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings"
                },
                "summary": "Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure."
                },
                "authors": [
                    {
                        "name": "Jan Elfes"
                    }
                ],
                "author_detail": {
                    "name": "Jan Elfes"
                },
                "author": "Jan Elfes",
                "arxiv_comment": "19 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14467v2",
                "updated": "2024-09-10T14:08:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    8,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-19T17:14:16Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    14,
                    16,
                    4,
                    201,
                    0
                ],
                "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality"
                },
                "summary": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}"
                },
                "authors": [
                    {
                        "name": "Jayr Pereira"
                    },
                    {
                        "name": "Andre Assumpcao"
                    },
                    {
                        "name": "Roberto Lotufo"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Lotufo"
                },
                "author": "Roberto Lotufo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06518v1",
                "updated": "2024-09-10T13:54:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    54,
                    4,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T13:54:04Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    54,
                    4,
                    1,
                    254,
                    0
                ],
                "title": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games"
                },
                "summary": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs."
                },
                "authors": [
                    {
                        "name": "Juhwan Choi"
                    },
                    {
                        "name": "YoungBin Kim"
                    }
                ],
                "author_detail": {
                    "name": "YoungBin Kim"
                },
                "author": "YoungBin Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07606v2",
                "updated": "2024-09-10T13:39:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    39,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2023-09-14T11:13:36Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    11,
                    13,
                    36,
                    3,
                    257,
                    0
                ],
                "title": "Zero-shot Audio Topic Reranking using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Audio Topic Reranking using Large Language Models"
                },
                "summary": "Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them."
                },
                "authors": [
                    {
                        "name": "Mengjie Qian"
                    },
                    {
                        "name": "Rao Ma"
                    },
                    {
                        "name": "Adian Liusie"
                    },
                    {
                        "name": "Erfan Loweimi"
                    },
                    {
                        "name": "Kate M. Knill"
                    },
                    {
                        "name": "Mark J. F. Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark J. F. Gales"
                },
                "author": "Mark J. F. Gales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10671v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10671v4",
                "updated": "2024-09-10T13:25:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    25,
                    53,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-15T12:35:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    12,
                    35,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "Qwen2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2 Technical Report"
                },
                "summary": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors."
                },
                "authors": [
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Chengpeng Li"
                    },
                    {
                        "name": "Chengyuan Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Jialin Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Jianxin Ma"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Jinze Bai"
                    },
                    {
                        "name": "Jinzheng He"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Keqin Chen"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Na Ni"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Ru Peng"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Ruize Gao"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Sinan Tan"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Wenbin Ge"
                    },
                    {
                        "name": "Xiaodong Deng"
                    },
                    {
                        "name": "Xiaohuan Zhou"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Xipin Wei"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Xuejing Liu"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yunfei Chu"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Zhifang Guo"
                    },
                    {
                        "name": "Zhihao Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Fan"
                },
                "author": "Zhihao Fan",
                "arxiv_comment": "26 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10671v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10671v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05211v2",
                "updated": "2024-09-10T13:21:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    21,
                    8,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-09T17:59:49Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    59,
                    49,
                    4,
                    222,
                    0
                ],
                "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
                },
                "summary": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Shaoqi Dong"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Yunsheng Wu"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Project Page: https://vita-home.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05568v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05568v4",
                "updated": "2024-09-10T13:08:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    8,
                    49,
                    1,
                    254,
                    0
                ],
                "published": "2024-06-08T20:19:35Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    20,
                    19,
                    35,
                    5,
                    160,
                    0
                ],
                "title": "SAMM: Sharded Automated Market Maker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAMM: Sharded Automated Market Maker"
                },
                "summary": "Automated Market Makers (AMMs) are a cornerstone of decentralized finance.\nThey are smart contracts (stateful programs) running on blockchains. They\nenable virtual token exchange: Traders swap tokens with the AMM for a fee,\nwhile liquidity providers supply liquidity and earn these fees. Demand for AMMs\nis growing rapidly, but our experiment-based estimates show that current\narchitectures cannot meet the projected demand by 2029. This is because the\nexecution of existing AMMs is non-parallelizable.\n  We present SAMM, an AMM comprising multiple shards. All shards are AMMs\nrunning on the same chain, but their independence enables parallel execution.\nUnlike classical sharding solutions, here security relies on incentive\ncompatibility. Therefore, SAMM introduces a novel fee design. Through analysis\nof Subgame-Perfect Nash Equilibria (SPNE), we show that SAMM incentivizes the\ndesired behavior: Liquidity providers balance liquidity among all shards,\novercoming destabilization attacks, and trades are evenly distributed. We\nvalidate our game-theoretic analysis with a simulation using real-world data.\n  We evaluate SAMM by implementing and deploying it on local testnets of the\nSui and Solana blockchains. To our knowledge, this is the first quantification\nof ``hot-contract'' performance. SAMM improves throughput by 5x and 16x,\nrespectively, potentially more with better parallelization of the underlying\nblockchains. It is directly deployable, mitigating the upcoming scaling\nbottleneck.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Market Makers (AMMs) are a cornerstone of decentralized finance.\nThey are smart contracts (stateful programs) running on blockchains. They\nenable virtual token exchange: Traders swap tokens with the AMM for a fee,\nwhile liquidity providers supply liquidity and earn these fees. Demand for AMMs\nis growing rapidly, but our experiment-based estimates show that current\narchitectures cannot meet the projected demand by 2029. This is because the\nexecution of existing AMMs is non-parallelizable.\n  We present SAMM, an AMM comprising multiple shards. All shards are AMMs\nrunning on the same chain, but their independence enables parallel execution.\nUnlike classical sharding solutions, here security relies on incentive\ncompatibility. Therefore, SAMM introduces a novel fee design. Through analysis\nof Subgame-Perfect Nash Equilibria (SPNE), we show that SAMM incentivizes the\ndesired behavior: Liquidity providers balance liquidity among all shards,\novercoming destabilization attacks, and trades are evenly distributed. We\nvalidate our game-theoretic analysis with a simulation using real-world data.\n  We evaluate SAMM by implementing and deploying it on local testnets of the\nSui and Solana blockchains. To our knowledge, this is the first quantification\nof ``hot-contract'' performance. SAMM improves throughput by 5x and 16x,\nrespectively, potentially more with better parallelization of the underlying\nblockchains. It is directly deployable, mitigating the upcoming scaling\nbottleneck."
                },
                "authors": [
                    {
                        "name": "Hongyin Chen"
                    },
                    {
                        "name": "Amit Vaisman"
                    },
                    {
                        "name": "Ittay Eyal"
                    }
                ],
                "author_detail": {
                    "name": "Ittay Eyal"
                },
                "author": "Ittay Eyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05568v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05568v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06393v3",
                "updated": "2024-09-10T12:58:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    58,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-04-09T15:35:52Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    15,
                    35,
                    52,
                    1,
                    100,
                    0
                ],
                "title": "MuPT: A Generative Symbolic Music Pretrained Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuPT: A Generative Symbolic Music Pretrained Transformer"
                },
                "summary": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions."
                },
                "authors": [
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Ziya Zhou"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Lejun Min"
                    },
                    {
                        "name": "Xueling Liu"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Fengze Han"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00222v2",
                "updated": "2024-09-10T12:57:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    57,
                    19,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-30T19:26:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    26,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Can Large Language Models Address Open-Target Stance Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Address Open-Target Stance Detection?"
                },
                "summary": "Stance detection (SD) assesses a text's position towards a target, typically\nlabeled as \"favor,\" \"against,\" or \"neutral.\" We introduce Open-Target Stance\nDetection (OTSD), where targets are neither seen during training nor provided\nas input. Evaluating Large Language Models (LLMs) like GPT-3.5, GPT-4o, Llama\n3, and Mistral, we compare their performance with the Target-Stance Extraction\n(TSE) approach, which has the advantage of using predefined targets. LLMs\nperform better than TSE in target generation when the real target is explicitly\nand not explicitly mentioned in the text. For stance detection, LLMs perform\nbetter in explicit scenarios but fail in non-explicit ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection (SD) assesses a text's position towards a target, typically\nlabeled as \"favor,\" \"against,\" or \"neutral.\" We introduce Open-Target Stance\nDetection (OTSD), where targets are neither seen during training nor provided\nas input. Evaluating Large Language Models (LLMs) like GPT-3.5, GPT-4o, Llama\n3, and Mistral, we compare their performance with the Target-Stance Extraction\n(TSE) approach, which has the advantage of using predefined targets. LLMs\nperform better than TSE in target generation when the real target is explicitly\nand not explicitly mentioned in the text. For stance detection, LLMs perform\nbetter in explicit scenarios but fail in non-explicit ones."
                },
                "authors": [
                    {
                        "name": "Abu Ubaida Akash"
                    },
                    {
                        "name": "Ahmed Fahmy"
                    },
                    {
                        "name": "Amine Trabelsi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Trabelsi"
                },
                "author": "Amine Trabelsi",
                "arxiv_comment": "12 pages; currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06450v1",
                "updated": "2024-09-10T12:12:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    12,
                    9,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T12:12:09Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    12,
                    9,
                    1,
                    254,
                    0
                ],
                "title": "Multimodal Large Language Model Driven Scenario Testing for Autonomous\n  Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Model Driven Scenario Testing for Autonomous\n  Vehicles"
                },
                "summary": "The generation of corner cases has become increasingly crucial for\nefficiently testing autonomous vehicles prior to road deployment. However,\nexisting methods struggle to accommodate diverse testing requirements and often\nlack the ability to generalize to unseen situations, thereby reducing the\nconvenience and usability of the generated scenarios. A method that facilitates\neasily controllable scenario generation for efficient autonomous vehicles (AV)\ntesting with realistic and challenging situations is greatly needed. To address\nthis, we proposed OmniTester: a multimodal Large Language Model (LLM) based\nframework that fully leverages the extensive world knowledge and reasoning\ncapabilities of LLMs. OmniTester is designed to generate realistic and diverse\nscenarios within a simulation environment, offering a robust solution for\ntesting and evaluating AVs. In addition to prompt engineering, we employ tools\nfrom Simulation of Urban Mobility to simplify the complexity of codes generated\nby LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a\nself-improvement mechanism to enhance the LLM's understanding of scenarios,\nthereby increasing its ability to produce more realistic scenes. In the\nexperiments, we demonstrated the controllability and realism of our approaches\nin generating three types of challenging and complex scenarios. Additionally,\nwe showcased its effectiveness in reconstructing new scenarios described in\ncrash report, driven by the generalization capability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation of corner cases has become increasingly crucial for\nefficiently testing autonomous vehicles prior to road deployment. However,\nexisting methods struggle to accommodate diverse testing requirements and often\nlack the ability to generalize to unseen situations, thereby reducing the\nconvenience and usability of the generated scenarios. A method that facilitates\neasily controllable scenario generation for efficient autonomous vehicles (AV)\ntesting with realistic and challenging situations is greatly needed. To address\nthis, we proposed OmniTester: a multimodal Large Language Model (LLM) based\nframework that fully leverages the extensive world knowledge and reasoning\ncapabilities of LLMs. OmniTester is designed to generate realistic and diverse\nscenarios within a simulation environment, offering a robust solution for\ntesting and evaluating AVs. In addition to prompt engineering, we employ tools\nfrom Simulation of Urban Mobility to simplify the complexity of codes generated\nby LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a\nself-improvement mechanism to enhance the LLM's understanding of scenarios,\nthereby increasing its ability to produce more realistic scenes. In the\nexperiments, we demonstrated the controllability and realism of our approaches\nin generating three types of challenging and complex scenarios. Additionally,\nwe showcased its effectiveness in reconstructing new scenarios described in\ncrash report, driven by the generalization capability of LLMs."
                },
                "authors": [
                    {
                        "name": "Qiujing Lu"
                    },
                    {
                        "name": "Xuanhan Wang"
                    },
                    {
                        "name": "Yiwei Jiang"
                    },
                    {
                        "name": "Guangming Zhao"
                    },
                    {
                        "name": "Mingyue Ma"
                    },
                    {
                        "name": "Shuo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Feng"
                },
                "author": "Shuo Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06446v1",
                "updated": "2024-09-10T12:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    1,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T12:01:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    12,
                    1,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data"
                },
                "summary": "Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness."
                },
                "authors": [
                    {
                        "name": "Hossein Hajipour"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Thorsten Holz"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "24 pages, 16 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09654v2",
                "updated": "2024-09-10T11:58:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    58,
                    23,
                    1,
                    254,
                    0
                ],
                "published": "2024-04-15T10:42:22Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    10,
                    42,
                    22,
                    0,
                    106,
                    0
                ],
                "title": "Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection"
                },
                "summary": "Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Shaofeng Cai"
                    },
                    {
                        "name": "Fang Deng"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    },
                    {
                        "name": "Junran Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junran Wu"
                },
                "author": "Junran Wu",
                "arxiv_comment": "Accepted by MM'24 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06443v1",
                "updated": "2024-09-10T11:49:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    49,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T11:49:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    49,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Knowledge Distillation via Query Selection for Detection Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation via Query Selection for Detection Transformer"
                },
                "summary": "Transformers have revolutionized the object detection landscape by\nintroducing DETRs, acclaimed for their simplicity and efficacy. Despite their\nadvantages, the substantial size of these models poses significant challenges\nfor practical deployment, particularly in resource-constrained environments.\nThis paper addresses the challenge of compressing DETR by leveraging knowledge\ndistillation, a technique that holds promise for maintaining model performance\nwhile reducing size. A critical aspect of DETRs' performance is their reliance\non queries to interpret object representations accurately. Traditional\ndistillation methods often focus exclusively on positive queries, identified\nthrough bipartite matching, neglecting the rich information present in\nhard-negative queries. Our visual analysis indicates that hard-negative\nqueries, focusing on foreground elements, are crucial for enhancing\ndistillation outcomes. To this end, we introduce a novel Group Query Selection\nstrategy, which diverges from traditional query selection in DETR distillation\nby segmenting queries based on their Generalized Intersection over Union (GIoU)\nwith ground truth objects, thereby uncovering valuable hard-negative queries\nfor distillation. Furthermore, we present the Knowledge Distillation via Query\nSelection for DETR (QSKD) framework, which incorporates Attention-Guided\nFeature Distillation (AGFD) and Local Alignment Prediction Distillation (LAPD).\nThese components optimize the distillation process by focusing on the most\ninformative aspects of the teacher model's intermediate features and output.\nOur comprehensive experimental evaluation of the MS-COCO dataset demonstrates\nthe effectiveness of our approach, significantly improving average precision\n(AP) across various DETR architectures without incurring substantial\ncomputational costs. Specifically, the AP of Conditional DETR ResNet-18\nincreased from 35.8 to 39.9.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have revolutionized the object detection landscape by\nintroducing DETRs, acclaimed for their simplicity and efficacy. Despite their\nadvantages, the substantial size of these models poses significant challenges\nfor practical deployment, particularly in resource-constrained environments.\nThis paper addresses the challenge of compressing DETR by leveraging knowledge\ndistillation, a technique that holds promise for maintaining model performance\nwhile reducing size. A critical aspect of DETRs' performance is their reliance\non queries to interpret object representations accurately. Traditional\ndistillation methods often focus exclusively on positive queries, identified\nthrough bipartite matching, neglecting the rich information present in\nhard-negative queries. Our visual analysis indicates that hard-negative\nqueries, focusing on foreground elements, are crucial for enhancing\ndistillation outcomes. To this end, we introduce a novel Group Query Selection\nstrategy, which diverges from traditional query selection in DETR distillation\nby segmenting queries based on their Generalized Intersection over Union (GIoU)\nwith ground truth objects, thereby uncovering valuable hard-negative queries\nfor distillation. Furthermore, we present the Knowledge Distillation via Query\nSelection for DETR (QSKD) framework, which incorporates Attention-Guided\nFeature Distillation (AGFD) and Local Alignment Prediction Distillation (LAPD).\nThese components optimize the distillation process by focusing on the most\ninformative aspects of the teacher model's intermediate features and output.\nOur comprehensive experimental evaluation of the MS-COCO dataset demonstrates\nthe effectiveness of our approach, significantly improving average precision\n(AP) across various DETR architectures without incurring substantial\ncomputational costs. Specifically, the AP of Conditional DETR ResNet-18\nincreased from 35.8 to 39.9."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Luting Wang"
                    },
                    {
                        "name": "Zongheng Tang"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Lijun Zhang"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06442v1",
                "updated": "2024-09-10T11:48:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T11:48:05Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "title": "Prompt2Fashion: An automatically generated fashion dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt2Fashion: An automatically generated fashion dataset"
                },
                "summary": "Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion."
                },
                "authors": [
                    {
                        "name": "Georgia Argyro"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04775v2",
                "updated": "2024-09-10T11:43:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    43,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-07T09:30:26Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    9,
                    30,
                    26,
                    5,
                    251,
                    0
                ],
                "title": "Leveraging LLMs, Graphs and Object Hierarchies for Task Planning in\n  Large-Scale Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs, Graphs and Object Hierarchies for Task Planning in\n  Large-Scale Environments"
                },
                "summary": "Planning methods struggle with computational intractability in solving\ntask-level problems in large-scale environments. This work explores leveraging\nthe commonsense knowledge encoded in LLMs to empower planning techniques to\ndeal with these complex scenarios. We achieve this by efficiently using LLMs to\nprune irrelevant components from the planning problem's state space,\nsubstantially simplifying its complexity. We demonstrate the efficacy of this\nsystem through extensive experiments within a household simulation environment,\nalongside real-world validation using a 7-DoF manipulator (video\nhttps://youtu.be/6ro2UOtOQS4).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning methods struggle with computational intractability in solving\ntask-level problems in large-scale environments. This work explores leveraging\nthe commonsense knowledge encoded in LLMs to empower planning techniques to\ndeal with these complex scenarios. We achieve this by efficiently using LLMs to\nprune irrelevant components from the planning problem's state space,\nsubstantially simplifying its complexity. We demonstrate the efficacy of this\nsystem through extensive experiments within a household simulation environment,\nalongside real-world validation using a 7-DoF manipulator (video\nhttps://youtu.be/6ro2UOtOQS4)."
                },
                "authors": [
                    {
                        "name": "Rodrigo Pérez-Dattari"
                    },
                    {
                        "name": "Zhaoting Li"
                    },
                    {
                        "name": "Robert Babuška"
                    },
                    {
                        "name": "Jens Kober"
                    },
                    {
                        "name": "Cosimo Della Santina"
                    }
                ],
                "author_detail": {
                    "name": "Cosimo Della Santina"
                },
                "author": "Cosimo Della Santina",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06433v1",
                "updated": "2024-09-10T11:31:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    31,
                    2,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T11:31:02Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    31,
                    2,
                    1,
                    254,
                    0
                ],
                "title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for\n  Scholarly Knowledge Organization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for\n  Scholarly Knowledge Organization"
                },
                "summary": "The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public."
                },
                "authors": [
                    {
                        "name": "Gollam Rabby"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Allard Oelen"
                    }
                ],
                "author_detail": {
                    "name": "Allard Oelen"
                },
                "author": "Allard Oelen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06416v1",
                "updated": "2024-09-10T10:55:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    55,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T10:55:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    55,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "Exploring the Integration of Large Language Models in Industrial Test\n  Maintenance Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Large Language Models in Industrial Test\n  Maintenance Processes"
                },
                "summary": "Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplored the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also proposed and demonstrated\nimplementations of two multi-agent architectures that can predict which test\ncases require maintenance following a change to the source code. Collectively,\nthese contributions advance our theoretical and practical understanding of how\nLLMs can be deployed to benefit industrial test maintenance processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplored the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also proposed and demonstrated\nimplementations of two multi-agent architectures that can predict which test\ncases require maintenance following a change to the source code. Collectively,\nthese contributions advance our theoretical and practical understanding of how\nLLMs can be deployed to benefit industrial test maintenance processes."
                },
                "authors": [
                    {
                        "name": "Ludvig Lemner"
                    },
                    {
                        "name": "Linnea Wahlgren"
                    },
                    {
                        "name": "Gregory Gay"
                    },
                    {
                        "name": "Nasser Mohammadiha"
                    },
                    {
                        "name": "Jingxiong Liu"
                    },
                    {
                        "name": "Joakim Wennerberg"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Wennerberg"
                },
                "author": "Joakim Wennerberg",
                "arxiv_comment": "Under submission to ACM TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06411v1",
                "updated": "2024-09-10T10:49:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    49,
                    38,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T10:49:38Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    49,
                    38,
                    1,
                    254,
                    0
                ],
                "title": "Length Desensitization in Directed Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length Desensitization in Directed Preference Optimization"
                },
                "summary": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "arxiv_comment": "21 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06377v1",
                "updated": "2024-09-10T09:58:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    58,
                    55,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T09:58:55Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    58,
                    55,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration"
                },
                "summary": "Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec."
                },
                "authors": [
                    {
                        "name": "Weicong Qin"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "arxiv_comment": "First 3 authors contributes equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03454v2",
                "updated": "2024-09-10T09:22:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    22,
                    26,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-05T12:06:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes"
                },
                "summary": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains."
                },
                "authors": [
                    {
                        "name": "Inacio Vieira"
                    },
                    {
                        "name": "Will Allred"
                    },
                    {
                        "name": "Séamus Lankford"
                    },
                    {
                        "name": "Sheila Castilho"
                    },
                    {
                        "name": "Andy Way"
                    }
                ],
                "author_detail": {
                    "name": "Andy Way"
                },
                "author": "Andy Way",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06351v1",
                "updated": "2024-09-10T09:10:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    10,
                    30,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T09:10:30Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    10,
                    30,
                    1,
                    254,
                    0
                ],
                "title": "MAGDA: Multi-agent guideline-driven diagnostic assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGDA: Multi-agent guideline-driven diagnostic assistance"
                },
                "summary": "In emergency departments, rural hospitals, or clinics in less developed\nregions, clinicians often lack fast image analysis by trained radiologists,\nwhich can have a detrimental effect on patients' healthcare. Large Language\nModels (LLMs) have the potential to alleviate some pressure from these\nclinicians by providing insights that can help them in their decision-making.\nWhile these LLMs achieve high test results on medical exams showcasing their\ngreat theoretical medical knowledge, they tend not to follow medical\nguidelines. In this work, we introduce a new approach for zero-shot\nguideline-driven decision support. We model a system of multiple LLM agents\naugmented with a contrastive vision-language model that collaborate to reach a\npatient diagnosis. After providing the agents with simple diagnostic\nguidelines, they will synthesize prompts and screen the image for findings\nfollowing these guidelines. Finally, they provide understandable\nchain-of-thought reasoning for their diagnosis, which is then self-refined to\nconsider inter-dependencies between diseases. As our method is zero-shot, it is\nadaptable to settings with rare diseases, where training data is limited, but\nexpert-crafted disease descriptions are available. We evaluate our method on\ntwo chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing\nperformance improvement over existing zero-shot methods and generalizability to\nrare diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In emergency departments, rural hospitals, or clinics in less developed\nregions, clinicians often lack fast image analysis by trained radiologists,\nwhich can have a detrimental effect on patients' healthcare. Large Language\nModels (LLMs) have the potential to alleviate some pressure from these\nclinicians by providing insights that can help them in their decision-making.\nWhile these LLMs achieve high test results on medical exams showcasing their\ngreat theoretical medical knowledge, they tend not to follow medical\nguidelines. In this work, we introduce a new approach for zero-shot\nguideline-driven decision support. We model a system of multiple LLM agents\naugmented with a contrastive vision-language model that collaborate to reach a\npatient diagnosis. After providing the agents with simple diagnostic\nguidelines, they will synthesize prompts and screen the image for findings\nfollowing these guidelines. Finally, they provide understandable\nchain-of-thought reasoning for their diagnosis, which is then self-refined to\nconsider inter-dependencies between diseases. As our method is zero-shot, it is\nadaptable to settings with rare diseases, where training data is limited, but\nexpert-crafted disease descriptions are available. We evaluate our method on\ntwo chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing\nperformance improvement over existing zero-shot methods and generalizability to\nrare diseases."
                },
                "authors": [
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Matthias Keicher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Keicher"
                },
                "author": "Matthias Keicher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06338v1",
                "updated": "2024-09-10T08:48:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:48:05Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "title": "Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long\n  Context Evaluation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long\n  Context Evaluation Tasks"
                },
                "summary": "We argue that there are two major distinct capabilities in long context\nunderstanding: retrieval and holistic understanding. Understanding and further\nimproving LLMs' long context capabilities would not be possible without knowing\nthe tasks' focus categories. We aim to automatically identify retrieval focused\nand holistic understanding focused problems from suites of benchmarks and\nquantitatively measure the difficulty within each focus. In this paper, we\npresent the Dolce framework, which parameterizes each problem by $\\lambda$\n(complexity) and $k$ (redundancy) and assigns to one of five predefined focus\ncategories. We propose to sample short contexts from the full context and\nestimate the probability an LLM solves the problem using the sampled spans. To\nfind the $\\lambda$ and $k$ for each problem, we further propose a mixture model\nof a non-parametric background noise component and a parametric/non-parametric\nhybrid oracle component, where we derive the probability functions\nparameterized by $\\lambda$ and $k$ for both the correct-or-wrong (COW) scenario\nand the partial-point-in-grading (PIG) scenario. Our proposed methods can\nidentify 0% to 67% of the problems are retrieval focused and 0% to 90% of the\nproblems are holistic understanding focused across 44 existing long context\nevaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that there are two major distinct capabilities in long context\nunderstanding: retrieval and holistic understanding. Understanding and further\nimproving LLMs' long context capabilities would not be possible without knowing\nthe tasks' focus categories. We aim to automatically identify retrieval focused\nand holistic understanding focused problems from suites of benchmarks and\nquantitatively measure the difficulty within each focus. In this paper, we\npresent the Dolce framework, which parameterizes each problem by $\\lambda$\n(complexity) and $k$ (redundancy) and assigns to one of five predefined focus\ncategories. We propose to sample short contexts from the full context and\nestimate the probability an LLM solves the problem using the sampled spans. To\nfind the $\\lambda$ and $k$ for each problem, we further propose a mixture model\nof a non-parametric background noise component and a parametric/non-parametric\nhybrid oracle component, where we derive the probability functions\nparameterized by $\\lambda$ and $k$ for both the correct-or-wrong (COW) scenario\nand the partial-point-in-grading (PIG) scenario. Our proposed methods can\nidentify 0% to 67% of the problems are retrieval focused and 0% to 90% of the\nproblems are holistic understanding focused across 44 existing long context\nevaluation tasks."
                },
                "authors": [
                    {
                        "name": "Zi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zi Yang"
                },
                "author": "Zi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06336v1",
                "updated": "2024-09-10T08:47:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    47,
                    23,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:47:23Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    47,
                    23,
                    1,
                    254,
                    0
                ],
                "title": "Towards Agentic AI on Particle Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic AI on Particle Accelerators"
                },
                "summary": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show two examples, where we demonstrate viability of such\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show two examples, where we demonstrate viability of such\narchitecture."
                },
                "authors": [
                    {
                        "name": "Antonin Sulc"
                    },
                    {
                        "name": "Thorsten Hellert"
                    },
                    {
                        "name": "Raimund Kammering"
                    },
                    {
                        "name": "Hayden Houscher"
                    },
                    {
                        "name": "Jason St. John"
                    }
                ],
                "author_detail": {
                    "name": "Jason St. John"
                },
                "author": "Jason St. John",
                "arxiv_comment": "4 pages, 3 figures, Machine Learning and the Physical Sciences at\n  Workshop at the 38th conference on Neural Information Processing Systems\n  (NeurIPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06328v1",
                "updated": "2024-09-10T08:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    33,
                    31,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    33,
                    31,
                    1,
                    254,
                    0
                ],
                "title": "Extracting Paragraphs from LLM Token Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Paragraphs from LLM Token Activations"
                },
                "summary": "Generative large language models (LLMs) excel in natural language processing\ntasks, yet their inner workings remain underexplored beyond token-level\npredictions. This study investigates the degree to which these models decide\nthe content of a paragraph at its onset, shedding light on their contextual\nunderstanding. By examining the information encoded in single-token\nactivations, specifically the \"\\textbackslash n\\textbackslash n\" double newline\ntoken, we demonstrate that patching these activations can transfer significant\ninformation about the context of the following paragraph, providing further\ninsights into the model's capacity to plan ahead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) excel in natural language processing\ntasks, yet their inner workings remain underexplored beyond token-level\npredictions. This study investigates the degree to which these models decide\nthe content of a paragraph at its onset, shedding light on their contextual\nunderstanding. By examining the information encoded in single-token\nactivations, specifically the \"\\textbackslash n\\textbackslash n\" double newline\ntoken, we demonstrate that patching these activations can transfer significant\ninformation about the context of the following paragraph, providing further\ninsights into the model's capacity to plan ahead."
                },
                "authors": [
                    {
                        "name": "Nicholas Pochinkov"
                    },
                    {
                        "name": "Angelo Benoit"
                    },
                    {
                        "name": "Lovkush Agarwal"
                    },
                    {
                        "name": "Zainab Ali Majid"
                    },
                    {
                        "name": "Lucile Ter-Minassian"
                    }
                ],
                "author_detail": {
                    "name": "Lucile Ter-Minassian"
                },
                "author": "Lucile Ter-Minassian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06327v1",
                "updated": "2024-09-10T08:32:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    32,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T08:32:29Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    32,
                    29,
                    1,
                    254,
                    0
                ],
                "title": "Spoofing-Aware Speaker Verification Robust Against Domain and Channel\n  Mismatches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoofing-Aware Speaker Verification Robust Against Domain and Channel\n  Mismatches"
                },
                "summary": "In real-world applications, it is challenging to build a speaker verification\nsystem that is simultaneously robust against common threats, including spoofing\nattacks, channel mismatch, and domain mismatch. Traditional automatic speaker\nverification (ASV) systems often tackle these issues separately, leading to\nsuboptimal performance when faced with simultaneous challenges. In this paper,\nwe propose an integrated framework that incorporates pair-wise learning and\nspoofing attack simulation into the meta-learning paradigm to enhance\nrobustness against these multifaceted threats. This novel approach employs an\nasymmetric dual-path model and a multi-task learning strategy to handle ASV,\nanti-spoofing, and spoofing-aware ASV tasks concurrently. A new testing\ndataset, CNComplex, is introduced to evaluate system performance under these\ncombined threats. Experimental results demonstrate that our integrated model\nsignificantly improves performance over traditional ASV systems across various\nscenarios, showcasing its potential for real-world deployment. Additionally,\nthe proposed framework's ability to generalize across different conditions\nhighlights its robustness and reliability, making it a promising solution for\npractical ASV applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world applications, it is challenging to build a speaker verification\nsystem that is simultaneously robust against common threats, including spoofing\nattacks, channel mismatch, and domain mismatch. Traditional automatic speaker\nverification (ASV) systems often tackle these issues separately, leading to\nsuboptimal performance when faced with simultaneous challenges. In this paper,\nwe propose an integrated framework that incorporates pair-wise learning and\nspoofing attack simulation into the meta-learning paradigm to enhance\nrobustness against these multifaceted threats. This novel approach employs an\nasymmetric dual-path model and a multi-task learning strategy to handle ASV,\nanti-spoofing, and spoofing-aware ASV tasks concurrently. A new testing\ndataset, CNComplex, is introduced to evaluate system performance under these\ncombined threats. Experimental results demonstrate that our integrated model\nsignificantly improves performance over traditional ASV systems across various\nscenarios, showcasing its potential for real-world deployment. Additionally,\nthe proposed framework's ability to generalize across different conditions\nhighlights its robustness and reliability, making it a promising solution for\npractical ASV applications."
                },
                "authors": [
                    {
                        "name": "Chang Zeng"
                    },
                    {
                        "name": "Xiaoxiao Miao"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Erica Cooper"
                    },
                    {
                        "name": "Junichi Yamagishi"
                    }
                ],
                "author_detail": {
                    "name": "Junichi Yamagishi"
                },
                "author": "Junichi Yamagishi",
                "arxiv_comment": "To appear in 2024 IEEE Spoken Language Technology Workshop, Dec\n  02-05, 2024, Macao, China",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20684v2",
                "updated": "2024-09-10T08:19:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    19,
                    8,
                    1,
                    254,
                    0
                ],
                "published": "2024-05-31T08:26:47Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    8,
                    26,
                    47,
                    4,
                    152,
                    0
                ],
                "title": "Joint Embeddings for Graph Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Embeddings for Graph Instruction Tuning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance in text\nunderstanding and have become an essential tool for building smart assistants.\nOriginally focusing on text, they have been enhanced with multimodal\ncapabilities in recent works that successfully built visual instruction\nfollowing assistants. As far as the graph modality goes, however, no such\nassistants have yet been developed. Graph structures are complex in that they\nrepresent relation between different features and are permutation invariant.\nMoreover, representing them in purely textual form does not always lead to good\nLLM performance even for finetuned models. As a result, there is a need to\ndevelop a new method to integrate graphs in LLMs for general graph\nunderstanding. This work explores the integration of the graph modality in LLM\nfor general graph instruction following tasks. It aims at producing a deep\nlearning model that enhances an underlying LLM with graph embeddings and trains\nit to understand them and to produce, given an instruction, an answer grounded\nin the graph representation. The approach performs significantly better than a\ngraph to text approach and remains consistent even for larger graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance in text\nunderstanding and have become an essential tool for building smart assistants.\nOriginally focusing on text, they have been enhanced with multimodal\ncapabilities in recent works that successfully built visual instruction\nfollowing assistants. As far as the graph modality goes, however, no such\nassistants have yet been developed. Graph structures are complex in that they\nrepresent relation between different features and are permutation invariant.\nMoreover, representing them in purely textual form does not always lead to good\nLLM performance even for finetuned models. As a result, there is a need to\ndevelop a new method to integrate graphs in LLMs for general graph\nunderstanding. This work explores the integration of the graph modality in LLM\nfor general graph instruction following tasks. It aims at producing a deep\nlearning model that enhances an underlying LLM with graph embeddings and trains\nit to understand them and to produce, given an instruction, an answer grounded\nin the graph representation. The approach performs significantly better than a\ngraph to text approach and remains consistent even for larger graphs."
                },
                "authors": [
                    {
                        "name": "Aaron Haag"
                    },
                    {
                        "name": "Vlad Argatu"
                    },
                    {
                        "name": "Oliver Lohse"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lohse"
                },
                "author": "Oliver Lohse",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13375v2",
                "updated": "2024-09-10T08:08:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    8,
                    40,
                    1,
                    254,
                    0
                ],
                "published": "2024-06-19T09:16:14Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    9,
                    16,
                    14,
                    2,
                    171,
                    0
                ],
                "title": "ALiiCE: Evaluating Positional Fine-grained Citation Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALiiCE: Evaluating Positional Fine-grained Citation Generation"
                },
                "summary": "Large Language Models (LLMs) can enhance the credibility and verifiability by\ngenerating text with citations. However, existing tasks and evaluation methods\nare predominantly limited to sentence-level statement, neglecting the\nsignificance of positional fine-grained citations that can appear anywhere\nwithin sentences. To facilitate further exploration of the fine-grained\ncitation generation, we propose ALiiCE, the first automatic evaluation\nframework for this task. Our framework first parses the sentence claim into\natomic claims via dependency analysis and then calculates citation quality at\nthe atomic claim level. ALiiCE introduces three novel metrics for positional\nfined-grained citation quality assessment, including positional fine-grained\ncitation recall and precision, and coefficient of variation of citation\npositions. We evaluate the positional fine-grained citation generation\nperformance of several LLMs on two long-form QA datasets. Our experiments and\nanalyses demonstrate the effectiveness and reasonableness of ALiiCE. The\nresults also indicate that existing LLMs still struggle to provide positional\nfine-grained citations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance the credibility and verifiability by\ngenerating text with citations. However, existing tasks and evaluation methods\nare predominantly limited to sentence-level statement, neglecting the\nsignificance of positional fine-grained citations that can appear anywhere\nwithin sentences. To facilitate further exploration of the fine-grained\ncitation generation, we propose ALiiCE, the first automatic evaluation\nframework for this task. Our framework first parses the sentence claim into\natomic claims via dependency analysis and then calculates citation quality at\nthe atomic claim level. ALiiCE introduces three novel metrics for positional\nfined-grained citation quality assessment, including positional fine-grained\ncitation recall and precision, and coefficient of variation of citation\npositions. We evaluate the positional fine-grained citation generation\nperformance of several LLMs on two long-form QA datasets. Our experiments and\nanalyses demonstrate the effectiveness and reasonableness of ALiiCE. The\nresults also indicate that existing LLMs still struggle to provide positional\nfine-grained citations."
                },
                "authors": [
                    {
                        "name": "Yilong Xu"
                    },
                    {
                        "name": "Jinhua Gao"
                    },
                    {
                        "name": "Xiaoming Yu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06299v1",
                "updated": "2024-09-10T07:53:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    53,
                    10,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:53:10Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    53,
                    10,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory"
                },
                "summary": "Recently, integrating visual foundation models into large language models\n(LLMs) to form video understanding systems has attracted widespread attention.\nMost of the existing models compress diverse semantic information within the\nwhole video and feed it into LLMs for content comprehension. While this method\nexcels in short video understanding, it may result in a blend of multiple event\ninformation in long videos due to coarse compression, which causes information\nredundancy. Consequently, the semantics of key events might be obscured within\nthe vast information that hinders the model's understanding capabilities. To\naddress this issue, we propose a Hierarchical Event-based Memory-enhanced LLM\n(HEM-LLM) for better understanding of long videos. Firstly, we design a novel\nadaptive sequence segmentation scheme to divide multiple events within long\nvideos. In this way, we can perform individual memory modeling for each event\nto establish intra-event contextual connections, thereby reducing information\nredundancy. Secondly, while modeling current event, we compress and inject the\ninformation of the previous event to enhance the long-term inter-event\ndependencies in videos. Finally, we perform extensive experiments on various\nvideo understanding tasks and the results show that our model achieves\nstate-of-the-art performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, integrating visual foundation models into large language models\n(LLMs) to form video understanding systems has attracted widespread attention.\nMost of the existing models compress diverse semantic information within the\nwhole video and feed it into LLMs for content comprehension. While this method\nexcels in short video understanding, it may result in a blend of multiple event\ninformation in long videos due to coarse compression, which causes information\nredundancy. Consequently, the semantics of key events might be obscured within\nthe vast information that hinders the model's understanding capabilities. To\naddress this issue, we propose a Hierarchical Event-based Memory-enhanced LLM\n(HEM-LLM) for better understanding of long videos. Firstly, we design a novel\nadaptive sequence segmentation scheme to divide multiple events within long\nvideos. In this way, we can perform individual memory modeling for each event\nto establish intra-event contextual connections, thereby reducing information\nredundancy. Secondly, while modeling current event, we compress and inject the\ninformation of the previous event to enhance the long-term inter-event\ndependencies in videos. Finally, we perform extensive experiments on various\nvideo understanding tasks and the results show that our model achieves\nstate-of-the-art performances."
                },
                "authors": [
                    {
                        "name": "Dingxin Cheng"
                    },
                    {
                        "name": "Mingda Li"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Yongxin Guo"
                    },
                    {
                        "name": "Bin Jiang"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06297v1",
                "updated": "2024-09-10T07:51:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    51,
                    53,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:51:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    51,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "User Preferences for Large Language Model versus Template-Based\n  Explanations of Movie Recommendations: A Pilot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Preferences for Large Language Model versus Template-Based\n  Explanations of Movie Recommendations: A Pilot Study"
                },
                "summary": "Recommender systems have become integral to our digital experiences, from\nonline shopping to streaming platforms. Still, the rationale behind their\nsuggestions often remains opaque to users. While some systems employ a\ngraph-based approach, offering inherent explainability through paths\nassociating recommended items and seed items, non-experts could not easily\nunderstand these explanations. A popular alternative is to convert graph-based\nexplanations into textual ones using a template and an algorithm, which we\ndenote here as ''template-based'' explanations. Yet, these can sometimes come\nacross as impersonal or uninspiring. A novel method would be to employ large\nlanguage models (LLMs) for this purpose, which we denote as ''LLM-based''. To\nassess the effectiveness of LLMs in generating more resonant explanations, we\nconducted a pilot study with 25 participants. They were presented with three\nexplanations: (1) traditional template-based, (2) LLM-based rephrasing of the\ntemplate output, and (3) purely LLM-based explanations derived from the\ngraph-based explanations. Although subject to high variance, preliminary\nfindings suggest that LLM-based explanations may provide a richer and more\nengaging user experience, further aligning with user expectations. This study\nsheds light on the potential limitations of current explanation methods and\noffers promising directions for leveraging large language models to improve\nuser satisfaction and trust in recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have become integral to our digital experiences, from\nonline shopping to streaming platforms. Still, the rationale behind their\nsuggestions often remains opaque to users. While some systems employ a\ngraph-based approach, offering inherent explainability through paths\nassociating recommended items and seed items, non-experts could not easily\nunderstand these explanations. A popular alternative is to convert graph-based\nexplanations into textual ones using a template and an algorithm, which we\ndenote here as ''template-based'' explanations. Yet, these can sometimes come\nacross as impersonal or uninspiring. A novel method would be to employ large\nlanguage models (LLMs) for this purpose, which we denote as ''LLM-based''. To\nassess the effectiveness of LLMs in generating more resonant explanations, we\nconducted a pilot study with 25 participants. They were presented with three\nexplanations: (1) traditional template-based, (2) LLM-based rephrasing of the\ntemplate output, and (3) purely LLM-based explanations derived from the\ngraph-based explanations. Although subject to high variance, preliminary\nfindings suggest that LLM-based explanations may provide a richer and more\nengaging user experience, further aligning with user expectations. This study\nsheds light on the potential limitations of current explanation methods and\noffers promising directions for leveraging large language models to improve\nuser satisfaction and trust in recommender systems."
                },
                "authors": [
                    {
                        "name": "Julien Albert"
                    },
                    {
                        "name": "Martin Balfroid"
                    },
                    {
                        "name": "Miriam Doh"
                    },
                    {
                        "name": "Jeremie Bogaert"
                    },
                    {
                        "name": "Luca La Fisca"
                    },
                    {
                        "name": "Liesbet De Vos"
                    },
                    {
                        "name": "Bryan Renard"
                    },
                    {
                        "name": "Vincent Stragier"
                    },
                    {
                        "name": "Emmanuel Jean"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Jean"
                },
                "author": "Emmanuel Jean",
                "arxiv_comment": "Presented to the Dutch-Belgian Workshop on Recommender Systems 2023\n  (14-15 December, 2023 - Antwerp, Belgium)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02897v3",
                "updated": "2024-09-10T07:43:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    43,
                    19,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-04T17:41:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    41,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA"
                },
                "summary": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Wanjun Gu"
                    },
                    {
                        "name": "Danqing Liu"
                    },
                    {
                        "name": "Minhao Zou"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06289v1",
                "updated": "2024-09-10T07:42:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    42,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T07:42:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    42,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Automate Strategy Finding with LLM in Quant investment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automate Strategy Finding with LLM in Quant investment"
                },
                "summary": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets."
                },
                "authors": [
                    {
                        "name": "Zhizhuo Kou"
                    },
                    {
                        "name": "Holam Yu"
                    },
                    {
                        "name": "Jingshu Peng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11569v2",
                "updated": "2024-09-10T07:34:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    34,
                    50,
                    1,
                    254,
                    0
                ],
                "published": "2024-06-17T14:06:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    6,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs"
                },
                "summary": "For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory."
                },
                "authors": [
                    {
                        "name": "Haifeng Wen"
                    },
                    {
                        "name": "Hong Xing"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "38 pages, 7 figures, submitted for possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06277v2",
                "updated": "2024-09-11T01:47:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    47,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-10T07:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    28,
                    13,
                    1,
                    254,
                    0
                ],
                "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret."
                },
                "authors": [
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02443v2",
                "updated": "2024-09-10T06:59:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    59,
                    37,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-04T04:41:15Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    41,
                    15,
                    2,
                    248,
                    0
                ],
                "title": "Exploring the applicability of Large Language Models to citation context\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the applicability of Large Language Models to citation context\n  analysis"
                },
                "summary": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses."
                },
                "authors": [
                    {
                        "name": "Kai Nishikawa"
                    },
                    {
                        "name": "Hitoshi Koshiba"
                    }
                ],
                "author_detail": {
                    "name": "Hitoshi Koshiba"
                },
                "author": "Hitoshi Koshiba",
                "arxiv_doi": "10.1007/s11192-024-05142-9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11192-024-05142-9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06241v1",
                "updated": "2024-09-10T06:17:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    17,
                    27,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T06:17:27Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    17,
                    27,
                    1,
                    254,
                    0
                ],
                "title": "DiPT: Enhancing LLM reasoning through diversified perspective-taking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiPT: Enhancing LLM reasoning through diversified perspective-taking"
                },
                "summary": "Existing work on improving language model reasoning typically explores a\nsingle solution path, which can be prone to errors. Inspired by\nperspective-taking in social studies, this paper introduces DiPT, a novel\napproach that complements current reasoning methods by explicitly incorporating\ndiversified viewpoints. This approach allows the model to gain a deeper\nunderstanding of the problem's context and identify the most effective solution\npath during the inference stage. Additionally, it provides a general\ndata-centric AI recipe for augmenting existing data to improve their quality\nfor fine-tuning.\n  Our empirical results demonstrate that DiPT can be flexibly integrated into\nexisting methods that focus on a single reasoning approach, enhancing their\nreasoning performance and stability when presented with paraphrased problems.\nFurthermore, we illustrate improved context understanding by maintaining the\nmodel's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning\nwith data enriched with diverse perspectives can boost the reasoning\ncapabilities of the model compared to fine-tuning with raw data alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on improving language model reasoning typically explores a\nsingle solution path, which can be prone to errors. Inspired by\nperspective-taking in social studies, this paper introduces DiPT, a novel\napproach that complements current reasoning methods by explicitly incorporating\ndiversified viewpoints. This approach allows the model to gain a deeper\nunderstanding of the problem's context and identify the most effective solution\npath during the inference stage. Additionally, it provides a general\ndata-centric AI recipe for augmenting existing data to improve their quality\nfor fine-tuning.\n  Our empirical results demonstrate that DiPT can be flexibly integrated into\nexisting methods that focus on a single reasoning approach, enhancing their\nreasoning performance and stability when presented with paraphrased problems.\nFurthermore, we illustrate improved context understanding by maintaining the\nmodel's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning\nwith data enriched with diverse perspectives can boost the reasoning\ncapabilities of the model compared to fine-tuning with raw data alone."
                },
                "authors": [
                    {
                        "name": "Hoang Anh Just"
                    },
                    {
                        "name": "Mahavir Dabas"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Ming Jin"
                    },
                    {
                        "name": "Ruoxi Jia"
                    }
                ],
                "author_detail": {
                    "name": "Ruoxi Jia"
                },
                "author": "Ruoxi Jia",
                "arxiv_comment": "LLM Reasoning with Perspectives, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05385v2",
                "updated": "2024-09-10T06:11:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    11,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T07:32:30Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    32,
                    30,
                    0,
                    253,
                    0
                ],
                "title": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models"
                },
                "summary": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability."
                },
                "authors": [
                    {
                        "name": "Hong Xingyun Hong"
                    },
                    {
                        "name": "Shao Yan Shao"
                    },
                    {
                        "name": "Wang Zhilin Wang"
                    },
                    {
                        "name": "Duan Manni Duan"
                    },
                    {
                        "name": "Jin Xiongnan"
                    }
                ],
                "author_detail": {
                    "name": "Jin Xiongnan"
                },
                "author": "Jin Xiongnan",
                "arxiv_comment": "This paper has been accepted by NLPCC-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01639v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01639v5",
                "updated": "2024-09-10T06:02:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    2,
                    25,
                    1,
                    254,
                    0
                ],
                "published": "2023-12-04T05:41:02Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    5,
                    41,
                    2,
                    0,
                    338,
                    0
                ],
                "title": "On the Effectiveness of Large Language Models in Domain-Specific Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effectiveness of Large Language Models in Domain-Specific Code\n  Generation"
                },
                "summary": "Large language models (LLMs) such as ChatGPT have shown remarkable\ncapabilities in code generation. Despite significant achievements, they rely on\nenormous training data to acquire a broad spectrum of open-domain knowledge.\nBesides, their evaluation revolves around open-domain benchmarks like\nHumanEval, which primarily consist of programming contests. Therefore, it is\nhard to fully characterize the intricacies and challenges associated with\nparticular domains (e.g., web, game, and math). In this paper, we conduct an\nin-depth study of the LLMs in domain-specific code generation. Our results\ndemonstrate that LLMs exhibit sub-optimal performance in generating\ndomain-specific code, due to their limited proficiency in utilizing\ndomain-specific libraries. We further observe that incorporating API knowledge\nas prompts can empower LLMs to generate more professional code. Based on these\nfindings, we further investigate how to effectively incorporate API knowledge\ninto the code generation process. We experiment with three strategies for\nincorporating domain knowledge, namely, external knowledge inquirer,\nchain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these\nstrategies as a new code generation approach called DomCoder. Experimental\nresults show that all strategies of DomCoder lead to improvement in the\neffectiveness of domain-specific code generation under certain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as ChatGPT have shown remarkable\ncapabilities in code generation. Despite significant achievements, they rely on\nenormous training data to acquire a broad spectrum of open-domain knowledge.\nBesides, their evaluation revolves around open-domain benchmarks like\nHumanEval, which primarily consist of programming contests. Therefore, it is\nhard to fully characterize the intricacies and challenges associated with\nparticular domains (e.g., web, game, and math). In this paper, we conduct an\nin-depth study of the LLMs in domain-specific code generation. Our results\ndemonstrate that LLMs exhibit sub-optimal performance in generating\ndomain-specific code, due to their limited proficiency in utilizing\ndomain-specific libraries. We further observe that incorporating API knowledge\nas prompts can empower LLMs to generate more professional code. Based on these\nfindings, we further investigate how to effectively incorporate API knowledge\ninto the code generation process. We experiment with three strategies for\nincorporating domain knowledge, namely, external knowledge inquirer,\nchain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these\nstrategies as a new code generation approach called DomCoder. Experimental\nresults show that all strategies of DomCoder lead to improvement in the\neffectiveness of domain-specific code generation under certain settings."
                },
                "authors": [
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Yalan Lin"
                    },
                    {
                        "name": "Yuhan Hu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Zhao Wei"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Juhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Juhong Wang"
                },
                "author": "Juhong Wang",
                "arxiv_comment": "Accepted by the ACM Transactions on Software Engineering and\n  Methodology (TOSEM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01639v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01639v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06223v1",
                "updated": "2024-09-10T05:26:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    26,
                    53,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T05:26:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    26,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models"
                },
                "summary": "The Audio Question Answering task includes audio event classification, audio\ncaptioning, and open ended reasoning. Recently, Audio Question Answering has\ngarnered attention due to the advent of Large Audio Language Models. Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext only Large Language Models through a projection module. While Large Audio\nLanguage Models excel in general audio understanding, they are limited in\ntemporal reasoning which may hinder their commercial applications and on device\ndeployment. This paper addresses these challenges and limitations in audio\ntemporal reasoning. First, we introduce a data augmentation technique for\ngenerating reliable audio temporal questions and answers using an LLM. Second,\nwe propose a continued finetuning curriculum learning strategy to specialize in\ntemporal reasoning without compromising performance on finetuned tasks.\nFinally, we develop a reliable and transparent automated metric, assisted by an\nLLM, to measure the correlation between Large Audio Language Model responses\nand ground truth data intelligently. We demonstrate the effectiveness of our\nproposed techniques using SOTA LALMs on public audio benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Audio Question Answering task includes audio event classification, audio\ncaptioning, and open ended reasoning. Recently, Audio Question Answering has\ngarnered attention due to the advent of Large Audio Language Models. Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext only Large Language Models through a projection module. While Large Audio\nLanguage Models excel in general audio understanding, they are limited in\ntemporal reasoning which may hinder their commercial applications and on device\ndeployment. This paper addresses these challenges and limitations in audio\ntemporal reasoning. First, we introduce a data augmentation technique for\ngenerating reliable audio temporal questions and answers using an LLM. Second,\nwe propose a continued finetuning curriculum learning strategy to specialize in\ntemporal reasoning without compromising performance on finetuned tasks.\nFinally, we develop a reliable and transparent automated metric, assisted by an\nLLM, to measure the correlation between Large Audio Language Model responses\nand ground truth data intelligently. We demonstrate the effectiveness of our\nproposed techniques using SOTA LALMs on public audio benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Arvind Krishna Sridhar"
                    },
                    {
                        "name": "Yinyi Guo"
                    },
                    {
                        "name": "Erik Visser"
                    }
                ],
                "author_detail": {
                    "name": "Erik Visser"
                },
                "author": "Erik Visser",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06211v1",
                "updated": "2024-09-10T04:34:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    34,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:34:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    34,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning"
                },
                "summary": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available."
                },
                "authors": [
                    {
                        "name": "Jaeseong Lee"
                    },
                    {
                        "name": "seung-won hwang"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Daniel F Campos"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06205v1",
                "updated": "2024-09-10T04:18:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    18,
                    49,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:18:49Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    18,
                    49,
                    1,
                    254,
                    0
                ],
                "title": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing\n  Behaviors with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing\n  Behaviors with LLMs"
                },
                "summary": "This paper introduces text-to-shape-display, a novel approach to generating\ndynamic shape changes in pin-based shape displays through natural language\ncommands. By leveraging large language models (LLMs) and AI-chaining, our\napproach allows users to author shape-changing behaviors on demand through text\nprompts without programming. We describe the foundational aspects necessary for\nsuch a system, including the identification of key generative elements\n(primitive, animation, and interaction) and design requirements to enhance user\ninteraction, based on formative exploration and iterative design processes.\nBased on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a\n24 x 24 shape display, which translates the user's textual command into\nexecutable code and allows for quick exploration through a web-based control\ninterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\nperformance evaluation and 2) user evaluation (N= 10). The study conclusions\nhighlight the ability to facilitate rapid ideation of a wide range of\nshape-changing behaviors with AI. However, the findings also expose\naccuracy-related challenges and limitations, prompting further exploration into\nrefining the framework for leveraging AI to better suit the unique requirements\nof shape-changing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces text-to-shape-display, a novel approach to generating\ndynamic shape changes in pin-based shape displays through natural language\ncommands. By leveraging large language models (LLMs) and AI-chaining, our\napproach allows users to author shape-changing behaviors on demand through text\nprompts without programming. We describe the foundational aspects necessary for\nsuch a system, including the identification of key generative elements\n(primitive, animation, and interaction) and design requirements to enhance user\ninteraction, based on formative exploration and iterative design processes.\nBased on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a\n24 x 24 shape display, which translates the user's textual command into\nexecutable code and allows for quick exploration through a web-based control\ninterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\nperformance evaluation and 2) user evaluation (N= 10). The study conclusions\nhighlight the ability to facilitate rapid ideation of a wide range of\nshape-changing behaviors with AI. However, the findings also expose\naccuracy-related challenges and limitations, prompting further exploration into\nrefining the framework for leveraging AI to better suit the unique requirements\nof shape-changing systems."
                },
                "authors": [
                    {
                        "name": "Wanli Qian"
                    },
                    {
                        "name": "Chenfeng Gao"
                    },
                    {
                        "name": "Anup Sathya"
                    },
                    {
                        "name": "Ryo Suzuki"
                    },
                    {
                        "name": "Ken Nakagaki"
                    }
                ],
                "author_detail": {
                    "name": "Ken Nakagaki"
                },
                "author": "Ken Nakagaki",
                "arxiv_doi": "10.1145/3654777.3676348",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3654777.3676348",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for ACM UIST 2024",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.12391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.12391v2",
                "updated": "2024-09-10T04:16:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    16,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2023-11-27T13:35:15Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    13,
                    35,
                    15,
                    0,
                    331,
                    0
                ],
                "title": "vTrain: A Simulation Framework for Evaluating Cost-effective and\n  Compute-optimal Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTrain: A Simulation Framework for Evaluating Cost-effective and\n  Compute-optimal Large Language Model Training"
                },
                "summary": "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget."
                },
                "authors": [
                    {
                        "name": "Jehyeon Bang"
                    },
                    {
                        "name": "Yujeong Choi"
                    },
                    {
                        "name": "Myeongwoo Kim"
                    },
                    {
                        "name": "Yongdeok Kim"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.12391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.12391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10863v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10863v3",
                "updated": "2024-09-10T04:00:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    0,
                    54,
                    1,
                    254,
                    0
                ],
                "published": "2023-10-16T22:23:18Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    22,
                    23,
                    18,
                    0,
                    289,
                    0
                ],
                "title": "Greedy Perspectives: Multi-Drone View Planning for Collaborative\n  Perception in Cluttered Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greedy Perspectives: Multi-Drone View Planning for Collaborative\n  Perception in Cluttered Environments"
                },
                "summary": "Deployment of teams of aerial robots could enable large-scale filming of\ndynamic groups of people (actors) in complex environments for applications in\nareas such as team sports and cinematography. Toward this end, methods for\nsubmodular maximization via sequential greedy planning can enable scalable\noptimization of camera views across teams of robots but face challenges with\nefficient coordination in cluttered environments. Obstacles can produce\nocclusions and increase chances of inter-robot collision which can violate\nrequirements for near-optimality guarantees. To coordinate teams of aerial\nrobots in filming groups of people in dense environments, a more general\nview-planning approach is required. We explore how collision and occlusion\nimpact performance in filming applications through the development of a\nmulti-robot multi-actor view planner with an occlusion-aware objective for\nfilming groups of people and compare with a formation planner and a greedy\nplanner that ignores inter-robot collisions. We evaluate our approach based on\nfive test environments and complex multi-actor behaviors. Compared with a\nformation planner, our sequential planner generates 14% greater view reward for\nfilming the actors in three scenarios and comparable performance to formation\nplanning on two others. We also observe near identical view rewards for\nsequential planning both with and without inter-robot collision constraints\nwhich indicates that robots are able to avoid collisions without impairing\nperformance in the perception task. Overall, we demonstrate effective\ncoordination of teams of aerial robots in environments cluttered with obstacles\nthat may cause collisions or occlusions and for filming groups that may split,\nmerge, or spread apart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of teams of aerial robots could enable large-scale filming of\ndynamic groups of people (actors) in complex environments for applications in\nareas such as team sports and cinematography. Toward this end, methods for\nsubmodular maximization via sequential greedy planning can enable scalable\noptimization of camera views across teams of robots but face challenges with\nefficient coordination in cluttered environments. Obstacles can produce\nocclusions and increase chances of inter-robot collision which can violate\nrequirements for near-optimality guarantees. To coordinate teams of aerial\nrobots in filming groups of people in dense environments, a more general\nview-planning approach is required. We explore how collision and occlusion\nimpact performance in filming applications through the development of a\nmulti-robot multi-actor view planner with an occlusion-aware objective for\nfilming groups of people and compare with a formation planner and a greedy\nplanner that ignores inter-robot collisions. We evaluate our approach based on\nfive test environments and complex multi-actor behaviors. Compared with a\nformation planner, our sequential planner generates 14% greater view reward for\nfilming the actors in three scenarios and comparable performance to formation\nplanning on two others. We also observe near identical view rewards for\nsequential planning both with and without inter-robot collision constraints\nwhich indicates that robots are able to avoid collisions without impairing\nperformance in the perception task. Overall, we demonstrate effective\ncoordination of teams of aerial robots in environments cluttered with obstacles\nthat may cause collisions or occlusions and for filming groups that may split,\nmerge, or spread apart."
                },
                "authors": [
                    {
                        "name": "Krishna Suresh"
                    },
                    {
                        "name": "Aditya Rauniyar"
                    },
                    {
                        "name": "Micah Corah"
                    },
                    {
                        "name": "Sebastian Scherer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Scherer"
                },
                "author": "Sebastian Scherer",
                "arxiv_comment": "IROS'24; 8 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10863v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10863v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06192v1",
                "updated": "2024-09-10T03:43:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    43,
                    26,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:43:26Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    43,
                    26,
                    1,
                    254,
                    0
                ],
                "title": "NOVI : Chatbot System for University Novice with BERT and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOVI : Chatbot System for University Novice with BERT and LLMs"
                },
                "summary": "To mitigate the difficulties of university freshmen in adapting to university\nlife, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes\npost and comment data from SKKU 'Everytime', a university community site.\nDeveloped using LangChain, NOVI's performance has been evaluated with a BLEU\nscore, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR\nscore. This approach is not only limited to help university freshmen but is\nalso expected to help various people adapting to new environments with\ndifferent data. This research explores the development and potential\napplication of new educational technology tools, contributing to easier social\nadaptation for beginners and settling a foundation for future advancement in\nLLM studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the difficulties of university freshmen in adapting to university\nlife, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes\npost and comment data from SKKU 'Everytime', a university community site.\nDeveloped using LangChain, NOVI's performance has been evaluated with a BLEU\nscore, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR\nscore. This approach is not only limited to help university freshmen but is\nalso expected to help various people adapting to new environments with\ndifferent data. This research explores the development and potential\napplication of new educational technology tools, contributing to easier social\nadaptation for beginners and settling a foundation for future advancement in\nLLM studies."
                },
                "authors": [
                    {
                        "name": "Yoonji Nam"
                    },
                    {
                        "name": "TaeWoong Seo"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Sangji Lee"
                    },
                    {
                        "name": "JaeEun Im"
                    }
                ],
                "author_detail": {
                    "name": "JaeEun Im"
                },
                "author": "JaeEun Im",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06185v1",
                "updated": "2024-09-10T03:26:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    26,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    26,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Unlock Novel Scientific Research Ideas?"
                },
                "summary": "\"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Sandeep Kumar"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Vinayak Goyal"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "arxiv_comment": "24 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06180v1",
                "updated": "2024-09-10T03:20:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    20,
                    56,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:20:56Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    20,
                    56,
                    1,
                    254,
                    0
                ],
                "title": "Optimizing Sample Size for Supervised Machine Learning with Bulk\n  Transcriptomic Sequencing: A Learning Curve Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Sample Size for Supervised Machine Learning with Bulk\n  Transcriptomic Sequencing: A Learning Curve Approach"
                },
                "summary": "Accurate sample classification using transcriptomics data is crucial for\nadvancing personalized medicine. Achieving this goal necessitates determining a\nsuitable sample size that ensures adequate statistical power without undue\nresource allocation. Current sample size calculation methods rely on\nassumptions and algorithms that may not align with supervised machine learning\ntechniques for sample classification. Addressing this critical methodological\ngap, we present a novel computational approach that establishes the\npower-versus-sample-size relationship by employing a data augmentation strategy\nfollowed by fitting a learning curve. We comprehensively evaluated its\nperformance for microRNA and RNA sequencing data, considering diverse data\ncharacteristics and algorithm configurations, based on a spectrum of evaluation\nmetrics. To foster accessibility and reproducibility, the Python and R code for\nimplementing our approach is available on GitHub. Its deployment will\nsignificantly facilitate the adoption of machine learning in transcriptomics\nstudies and accelerate their translation into clinically useful classifiers for\npersonalized treatment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate sample classification using transcriptomics data is crucial for\nadvancing personalized medicine. Achieving this goal necessitates determining a\nsuitable sample size that ensures adequate statistical power without undue\nresource allocation. Current sample size calculation methods rely on\nassumptions and algorithms that may not align with supervised machine learning\ntechniques for sample classification. Addressing this critical methodological\ngap, we present a novel computational approach that establishes the\npower-versus-sample-size relationship by employing a data augmentation strategy\nfollowed by fitting a learning curve. We comprehensively evaluated its\nperformance for microRNA and RNA sequencing data, considering diverse data\ncharacteristics and algorithm configurations, based on a spectrum of evaluation\nmetrics. To foster accessibility and reproducibility, the Python and R code for\nimplementing our approach is available on GitHub. Its deployment will\nsignificantly facilitate the adoption of machine learning in transcriptomics\nstudies and accelerate their translation into clinically useful classifiers for\npersonalized treatment."
                },
                "authors": [
                    {
                        "name": "Yunhui Qi"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Li-Xuan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Li-Xuan Qin"
                },
                "author": "Li-Xuan Qin",
                "arxiv_comment": "34 pages, 6 figures and extended data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06177v1",
                "updated": "2024-09-10T03:12:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    12,
                    39,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:12:39Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    12,
                    39,
                    1,
                    254,
                    0
                ],
                "title": "HierLLM: Hierarchical Large Language Model for Question Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierLLM: Hierarchical Large Language Model for Question Recommendation"
                },
                "summary": "Question recommendation is a task that sequentially recommends questions for\nstudents to enhance their learning efficiency. That is, given the learning\nhistory and learning target of a student, a question recommender is supposed to\nselect the question that will bring the most improvement for students. Previous\nmethods typically model the question recommendation as a sequential\ndecision-making problem, estimating students' learning state with the learning\nhistory, and feeding the learning state with the learning target to a neural\nnetwork to select the recommended question from a question set. However,\nprevious methods are faced with two challenges: (1) learning history is\nunavailable in the cold start scenario, which makes the recommender generate\ninappropriate recommendations; (2) the size of the question set is much large,\nwhich makes it difficult for the recommender to select the best question\nprecisely. To address the challenges, we propose a method called hierarchical\nlarge language model for question recommendation (HierLLM), which is a\nLLM-based hierarchical structure. The LLM-based structure enables HierLLM to\ntackle the cold start issue with the strong reasoning abilities of LLM. The\nhierarchical structure takes advantage of the fact that the number of concepts\nis significantly smaller than the number of questions, narrowing the range of\nselectable questions by first identifying the relevant concept for the\nto-recommend question, and then selecting the recommended question based on\nthat concept. This hierarchical structure reduces the difficulty of the\nrecommendation.To investigate the performance of HierLLM, we conduct extensive\nexperiments, and the results demonstrate the outstanding performance of\nHierLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question recommendation is a task that sequentially recommends questions for\nstudents to enhance their learning efficiency. That is, given the learning\nhistory and learning target of a student, a question recommender is supposed to\nselect the question that will bring the most improvement for students. Previous\nmethods typically model the question recommendation as a sequential\ndecision-making problem, estimating students' learning state with the learning\nhistory, and feeding the learning state with the learning target to a neural\nnetwork to select the recommended question from a question set. However,\nprevious methods are faced with two challenges: (1) learning history is\nunavailable in the cold start scenario, which makes the recommender generate\ninappropriate recommendations; (2) the size of the question set is much large,\nwhich makes it difficult for the recommender to select the best question\nprecisely. To address the challenges, we propose a method called hierarchical\nlarge language model for question recommendation (HierLLM), which is a\nLLM-based hierarchical structure. The LLM-based structure enables HierLLM to\ntackle the cold start issue with the strong reasoning abilities of LLM. The\nhierarchical structure takes advantage of the fact that the number of concepts\nis significantly smaller than the number of questions, narrowing the range of\nselectable questions by first identifying the relevant concept for the\nto-recommend question, and then selecting the recommended question based on\nthat concept. This hierarchical structure reduces the difficulty of the\nrecommendation.To investigate the performance of HierLLM, we conduct extensive\nexperiments, and the results demonstrate the outstanding performance of\nHierLLM."
                },
                "authors": [
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Haipeng Liu"
                    },
                    {
                        "name": "Ting Long"
                    }
                ],
                "author_detail": {
                    "name": "Ting Long"
                },
                "author": "Ting Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06173v1",
                "updated": "2024-09-10T03:06:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    6,
                    17,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T03:06:17Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    6,
                    17,
                    1,
                    254,
                    0
                ],
                "title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks"
                },
                "summary": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors."
                },
                "authors": [
                    {
                        "name": "Georgios Chochlakis"
                    },
                    {
                        "name": "Niyantha Maruthu Pandiyan"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Shrikanth Narayanan"
                },
                "author": "Shrikanth Narayanan",
                "arxiv_comment": "5 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02076v2",
                "updated": "2024-09-10T02:43:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    43,
                    36,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-03T17:25:54Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    25,
                    54,
                    1,
                    247,
                    0
                ],
                "title": "Spinning the Golden Thread: Benchmarking Long-Form Generation in\n  long-context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spinning the Golden Thread: Benchmarking Long-Form Generation in\n  long-context LLMs"
                },
                "summary": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, Spinning the Golden\nThread (SGT), which tests models' ability to identify specific events within\ngenerated long text sequences. In this benchmark, we prompt long-context LMs to\ncreate long-form text that must include particular events or constraints and\nevaluate their ability to incorporate these elements. We evaluated ten\nlong-context LMs across four distinct scenarios, three types of prompt\ninstructions, and two different generation-length settings (16K and 32K).\nAlthough these models perform well on NIAH benchmarks, none demonstrated\nsatisfactory performance on the Spinning the Golden Thread, raising concerns\nabout their ability to generate coherent long-form text that follows\ninstructions. Additionally, as the length of the generated text increases, all\nmodels exhibit a significant drop in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, Spinning the Golden\nThread (SGT), which tests models' ability to identify specific events within\ngenerated long text sequences. In this benchmark, we prompt long-context LMs to\ncreate long-form text that must include particular events or constraints and\nevaluate their ability to incorporate these elements. We evaluated ten\nlong-context LMs across four distinct scenarios, three types of prompt\ninstructions, and two different generation-length settings (16K and 32K).\nAlthough these models perform well on NIAH benchmarks, none demonstrated\nsatisfactory performance on the Spinning the Golden Thread, raising concerns\nabout their ability to generate coherent long-form text that follows\ninstructions. Additionally, as the length of the generated text increases, all\nmodels exhibit a significant drop in performance."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06164v1",
                "updated": "2024-09-10T02:22:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    22,
                    50,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T02:22:50Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    22,
                    50,
                    1,
                    254,
                    0
                ],
                "title": "Deep Learning and Large Language Models for Audio and Text Analysis in\n  Predicting Suicidal Acts in Chinese Psychological Support Hotlines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning and Large Language Models for Audio and Text Analysis in\n  Predicting Suicidal Acts in Chinese Psychological Support Hotlines"
                },
                "summary": "Suicide is a pressing global issue, demanding urgent and effective preventive\ninterventions. Among the various strategies in place, psychological support\nhotlines had proved as a potent intervention method. Approximately two million\npeople in China attempt suicide annually, with many individuals making multiple\nattempts. Prompt identification and intervention for high-risk individuals are\ncrucial to preventing tragedies. With the rapid advancement of artificial\nintelligence (AI), especially the development of large-scale language models\n(LLMs), new technological tools have been introduced to the field of mental\nhealth. This study included 1284 subjects, and was designed to validate whether\ndeep learning models and LLMs, using audio and transcribed text from support\nhotlines, can effectively predict suicide risk. We proposed a simple LLM-based\npipeline that first summarizes transcribed text from approximately one hour of\nspeech to extract key features, and then predict suicidial bahaviours in the\nfuture. We compared our LLM-based method with the traditional manual scale\napproach in a clinical setting and with five advanced deep learning models.\nSurprisingly, the proposed simple LLM pipeline achieved strong performance on a\ntest set of 46 subjects, with an F1 score of 76\\% when combined with manual\nscale rating. This is 7\\% higher than the best speech-based deep learning\nmodels and represents a 27.82\\% point improvement in F1 score compared to using\nthe manual scale apporach alone. Our study explores new applications of LLMs\nand demonstrates their potential for future use in suicide prevention efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suicide is a pressing global issue, demanding urgent and effective preventive\ninterventions. Among the various strategies in place, psychological support\nhotlines had proved as a potent intervention method. Approximately two million\npeople in China attempt suicide annually, with many individuals making multiple\nattempts. Prompt identification and intervention for high-risk individuals are\ncrucial to preventing tragedies. With the rapid advancement of artificial\nintelligence (AI), especially the development of large-scale language models\n(LLMs), new technological tools have been introduced to the field of mental\nhealth. This study included 1284 subjects, and was designed to validate whether\ndeep learning models and LLMs, using audio and transcribed text from support\nhotlines, can effectively predict suicide risk. We proposed a simple LLM-based\npipeline that first summarizes transcribed text from approximately one hour of\nspeech to extract key features, and then predict suicidial bahaviours in the\nfuture. We compared our LLM-based method with the traditional manual scale\napproach in a clinical setting and with five advanced deep learning models.\nSurprisingly, the proposed simple LLM pipeline achieved strong performance on a\ntest set of 46 subjects, with an F1 score of 76\\% when combined with manual\nscale rating. This is 7\\% higher than the best speech-based deep learning\nmodels and represents a 27.82\\% point improvement in F1 score compared to using\nthe manual scale apporach alone. Our study explores new applications of LLMs\nand demonstrates their potential for future use in suicide prevention efforts."
                },
                "authors": [
                    {
                        "name": "Yining Chen"
                    },
                    {
                        "name": "Jianqiang Li"
                    },
                    {
                        "name": "Changwei Song"
                    },
                    {
                        "name": "Qing Zhao"
                    },
                    {
                        "name": "Yongsheng Tong"
                    },
                    {
                        "name": "Guanghui Fu"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui Fu"
                },
                "author": "Guanghui Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09895v3",
                "updated": "2024-09-10T02:12:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    12,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-19T11:09:12Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "title": "Performance Law of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Law of Large Language Models"
                },
                "summary": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments."
                },
                "authors": [
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Personal opinions of the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v2",
                "updated": "2024-09-10T02:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    1,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge\n  Discovery"
                },
                "summary": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access\nexternal databases, thereby enhancing the generation quality of large language\nmodels (LLMs) through optimized context. However, the existing retrieval\nmethods are constrained inherently, as they can only perform relevance matching\nbetween explicitly stated queries and well-formed knowledge, but unable to\nhandle tasks involving ambiguous information needs or unstructured knowledge.\nConsequently, existing RAG systems are primarily effective for straightforward\nquestion-answering tasks. In this work, we propose MemoRAG, a novel\nretrieval-augmented generation paradigm empowered by long-term memory. MemoRAG\nadopts a dual-system architecture. On the one hand, it employs a light but\nlong-range LLM to form the global memory of database. Once a task is presented,\nit generates draft answers, cluing the retrieval tools to locate useful\ninformation within the database. On the other hand, it leverages an expensive\nbut expressive LLM, which generates the ultimate answer based on the retrieved\ninformation. Building on this general framework, we further optimize MemoRAG's\nperformance by enhancing its cluing mechanism and memorization capacity. In our\nexperiment, MemoRAG achieves superior performance across a variety of\nevaluation tasks, including both complex ones where conventional RAG fails and\nstraightforward ones where RAG is commonly applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access\nexternal databases, thereby enhancing the generation quality of large language\nmodels (LLMs) through optimized context. However, the existing retrieval\nmethods are constrained inherently, as they can only perform relevance matching\nbetween explicitly stated queries and well-formed knowledge, but unable to\nhandle tasks involving ambiguous information needs or unstructured knowledge.\nConsequently, existing RAG systems are primarily effective for straightforward\nquestion-answering tasks. In this work, we propose MemoRAG, a novel\nretrieval-augmented generation paradigm empowered by long-term memory. MemoRAG\nadopts a dual-system architecture. On the one hand, it employs a light but\nlong-range LLM to form the global memory of database. Once a task is presented,\nit generates draft answers, cluing the retrieval tools to locate useful\ninformation within the database. On the other hand, it leverages an expensive\nbut expressive LLM, which generates the ultimate answer based on the retrieved\ninformation. Building on this general framework, we further optimize MemoRAG's\nperformance by enhancing its cluing mechanism and memorization capacity. In our\nexperiment, MemoRAG achieves superior performance across a variety of\nevaluation tasks, including both complex ones where conventional RAG fails and\nstraightforward ones where RAG is commonly applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Technical Report. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06131v1",
                "updated": "2024-09-10T00:59:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    0,
                    59,
                    18,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T00:59:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    0,
                    59,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn,\n  Focus, and Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn,\n  Focus, and Review"
                },
                "summary": "Large Language Model (LLM) pretraining traditionally relies on autoregressive\nlanguage modeling on randomly sampled data blocks from web-scale datasets. We\ntake inspiration from human learning techniques like spaced repetition to\nhypothesize that random data sampling for LLMs leads to high training cost and\nlow quality models which tend to forget data. In order to effectively commit\nweb-scale information to long-term memory, we propose the LFR (Learn, Focus,\nand Review) pedagogy, a new dynamic training paradigm which focuses and\nrepeatedly reviews complex data blocks at systematic intervals based on the\nmodel's learning pace and progress. LFR records the model perplexities for\ndifferent data blocks and frequently revisits blocks with higher perplexity\nwhich are more likely to be forgotten. We pretrain the GPT-2 models (124M -\n1.5B) from scratch on the OpenWebText dataset using LFR. We test on downstream\ntasks from the language modeling, question answering, translation, and problem\nsolving domains to achieve consistently lower perplexity and higher accuracy\nthan the baseline OpenAI models, while obtaining a 20x pretraining speed-up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) pretraining traditionally relies on autoregressive\nlanguage modeling on randomly sampled data blocks from web-scale datasets. We\ntake inspiration from human learning techniques like spaced repetition to\nhypothesize that random data sampling for LLMs leads to high training cost and\nlow quality models which tend to forget data. In order to effectively commit\nweb-scale information to long-term memory, we propose the LFR (Learn, Focus,\nand Review) pedagogy, a new dynamic training paradigm which focuses and\nrepeatedly reviews complex data blocks at systematic intervals based on the\nmodel's learning pace and progress. LFR records the model perplexities for\ndifferent data blocks and frequently revisits blocks with higher perplexity\nwhich are more likely to be forgotten. We pretrain the GPT-2 models (124M -\n1.5B) from scratch on the OpenWebText dataset using LFR. We test on downstream\ntasks from the language modeling, question answering, translation, and problem\nsolving domains to achieve consistently lower perplexity and higher accuracy\nthan the baseline OpenAI models, while obtaining a 20x pretraining speed-up."
                },
                "authors": [
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Jui-Nan Yen"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02636v2",
                "updated": "2024-09-10T00:18:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    0,
                    18,
                    2,
                    1,
                    254,
                    0
                ],
                "published": "2024-02-04T23:04:02Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    23,
                    4,
                    2,
                    6,
                    35,
                    0
                ],
                "title": "Can Large Language Models Learn Independent Causal Mechanisms?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Learn Independent Causal Mechanisms?"
                },
                "summary": "Despite impressive performance on language modelling and complex reasoning\ntasks, Large Language Models (LLMs) fall short on the same tasks in uncommon\nsettings or with distribution shifts, exhibiting a lack of generalisation\nability. By contrast, systems such as causal models, that learn abstract\nvariables and causal relationships, can demonstrate increased robustness\nagainst changes in the distribution. One reason for this success is the\nexistence and use of Independent Causal Mechanisms (ICMs) representing\nhigh-level concepts that only sparsely interact. In this work, we apply two\nconcepts from causality to learn ICMs within LLMs. We develop a new LLM\narchitecture composed of multiple sparsely interacting language modelling\nmodules. We show that such causal constraints can improve out-of-distribution\nperformance on abstract and causal reasoning tasks. We also investigate the\nlevel of independence and domain specialisation and show that LLMs rely on\npre-trained partially domain-invariant mechanisms resilient to fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive performance on language modelling and complex reasoning\ntasks, Large Language Models (LLMs) fall short on the same tasks in uncommon\nsettings or with distribution shifts, exhibiting a lack of generalisation\nability. By contrast, systems such as causal models, that learn abstract\nvariables and causal relationships, can demonstrate increased robustness\nagainst changes in the distribution. One reason for this success is the\nexistence and use of Independent Causal Mechanisms (ICMs) representing\nhigh-level concepts that only sparsely interact. In this work, we apply two\nconcepts from causality to learn ICMs within LLMs. We develop a new LLM\narchitecture composed of multiple sparsely interacting language modelling\nmodules. We show that such causal constraints can improve out-of-distribution\nperformance on abstract and causal reasoning tasks. We also investigate the\nlevel of independence and domain specialisation and show that LLMs rely on\npre-trained partially domain-invariant mechanisms resilient to fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gaël Gendron"
                    },
                    {
                        "name": "Bao Trung Nguyen"
                    },
                    {
                        "name": "Alex Yuxuan Peng"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Gillian Dobbie"
                    }
                ],
                "author_detail": {
                    "name": "Gillian Dobbie"
                },
                "author": "Gillian Dobbie",
                "arxiv_comment": "20 pages, 7 pages for the main paper and 13 pages for references and\n  appendices, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; I.2.6; I.2.7; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06098v1",
                "updated": "2024-09-09T22:33:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    33,
                    11,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T22:33:11Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    33,
                    11,
                    0,
                    253,
                    0
                ],
                "title": "Positioning of a Next Generation Mobile Cell to Maximise Aggregate\n  Network Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning of a Next Generation Mobile Cell to Maximise Aggregate\n  Network Capacity"
                },
                "summary": "In wireless communications, the need to cover operation areas, such as\nseaports, is at the forefront of discussion, especially regarding network\ncapacity provisioning. Radio network planning typically involves determining\nthe number of fixed cells, considering link budgets and deploying them\ngeometrically centered across targeted areas. This paper proposes a solution to\ndetermine the optimal position for a mobile cell, considering 3GPP path loss\nmodels. The optimal position for the mobile cell maximises the aggregate\nnetwork capacity offered to a set of User Equipments (UEs), with gains up to\n187% compared to the positioning of the mobile cell at the UEs geometrical\ncenter. The proposed solution can be used by network planners and integrated\ninto network optimisation tools. This has the potential to reduce costs\nassociated with the Radio Access Network (RAN) planning by enhancing\nflexibility for on-demand deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In wireless communications, the need to cover operation areas, such as\nseaports, is at the forefront of discussion, especially regarding network\ncapacity provisioning. Radio network planning typically involves determining\nthe number of fixed cells, considering link budgets and deploying them\ngeometrically centered across targeted areas. This paper proposes a solution to\ndetermine the optimal position for a mobile cell, considering 3GPP path loss\nmodels. The optimal position for the mobile cell maximises the aggregate\nnetwork capacity offered to a set of User Equipments (UEs), with gains up to\n187% compared to the positioning of the mobile cell at the UEs geometrical\ncenter. The proposed solution can be used by network planners and integrated\ninto network optimisation tools. This has the potential to reduce costs\nassociated with the Radio Access Network (RAN) planning by enhancing\nflexibility for on-demand deployments."
                },
                "authors": [
                    {
                        "name": "Paulo Furtado Correia"
                    },
                    {
                        "name": "Andre Coelho"
                    },
                    {
                        "name": "Manuel Ricardo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Ricardo"
                },
                "author": "Manuel Ricardo",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06097v1",
                "updated": "2024-09-09T22:29:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    29,
                    35,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T22:29:35Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    29,
                    35,
                    0,
                    253,
                    0
                ],
                "title": "ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information\n  in Task-Oriented Dialog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information\n  in Task-Oriented Dialog"
                },
                "summary": "We introduce ClarQ-LLM, an evaluation framework consisting of bilingual\nEnglish-Chinese conversation tasks, conversational agents and evaluation\nmetrics, designed to serve as a strong benchmark for assessing agents' ability\nto ask clarification questions in task-oriented dialogues. The benchmark\nincludes 31 different task types, each with 10 unique dialogue scenarios\nbetween information seeker and provider agents. The scenarios require the\nseeker to ask questions to resolve uncertainty and gather necessary information\nto complete tasks. Unlike traditional benchmarks that evaluate agents based on\nfixed dialogue content, ClarQ-LLM includes a provider conversational agent to\nreplicate the original human provider in the benchmark. This allows both\ncurrent and future seeker agents to test their ability to complete information\ngathering tasks through dialogue by directly interacting with our provider\nagent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of\nonly 60.05\\%, showing that ClarQ-LLM presents a strong challenge for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ClarQ-LLM, an evaluation framework consisting of bilingual\nEnglish-Chinese conversation tasks, conversational agents and evaluation\nmetrics, designed to serve as a strong benchmark for assessing agents' ability\nto ask clarification questions in task-oriented dialogues. The benchmark\nincludes 31 different task types, each with 10 unique dialogue scenarios\nbetween information seeker and provider agents. The scenarios require the\nseeker to ask questions to resolve uncertainty and gather necessary information\nto complete tasks. Unlike traditional benchmarks that evaluate agents based on\nfixed dialogue content, ClarQ-LLM includes a provider conversational agent to\nreplicate the original human provider in the benchmark. This allows both\ncurrent and future seeker agents to test their ability to complete information\ngathering tasks through dialogue by directly interacting with our provider\nagent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of\nonly 60.05\\%, showing that ClarQ-LLM presents a strong challenge for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Yujian Gan"
                    },
                    {
                        "name": "Changling Li"
                    },
                    {
                        "name": "Jinxia Xie"
                    },
                    {
                        "name": "Luou Wen"
                    },
                    {
                        "name": "Matthew Purver"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06080v1",
                "updated": "2024-09-09T21:26:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    21,
                    26,
                    32,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T21:26:32Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    21,
                    26,
                    32,
                    0,
                    253,
                    0
                ],
                "title": "Regression with Large Language Models for Materials and Molecular\n  Property Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression with Large Language Models for Materials and Molecular\n  Property Prediction"
                },
                "summary": "We demonstrate the ability of large language models (LLMs) to perform\nmaterial and molecular property regression tasks, a significant deviation from\nthe conventional LLM use case. We benchmark the Large Language Model Meta AI\n(LLaMA) 3 on several molecular properties in the QM9 dataset and 24 materials\nproperties. Only composition-based input strings are used as the model input\nand we fine tune on only the generative loss. We broadly find that LLaMA 3,\nwhen fine-tuned using the SMILES representation of molecules, provides useful\nregression results which can rival standard materials property prediction\nmodels like random forest or fully connected neural networks on the QM9\ndataset. Not surprisingly, LLaMA 3 errors are 5-10x higher than those of the\nstate-of-the-art models that were trained using far more granular\nrepresentation of molecules (e.g., atom types and their coordinates) for the\nsame task. Interestingly, LLaMA 3 provides improved predictions compared to\nGPT-3.5 and GPT-4o. This work highlights the versatility of LLMs, suggesting\nthat LLM-like generative models can potentially transcend their traditional\napplications to tackle complex physical phenomena, thus paving the way for\nfuture research and applications in chemistry, materials science and other\nscientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate the ability of large language models (LLMs) to perform\nmaterial and molecular property regression tasks, a significant deviation from\nthe conventional LLM use case. We benchmark the Large Language Model Meta AI\n(LLaMA) 3 on several molecular properties in the QM9 dataset and 24 materials\nproperties. Only composition-based input strings are used as the model input\nand we fine tune on only the generative loss. We broadly find that LLaMA 3,\nwhen fine-tuned using the SMILES representation of molecules, provides useful\nregression results which can rival standard materials property prediction\nmodels like random forest or fully connected neural networks on the QM9\ndataset. Not surprisingly, LLaMA 3 errors are 5-10x higher than those of the\nstate-of-the-art models that were trained using far more granular\nrepresentation of molecules (e.g., atom types and their coordinates) for the\nsame task. Interestingly, LLaMA 3 provides improved predictions compared to\nGPT-3.5 and GPT-4o. This work highlights the versatility of LLMs, suggesting\nthat LLM-like generative models can potentially transcend their traditional\napplications to tackle complex physical phenomena, thus paving the way for\nfuture research and applications in chemistry, materials science and other\nscientific domains."
                },
                "authors": [
                    {
                        "name": "Ryan Jacobs"
                    },
                    {
                        "name": "Maciej P. Polak"
                    },
                    {
                        "name": "Lane E. Schultz"
                    },
                    {
                        "name": "Hamed Mahdavi"
                    },
                    {
                        "name": "Vasant Honavar"
                    },
                    {
                        "name": "Dane Morgan"
                    }
                ],
                "author_detail": {
                    "name": "Dane Morgan"
                },
                "author": "Dane Morgan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06072v1",
                "updated": "2024-09-09T21:12:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    21,
                    12,
                    3,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T21:12:03Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    21,
                    12,
                    3,
                    0,
                    253,
                    0
                ],
                "title": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud &\n  Abuse Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud &\n  Abuse Detection"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks. However, their practical application in\nhigh-stake domains, such as fraud and abuse detection, remains an area that\nrequires further exploration. The existing applications often narrowly focus on\nspecific tasks like toxicity or hate speech detection. In this paper, we\npresent a comprehensive benchmark suite designed to assess the performance of\nLLMs in identifying and mitigating fraudulent and abusive language across\nvarious real-world scenarios. Our benchmark encompasses a diverse set of tasks,\nincluding detecting spam emails, hate speech, misogynistic language, and more.\nWe evaluated several state-of-the-art LLMs, including models from Anthropic,\nMistral AI, and the AI21 family, to provide a comprehensive assessment of their\ncapabilities in this critical domain. The results indicate that while LLMs\nexhibit proficient baseline performance in individual fraud and abuse detection\ntasks, their performance varies considerably across tasks, particularly\nstruggling with tasks that demand nuanced pragmatic reasoning, such as\nidentifying diverse forms of misogynistic language. These findings have\nimportant implications for the responsible development and deployment of LLMs\nin high-risk applications. Our benchmark suite can serve as a tool for\nresearchers and practitioners to systematically evaluate LLMs for multi-task\nfraud detection and drive the creation of more robust, trustworthy, and\nethically-aligned systems for fraud and abuse detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks. However, their practical application in\nhigh-stake domains, such as fraud and abuse detection, remains an area that\nrequires further exploration. The existing applications often narrowly focus on\nspecific tasks like toxicity or hate speech detection. In this paper, we\npresent a comprehensive benchmark suite designed to assess the performance of\nLLMs in identifying and mitigating fraudulent and abusive language across\nvarious real-world scenarios. Our benchmark encompasses a diverse set of tasks,\nincluding detecting spam emails, hate speech, misogynistic language, and more.\nWe evaluated several state-of-the-art LLMs, including models from Anthropic,\nMistral AI, and the AI21 family, to provide a comprehensive assessment of their\ncapabilities in this critical domain. The results indicate that while LLMs\nexhibit proficient baseline performance in individual fraud and abuse detection\ntasks, their performance varies considerably across tasks, particularly\nstruggling with tasks that demand nuanced pragmatic reasoning, such as\nidentifying diverse forms of misogynistic language. These findings have\nimportant implications for the responsible development and deployment of LLMs\nin high-risk applications. Our benchmark suite can serve as a tool for\nresearchers and practitioners to systematically evaluate LLMs for multi-task\nfraud detection and drive the creation of more robust, trustworthy, and\nethically-aligned systems for fraud and abuse detection."
                },
                "authors": [
                    {
                        "name": "Joymallya Chakraborty"
                    },
                    {
                        "name": "Wei Xia"
                    },
                    {
                        "name": "Anirban Majumder"
                    },
                    {
                        "name": "Dan Ma"
                    },
                    {
                        "name": "Walid Chaabene"
                    },
                    {
                        "name": "Naveed Janvekar"
                    }
                ],
                "author_detail": {
                    "name": "Naveed Janvekar"
                },
                "author": "Naveed Janvekar",
                "arxiv_comment": "12 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12192v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12192v3",
                "updated": "2024-09-09T21:07:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    21,
                    7,
                    47,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-16T21:36:43Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    21,
                    36,
                    43,
                    1,
                    198,
                    0
                ],
                "title": "Towards Dataset-scale and Feature-oriented Evaluation of Text\n  Summarization in Large Language Model Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Dataset-scale and Feature-oriented Evaluation of Text\n  Summarization in Large Language Model Prompts"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering\nhave made chatbot customization more accessible, significantly reducing\nbarriers to tasks that previously required programming skills. However, prompt\nevaluation, especially at the dataset scale, remains complex due to the need to\nassess prompts across thousands of test instances within a dataset. Our study,\nbased on a comprehensive literature review and pilot study, summarized five\ncritical challenges in prompt evaluation. In response, we introduce a\nfeature-oriented workflow for systematic prompt evaluation. In the context of\ntext summarization, our workflow advocates evaluation with summary\ncharacteristics (feature metrics) such as complexity, formality, or\nnaturalness, instead of using traditional quality metrics like ROUGE. This\ndesign choice enables a more user-friendly evaluation of prompts, as it guides\nusers in sorting through the ambiguity inherent in natural language. To support\nthis workflow, we introduce Awesum, a visual analytics system that facilitates\nidentifying optimal prompt refinements for text summarization through\ninteractive visualizations, featuring a novel Prompt Comparator design that\nemploys a BubbleSet-inspired design enhanced by dimensionality reduction\ntechniques. We evaluate the effectiveness and general applicability of the\nsystem with practitioners from various domains and found that (1) our design\nhelps overcome the learning curve for non-technical people to conduct a\nsystematic evaluation of summarization prompts, and (2) our feature-oriented\nworkflow has the potential to generalize to other NLG and image-generation\ntasks. For future works, we advocate moving towards feature-oriented evaluation\nof LLM prompts and discuss unsolved challenges in terms of human-agent\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering\nhave made chatbot customization more accessible, significantly reducing\nbarriers to tasks that previously required programming skills. However, prompt\nevaluation, especially at the dataset scale, remains complex due to the need to\nassess prompts across thousands of test instances within a dataset. Our study,\nbased on a comprehensive literature review and pilot study, summarized five\ncritical challenges in prompt evaluation. In response, we introduce a\nfeature-oriented workflow for systematic prompt evaluation. In the context of\ntext summarization, our workflow advocates evaluation with summary\ncharacteristics (feature metrics) such as complexity, formality, or\nnaturalness, instead of using traditional quality metrics like ROUGE. This\ndesign choice enables a more user-friendly evaluation of prompts, as it guides\nusers in sorting through the ambiguity inherent in natural language. To support\nthis workflow, we introduce Awesum, a visual analytics system that facilitates\nidentifying optimal prompt refinements for text summarization through\ninteractive visualizations, featuring a novel Prompt Comparator design that\nemploys a BubbleSet-inspired design enhanced by dimensionality reduction\ntechniques. We evaluate the effectiveness and general applicability of the\nsystem with practitioners from various domains and found that (1) our design\nhelps overcome the learning curve for non-technical people to conduct a\nsystematic evaluation of summarization prompts, and (2) our feature-oriented\nworkflow has the potential to generalize to other NLG and image-generation\ntasks. For future works, we advocate moving towards feature-oriented evaluation\nof LLM prompts and discuss unsolved challenges in terms of human-agent\ninteraction."
                },
                "authors": [
                    {
                        "name": "Sam Yu-Te Lee"
                    },
                    {
                        "name": "Aryaman Bahukhandi"
                    },
                    {
                        "name": "Dongyu Liu"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_doi": "10.1109/TVCG.2024.3456398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVCG.2024.3456398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12192v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12192v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06062v1",
                "updated": "2024-09-09T20:52:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    20,
                    52,
                    25,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T20:52:25Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    20,
                    52,
                    25,
                    0,
                    253,
                    0
                ],
                "title": "Retrieval Augmented Correction of Named Entity Speech Recognition Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Correction of Named Entity Speech Recognition Errors"
                },
                "summary": "In recent years, end-to-end automatic speech recognition (ASR) systems have\nproven themselves remarkably accurate and performant, but these systems still\nhave a significant error rate for entity names which appear infrequently in\ntheir training data. In parallel to the rise of end-to-end ASR systems, large\nlanguage models (LLMs) have proven to be a versatile tool for various natural\nlanguage processing (NLP) tasks. In NLP tasks where a database of relevant\nknowledge is available, retrieval augmented generation (RAG) has achieved\nimpressive results when used with LLMs. In this work, we propose a RAG-like\ntechnique for correcting speech recognition entity name errors. Our approach\nuses a vector database to index a set of relevant entities. At runtime,\ndatabase queries are generated from possibly errorful textual ASR hypotheses,\nand the entities retrieved using these queries are fed, along with the ASR\nhypotheses, to an LLM which has been adapted to correct ASR errors. Overall,\nour best system achieves 33%-39% relative word error rate reductions on\nsynthetic test sets focused on voice assistant queries of rare music entities\nwithout regressing on the STOP test set, a publicly available voice assistant\ntest set covering many domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, end-to-end automatic speech recognition (ASR) systems have\nproven themselves remarkably accurate and performant, but these systems still\nhave a significant error rate for entity names which appear infrequently in\ntheir training data. In parallel to the rise of end-to-end ASR systems, large\nlanguage models (LLMs) have proven to be a versatile tool for various natural\nlanguage processing (NLP) tasks. In NLP tasks where a database of relevant\nknowledge is available, retrieval augmented generation (RAG) has achieved\nimpressive results when used with LLMs. In this work, we propose a RAG-like\ntechnique for correcting speech recognition entity name errors. Our approach\nuses a vector database to index a set of relevant entities. At runtime,\ndatabase queries are generated from possibly errorful textual ASR hypotheses,\nand the entities retrieved using these queries are fed, along with the ASR\nhypotheses, to an LLM which has been adapted to correct ASR errors. Overall,\nour best system achieves 33%-39% relative word error rate reductions on\nsynthetic test sets focused on voice assistant queries of rare music entities\nwithout regressing on the STOP test set, a publicly available voice assistant\ntest set covering many domains."
                },
                "authors": [
                    {
                        "name": "Ernest Pusateri"
                    },
                    {
                        "name": "Anmol Walia"
                    },
                    {
                        "name": "Anirudh Kashi"
                    },
                    {
                        "name": "Bortik Bandyopadhyay"
                    },
                    {
                        "name": "Nadia Hyder"
                    },
                    {
                        "name": "Sayantan Mahinder"
                    },
                    {
                        "name": "Raviteja Anantha"
                    },
                    {
                        "name": "Daben Liu"
                    },
                    {
                        "name": "Sashank Gondala"
                    }
                ],
                "author_detail": {
                    "name": "Sashank Gondala"
                },
                "author": "Sashank Gondala",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05465v2",
                "updated": "2024-09-09T20:47:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    20,
                    47,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-08T13:01:13Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    13,
                    1,
                    13,
                    5,
                    160,
                    0
                ],
                "title": "Metaverse for Safer Roadways: An Immersive Digital Twin Framework for\n  Exploring Human-Autonomy Coexistence in Urban Transportation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaverse for Safer Roadways: An Immersive Digital Twin Framework for\n  Exploring Human-Autonomy Coexistence in Urban Transportation Systems"
                },
                "summary": "Societal-scale deployment of autonomous vehicles requires them to coexist\nwith human drivers, necessitating mutual understanding and coordination among\nthese entities. However, purely real-world or simulation-based experiments\ncannot be employed to explore such complex interactions due to safety and\nreliability concerns, respectively. Consequently, this work presents an\nimmersive digital twin framework to explore and experiment with the interaction\ndynamics between autonomous and non-autonomous traffic participants.\nParticularly, we employ a mixed-reality human-machine interface to allow human\ndrivers and autonomous agents to observe and interact with each other for\ntesting edge-case scenarios while ensuring safety at all times. To validate the\nversatility of the proposed framework's modular architecture, we first present\na discussion on a set of user experience experiments encompassing 4 different\nlevels of immersion with 4 distinct user interfaces. We then present a case\nstudy of uncontrolled intersection traversal to demonstrate the efficacy of the\nproposed framework in validating the interactions of a primary human-driven,\nautonomous, and connected autonomous vehicle with a secondary semi-autonomous\nvehicle. The proposed framework has been openly released to guide the future of\nautonomy-oriented digital twins and research on human-autonomy coexistence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Societal-scale deployment of autonomous vehicles requires them to coexist\nwith human drivers, necessitating mutual understanding and coordination among\nthese entities. However, purely real-world or simulation-based experiments\ncannot be employed to explore such complex interactions due to safety and\nreliability concerns, respectively. Consequently, this work presents an\nimmersive digital twin framework to explore and experiment with the interaction\ndynamics between autonomous and non-autonomous traffic participants.\nParticularly, we employ a mixed-reality human-machine interface to allow human\ndrivers and autonomous agents to observe and interact with each other for\ntesting edge-case scenarios while ensuring safety at all times. To validate the\nversatility of the proposed framework's modular architecture, we first present\na discussion on a set of user experience experiments encompassing 4 different\nlevels of immersion with 4 distinct user interfaces. We then present a case\nstudy of uncontrolled intersection traversal to demonstrate the efficacy of the\nproposed framework in validating the interactions of a primary human-driven,\nautonomous, and connected autonomous vehicle with a secondary semi-autonomous\nvehicle. The proposed framework has been openly released to guide the future of\nautonomy-oriented digital twins and research on human-autonomy coexistence."
                },
                "authors": [
                    {
                        "name": "Tanmay Vilas Samak"
                    },
                    {
                        "name": "Chinmay Vilas Samak"
                    },
                    {
                        "name": "Venkat Narayan Krovi"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Narayan Krovi"
                },
                "author": "Venkat Narayan Krovi",
                "arxiv_comment": "Accepted at IEEE Conference on Telepresence (TELE) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18276v3",
                "updated": "2024-09-09T20:01:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    20,
                    1,
                    22,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-23T21:18:31Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    21,
                    18,
                    31,
                    1,
                    205,
                    0
                ],
                "title": "Rome was Not Built in a Single Step: Hierarchical Prompting for\n  LLM-based Chip Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rome was Not Built in a Single Step: Hierarchical Prompting for\n  LLM-based Chip Design"
                },
                "summary": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM"
                },
                "authors": [
                    {
                        "name": "Andre Nakkab"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Siddharth Garg"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Garg"
                },
                "author": "Siddharth Garg",
                "arxiv_comment": "Accepted at MLCAD '24. 10 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13598v2",
                "updated": "2024-09-09T19:51:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    19,
                    51,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-02-21T08:03:27Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    8,
                    3,
                    27,
                    2,
                    52,
                    0
                ],
                "title": "User-LLM: Efficient LLM Contextualization with User Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-LLM: Efficient LLM Contextualization with User Embeddings"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, but effectively incorporating complex and potentially noisy user\ntimeline data into LLMs remains a challenge. Current approaches often involve\ntranslating user timelines into text descriptions before feeding them to LLMs,\nwhich can be inefficient and may not fully capture the nuances of user\nbehavior. Inspired by how LLMs are effectively integrated with images through\ndirect embeddings, we propose User-LLM, a novel framework that leverages user\nembeddings to directly contextualize LLMs with user history interactions. These\nembeddings, generated by a user encoder pretrained using self-supervised\nlearning on diverse user interactions, capture latent user behaviors and\ninterests as well as their evolution over time. We integrate these user\nembeddings with LLMs through cross-attention, enabling LLMs to dynamically\nadapt their responses based on the context of a user's past actions and\npreferences.\n  Our approach achieves significant efficiency gains by representing user\ntimelines directly as embeddings, leading to substantial inference speedups of\nup to 78.1X. Comprehensive experiments on MovieLens, Amazon Review, and Google\nLocal Review datasets demonstrate that User-LLM outperforms text-prompt-based\ncontextualization on tasks requiring deep user understanding, with improvements\nof up to 16.33%, particularly excelling on long sequences that capture subtle\nshifts in user behavior. Furthermore, the incorporation of Perceiver layers\nstreamlines the integration between user encoders and LLMs, yielding additional\ncomputational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success across various\ndomains, but effectively incorporating complex and potentially noisy user\ntimeline data into LLMs remains a challenge. Current approaches often involve\ntranslating user timelines into text descriptions before feeding them to LLMs,\nwhich can be inefficient and may not fully capture the nuances of user\nbehavior. Inspired by how LLMs are effectively integrated with images through\ndirect embeddings, we propose User-LLM, a novel framework that leverages user\nembeddings to directly contextualize LLMs with user history interactions. These\nembeddings, generated by a user encoder pretrained using self-supervised\nlearning on diverse user interactions, capture latent user behaviors and\ninterests as well as their evolution over time. We integrate these user\nembeddings with LLMs through cross-attention, enabling LLMs to dynamically\nadapt their responses based on the context of a user's past actions and\npreferences.\n  Our approach achieves significant efficiency gains by representing user\ntimelines directly as embeddings, leading to substantial inference speedups of\nup to 78.1X. Comprehensive experiments on MovieLens, Amazon Review, and Google\nLocal Review datasets demonstrate that User-LLM outperforms text-prompt-based\ncontextualization on tasks requiring deep user understanding, with improvements\nof up to 16.33%, particularly excelling on long sequences that capture subtle\nshifts in user behavior. Furthermore, the incorporation of Perceiver layers\nstreamlines the integration between user encoders and LLMs, yielding additional\ncomputational savings."
                },
                "authors": [
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Devora Berlowitz"
                    },
                    {
                        "name": "Sushant Prakash"
                    },
                    {
                        "name": "Bradley Green"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Jun Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xie"
                },
                "author": "Jun Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15847v3",
                "updated": "2024-09-09T19:36:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    19,
                    36,
                    55,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-22T17:59:45Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    17,
                    59,
                    45,
                    0,
                    204,
                    0
                ],
                "title": "LLMmap: Fingerprinting For Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMmap: Fingerprinting For Large Language Models"
                },
                "summary": "We introduce LLMmap, a first-generation fingerprinting technique targeted at\nLLM-integrated applications. LLMmap employs an active fingerprinting approach,\nsending carefully crafted queries to the application and analyzing the\nresponses to identify the specific LLM version in use. Our query selection is\ninformed by domain expertise on how LLMs generate uniquely identifiable\nresponses to thematically varied prompts. With as few as 8 interactions, LLMmap\ncan accurately identify 42 different LLM versions with over 95% accuracy. More\nimportantly, LLMmap is designed to be robust across different application\nlayers, allowing it to identify LLM versions--whether open-source or\nproprietary--from various vendors, operating under various unknown system\nprompts, stochastic sampling hyperparameters, and even complex generation\nframeworks such as RAG or Chain-of-Thought. We discuss potential mitigations\nand demonstrate that, against resourceful adversaries, effective\ncountermeasures may be challenging or even unrealizable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LLMmap, a first-generation fingerprinting technique targeted at\nLLM-integrated applications. LLMmap employs an active fingerprinting approach,\nsending carefully crafted queries to the application and analyzing the\nresponses to identify the specific LLM version in use. Our query selection is\ninformed by domain expertise on how LLMs generate uniquely identifiable\nresponses to thematically varied prompts. With as few as 8 interactions, LLMmap\ncan accurately identify 42 different LLM versions with over 95% accuracy. More\nimportantly, LLMmap is designed to be robust across different application\nlayers, allowing it to identify LLM versions--whether open-source or\nproprietary--from various vendors, operating under various unknown system\nprompts, stochastic sampling hyperparameters, and even complex generation\nframeworks such as RAG or Chain-of-Thought. We discuss potential mitigations\nand demonstrate that, against resourceful adversaries, effective\ncountermeasures may be challenging or even unrealizable."
                },
                "authors": [
                    {
                        "name": "Dario Pasquini"
                    },
                    {
                        "name": "Evgenios M. Kornaropoulos"
                    },
                    {
                        "name": "Giuseppe Ateniese"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Ateniese"
                },
                "author": "Giuseppe Ateniese",
                "arxiv_comment": "version 0.2 (more experiments)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11795v2",
                "updated": "2024-09-09T18:57:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    57,
                    1,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-21T17:36:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    36,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large\n  Language Model"
                },
                "summary": "In the realm of multimodal research, numerous studies leverage substantial\nimage-text pairs to conduct modal alignment learning, transforming Large\nLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety of\nvisual-language tasks. The prevailing methodologies primarily fall into two\ncategories: self-attention-based and cross-attention-based methods. While\nself-attention-based methods offer superior data efficiency due to their simple\nMLP architecture, they often suffer from lower computational efficiency due to\nconcatenating visual and textual tokens as input for LLM. Conversely,\ncross-attention-based methods, although less data-efficient due to additional\nlearnable parameters, exhibit higher computational efficiency by avoiding long\nsequence input for LLM. To address these trade-offs, we introduce the\nData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).\nWithout introducing additional modules or learnable parameters, EE-MLLM\nachieves both data and compute efficiency. Specifically, we modify the original\nself-attention mechanism in MLLM to a composite attention mechanism. This\nmechanism has two key characteristics: 1) Eliminating the computational\noverhead of self-attention within visual tokens to achieve compute efficiency,\nand 2) Reusing the weights on each layer of LLM to facilitate effective\nmodality alignment between vision and language for data efficiency.\nExperimental results demonstrate the effectiveness of EE-MLLM across a range of\nbenchmarks, including general-purpose datasets like MMBench and SeedBench, as\nwell as fine-grained tasks such as TextVQA and DocVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of multimodal research, numerous studies leverage substantial\nimage-text pairs to conduct modal alignment learning, transforming Large\nLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety of\nvisual-language tasks. The prevailing methodologies primarily fall into two\ncategories: self-attention-based and cross-attention-based methods. While\nself-attention-based methods offer superior data efficiency due to their simple\nMLP architecture, they often suffer from lower computational efficiency due to\nconcatenating visual and textual tokens as input for LLM. Conversely,\ncross-attention-based methods, although less data-efficient due to additional\nlearnable parameters, exhibit higher computational efficiency by avoiding long\nsequence input for LLM. To address these trade-offs, we introduce the\nData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).\nWithout introducing additional modules or learnable parameters, EE-MLLM\nachieves both data and compute efficiency. Specifically, we modify the original\nself-attention mechanism in MLLM to a composite attention mechanism. This\nmechanism has two key characteristics: 1) Eliminating the computational\noverhead of self-attention within visual tokens to achieve compute efficiency,\nand 2) Reusing the weights on each layer of LLM to facilitate effective\nmodality alignment between vision and language for data efficiency.\nExperimental results demonstrate the effectiveness of EE-MLLM across a range of\nbenchmarks, including general-purpose datasets like MMBench and SeedBench, as\nwell as fine-grained tasks such as TextVQA and DocVQA."
                },
                "authors": [
                    {
                        "name": "Feipeng Ma"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Siying Wu"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Yueyi Zhang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Sun"
                },
                "author": "Xiaoyan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05990v1",
                "updated": "2024-09-09T18:34:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    34,
                    26,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T18:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    34,
                    26,
                    0,
                    253,
                    0
                ],
                "title": "FairHome: A Fair Housing and Fair Lending Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairHome: A Fair Housing and Fair Lending Dataset"
                },
                "summary": "We present a Fair Housing and Fair Lending dataset (FairHome): A dataset with\naround 75,000 examples across 9 protected categories. To the best of our\nknowledge, FairHome is the first publicly available dataset labeled with binary\nlabels for compliance risk in the housing domain. We demonstrate the usefulness\nand effectiveness of such a dataset by training a classifier and using it to\ndetect potential violations when using a large language model (LLM) in the\ncontext of real-estate transactions. We benchmark the trained classifier\nagainst state-of-the-art LLMs including GPT-3.5, GPT-4, LLaMA-3, and Mistral\nLarge in both zero-shot and few-shot contexts. Our classifier outperformed with\nan F1-score of 0.91, underscoring the effectiveness of our dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Fair Housing and Fair Lending dataset (FairHome): A dataset with\naround 75,000 examples across 9 protected categories. To the best of our\nknowledge, FairHome is the first publicly available dataset labeled with binary\nlabels for compliance risk in the housing domain. We demonstrate the usefulness\nand effectiveness of such a dataset by training a classifier and using it to\ndetect potential violations when using a large language model (LLM) in the\ncontext of real-estate transactions. We benchmark the trained classifier\nagainst state-of-the-art LLMs including GPT-3.5, GPT-4, LLaMA-3, and Mistral\nLarge in both zero-shot and few-shot contexts. Our classifier outperformed with\nan F1-score of 0.91, underscoring the effectiveness of our dataset."
                },
                "authors": [
                    {
                        "name": "Anusha Bagalkotkar"
                    },
                    {
                        "name": "Aveek Karmakar"
                    },
                    {
                        "name": "Gabriel Arnson"
                    },
                    {
                        "name": "Ondrej Linda"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Linda"
                },
                "arxiv_affiliation": "Zillow Group",
                "author": "Ondrej Linda",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04249v2",
                "updated": "2024-09-09T18:25:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    25,
                    1,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T12:55:49Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    55,
                    49,
                    4,
                    250,
                    0
                ],
                "title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge\n  Devices"
                },
                "summary": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models."
                },
                "authors": [
                    {
                        "name": "Xueyuan Han"
                    },
                    {
                        "name": "Zinuo Cai"
                    },
                    {
                        "name": "Yichu Zhang"
                    },
                    {
                        "name": "Chongxin Fan"
                    },
                    {
                        "name": "Junhan Liu"
                    },
                    {
                        "name": "Ruhui Ma"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "Accepted by the 42nd IEEE International Conference on Computer Design\n  (ICCD 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05977v1",
                "updated": "2024-09-09T18:21:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    28,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T18:21:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "AI for Mathematics Mathematical Formalized Problem Solving and Theorem\n  Proving in Different Fields in Lean4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI for Mathematics Mathematical Formalized Problem Solving and Theorem\n  Proving in Different Fields in Lean4"
                },
                "summary": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra."
                },
                "authors": [
                    {
                        "name": "Xichen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xichen Tang"
                },
                "author": "Xichen Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05976v1",
                "updated": "2024-09-09T18:21:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    23,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T18:21:23Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    23,
                    0,
                    253,
                    0
                ],
                "title": "FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous\n  Low-Rank Adaptations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous\n  Low-Rank Adaptations"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has been pivotal in\nadvancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks\nthrough fine-tuning. Federated learning (FL) further enhances fine-tuning in a\nprivacy-aware manner by utilizing clients' local data through in-situ\ncomputation, eliminating the need for data movement. However, fine-tuning LLMs,\ngiven their massive scale of parameters, poses challenges for clients with\nconstrained and heterogeneous resources in FL. Previous methods employed\nlow-rank adaptation (LoRA) for efficient federated fine-tuning but utilized\ntraditional FL aggregation strategies on LoRA adapters. These approaches led to\nmathematically inaccurate aggregation noise, reducing fine-tuning effectiveness\nand failing to address heterogeneous LoRAs. In this work, we first highlight\nthe mathematical incorrectness of LoRA aggregation in existing federated\nfine-tuning methods. We introduce a new approach called FLORA that enables\nfederated fine-tuning on heterogeneous LoRA adapters across clients through a\nnovel stacking-based aggregation method. Our approach is noise-free and\nseamlessly supports heterogeneous LoRA adapters. Extensive experiments\ndemonstrate FLORA' s superior performance in both homogeneous and heterogeneous\nsettings, surpassing state-of-the-art methods. We envision this work as a\nmilestone for efficient, privacy-preserving, and accurate federated fine-tuning\nof LLMs. Our code is available at https://github.com/ATP-1010/FederatedLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has been pivotal in\nadvancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks\nthrough fine-tuning. Federated learning (FL) further enhances fine-tuning in a\nprivacy-aware manner by utilizing clients' local data through in-situ\ncomputation, eliminating the need for data movement. However, fine-tuning LLMs,\ngiven their massive scale of parameters, poses challenges for clients with\nconstrained and heterogeneous resources in FL. Previous methods employed\nlow-rank adaptation (LoRA) for efficient federated fine-tuning but utilized\ntraditional FL aggregation strategies on LoRA adapters. These approaches led to\nmathematically inaccurate aggregation noise, reducing fine-tuning effectiveness\nand failing to address heterogeneous LoRAs. In this work, we first highlight\nthe mathematical incorrectness of LoRA aggregation in existing federated\nfine-tuning methods. We introduce a new approach called FLORA that enables\nfederated fine-tuning on heterogeneous LoRA adapters across clients through a\nnovel stacking-based aggregation method. Our approach is noise-free and\nseamlessly supports heterogeneous LoRA adapters. Extensive experiments\ndemonstrate FLORA' s superior performance in both homogeneous and heterogeneous\nsettings, surpassing state-of-the-art methods. We envision this work as a\nmilestone for efficient, privacy-preserving, and accurate federated fine-tuning\nof LLMs. Our code is available at https://github.com/ATP-1010/FederatedLLM."
                },
                "authors": [
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05865v1",
                "updated": "2024-09-09T17:59:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    50,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:50Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    50,
                    0,
                    253,
                    0
                ],
                "title": "Robot Utility Models: General Policies for Zero-Shot Deployment in New\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot Utility Models: General Policies for Zero-Shot Deployment in New\n  Environments"
                },
                "summary": "Robot models, particularly those trained with large amounts of data, have\nrecently shown a plethora of real-world manipulation and navigation\ncapabilities. Several independent efforts have shown that given sufficient\ntraining data in an environment, robot policies can generalize to demonstrated\nvariations in that environment. However, needing to finetune robot models to\nevery new environment stands in stark contrast to models in language or vision\nthat can be deployed zero-shot for open-world problems. In this work, we\npresent Robot Utility Models (RUMs), a framework for training and deploying\nzero-shot robot policies that can directly generalize to new environments\nwithout any finetuning. To create RUMs efficiently, we develop new tools to\nquickly collect data for mobile manipulation tasks, integrate such data into a\npolicy with multi-modal imitation learning, and deploy policies on-device on\nHello Robot Stretch, a cheap commodity robot, with an external mLLM verifier\nfor retrying. We train five such utility models for opening cabinet doors,\nopening drawers, picking up napkins, picking up paper bags, and reorienting\nfallen objects. Our system, on average, achieves 90% success rate in unseen,\nnovel environments interacting with unseen objects. Moreover, the utility\nmodels can also succeed in different robot and camera set-ups with no further\ndata, training, or fine-tuning. Primary among our lessons are the importance of\ntraining data over training algorithm and policy class, guidance about data\nscaling, necessity for diverse yet high-quality demonstrations, and a recipe\nfor robot introspection and retrying to improve performance on individual\nenvironments. Our code, data, models, hardware designs, as well as our\nexperiment and deployment videos are open sourced and can be found on our\nproject website: https://robotutilitymodels.com",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot models, particularly those trained with large amounts of data, have\nrecently shown a plethora of real-world manipulation and navigation\ncapabilities. Several independent efforts have shown that given sufficient\ntraining data in an environment, robot policies can generalize to demonstrated\nvariations in that environment. However, needing to finetune robot models to\nevery new environment stands in stark contrast to models in language or vision\nthat can be deployed zero-shot for open-world problems. In this work, we\npresent Robot Utility Models (RUMs), a framework for training and deploying\nzero-shot robot policies that can directly generalize to new environments\nwithout any finetuning. To create RUMs efficiently, we develop new tools to\nquickly collect data for mobile manipulation tasks, integrate such data into a\npolicy with multi-modal imitation learning, and deploy policies on-device on\nHello Robot Stretch, a cheap commodity robot, with an external mLLM verifier\nfor retrying. We train five such utility models for opening cabinet doors,\nopening drawers, picking up napkins, picking up paper bags, and reorienting\nfallen objects. Our system, on average, achieves 90% success rate in unseen,\nnovel environments interacting with unseen objects. Moreover, the utility\nmodels can also succeed in different robot and camera set-ups with no further\ndata, training, or fine-tuning. Primary among our lessons are the importance of\ntraining data over training algorithm and policy class, guidance about data\nscaling, necessity for diverse yet high-quality demonstrations, and a recipe\nfor robot introspection and retrying to improve performance on individual\nenvironments. Our code, data, models, hardware designs, as well as our\nexperiment and deployment videos are open sourced and can be found on our\nproject website: https://robotutilitymodels.com"
                },
                "authors": [
                    {
                        "name": "Haritheja Etukuru"
                    },
                    {
                        "name": "Norihito Naka"
                    },
                    {
                        "name": "Zijin Hu"
                    },
                    {
                        "name": "Seungjae Lee"
                    },
                    {
                        "name": "Julian Mehu"
                    },
                    {
                        "name": "Aaron Edsinger"
                    },
                    {
                        "name": "Chris Paxton"
                    },
                    {
                        "name": "Soumith Chintala"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Nur Muhammad Mahi Shafiullah"
                    }
                ],
                "author_detail": {
                    "name": "Nur Muhammad Mahi Shafiullah"
                },
                "author": "Nur Muhammad Mahi Shafiullah",
                "arxiv_comment": "Project website https://robotutilitymodels.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05864v1",
                "updated": "2024-09-09T17:59:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    45,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:45Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    45,
                    0,
                    253,
                    0
                ],
                "title": "Neural MP: A Generalist Neural Motion Planner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural MP: A Generalist Neural Motion Planner"
                },
                "summary": "The current paradigm for motion planning generates solutions from scratch for\nevery new problem, which consumes significant amounts of time and computational\nresources. For complex, cluttered scenes, motion planning approaches can often\ntake minutes to produce a solution, while humans are able to accurately and\nsafely reach any goal in seconds by leveraging their prior experience. We seek\nto do the same by applying data-driven learning at scale to the problem of\nmotion planning. Our approach builds a large number of complex scenes in\nsimulation, collects expert data from a motion planner, then distills it into a\nreactive generalist policy. We then combine this with lightweight optimization\nto obtain a safe path for real world deployment. We perform a thorough\nevaluation of our method on 64 motion planning tasks across four diverse\nenvironments with randomized poses, scenes and obstacles, in the real world,\ndemonstrating an improvement of 23%, 17% and 79% motion planning success rate\nover state of the art sampling, optimization and learning based planning\nmethods. Video results available at mihdalal.github.io/neuralmotionplanner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current paradigm for motion planning generates solutions from scratch for\nevery new problem, which consumes significant amounts of time and computational\nresources. For complex, cluttered scenes, motion planning approaches can often\ntake minutes to produce a solution, while humans are able to accurately and\nsafely reach any goal in seconds by leveraging their prior experience. We seek\nto do the same by applying data-driven learning at scale to the problem of\nmotion planning. Our approach builds a large number of complex scenes in\nsimulation, collects expert data from a motion planner, then distills it into a\nreactive generalist policy. We then combine this with lightweight optimization\nto obtain a safe path for real world deployment. We perform a thorough\nevaluation of our method on 64 motion planning tasks across four diverse\nenvironments with randomized poses, scenes and obstacles, in the real world,\ndemonstrating an improvement of 23%, 17% and 79% motion planning success rate\nover state of the art sampling, optimization and learning based planning\nmethods. Video results available at mihdalal.github.io/neuralmotionplanner"
                },
                "authors": [
                    {
                        "name": "Murtaza Dalal"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Russell Mendonca"
                    },
                    {
                        "name": "Youssef Khaky"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Deepak Pathak"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Pathak"
                },
                "author": "Deepak Pathak",
                "arxiv_comment": "Website at mihdalal.github.io/neuralmotionplanner. Main paper: 7\n  pages, 4 figures, 2 tables. Appendix: 9 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14770v5",
                "updated": "2024-09-09T17:37:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    37,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2023-05-24T06:19:14Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    6,
                    19,
                    14,
                    2,
                    144,
                    0
                ],
                "title": "Using Natural Language Explanations to Rescale Human Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Natural Language Explanations to Rescale Human Judgments"
                },
                "summary": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric."
                },
                "authors": [
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Jifan Chen"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "arxiv_comment": "Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05824v1",
                "updated": "2024-09-09T17:30:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    30,
                    20,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:30:20Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    30,
                    20,
                    0,
                    253,
                    0
                ],
                "title": "Are Large Language Models a Threat to Programming Platforms? An\n  Exploratory Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models a Threat to Programming Platforms? An\n  Exploratory Study"
                },
                "summary": "Competitive programming platforms like LeetCode, Codeforces, and HackerRank\nevaluate programming skills, often used by recruiters for screening. With the\nrise of advanced Large Language Models (LLMs) such as ChatGPT, Gemini, and Meta\nAI, their problem-solving ability on these platforms needs assessment. This\nstudy explores LLMs' ability to tackle diverse programming challenges across\nplatforms with varying difficulty, offering insights into their real-time and\noffline performance and comparing them with human programmers.\n  We tested 98 problems from LeetCode, 126 from Codeforces, covering 15\ncategories. Nine online contests from Codeforces and LeetCode were conducted,\nalong with two certification tests on HackerRank, to assess real-time\nperformance. Prompts and feedback mechanisms were used to guide LLMs, and\ncorrelations were explored across different scenarios.\n  LLMs, like ChatGPT (71.43% success on LeetCode), excelled in LeetCode and\nHackerRank certifications but struggled in virtual contests, particularly on\nCodeforces. They performed better than users in LeetCode archives, excelling in\ntime and memory efficiency but underperforming in harder Codeforces contests.\nWhile not immediately threatening, LLMs performance on these platforms is\nconcerning, and future improvements will need addressing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive programming platforms like LeetCode, Codeforces, and HackerRank\nevaluate programming skills, often used by recruiters for screening. With the\nrise of advanced Large Language Models (LLMs) such as ChatGPT, Gemini, and Meta\nAI, their problem-solving ability on these platforms needs assessment. This\nstudy explores LLMs' ability to tackle diverse programming challenges across\nplatforms with varying difficulty, offering insights into their real-time and\noffline performance and comparing them with human programmers.\n  We tested 98 problems from LeetCode, 126 from Codeforces, covering 15\ncategories. Nine online contests from Codeforces and LeetCode were conducted,\nalong with two certification tests on HackerRank, to assess real-time\nperformance. Prompts and feedback mechanisms were used to guide LLMs, and\ncorrelations were explored across different scenarios.\n  LLMs, like ChatGPT (71.43% success on LeetCode), excelled in LeetCode and\nHackerRank certifications but struggled in virtual contests, particularly on\nCodeforces. They performed better than users in LeetCode archives, excelling in\ntime and memory efficiency but underperforming in harder Codeforces contests.\nWhile not immediately threatening, LLMs performance on these platforms is\nconcerning, and future improvements will need addressing."
                },
                "authors": [
                    {
                        "name": "Md Mustakim Billah"
                    },
                    {
                        "name": "Palash Ranjan Roy"
                    },
                    {
                        "name": "Zadia Codabux"
                    },
                    {
                        "name": "Banani Roy"
                    }
                ],
                "author_detail": {
                    "name": "Banani Roy"
                },
                "author": "Banani Roy",
                "arxiv_doi": "10.1145/3674805.3686689",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3686689",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in ESEM 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05816v1",
                "updated": "2024-09-09T17:23:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    23,
                    29,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:23:29Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    23,
                    29,
                    0,
                    253,
                    0
                ],
                "title": "Improving Pretraining Data Using Perplexity Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Pretraining Data Using Perplexity Correlations"
                },
                "summary": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier."
                },
                "authors": [
                    {
                        "name": "Tristan Thrush"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05806v1",
                "updated": "2024-09-09T17:11:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:11:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "Benchmarking Chinese Knowledge Rectification in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Chinese Knowledge Rectification in Large Language Models"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Tianhe Lu"
                    },
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work; code and dataset are available at\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05782v1",
                "updated": "2024-09-09T16:45:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    45,
                    26,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:45:26Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    45,
                    26,
                    0,
                    253,
                    0
                ],
                "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Neural Network Scaling Laws and Scale-time Equivalence"
                },
                "summary": "As neural networks continue to grow in size but datasets might not, it is\nvital to understand how much performance improvement can be expected: is it\nmore important to scale network size or data volume? Thus, neural network\nscaling laws, which characterize how test error varies with network size and\ndata volume, have become increasingly important. However, existing scaling laws\nare often applicable only in limited regimes and often do not incorporate or\npredict well-known phenomena such as double descent. Here, we present a novel\ntheoretical characterization of how three factors -- model size, training time,\nand data volume -- interact to determine the performance of deep neural\nnetworks. We first establish a theoretical and empirical equivalence between\nscaling the size of a neural network and increasing its training time\nproportionally. Scale-time equivalence challenges the current practice, wherein\nlarge models are trained for small durations, and suggests that smaller models\ntrained over extended periods could match their efficacy. It also leads to a\nnovel method for predicting the performance of large-scale networks from\nsmall-scale networks trained for extended epochs, and vice versa. We next\ncombine scale-time equivalence with a linear model analysis of double descent\nto obtain a unified theoretical scaling law, which we confirm with experiments\nacross vision benchmarks and network architectures. These laws explain several\npreviously unexplained phenomena: reduced data requirements for generalization\nin larger models, heightened sensitivity to label noise in overparameterized\nmodels, and instances where increasing model scale does not necessarily enhance\nperformance. Our findings hold significant implications for the practical\ndeployment of neural networks, offering a more accessible and efficient path to\ntraining and fine-tuning large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As neural networks continue to grow in size but datasets might not, it is\nvital to understand how much performance improvement can be expected: is it\nmore important to scale network size or data volume? Thus, neural network\nscaling laws, which characterize how test error varies with network size and\ndata volume, have become increasingly important. However, existing scaling laws\nare often applicable only in limited regimes and often do not incorporate or\npredict well-known phenomena such as double descent. Here, we present a novel\ntheoretical characterization of how three factors -- model size, training time,\nand data volume -- interact to determine the performance of deep neural\nnetworks. We first establish a theoretical and empirical equivalence between\nscaling the size of a neural network and increasing its training time\nproportionally. Scale-time equivalence challenges the current practice, wherein\nlarge models are trained for small durations, and suggests that smaller models\ntrained over extended periods could match their efficacy. It also leads to a\nnovel method for predicting the performance of large-scale networks from\nsmall-scale networks trained for extended epochs, and vice versa. We next\ncombine scale-time equivalence with a linear model analysis of double descent\nto obtain a unified theoretical scaling law, which we confirm with experiments\nacross vision benchmarks and network architectures. These laws explain several\npreviously unexplained phenomena: reduced data requirements for generalization\nin larger models, heightened sensitivity to label noise in overparameterized\nmodels, and instances where increasing model scale does not necessarily enhance\nperformance. Our findings hold significant implications for the practical\ndeployment of neural networks, offering a more accessible and efficient path to\ntraining and fine-tuning large models."
                },
                "authors": [
                    {
                        "name": "Akhilan Boopathy"
                    },
                    {
                        "name": "Ila Fiete"
                    }
                ],
                "author_detail": {
                    "name": "Ila Fiete"
                },
                "author": "Ila Fiete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14774v2",
                "updated": "2024-09-09T16:41:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    41,
                    36,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-27T04:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    31,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning"
                },
                "summary": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings."
                },
                "authors": [
                    {
                        "name": "Simran Kaur"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14417v2",
                "updated": "2024-09-09T16:34:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    34,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-19T15:42:49Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    15,
                    42,
                    49,
                    4,
                    201,
                    0
                ],
                "title": "Mixture of Experts with Mixture of Precisions for Tuning Quality of\n  Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts with Mixture of Precisions for Tuning Quality of\n  Service"
                },
                "summary": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 3.81 to 4.00, 13.59 to 14.17, and 7.24 to 7.40\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 3.81 to 4.00, 13.59 to 14.17, and 7.24 to 7.40\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important."
                },
                "authors": [
                    {
                        "name": "HamidReza Imani"
                    },
                    {
                        "name": "Abdolah Amirany"
                    },
                    {
                        "name": "Tarek El-Ghazawi"
                    }
                ],
                "author_detail": {
                    "name": "Tarek El-Ghazawi"
                },
                "author": "Tarek El-Ghazawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05771v1",
                "updated": "2024-09-09T16:33:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    33,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:33:16Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    33,
                    16,
                    0,
                    253,
                    0
                ],
                "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models"
                },
                "summary": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties."
                },
                "authors": [
                    {
                        "name": "Emily Cheng"
                    },
                    {
                        "name": "Richard J. Antonello"
                    }
                ],
                "author_detail": {
                    "name": "Richard J. Antonello"
                },
                "author": "Richard J. Antonello",
                "arxiv_comment": "Equal contribution from both authors. Submitted to NeurIPS NeuroAI\n  workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05768v1",
                "updated": "2024-09-09T16:32:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    32,
                    14,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:32:14Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    32,
                    14,
                    0,
                    253,
                    0
                ],
                "title": "Model Input Verification of Large Scale Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Input Verification of Large Scale Simulations"
                },
                "summary": "Reliable simulations are critical for analyzing and understanding complex\nsystems, but their accuracy depends on correct input data. Incorrect inputs\nsuch as invalid or out-of-range values, missing data, and format\ninconsistencies can cause simulation crashes or unnoticed result distortions,\nultimately undermining the validity of the conclusions. This paper presents a\nmethodology for verifying the validity of input data in simulations, a process\nwe term model input verification (MIV). We implement this approach in FabGuard,\na toolset that uses established data schema and validation tools for the\nspecific needs of simulation modeling. We introduce a formalism for\ncategorizing MIV patterns and offer a streamlined verification pipeline that\nintegrates into existing simulation workflows. FabGuard's applicability is\ndemonstrated across three diverse domains: conflict-driven migration, disaster\nevacuation, and disease spread models. We also explore the use of Large\nLanguage Models (LLMs) for automating constraint generation and inference. In a\ncase study with a migration simulation, LLMs not only correctly inferred 22 out\nof 23 developer-defined constraints, but also identified errors in existing\nconstraints and proposed new, valid constraints. Our evaluation demonstrates\nthat MIV is feasible on large datasets, with FabGuard efficiently processing\n12,000 input files in 140 seconds and maintaining consistent performance across\nvarying file sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable simulations are critical for analyzing and understanding complex\nsystems, but their accuracy depends on correct input data. Incorrect inputs\nsuch as invalid or out-of-range values, missing data, and format\ninconsistencies can cause simulation crashes or unnoticed result distortions,\nultimately undermining the validity of the conclusions. This paper presents a\nmethodology for verifying the validity of input data in simulations, a process\nwe term model input verification (MIV). We implement this approach in FabGuard,\na toolset that uses established data schema and validation tools for the\nspecific needs of simulation modeling. We introduce a formalism for\ncategorizing MIV patterns and offer a streamlined verification pipeline that\nintegrates into existing simulation workflows. FabGuard's applicability is\ndemonstrated across three diverse domains: conflict-driven migration, disaster\nevacuation, and disease spread models. We also explore the use of Large\nLanguage Models (LLMs) for automating constraint generation and inference. In a\ncase study with a migration simulation, LLMs not only correctly inferred 22 out\nof 23 developer-defined constraints, but also identified errors in existing\nconstraints and proposed new, valid constraints. Our evaluation demonstrates\nthat MIV is feasible on large datasets, with FabGuard efficiently processing\n12,000 input files in 140 seconds and maintaining consistent performance across\nvarying file sizes."
                },
                "authors": [
                    {
                        "name": "Rumyana Neykova"
                    },
                    {
                        "name": "Derek Groen"
                    }
                ],
                "author_detail": {
                    "name": "Derek Groen"
                },
                "author": "Derek Groen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10999v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10999v3",
                "updated": "2024-09-09T16:28:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    28,
                    9,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-16T16:25:22Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    25,
                    22,
                    6,
                    168,
                    0
                ],
                "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions"
                },
                "summary": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications."
                },
                "authors": [
                    {
                        "name": "Liman Wang"
                    },
                    {
                        "name": "Hanyang Zhong"
                    },
                    {
                        "name": "Wenting Cao"
                    },
                    {
                        "name": "Zeyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zeyuan Sun"
                },
                "author": "Zeyuan Sun",
                "arxiv_comment": "This article is currently under review. All data will be open on\n  GitHub once the review is complete.\n  https://github.com/limanwang/Balancing-Rigor-and-Utility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10999v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10999v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05747v1",
                "updated": "2024-09-09T16:02:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    2,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:02:27Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    2,
                    27,
                    0,
                    253,
                    0
                ],
                "title": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI)\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI)\n  System"
                },
                "summary": "This paper presents a novel conversational AI-enabled active ideation\ninterface as a creative idea-generation tool to assist novice designers in\nmitigating the initial latency and ideation bottlenecks that are commonly\nobserved. It is a dynamic, interactive, and contextually responsive approach,\nactively involving a large language model (LLM) from the domain of natural\nlanguage processing (NLP) in artificial intelligence (AI) to produce multiple\nstatements of potential ideas for different design problems. Integrating such\nAI models with ideation creates what we refer to as an Active Ideation\nscenario, which helps foster continuous dialogue-based interaction,\ncontext-sensitive conversation, and prolific idea generation. A pilot study was\nconducted with thirty novice designers to generate ideas for given problems\nusing traditional methods and the new CAI-based interface. The key parameters\nof fluency, novelty, and variety were used to compare the outcomes\nqualitatively by a panel of experts. The findings demonstrated the\neffectiveness of the proposed tool for generating prolific, diverse and novel\nideas. The interface was enhanced by incorporating a prompt-engineered\nstructured dialogue style for each ideation stage to make it uniform and more\nconvenient for the designers. The resulting responses of such a structured CAI\ninterface were found to be more succinct and aligned towards the subsequent\ndesign stage, namely conceptualization. The paper thus established the rich\npotential of using Generative AI (Gen-AI) for the early ill-structured phase of\nthe creative product design process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel conversational AI-enabled active ideation\ninterface as a creative idea-generation tool to assist novice designers in\nmitigating the initial latency and ideation bottlenecks that are commonly\nobserved. It is a dynamic, interactive, and contextually responsive approach,\nactively involving a large language model (LLM) from the domain of natural\nlanguage processing (NLP) in artificial intelligence (AI) to produce multiple\nstatements of potential ideas for different design problems. Integrating such\nAI models with ideation creates what we refer to as an Active Ideation\nscenario, which helps foster continuous dialogue-based interaction,\ncontext-sensitive conversation, and prolific idea generation. A pilot study was\nconducted with thirty novice designers to generate ideas for given problems\nusing traditional methods and the new CAI-based interface. The key parameters\nof fluency, novelty, and variety were used to compare the outcomes\nqualitatively by a panel of experts. The findings demonstrated the\neffectiveness of the proposed tool for generating prolific, diverse and novel\nideas. The interface was enhanced by incorporating a prompt-engineered\nstructured dialogue style for each ideation stage to make it uniform and more\nconvenient for the designers. The resulting responses of such a structured CAI\ninterface were found to be more succinct and aligned towards the subsequent\ndesign stage, namely conceptualization. The paper thus established the rich\npotential of using Generative AI (Gen-AI) for the early ill-structured phase of\nthe creative product design process."
                },
                "authors": [
                    {
                        "name": "B. Sankar"
                    },
                    {
                        "name": "Dibakar Sen"
                    }
                ],
                "author_detail": {
                    "name": "Dibakar Sen"
                },
                "author": "Dibakar Sen",
                "arxiv_comment": "21 pages, 16 figures, AIEDAM Journal Article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]