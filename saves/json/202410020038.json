[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v2",
                "updated": "2024-09-27T03:31:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    31,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.20566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20566v1",
                "updated": "2024-09-30T17:59:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    34,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:59:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning"
                },
                "summary": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development."
                },
                "authors": [
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Aleksei Timofeev"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Jean-Philippe Fauconnier"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Yinfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Yang"
                },
                "author": "Yinfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20565v1",
                "updated": "2024-09-30T17:59:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    33,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:59:33Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    33,
                    0,
                    274,
                    0
                ],
                "title": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation\n  of LLM-Generated Medical Explanatory Arguments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation\n  of LLM-Generated Medical Explanatory Arguments"
                },
                "summary": "Evaluating LLM-generated text has become a key challenge, especially in\ndomain-specific contexts like the medical field. This work introduces a novel\nevaluation methodology for LLM-generated medical explanatory arguments, relying\non Proxy Tasks and rankings to closely align results with human evaluation\ncriteria, overcoming the biases typically seen in LLMs used as judges. We\ndemonstrate that the proposed evaluators are robust against adversarial\nattacks, including the assessment of non-argumentative text. Additionally, the\nhuman-crafted arguments needed to train the evaluators are minimized to just\none example per Proxy Task. By examining multiple LLM-generated arguments, we\nestablish a methodology for determining whether a Proxy Task is suitable for\nevaluating LLM-generated medical explanatory arguments, requiring only five\nexamples and two human experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-generated text has become a key challenge, especially in\ndomain-specific contexts like the medical field. This work introduces a novel\nevaluation methodology for LLM-generated medical explanatory arguments, relying\non Proxy Tasks and rankings to closely align results with human evaluation\ncriteria, overcoming the biases typically seen in LLMs used as judges. We\ndemonstrate that the proposed evaluators are robust against adversarial\nattacks, including the assessment of non-argumentative text. Additionally, the\nhuman-crafted arguments needed to train the evaluators are minimized to just\none example per Proxy Task. By examining multiple LLM-generated arguments, we\nestablish a methodology for determining whether a Proxy Task is suitable for\nevaluating LLM-generated medical explanatory arguments, requiring only five\nexamples and two human experts."
                },
                "authors": [
                    {
                        "name": "Iker De la Iglesia"
                    },
                    {
                        "name": "Iakes Goenaga"
                    },
                    {
                        "name": "Johanna Ramirez-Romero"
                    },
                    {
                        "name": "Jose Maria Villa-Gonzalez"
                    },
                    {
                        "name": "Josu Goikoetxea"
                    },
                    {
                        "name": "Ander Barrena"
                    }
                ],
                "author_detail": {
                    "name": "Ander Barrena"
                },
                "author": "Ander Barrena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20564v1",
                "updated": "2024-09-30T17:59:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    19,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:59:19Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    19,
                    0,
                    274,
                    0
                ],
                "title": "Predicting the rate of fast radio bursts in globular clusters from\n  binary black hole observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the rate of fast radio bursts in globular clusters from\n  binary black hole observations"
                },
                "summary": "The repeating fast radio burst (FRB) source in an old globular cluster (GC)\nin M81 proves that FRBs, which are typically associated with young magnetars,\ncan also occur in old stellar populations. A potential explanation is\nsuper-Chandrasekhar binary white dwarf (BWD) coalescences, which may produce\nFRB-emitting neutron stars. GCs can also give rise to binary black hole (BBH)\nmergers detectable with gravitational waves, and the BWD coalescence rate from\nGCs is correlated with their BBH merger rate. For the first time, we combine\nindependent observations of gravitational waves and FRBs to infer the origins\nof FRB sources. We use GC formation histories inferred from BBH observations to\npredict the rate of super-Chandrasekhar BWD coalescences originating from GCs\nas a function of redshift. We explore mass-loss and mass-conserved scenarios\nfor BWD coalescences and find that the coalescence rates evolve differently\nacross redshift in these two cases. In the mass-loss scenario, the BWD\ncoalescence rates decrease with increasing redshift, similar to some recent\nmeasurements of the FRB rate as a function of redshift. We show that GCs could\ncontribute $\\lesssim 1\\%$ to the total FRB source formation rates in the local\nUniverse. Our multi-messenger approach also offers a novel method to better\nconstrain the GC population using both FRB and gravitational wave observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The repeating fast radio burst (FRB) source in an old globular cluster (GC)\nin M81 proves that FRBs, which are typically associated with young magnetars,\ncan also occur in old stellar populations. A potential explanation is\nsuper-Chandrasekhar binary white dwarf (BWD) coalescences, which may produce\nFRB-emitting neutron stars. GCs can also give rise to binary black hole (BBH)\nmergers detectable with gravitational waves, and the BWD coalescence rate from\nGCs is correlated with their BBH merger rate. For the first time, we combine\nindependent observations of gravitational waves and FRBs to infer the origins\nof FRB sources. We use GC formation histories inferred from BBH observations to\npredict the rate of super-Chandrasekhar BWD coalescences originating from GCs\nas a function of redshift. We explore mass-loss and mass-conserved scenarios\nfor BWD coalescences and find that the coalescence rates evolve differently\nacross redshift in these two cases. In the mass-loss scenario, the BWD\ncoalescence rates decrease with increasing redshift, similar to some recent\nmeasurements of the FRB rate as a function of redshift. We show that GCs could\ncontribute $\\lesssim 1\\%$ to the total FRB source formation rates in the local\nUniverse. Our multi-messenger approach also offers a novel method to better\nconstrain the GC population using both FRB and gravitational wave observations."
                },
                "authors": [
                    {
                        "name": "Aryamann Rao"
                    },
                    {
                        "name": "Claire S. Ye"
                    },
                    {
                        "name": "Maya Fishbach"
                    }
                ],
                "author_detail": {
                    "name": "Maya Fishbach"
                },
                "author": "Maya Fishbach",
                "arxiv_comment": "12 pages, 4 figures, 1 table. Submitted to ApJL. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20557v1",
                "updated": "2024-09-30T17:57:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    57,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:57:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    57,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in\n  Instructional Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in\n  Instructional Videos"
                },
                "summary": "Goal-oriented planning, or anticipating a series of actions that transition\nan agent from its current state to a predefined objective, is crucial for\ndeveloping intelligent assistants aiding users in daily procedural tasks. The\nproblem presents significant challenges due to the need for comprehensive\nknowledge of temporal and hierarchical task structures, as well as strong\ncapabilities in reasoning and planning. To achieve this, prior work typically\nrelies on extensive training on the target dataset, which often results in\nsignificant dataset bias and a lack of generalization to unseen tasks. In this\nwork, we introduce VidAssist, an integrated framework designed for\nzero/few-shot goal-oriented planning in instructional videos. VidAssist\nleverages large language models (LLMs) as both the knowledge base and the\nassessment tool for generating and evaluating action plans, thus overcoming the\nchallenges of acquiring procedural knowledge from small-scale, low-diversity\ndatasets. Moreover, VidAssist employs a breadth-first search algorithm for\noptimal plan generation, in which a composite of value functions designed for\ngoal-oriented planning is utilized to assess the predicted actions at each\nstep. Extensive experiments demonstrate that VidAssist offers a unified\nframework for different goal-oriented planning setups, e.g., visual planning\nfor assistance (VPA) and procedural planning (PP), and achieves remarkable\nperformance in zero-shot and few-shot setups. Specifically, our few-shot model\noutperforms the prior fully supervised state-of-the-art method by +7.7% in VPA\nand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,\nand models are publicly available at https://sites.google.com/view/vidassist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented planning, or anticipating a series of actions that transition\nan agent from its current state to a predefined objective, is crucial for\ndeveloping intelligent assistants aiding users in daily procedural tasks. The\nproblem presents significant challenges due to the need for comprehensive\nknowledge of temporal and hierarchical task structures, as well as strong\ncapabilities in reasoning and planning. To achieve this, prior work typically\nrelies on extensive training on the target dataset, which often results in\nsignificant dataset bias and a lack of generalization to unseen tasks. In this\nwork, we introduce VidAssist, an integrated framework designed for\nzero/few-shot goal-oriented planning in instructional videos. VidAssist\nleverages large language models (LLMs) as both the knowledge base and the\nassessment tool for generating and evaluating action plans, thus overcoming the\nchallenges of acquiring procedural knowledge from small-scale, low-diversity\ndatasets. Moreover, VidAssist employs a breadth-first search algorithm for\noptimal plan generation, in which a composite of value functions designed for\ngoal-oriented planning is utilized to assess the predicted actions at each\nstep. Extensive experiments demonstrate that VidAssist offers a unified\nframework for different goal-oriented planning setups, e.g., visual planning\nfor assistance (VPA) and procedural planning (PP), and achieves remarkable\nperformance in zero-shot and few-shot setups. Specifically, our few-shot model\noutperforms the prior fully supervised state-of-the-art method by +7.7% in VPA\nand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,\nand models are publicly available at https://sites.google.com/view/vidassist."
                },
                "authors": [
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Fu-Jen Chu"
                    },
                    {
                        "name": "Kris Kitani"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Xitong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Yang"
                },
                "author": "Xitong Yang",
                "arxiv_comment": "Accepted by ECCV 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20551v1",
                "updated": "2024-09-30T17:52:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    52,
                    5,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:52:05Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    52,
                    5,
                    0,
                    274,
                    0
                ],
                "title": "UniAff: A Unified Representation of Affordances for Tool Usage and\n  Articulation with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAff: A Unified Representation of Affordances for Tool Usage and\n  Articulation with Vision-Language Models"
                },
                "summary": "Previous studies on robotic manipulation are based on a limited understanding\nof the underlying 3D motion constraints and affordances. To address these\nchallenges, we propose a comprehensive paradigm, termed UniAff, that integrates\n3D object-centric manipulation and task understanding in a unified formulation.\nSpecifically, we constructed a dataset labeled with manipulation-related key\nattributes, comprising 900 articulated objects from 19 categories and 600 tools\nfrom 12 categories. Furthermore, we leverage MLLMs to infer object-centric\nrepresentations for manipulation tasks, including affordance recognition and\nreasoning about 3D motion constraints. Comprehensive experiments in both\nsimulation and real-world settings indicate that UniAff significantly improves\nthe generalization of robotic manipulation for tools and articulated objects.\nWe hope that UniAff will serve as a general baseline for unified robotic\nmanipulation tasks in the future. Images, videos, dataset, and code are\npublished on the project website at:https://sites.google.com/view/uni-aff/home",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies on robotic manipulation are based on a limited understanding\nof the underlying 3D motion constraints and affordances. To address these\nchallenges, we propose a comprehensive paradigm, termed UniAff, that integrates\n3D object-centric manipulation and task understanding in a unified formulation.\nSpecifically, we constructed a dataset labeled with manipulation-related key\nattributes, comprising 900 articulated objects from 19 categories and 600 tools\nfrom 12 categories. Furthermore, we leverage MLLMs to infer object-centric\nrepresentations for manipulation tasks, including affordance recognition and\nreasoning about 3D motion constraints. Comprehensive experiments in both\nsimulation and real-world settings indicate that UniAff significantly improves\nthe generalization of robotic manipulation for tools and articulated objects.\nWe hope that UniAff will serve as a general baseline for unified robotic\nmanipulation tasks in the future. Images, videos, dataset, and code are\npublished on the project website at:https://sites.google.com/view/uni-aff/home"
                },
                "authors": [
                    {
                        "name": "Qiaojun Yu"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Xibin Yuan"
                    },
                    {
                        "name": "Zhengkai Jiang"
                    },
                    {
                        "name": "Ce Hao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haonan Chang"
                    },
                    {
                        "name": "Junbo Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Cewu Lu"
                    }
                ],
                "author_detail": {
                    "name": "Cewu Lu"
                },
                "author": "Cewu Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20550v1",
                "updated": "2024-09-30T17:51:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    15,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    15,
                    0,
                    274,
                    0
                ],
                "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation"
                },
                "summary": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination"
                },
                "authors": [
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "11 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11634v2",
                "updated": "2024-09-30T17:51:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    11,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-17T15:14:10Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    14,
                    10,
                    0,
                    169,
                    0
                ],
                "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating\n  Test-Taking Strategies from Benchmark Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating\n  Test-Taking Strategies from Benchmark Performance"
                },
                "summary": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter."
                },
                "authors": [
                    {
                        "name": "Kyle Moore"
                    },
                    {
                        "name": "Jesse Roberts"
                    },
                    {
                        "name": "Thao Pham"
                    },
                    {
                        "name": "Oseremhen Ewaleifoh"
                    },
                    {
                        "name": "Doug Fisher"
                    }
                ],
                "author_detail": {
                    "name": "Doug Fisher"
                },
                "author": "Doug Fisher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20548v1",
                "updated": "2024-09-30T17:49:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    49,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:49:09Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    49,
                    9,
                    0,
                    274,
                    0
                ],
                "title": "Robi Butler: Remote Multimodal Interactions with Household Robot\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robi Butler: Remote Multimodal Interactions with Household Robot\n  Assistant"
                },
                "summary": "In this paper, we introduce Robi Butler, a novel household robotic system\nthat enables multimodal interactions with remote users. Building on the\nadvanced communication interfaces, Robi Butler allows users to monitor the\nrobot's status, send text or voice instructions, and select target objects by\nhand pointing. At the core of our system is a high-level behavior module,\npowered by Large Language Models (LLMs), that interprets multimodal\ninstructions to generate action plans. These plans are composed of a set of\nopen vocabulary primitives supported by Vision Language Models (VLMs) that\nhandle both text and pointing queries. The integration of the above components\nallows Robi Butler to ground remote multimodal instructions in the real-world\nhome environment in a zero-shot manner. We demonstrate the effectiveness and\nefficiency of this system using a variety of daily household tasks that involve\nremote users giving multimodal instructions. Additionally, we conducted a user\nstudy to analyze how multimodal interactions affect efficiency and user\nexperience during remote human-robot interaction and discuss the potential\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Robi Butler, a novel household robotic system\nthat enables multimodal interactions with remote users. Building on the\nadvanced communication interfaces, Robi Butler allows users to monitor the\nrobot's status, send text or voice instructions, and select target objects by\nhand pointing. At the core of our system is a high-level behavior module,\npowered by Large Language Models (LLMs), that interprets multimodal\ninstructions to generate action plans. These plans are composed of a set of\nopen vocabulary primitives supported by Vision Language Models (VLMs) that\nhandle both text and pointing queries. The integration of the above components\nallows Robi Butler to ground remote multimodal instructions in the real-world\nhome environment in a zero-shot manner. We demonstrate the effectiveness and\nefficiency of this system using a variety of daily household tasks that involve\nremote users giving multimodal instructions. Additionally, we conducted a user\nstudy to analyze how multimodal interactions affect efficiency and user\nexperience during remote human-robot interaction and discuss the potential\nimprovements."
                },
                "authors": [
                    {
                        "name": "Anxing Xiao"
                    },
                    {
                        "name": "Nuwan Janaka"
                    },
                    {
                        "name": "Tianrun Hu"
                    },
                    {
                        "name": "Anshul Gupta"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Cunjun Yu"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20547v1",
                "updated": "2024-09-30T17:48:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    48,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:48:22Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    48,
                    22,
                    0,
                    274,
                    0
                ],
                "title": "Annealing Flow Generative Model Towards Sampling High-Dimensional and\n  Multi-Modal Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annealing Flow Generative Model Towards Sampling High-Dimensional and\n  Multi-Modal Distributions"
                },
                "summary": "Sampling from high-dimensional, multi-modal distributions remains a\nfundamental challenge across domains such as statistical Bayesian inference and\nphysics-based machine learning. In this paper, we propose Annealing Flow (AF),\na continuous normalizing flow-based approach designed to sample from\nhigh-dimensional and multi-modal distributions. The key idea is to learn a\ncontinuous normalizing flow-based transport map, guided by annealing, to\ntransition samples from an easy-to-sample distribution to the target\ndistribution, facilitating effective exploration of modes in high-dimensional\nspaces. Unlike many existing methods, AF training does not rely on samples from\nthe target distribution. AF ensures effective and balanced mode exploration,\nachieves linear complexity in sample size and dimensions, and circumvents\ninefficient mixing times. We demonstrate the superior performance of AF\ncompared to state-of-the-art methods through extensive experiments on various\nchallenging distributions and real-world datasets, particularly in\nhigh-dimensional and multi-modal settings. We also highlight the potential of\nAF for sampling the least favorable distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling from high-dimensional, multi-modal distributions remains a\nfundamental challenge across domains such as statistical Bayesian inference and\nphysics-based machine learning. In this paper, we propose Annealing Flow (AF),\na continuous normalizing flow-based approach designed to sample from\nhigh-dimensional and multi-modal distributions. The key idea is to learn a\ncontinuous normalizing flow-based transport map, guided by annealing, to\ntransition samples from an easy-to-sample distribution to the target\ndistribution, facilitating effective exploration of modes in high-dimensional\nspaces. Unlike many existing methods, AF training does not rely on samples from\nthe target distribution. AF ensures effective and balanced mode exploration,\nachieves linear complexity in sample size and dimensions, and circumvents\ninefficient mixing times. We demonstrate the superior performance of AF\ncompared to state-of-the-art methods through extensive experiments on various\nchallenging distributions and real-world datasets, particularly in\nhigh-dimensional and multi-modal settings. We also highlight the potential of\nAF for sampling the least favorable distributions."
                },
                "authors": [
                    {
                        "name": "Dongze Wu"
                    },
                    {
                        "name": "Yao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yao Xie"
                },
                "author": "Yao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01261v2",
                "updated": "2024-09-30T17:39:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    39,
                    59,
                    0,
                    274,
                    0
                ],
                "published": "2024-04-01T17:33:38Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    33,
                    38,
                    0,
                    92,
                    0
                ],
                "title": "FABLES: Evaluating faithfulness and content selection in book-length\n  summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FABLES: Evaluating faithfulness and content selection in book-length\n  summarization"
                },
                "summary": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook."
                },
                "authors": [
                    {
                        "name": "Yekyung Kim"
                    },
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Aparna Garimella"
                    },
                    {
                        "name": "Varun Manjunatha"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "preprint - 39 pages",
                "arxiv_journal_ref": "1st Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20536v1",
                "updated": "2024-09-30T17:39:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    39,
                    38,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:39:38Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    39,
                    38,
                    0,
                    274,
                    0
                ],
                "title": "Best Practices for Responsible Machine Learning in Credit Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best Practices for Responsible Machine Learning in Credit Scoring"
                },
                "summary": "The widespread use of machine learning in credit scoring has brought\nsignificant advancements in risk assessment and decision-making. However, it\nhas also raised concerns about potential biases, discrimination, and lack of\ntransparency in these automated systems. This tutorial paper performed a\nnon-systematic literature review to guide best practices for developing\nresponsible machine learning models in credit scoring, focusing on fairness,\nreject inference, and explainability. We discuss definitions, metrics, and\ntechniques for mitigating biases and ensuring equitable outcomes across\ndifferent groups. Additionally, we address the issue of limited data\nrepresentativeness by exploring reject inference methods that incorporate\ninformation from rejected loan applications. Finally, we emphasize the\nimportance of transparency and explainability in credit models, discussing\ntechniques that provide insights into the decision-making process and enable\nindividuals to understand and potentially improve their creditworthiness. By\nadopting these best practices, financial institutions can harness the power of\nmachine learning while upholding ethical and responsible lending practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of machine learning in credit scoring has brought\nsignificant advancements in risk assessment and decision-making. However, it\nhas also raised concerns about potential biases, discrimination, and lack of\ntransparency in these automated systems. This tutorial paper performed a\nnon-systematic literature review to guide best practices for developing\nresponsible machine learning models in credit scoring, focusing on fairness,\nreject inference, and explainability. We discuss definitions, metrics, and\ntechniques for mitigating biases and ensuring equitable outcomes across\ndifferent groups. Additionally, we address the issue of limited data\nrepresentativeness by exploring reject inference methods that incorporate\ninformation from rejected loan applications. Finally, we emphasize the\nimportance of transparency and explainability in credit models, discussing\ntechniques that provide insights into the decision-making process and enable\nindividuals to understand and potentially improve their creditworthiness. By\nadopting these best practices, financial institutions can harness the power of\nmachine learning while upholding ethical and responsible lending practices."
                },
                "authors": [
                    {
                        "name": "Giovani Valdrighi"
                    },
                    {
                        "name": "Athyrson M. Ribeiro"
                    },
                    {
                        "name": "Jansen S. B. Pereira"
                    },
                    {
                        "name": "Vitoria Guardieiro"
                    },
                    {
                        "name": "Arthur Hendricks"
                    },
                    {
                        "name": "Décio Miranda Filho"
                    },
                    {
                        "name": "Juan David Nieto Garcia"
                    },
                    {
                        "name": "Felipe F. Bocca"
                    },
                    {
                        "name": "Thalita B. Veronese"
                    },
                    {
                        "name": "Lucas Wanner"
                    },
                    {
                        "name": "Marcos Medeiros Raimundo"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Medeiros Raimundo"
                },
                "author": "Marcos Medeiros Raimundo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20533v1",
                "updated": "2024-09-30T17:37:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    37,
                    31,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:37:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    37,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "The eventful life of a luminous galaxy at z = 14: metal enrichment,\n  feedback, and low gas fraction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The eventful life of a luminous galaxy at z = 14: metal enrichment,\n  feedback, and low gas fraction?"
                },
                "summary": "JADES-GS-z14-0 is the most distant spectroscopically confirmed galaxy so far,\nat $z>14$. With a UV magnitude of -20.81, it is one of the most luminous\ngalaxies at cosmic dawn and its half-light radius of 260 pc means that stars\ndominate the observed UV emission. We report the ALMA detection of\n[OIII]88$\\mu$m line emission with a significance of 6.67$\\sigma$ and at a\nfrequency of 223.524 GHz, corresponding to a redshift of $14.1796\\pm0.0007$,\nwhich is consistent with the candidate CIII] line detected in the NIRSpec\nspectrum. At this spectroscopic redshift, the Lyman break identified with\nNIRSpec requires a damped Lyman-$\\alpha$ absorber with a column density of\n$\\log(N_{\\rm HI}/\\mathrm{cm}^{-2})=22.23$. The total [OIII]88$\\mu$m luminosity\n(log($(L_{\\rm [OIII]}/L_\\odot) = 8.3\\pm0.1$) is fully consistent with the local\n$L_{\\rm [OIII]}-SFR$ relation. Based on the ${L_{\\rm [OIII]}/SFR}$, we infer a\ngas-phase metallicity $>0.1~{\\rm Z_{\\rm \\odot}}$, which is somewhat unexpected\ngiven the weakness of the UV emission lines. Using prospector SED modeling and\ncombining the ALMA data with JWST observations, we find $Z=0.17~{Z_{\\rm\n\\odot}}$ and an escape fraction of ionizing photons of 20%, which is necessary\nto explain the UV spectrum. We measure an [O III]5007\\r{A}/[O III]88$\\mu$m line\nflux ratio between 1 and 10, resulting in an upper limit to the electron\ndensity of roughly 300 cm$^{-3}$, which is lower than those measured in other\nhigh-$z$ luminous galaxies. The [OIII]88$\\mu$m emission line is spectrally\nresolved, with a FWHM of 100 km/s, resulting in a dynamical mass of\n$\\log$(M$_{\\rm dyn}/M_\\odot$) = 9.0$\\pm0.2$. This value is comparable to the\nstellar mass derived from the SED fitting, which implies a very low gas\nfraction. Past radiation-driven outflows may have cleared the galaxy from the\ngas, reducing the gas fraction and thus increasing the escape fraction of\nionizing photons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES-GS-z14-0 is the most distant spectroscopically confirmed galaxy so far,\nat $z>14$. With a UV magnitude of -20.81, it is one of the most luminous\ngalaxies at cosmic dawn and its half-light radius of 260 pc means that stars\ndominate the observed UV emission. We report the ALMA detection of\n[OIII]88$\\mu$m line emission with a significance of 6.67$\\sigma$ and at a\nfrequency of 223.524 GHz, corresponding to a redshift of $14.1796\\pm0.0007$,\nwhich is consistent with the candidate CIII] line detected in the NIRSpec\nspectrum. At this spectroscopic redshift, the Lyman break identified with\nNIRSpec requires a damped Lyman-$\\alpha$ absorber with a column density of\n$\\log(N_{\\rm HI}/\\mathrm{cm}^{-2})=22.23$. The total [OIII]88$\\mu$m luminosity\n(log($(L_{\\rm [OIII]}/L_\\odot) = 8.3\\pm0.1$) is fully consistent with the local\n$L_{\\rm [OIII]}-SFR$ relation. Based on the ${L_{\\rm [OIII]}/SFR}$, we infer a\ngas-phase metallicity $>0.1~{\\rm Z_{\\rm \\odot}}$, which is somewhat unexpected\ngiven the weakness of the UV emission lines. Using prospector SED modeling and\ncombining the ALMA data with JWST observations, we find $Z=0.17~{Z_{\\rm\n\\odot}}$ and an escape fraction of ionizing photons of 20%, which is necessary\nto explain the UV spectrum. We measure an [O III]5007\\r{A}/[O III]88$\\mu$m line\nflux ratio between 1 and 10, resulting in an upper limit to the electron\ndensity of roughly 300 cm$^{-3}$, which is lower than those measured in other\nhigh-$z$ luminous galaxies. The [OIII]88$\\mu$m emission line is spectrally\nresolved, with a FWHM of 100 km/s, resulting in a dynamical mass of\n$\\log$(M$_{\\rm dyn}/M_\\odot$) = 9.0$\\pm0.2$. This value is comparable to the\nstellar mass derived from the SED fitting, which implies a very low gas\nfraction. Past radiation-driven outflows may have cleared the galaxy from the\ngas, reducing the gas fraction and thus increasing the escape fraction of\nionizing photons."
                },
                "authors": [
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Xihan Ji"
                    },
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Fengwu Sun"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Tom J. L. C. Bakx"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Jorge A. Zavala"
                    },
                    {
                        "name": "Kevin Hainline"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Stacey Alberts"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stéphane Charlot"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Peter Jakobsen"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Chris Willott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Willott"
                },
                "author": "Chris Willott",
                "arxiv_comment": "12 pages, 7 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00222v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00222v4",
                "updated": "2024-09-30T17:37:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    37,
                    16,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-30T19:26:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    26,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Can Large Language Models Address Open-Target Stance Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Address Open-Target Stance Detection?"
                },
                "summary": "Stance detection (SD) identifies a text's position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs)\nGPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the only\nexisting work, Target-Stance Extraction (TSE), which benefits from predefined\ntargets. Unlike TSE, OTSD removes the dependency of a predefined list, making\ntarget generation and evaluation more challenging. We also provide a metric for\nevaluating target quality that correlates well with human judgment. Our\nexperiments reveal that LLMs outperform TSE in target generation when the real\ntarget is explicitly and not explicitly mentioned in the text. Likewise, for\nstance detection, LLMs excel in explicit cases with comparable performance in\nnon-explicit in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection (SD) identifies a text's position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs)\nGPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the only\nexisting work, Target-Stance Extraction (TSE), which benefits from predefined\ntargets. Unlike TSE, OTSD removes the dependency of a predefined list, making\ntarget generation and evaluation more challenging. We also provide a metric for\nevaluating target quality that correlates well with human judgment. Our\nexperiments reveal that LLMs outperform TSE in target generation when the real\ntarget is explicitly and not explicitly mentioned in the text. Likewise, for\nstance detection, LLMs excel in explicit cases with comparable performance in\nnon-explicit in general."
                },
                "authors": [
                    {
                        "name": "Abu Ubaida Akash"
                    },
                    {
                        "name": "Ahmed Fahmy"
                    },
                    {
                        "name": "Amine Trabelsi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Trabelsi"
                },
                "author": "Amine Trabelsi",
                "arxiv_comment": "14 pages; currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00222v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00222v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12098v2",
                "updated": "2024-09-30T17:22:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    22,
                    55,
                    0,
                    274,
                    0
                ],
                "published": "2023-11-20T19:00:01Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    19,
                    0,
                    1,
                    0,
                    324,
                    0
                ],
                "title": "Union Through UNITY: Cosmology with 2,000 SNe Using a Unified Bayesian\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Union Through UNITY: Cosmology with 2,000 SNe Using a Unified Bayesian\n  Framework"
                },
                "summary": "Type Ia supernovae (SNe Ia) were instrumental in establishing the\nacceleration of the universe's expansion. By virtue of their combination of\ndistance reach, precision, and prevalence, they continue to provide key\ncosmological constraints, complementing other cosmological probes. Individual\nSN surveys cover only over about a factor of two in redshift, so compilations\nof multiple SN datasets are strongly beneficial. We assemble an up-to-date\n\"Union\" compilation of 2087 cosmologically useful SNe Ia from 24 datasets\n(\"Union3\"). We take care to put all SNe on the same distance scale and update\nthe light-curve fitting with SALT3 to use the full rest-frame optical. Over the\nnext few years, the number of cosmologically useful SNe Ia will increase by\nmore than a factor of ten, and keeping systematic uncertainties subdominant\nwill be more challenging than ever. We discuss the importance of treating\noutliers, selection effects, light-curve shape and color populations and\nstandardization relations, unexplained dispersion, and heterogeneous\nobservations simultaneously. We present an updated Bayesian framework, called\nUNITY1.5 (Unified Nonlinear Inference for Type-Ia cosmologY), that incorporates\nsignificant improvements in our ability to model selection effects,\nstandardization, and systematic uncertainties compared to earlier analyses. As\nan analysis byproduct, we also recover the posterior of the SN-only\npeculiar-velocity field, although we do not interpret it in this work. We\ncompute updated cosmological constraints with Union3 and UNITY1.5, finding weak\n1.7--2.6 sigma tension with LambdaCDM and possible evidence for thawing dark\nenergy (w0 > -1, wa < 0). We release our SN distances, light-curve fits, and\nUNITY1.5 framework to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type Ia supernovae (SNe Ia) were instrumental in establishing the\nacceleration of the universe's expansion. By virtue of their combination of\ndistance reach, precision, and prevalence, they continue to provide key\ncosmological constraints, complementing other cosmological probes. Individual\nSN surveys cover only over about a factor of two in redshift, so compilations\nof multiple SN datasets are strongly beneficial. We assemble an up-to-date\n\"Union\" compilation of 2087 cosmologically useful SNe Ia from 24 datasets\n(\"Union3\"). We take care to put all SNe on the same distance scale and update\nthe light-curve fitting with SALT3 to use the full rest-frame optical. Over the\nnext few years, the number of cosmologically useful SNe Ia will increase by\nmore than a factor of ten, and keeping systematic uncertainties subdominant\nwill be more challenging than ever. We discuss the importance of treating\noutliers, selection effects, light-curve shape and color populations and\nstandardization relations, unexplained dispersion, and heterogeneous\nobservations simultaneously. We present an updated Bayesian framework, called\nUNITY1.5 (Unified Nonlinear Inference for Type-Ia cosmologY), that incorporates\nsignificant improvements in our ability to model selection effects,\nstandardization, and systematic uncertainties compared to earlier analyses. As\nan analysis byproduct, we also recover the posterior of the SN-only\npeculiar-velocity field, although we do not interpret it in this work. We\ncompute updated cosmological constraints with Union3 and UNITY1.5, finding weak\n1.7--2.6 sigma tension with LambdaCDM and possible evidence for thawing dark\nenergy (w0 > -1, wa < 0). We release our SN distances, light-curve fits, and\nUNITY1.5 framework to the community."
                },
                "authors": [
                    {
                        "name": "David Rubin"
                    },
                    {
                        "name": "Greg Aldering"
                    },
                    {
                        "name": "Marc Betoule"
                    },
                    {
                        "name": "Andy Fruchter"
                    },
                    {
                        "name": "Xiaosheng Huang"
                    },
                    {
                        "name": "Alex G. Kim"
                    },
                    {
                        "name": "Chris Lidman"
                    },
                    {
                        "name": "Eric Linder"
                    },
                    {
                        "name": "Saul Perlmutter"
                    },
                    {
                        "name": "Pilar Ruiz-Lapuente"
                    },
                    {
                        "name": "Nao Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Nao Suzuki"
                },
                "author": "Nao Suzuki",
                "arxiv_comment": "Under review at ApJ, 12 new figures, no changes to cosmological\n  results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00746v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00746v7",
                "updated": "2024-09-30T17:22:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    22,
                    1,
                    0,
                    274,
                    0
                ],
                "published": "2024-02-01T16:40:32Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    16,
                    40,
                    32,
                    3,
                    32,
                    0
                ],
                "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System"
                },
                "summary": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00746v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00746v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20519v1",
                "updated": "2024-09-30T17:20:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    20,
                    19,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:20:19Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    20,
                    19,
                    0,
                    274,
                    0
                ],
                "title": "A physically-motivated template set for high-z galaxy SED fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A physically-motivated template set for high-z galaxy SED fitting"
                },
                "summary": "We introduce a new physically-motivated spectral template set for fitting the\nspectral energy distributions (SEDs) of high-z galaxies. We use the public\ngalaxy formation code ARES to generate star formation histories of thirteen\nrepresentative galaxies with diverse masses and generate their predicted\nspectra across a set of redshifts at z > 6. The model parameters are calibrated\nto reproduce the properties of z > 6 galaxies observed by HST. Motivated by the\napparent importance of bursty star formation at high redshifts, we also include\ntemplates with recent starbursts. We use these templates with the SED-fitting\ncode EAZY to analyze both an independent theoretical model and a public sample\nof JWST-observed galaxies from the JADES survey. The comparison with a\nsemi-analytic model demonstrates that our fitting framework accurately measures\nthe galaxy properties, even when the underlying assumptions of the model differ\nfrom ours. Our preliminary application to JWST data shows that galaxies at z >\n8 are often bursty (especially at small galaxy masses), follow a star-forming\nmain sequence similar to those at lower redshift (albeit with a higher\nnormalization), and form stars earlier than expected in ares. Our SED-fitting\nframework is very fast (thanks to the efficiency of EAZY) but provides full\ninferred star formation histories for each source. Additionally, it enables a\ndirect comparison to theoretical models and helps point toward improvements\nnecessary in those models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new physically-motivated spectral template set for fitting the\nspectral energy distributions (SEDs) of high-z galaxies. We use the public\ngalaxy formation code ARES to generate star formation histories of thirteen\nrepresentative galaxies with diverse masses and generate their predicted\nspectra across a set of redshifts at z > 6. The model parameters are calibrated\nto reproduce the properties of z > 6 galaxies observed by HST. Motivated by the\napparent importance of bursty star formation at high redshifts, we also include\ntemplates with recent starbursts. We use these templates with the SED-fitting\ncode EAZY to analyze both an independent theoretical model and a public sample\nof JWST-observed galaxies from the JADES survey. The comparison with a\nsemi-analytic model demonstrates that our fitting framework accurately measures\nthe galaxy properties, even when the underlying assumptions of the model differ\nfrom ours. Our preliminary application to JWST data shows that galaxies at z >\n8 are often bursty (especially at small galaxy masses), follow a star-forming\nmain sequence similar to those at lower redshift (albeit with a higher\nnormalization), and form stars earlier than expected in ares. Our SED-fitting\nframework is very fast (thanks to the efficiency of EAZY) but provides full\ninferred star formation histories for each source. Additionally, it enables a\ndirect comparison to theoretical models and helps point toward improvements\nnecessary in those models."
                },
                "authors": [
                    {
                        "name": "Judah Luberto"
                    },
                    {
                        "name": "Steven Furlanetto"
                    },
                    {
                        "name": "Jordan Mirocha"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Mirocha"
                },
                "author": "Jordan Mirocha",
                "arxiv_comment": "31 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v3",
                "updated": "2024-10-01T08:50:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    50,
                    1,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20512v1",
                "updated": "2024-09-30T17:13:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    13,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:13:40Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    13,
                    40,
                    0,
                    274,
                    0
                ],
                "title": "Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis\n  of Perovskite via Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis\n  of Perovskite via Language Models"
                },
                "summary": "The challenge of accurately predicting toxicity of industrial solvents used\nin perovskite synthesis is a necessary undertaking but is limited by a lack of\na targeted and structured toxicity data. This paper presents a novel framework\nthat combines an automated data extraction using language models, and an\nuncertainty-informed prediction model to fill data gaps and improve prediction\nconfidence. First, we have utilized and compared two approaches to\nautomatically extract relevant data from a corpus of scientific literature on\nsolvents used in perovskite synthesis: smaller bidirectional language models\nlike BERT and ELMo are used for their repeatability and deterministic outputs,\nwhile autoregressive large language model (LLM) such as GPT-3.5 is used to\nleverage its larger training corpus and better response generation. Our novel\n'prompting and verification' technique integrated with an LLM aims at targeted\nextraction and refinement, thereby reducing hallucination and improving the\nquality of the extracted data using the LLM. Next, the extracted data is fed\ninto our pre-trained multi-task binary classification deep learning to predict\nthe ED nature of extracted solvents. We have used a Shannon entropy-based\nuncertainty quantification utilizing the class probabilities obtained from the\nclassification model to quantify uncertainty and identify data gaps in our\npredictions. This approach leads to the curation of a structured dataset for\nsolvents used in perovskite synthesis and their uncertainty-informed virtual\ntoxicity assessment. Additionally, chord diagrams have been used to visualize\nsolvent interactions and prioritize those with potential hazards, revealing\nthat 70% of the solvent interactions were primarily associated with two\nspecific perovskites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of accurately predicting toxicity of industrial solvents used\nin perovskite synthesis is a necessary undertaking but is limited by a lack of\na targeted and structured toxicity data. This paper presents a novel framework\nthat combines an automated data extraction using language models, and an\nuncertainty-informed prediction model to fill data gaps and improve prediction\nconfidence. First, we have utilized and compared two approaches to\nautomatically extract relevant data from a corpus of scientific literature on\nsolvents used in perovskite synthesis: smaller bidirectional language models\nlike BERT and ELMo are used for their repeatability and deterministic outputs,\nwhile autoregressive large language model (LLM) such as GPT-3.5 is used to\nleverage its larger training corpus and better response generation. Our novel\n'prompting and verification' technique integrated with an LLM aims at targeted\nextraction and refinement, thereby reducing hallucination and improving the\nquality of the extracted data using the LLM. Next, the extracted data is fed\ninto our pre-trained multi-task binary classification deep learning to predict\nthe ED nature of extracted solvents. We have used a Shannon entropy-based\nuncertainty quantification utilizing the class probabilities obtained from the\nclassification model to quantify uncertainty and identify data gaps in our\npredictions. This approach leads to the curation of a structured dataset for\nsolvents used in perovskite synthesis and their uncertainty-informed virtual\ntoxicity assessment. Additionally, chord diagrams have been used to visualize\nsolvent interactions and prioritize those with potential hazards, revealing\nthat 70% of the solvent interactions were primarily associated with two\nspecific perovskites."
                },
                "authors": [
                    {
                        "name": "Arpan Mukherjee"
                    },
                    {
                        "name": "Deepesh Giri"
                    },
                    {
                        "name": "Krishna Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Krishna Rajan"
                },
                "author": "Krishna Rajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17041v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17041v3",
                "updated": "2024-09-30T17:12:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    12,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2023-11-28T18:53:06Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    18,
                    53,
                    6,
                    1,
                    332,
                    0
                ],
                "title": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties"
                },
                "summary": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}."
                },
                "authors": [
                    {
                        "name": "Keunwoo Peter Yu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Fengyuan Hu"
                    },
                    {
                        "name": "Shane Storks"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "16 pages, LaTeX; Accepted to EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17041v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17041v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20507v1",
                "updated": "2024-09-30T17:09:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    9,
                    18,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:09:18Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    9,
                    18,
                    0,
                    274,
                    0
                ],
                "title": "Constraining Cosmology with Simulation-based inference and Optical\n  Galaxy Cluster Abundance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining Cosmology with Simulation-based inference and Optical\n  Galaxy Cluster Abundance"
                },
                "summary": "We test the robustness of simulation-based inference (SBI) in the context of\ncosmological parameter estimation from galaxy cluster counts and masses in\nsimulated optical datasets. We construct ``simulations'' using analytical\nmodels for the galaxy cluster halo mass function (HMF) and for the observed\nrichness (number of observed member galaxies) to train and test the SBI method.\nWe compare the SBI parameter posterior samples to those from an MCMC analysis\nthat uses the same analytical models to construct predictions of the observed\ndata vector. The two methods exhibit comparable performance, with reliable\nconstraints derived for the primary cosmological parameters, ($\\Omega_m$ and\n$\\sigma_8$), and richness-mass relation parameters. We also perform\nout-of-domain tests with observables constructed from galaxy cluster-sized\nhalos in the Quijote simulations. Again, the SBI and MCMC results have\ncomparable posteriors, with similar uncertainties and biases. Unsurprisingly,\nupon evaluating the SBI method on thousands of simulated data vectors that span\nthe parameter space, SBI exhibits worsened posterior calibration metrics in the\nout-of-domain application. We note that such calibration tests with MCMC is\nless computationally feasible and highlight the potential use of SBI to\nstress-test limitations of analytical models, such as in the use for\nconstructing models for inference with MCMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We test the robustness of simulation-based inference (SBI) in the context of\ncosmological parameter estimation from galaxy cluster counts and masses in\nsimulated optical datasets. We construct ``simulations'' using analytical\nmodels for the galaxy cluster halo mass function (HMF) and for the observed\nrichness (number of observed member galaxies) to train and test the SBI method.\nWe compare the SBI parameter posterior samples to those from an MCMC analysis\nthat uses the same analytical models to construct predictions of the observed\ndata vector. The two methods exhibit comparable performance, with reliable\nconstraints derived for the primary cosmological parameters, ($\\Omega_m$ and\n$\\sigma_8$), and richness-mass relation parameters. We also perform\nout-of-domain tests with observables constructed from galaxy cluster-sized\nhalos in the Quijote simulations. Again, the SBI and MCMC results have\ncomparable posteriors, with similar uncertainties and biases. Unsurprisingly,\nupon evaluating the SBI method on thousands of simulated data vectors that span\nthe parameter space, SBI exhibits worsened posterior calibration metrics in the\nout-of-domain application. We note that such calibration tests with MCMC is\nless computationally feasible and highlight the potential use of SBI to\nstress-test limitations of analytical models, such as in the use for\nconstructing models for inference with MCMC."
                },
                "authors": [
                    {
                        "name": "Moonzarin Reza"
                    },
                    {
                        "name": "Yuanyuan Zhang"
                    },
                    {
                        "name": "Camille Avestruz"
                    },
                    {
                        "name": "Louis E. Strigari"
                    },
                    {
                        "name": "Simone Shevchuk"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Villaescusa-Navarro"
                },
                "author": "Francisco Villaescusa-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20502v1",
                "updated": "2024-09-30T17:02:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    2,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:02:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    2,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "COLLAGE: Collaborative Human-Agent Interaction Generation using\n  Hierarchical Latent Diffusion and Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLLAGE: Collaborative Human-Agent Interaction Generation using\n  Hierarchical Latent Diffusion and Language Models"
                },
                "summary": "We propose a novel framework COLLAGE for generating collaborative\nagent-object-agent interactions by leveraging large language models (LLMs) and\nhierarchical motion-specific vector-quantized variational autoencoders\n(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by\nincorporating the knowledge and reasoning abilities of LLMs to guide a\ngenerative diffusion model. The hierarchical VQ-VAE architecture captures\ndifferent motion-specific characteristics at multiple levels of abstraction,\navoiding redundant concepts and enabling efficient multi-resolution\nrepresentation. We introduce a diffusion model that operates in the latent\nspace and incorporates LLM-generated motion planning cues to guide the\ndenoising process, resulting in prompt-specific motion generation with greater\ncontrol and diversity. Experimental results on the CORE-4D, and InterHuman\ndatasets demonstrate the effectiveness of our approach in generating realistic\nand diverse collaborative human-object-human interactions, outperforming\nstate-of-the-art methods. Our work opens up new possibilities for modeling\ncomplex interactions in various domains, such as robotics, graphics and\ncomputer vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework COLLAGE for generating collaborative\nagent-object-agent interactions by leveraging large language models (LLMs) and\nhierarchical motion-specific vector-quantized variational autoencoders\n(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by\nincorporating the knowledge and reasoning abilities of LLMs to guide a\ngenerative diffusion model. The hierarchical VQ-VAE architecture captures\ndifferent motion-specific characteristics at multiple levels of abstraction,\navoiding redundant concepts and enabling efficient multi-resolution\nrepresentation. We introduce a diffusion model that operates in the latent\nspace and incorporates LLM-generated motion planning cues to guide the\ndenoising process, resulting in prompt-specific motion generation with greater\ncontrol and diversity. Experimental results on the CORE-4D, and InterHuman\ndatasets demonstrate the effectiveness of our approach in generating realistic\nand diverse collaborative human-object-human interactions, outperforming\nstate-of-the-art methods. Our work opens up new possibilities for modeling\ncomplex interactions in various domains, such as robotics, graphics and\ncomputer vision."
                },
                "authors": [
                    {
                        "name": "Divyanshu Daiya"
                    },
                    {
                        "name": "Damon Conover"
                    },
                    {
                        "name": "Aniket Bera"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Bera"
                },
                "author": "Aniket Bera",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.11390v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.11390v3",
                "updated": "2024-09-30T16:41:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    41,
                    29,
                    0,
                    274,
                    0
                ],
                "published": "2023-07-21T07:06:56Z",
                "published_parsed": [
                    2023,
                    7,
                    21,
                    7,
                    6,
                    56,
                    4,
                    202,
                    0
                ],
                "title": "Fast spatial simulation of extreme high-resolution radar precipitation\n  data using INLA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast spatial simulation of extreme high-resolution radar precipitation\n  data using INLA"
                },
                "summary": "Aiming to deliver improved precipitation simulations for hydrological impact\nassessment studies, we develop a methodology for modelling and simulating\nhigh-dimensional spatial precipitation extremes, focusing on both their\nmarginal distributions and tail dependence structures. Tail dependence is\ncrucial for assessing the consequences of extreme precipitation events, yet\nmost stochastic weather generators do not attempt to capture this property. The\nspatial distribution of precipitation occurrences is modelled with four\ncompeting models, while the spatial distribution of nonzero extreme\nprecipitation intensities are modelled with a latent Gaussian version of the\nspatial conditional extremes model. Nonzero precipitation marginal\ndistributions are modelled using latent Gaussian models with gamma and\ngeneralised Pareto likelihoods. Fast inference is achieved using integrated\nnested Laplace approximations (INLA). We model and simulate spatial\nprecipitation extremes in Central Norway, using 13 years of hourly radar data\nwith a spatial resolution of $1 \\times 1$~km$^2$, over an area of size\n$6461$~km$^2$, to describe the behaviour of extreme precipitation over a small\ndrainage area. Inference on this high-dimensional data set is achieved within\nhours, and the simulations capture the main trends of the observed\nprecipitation well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aiming to deliver improved precipitation simulations for hydrological impact\nassessment studies, we develop a methodology for modelling and simulating\nhigh-dimensional spatial precipitation extremes, focusing on both their\nmarginal distributions and tail dependence structures. Tail dependence is\ncrucial for assessing the consequences of extreme precipitation events, yet\nmost stochastic weather generators do not attempt to capture this property. The\nspatial distribution of precipitation occurrences is modelled with four\ncompeting models, while the spatial distribution of nonzero extreme\nprecipitation intensities are modelled with a latent Gaussian version of the\nspatial conditional extremes model. Nonzero precipitation marginal\ndistributions are modelled using latent Gaussian models with gamma and\ngeneralised Pareto likelihoods. Fast inference is achieved using integrated\nnested Laplace approximations (INLA). We model and simulate spatial\nprecipitation extremes in Central Norway, using 13 years of hourly radar data\nwith a spatial resolution of $1 \\times 1$~km$^2$, over an area of size\n$6461$~km$^2$, to describe the behaviour of extreme precipitation over a small\ndrainage area. Inference on this high-dimensional data set is achieved within\nhours, and the simulations capture the main trends of the observed\nprecipitation well."
                },
                "authors": [
                    {
                        "name": "Silius M. Vandeskog"
                    },
                    {
                        "name": "Raphaël Huser"
                    },
                    {
                        "name": "Oddbjørn Bruland"
                    },
                    {
                        "name": "Sara Martino"
                    }
                ],
                "author_detail": {
                    "name": "Sara Martino"
                },
                "author": "Sara Martino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.11390v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.11390v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01270v2",
                "updated": "2024-09-30T16:39:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    39,
                    51,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-01T13:21:33Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    21,
                    33,
                    0,
                    183,
                    0
                ],
                "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit\n  Biases in LLM Open-ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit\n  Biases in LLM Open-ended Text Generation"
                },
                "summary": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured."
                },
                "authors": [
                    {
                        "name": "Serene Lim"
                    },
                    {
                        "name": "María Pérez-Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "María Pérez-Ortiz"
                },
                "author": "María Pérez-Ortiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03843v3",
                "updated": "2024-09-30T16:16:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    16,
                    4,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-06T08:21:30Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    8,
                    21,
                    30,
                    3,
                    158,
                    0
                ],
                "title": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning\n  of Large Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts."
                },
                "authors": [
                    {
                        "name": "Jianben He"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Shiyi Liu"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Claudio Silva"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08648v2",
                "updated": "2024-09-30T16:15:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    15,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-12T21:17:02Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    21,
                    17,
                    2,
                    2,
                    164,
                    0
                ],
                "title": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large\n  Language Models"
                },
                "summary": "When humans create sculptures, we are able to reason about how geometrically\nwe need to alter the clay state to reach our target goal. We are not computing\npoint-wise similarity metrics, or reasoning about low-level positioning of our\ntools, but instead determining the higher-level changes that need to be made.\nIn this work, we propose LLM-Craft, a novel pipeline that leverages large\nlanguage models (LLMs) to iteratively reason about and generate\ndeformation-based crafting action sequences. We simplify and couple the state\nand action representations to further encourage shape-based reasoning. To the\nbest of our knowledge, LLM-Craft is the first system successfully leveraging\nLLMs for complex deformable object interactions. Through our experiments, we\ndemonstrate that with the LLM-Craft framework, LLMs are able to successfully\nreason about the deformation behavior of elasto-plastic objects. Furthermore,\nwe find that LLM-Craft is able to successfully create a set of simple letter\nshapes. Finally, we explore extending the framework to reaching more ambiguous\nsemantic goals, such as \"thinner\" or \"bumpy\". For videos please see our\nwebsite: https://sites.google.com/andrew.cmu.edu/llmcraft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When humans create sculptures, we are able to reason about how geometrically\nwe need to alter the clay state to reach our target goal. We are not computing\npoint-wise similarity metrics, or reasoning about low-level positioning of our\ntools, but instead determining the higher-level changes that need to be made.\nIn this work, we propose LLM-Craft, a novel pipeline that leverages large\nlanguage models (LLMs) to iteratively reason about and generate\ndeformation-based crafting action sequences. We simplify and couple the state\nand action representations to further encourage shape-based reasoning. To the\nbest of our knowledge, LLM-Craft is the first system successfully leveraging\nLLMs for complex deformable object interactions. Through our experiments, we\ndemonstrate that with the LLM-Craft framework, LLMs are able to successfully\nreason about the deformation behavior of elasto-plastic objects. Furthermore,\nwe find that LLM-Craft is able to successfully create a set of simple letter\nshapes. Finally, we explore extending the framework to reaching more ambiguous\nsemantic goals, such as \"thinner\" or \"bumpy\". For videos please see our\nwebsite: https://sites.google.com/andrew.cmu.edu/llmcraft."
                },
                "authors": [
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20447v1",
                "updated": "2024-09-30T16:05:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    5,
                    29,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T16:05:29Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    5,
                    29,
                    0,
                    274,
                    0
                ],
                "title": "POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator"
                },
                "summary": "Neural Architecture Search (NAS) automates neural network design, reducing\ndependence on human expertise. While NAS methods are computationally intensive\nand dataset-specific, auxiliary predictors reduce the models needing training,\ndecreasing search time. This strategy is used to generate architectures\nsatisfying multiple computational constraints. Recently, Transferable NAS has\nemerged, generalizing the search process from dataset-dependent to\ntask-dependent. In this field, DiffusionNAG is a state-of-the-art method. This\ndiffusion-based approach streamlines computation, generating architectures\noptimized for accuracy on unseen datasets without further adaptation. However,\nby focusing solely on accuracy, DiffusionNAG overlooks other crucial objectives\nlike model complexity, computational efficiency, and inference latency --\nfactors essential for deploying models in resource-constrained environments.\nThis paper introduces the Pareto-Optimal Many-Objective Neural Architecture\nGenerator (POMONAG), extending DiffusionNAG via a many-objective diffusion\nprocess. POMONAG simultaneously considers accuracy, number of parameters,\nmultiply-accumulate operations (MACs), and inference latency. It integrates\nPerformance Predictor models to estimate these metrics and guide diffusion\ngradients. POMONAG's optimization is enhanced by expanding its training\nMeta-Dataset, applying Pareto Front Filtering, and refining embeddings for\nconditional generation. These enhancements enable POMONAG to generate\nPareto-optimal architectures that outperform the previous state-of-the-art in\nperformance and efficiency. Results were validated on two search spaces --\nNASBench201 and MobileNetV3 -- and evaluated across 15 image classification\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) automates neural network design, reducing\ndependence on human expertise. While NAS methods are computationally intensive\nand dataset-specific, auxiliary predictors reduce the models needing training,\ndecreasing search time. This strategy is used to generate architectures\nsatisfying multiple computational constraints. Recently, Transferable NAS has\nemerged, generalizing the search process from dataset-dependent to\ntask-dependent. In this field, DiffusionNAG is a state-of-the-art method. This\ndiffusion-based approach streamlines computation, generating architectures\noptimized for accuracy on unseen datasets without further adaptation. However,\nby focusing solely on accuracy, DiffusionNAG overlooks other crucial objectives\nlike model complexity, computational efficiency, and inference latency --\nfactors essential for deploying models in resource-constrained environments.\nThis paper introduces the Pareto-Optimal Many-Objective Neural Architecture\nGenerator (POMONAG), extending DiffusionNAG via a many-objective diffusion\nprocess. POMONAG simultaneously considers accuracy, number of parameters,\nmultiply-accumulate operations (MACs), and inference latency. It integrates\nPerformance Predictor models to estimate these metrics and guide diffusion\ngradients. POMONAG's optimization is enhanced by expanding its training\nMeta-Dataset, applying Pareto Front Filtering, and refining embeddings for\nconditional generation. These enhancements enable POMONAG to generate\nPareto-optimal architectures that outperform the previous state-of-the-art in\nperformance and efficiency. Results were validated on two search spaces --\nNASBench201 and MobileNetV3 -- and evaluated across 15 image classification\ndatasets."
                },
                "authors": [
                    {
                        "name": "Eugenio Lomurno"
                    },
                    {
                        "name": "Samuele Mariani"
                    },
                    {
                        "name": "Matteo Monti"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20441v2",
                "updated": "2024-10-01T06:03:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    3,
                    22,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T16:00:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Instance-adaptive Zero-shot Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance-adaptive Zero-shot Chain-of-Thought Prompting"
                },
                "summary": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Shaotian Yan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Renchu Guan"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16990v2",
                "updated": "2024-09-30T15:49:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    49,
                    6,
                    0,
                    274,
                    0
                ],
                "published": "2024-01-30T13:22:21Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    13,
                    22,
                    21,
                    1,
                    30,
                    0
                ],
                "title": "Recovery and inference of causal effects with sequential adjustment for\n  confounding and attrition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovery and inference of causal effects with sequential adjustment for\n  confounding and attrition"
                },
                "summary": "Confounding bias and selection bias are two significant challenges to the\nvalidity of conclusions drawn from applied causal inference. The latter can\nstem from informative missingness, such as in cases of attrition. We introduce\nthe Sequential Adjustment Criteria (SAC), which extend available graphical\nconditions for recovering causal effects using sequential regressions, allowing\nfor the inclusion of post-exposure and forbidden variables in the admissible\nadjustment sets. We propose an estimator for the recovered Average Treatment\nEffect (ATE) based on Targeted Minimum-Loss Estimation (TMLE), which enjoys\nmultiple robustness under certain conditions. This approach ensures consistency\neven in scenarios where the Double Inverse Probability Weighting (DIPW) and the\nna\\\"ive plug-in sequential regressions approaches fall short. Through a\nsimulation study, we assess the performance of the proposed estimator against\nalternative methods across different graph setups and model specification\nscenarios. As a motivating application, we examine the effect of\npharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD)\nupon the scores obtained by diagnosed Norwegian schoolchildren in national\ntests using observational data ($n=9,352$). Our findings align with the\naccumulated clinical evidence, affirming a positive but small impact of\nmedication on academic achievement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confounding bias and selection bias are two significant challenges to the\nvalidity of conclusions drawn from applied causal inference. The latter can\nstem from informative missingness, such as in cases of attrition. We introduce\nthe Sequential Adjustment Criteria (SAC), which extend available graphical\nconditions for recovering causal effects using sequential regressions, allowing\nfor the inclusion of post-exposure and forbidden variables in the admissible\nadjustment sets. We propose an estimator for the recovered Average Treatment\nEffect (ATE) based on Targeted Minimum-Loss Estimation (TMLE), which enjoys\nmultiple robustness under certain conditions. This approach ensures consistency\neven in scenarios where the Double Inverse Probability Weighting (DIPW) and the\nna\\\"ive plug-in sequential regressions approaches fall short. Through a\nsimulation study, we assess the performance of the proposed estimator against\nalternative methods across different graph setups and model specification\nscenarios. As a motivating application, we examine the effect of\npharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD)\nupon the scores obtained by diagnosed Norwegian schoolchildren in national\ntests using observational data ($n=9,352$). Our findings align with the\naccumulated clinical evidence, affirming a positive but small impact of\nmedication on academic achievement."
                },
                "authors": [
                    {
                        "name": "Johan de Aguas"
                    },
                    {
                        "name": "Johan Pensar"
                    },
                    {
                        "name": "Tomás Varnet Pérez"
                    },
                    {
                        "name": "Guido Biele"
                    }
                ],
                "author_detail": {
                    "name": "Guido Biele"
                },
                "author": "Guido Biele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62A09, 62D20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09972v3",
                "updated": "2024-09-30T15:43:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    43,
                    48,
                    0,
                    274,
                    0
                ],
                "published": "2023-11-16T15:47:30Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    15,
                    47,
                    30,
                    3,
                    320,
                    0
                ],
                "title": "Inference in Auctions with Many Bidders Using Transaction Prices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in Auctions with Many Bidders Using Transaction Prices"
                },
                "summary": "This paper studies inference in first- and second-price sealed-bid auctions\nwith many bidders, using an asymptotic framework where the number of bidders\nincreases while the number of auctions remains fixed. Relevant applications\ninclude online, treasury, spectrum, and art auctions. Our approach enables\nasymptotically exact inference on key features such as the winner's expected\nutility, the seller's expected revenue, and the tail of the valuation\ndistribution using only transaction price data. Our simulations demonstrate the\naccuracy of the methods in finite samples. We apply our methods to Hong Kong\nvehicle license auctions, focusing on high-priced, single-letter plates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies inference in first- and second-price sealed-bid auctions\nwith many bidders, using an asymptotic framework where the number of bidders\nincreases while the number of auctions remains fixed. Relevant applications\ninclude online, treasury, spectrum, and art auctions. Our approach enables\nasymptotically exact inference on key features such as the winner's expected\nutility, the seller's expected revenue, and the tail of the valuation\ndistribution using only transaction price data. Our simulations demonstrate the\naccuracy of the methods in finite samples. We apply our methods to Hong Kong\nvehicle license auctions, focusing on high-priced, single-letter plates."
                },
                "authors": [
                    {
                        "name": "Federico A. Bugni"
                    },
                    {
                        "name": "Yulong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Wang"
                },
                "author": "Yulong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20410v1",
                "updated": "2024-09-30T15:38:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    38,
                    33,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:38:33Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    38,
                    33,
                    0,
                    274,
                    0
                ],
                "title": "Does Positive Reinforcement Work?: A Quasi-Experimental Study of the\n  Effects of Positive Feedback on Reddit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Positive Reinforcement Work?: A Quasi-Experimental Study of the\n  Effects of Positive Feedback on Reddit"
                },
                "summary": "Social media platform design often incorporates explicit signals of positive\nfeedback. Some moderators provide positive feedback with the goal of positive\nreinforcement, but are often unsure of their ability to actually influence user\nbehavior. Despite its widespread use and theory touting positive feedback as\ncrucial for user motivation, its effect on recipients is relatively unknown.\nThis paper examines how positive feedback impacts Reddit users and evaluates\nits differential effects to understand who benefits most from receiving\npositive feedback. Through a causal inference study of 11M posts across 4\nmonths, we find that users who received positive feedback made more frequent\n(2% per day) and higher quality (57% higher score; 2% fewer removals per day)\nposts compared to a set of matched control users. Our findings highlight the\nneed for platforms and communities to expand their perspective on moderation\nand complement punitive approaches with positive reinforcement strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platform design often incorporates explicit signals of positive\nfeedback. Some moderators provide positive feedback with the goal of positive\nreinforcement, but are often unsure of their ability to actually influence user\nbehavior. Despite its widespread use and theory touting positive feedback as\ncrucial for user motivation, its effect on recipients is relatively unknown.\nThis paper examines how positive feedback impacts Reddit users and evaluates\nits differential effects to understand who benefits most from receiving\npositive feedback. Through a causal inference study of 11M posts across 4\nmonths, we find that users who received positive feedback made more frequent\n(2% per day) and higher quality (57% higher score; 2% fewer removals per day)\nposts compared to a set of matched control users. Our findings highlight the\nneed for platforms and communities to expand their perspective on moderation\nand complement punitive approaches with positive reinforcement strategies."
                },
                "authors": [
                    {
                        "name": "Charlotte Lambert"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Eshwar Chandrasekharan"
                    }
                ],
                "author_detail": {
                    "name": "Eshwar Chandrasekharan"
                },
                "author": "Eshwar Chandrasekharan",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14744v2",
                "updated": "2024-09-30T15:36:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    36,
                    26,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-23T06:42:21Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    6,
                    42,
                    21,
                    0,
                    267,
                    0
                ],
                "title": "LINKAGE: Listwise Ranking among Varied-Quality References for\n  Non-Factoid QA Evaluation via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINKAGE: Listwise Ranking among Varied-Quality References for\n  Non-Factoid QA Evaluation via LLMs"
                },
                "summary": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to\ndiverse potential answers and no objective criterion. The commonly used\nautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measure\nsemantic similarities or answers from different perspectives. Recently, Large\nLanguage Models (LLMs) have been resorted to for NFQA evaluation due to their\ncompelling performance on various NLP tasks. Common approaches include\npointwise scoring of each candidate answer and pairwise comparisons between\nanswers. Inspired by the evolution from pointwise to pairwise to listwise in\nlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,\nthat utilizes LLMs to rank candidate answers in a list of reference answers\nsorted by descending quality. Moreover, for NF questions that do not have\nmulti-grade or any golden answers, we leverage LLMs to generate the reference\nanswer list of various quality to facilitate the listwise evaluation. Extensive\nexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and\nWebGLM show that our method has significantly higher correlations with human\nannotations compared to automatic scores and common pointwise and pairwise\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to\ndiverse potential answers and no objective criterion. The commonly used\nautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measure\nsemantic similarities or answers from different perspectives. Recently, Large\nLanguage Models (LLMs) have been resorted to for NFQA evaluation due to their\ncompelling performance on various NLP tasks. Common approaches include\npointwise scoring of each candidate answer and pairwise comparisons between\nanswers. Inspired by the evolution from pointwise to pairwise to listwise in\nlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,\nthat utilizes LLMs to rank candidate answers in a list of reference answers\nsorted by descending quality. Moreover, for NF questions that do not have\nmulti-grade or any golden answers, we leverage LLMs to generate the reference\nanswer list of various quality to facilitate the listwise evaluation. Extensive\nexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and\nWebGLM show that our method has significantly higher correlations with human\nannotations compared to automatic scores and common pointwise and pairwise\napproaches."
                },
                "authors": [
                    {
                        "name": "Sihui Yang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Wanqing Cui"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Published as a conference paper at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01528v3",
                "updated": "2024-09-30T15:30:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    30,
                    25,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-03T16:58:17Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    58,
                    17,
                    0,
                    155,
                    0
                ],
                "title": "Physics-Informed Neural Networks for Dynamic Process Operations with\n  Limited Physical Knowledge and Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Neural Networks for Dynamic Process Operations with\n  Limited Physical Knowledge and Data"
                },
                "summary": "In chemical engineering, process data are expensive to acquire, and complex\nphenomena are difficult to fully model. We explore the use of physics-informed\nneural networks (PINNs) for modeling dynamic processes with incomplete\nmechanistic semi-explicit differential-algebraic equation systems and scarce\nprocess data. In particular, we focus on estimating states for which neither\ndirect observational data nor constitutive equations are available. We propose\nan easy-to-apply heuristic to assess whether estimation of such states may be\npossible. As numerical examples, we consider a continuously stirred tank\nreactor and a liquid-liquid separator. We find that PINNs can infer\nimmeasurable states with reasonable accuracy, even if respective constitutive\nequations are unknown. We thus show that PINNs are capable of modeling\nprocesses when relatively few experimental data and only partially known\nmechanistic descriptions are available, and conclude that they constitute a\npromising avenue that warrants further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In chemical engineering, process data are expensive to acquire, and complex\nphenomena are difficult to fully model. We explore the use of physics-informed\nneural networks (PINNs) for modeling dynamic processes with incomplete\nmechanistic semi-explicit differential-algebraic equation systems and scarce\nprocess data. In particular, we focus on estimating states for which neither\ndirect observational data nor constitutive equations are available. We propose\nan easy-to-apply heuristic to assess whether estimation of such states may be\npossible. As numerical examples, we consider a continuously stirred tank\nreactor and a liquid-liquid separator. We find that PINNs can infer\nimmeasurable states with reasonable accuracy, even if respective constitutive\nequations are unknown. We thus show that PINNs are capable of modeling\nprocesses when relatively few experimental data and only partially known\nmechanistic descriptions are available, and conclude that they constitute a\npromising avenue that warrants further investigation."
                },
                "authors": [
                    {
                        "name": "Mehmet Velioglu"
                    },
                    {
                        "name": "Song Zhai"
                    },
                    {
                        "name": "Sophia Rupprecht"
                    },
                    {
                        "name": "Alexander Mitsos"
                    },
                    {
                        "name": "Andreas Jupke"
                    },
                    {
                        "name": "Manuel Dahmen"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Dahmen"
                },
                "author": "Manuel Dahmen",
                "arxiv_comment": "manuscript (35 pages, 10 figures, 11 tables), supporting materials\n  (15 pages, 4 figures, 5 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20385v1",
                "updated": "2024-09-30T15:20:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    20,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:20:58Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    20,
                    58,
                    0,
                    274,
                    0
                ],
                "title": "Wait, but Tylenol is Acetaminophen... Investigating and Improving\n  Language Models' Ability to Resist Requests for Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wait, but Tylenol is Acetaminophen... Investigating and Improving\n  Language Models' Ability to Resist Requests for Misinformation"
                },
                "summary": "Background: Large language models (LLMs) are trained to follow directions,\nbut this introduces a vulnerability to blindly comply with user requests even\nif they generate wrong information. In medicine, this could accelerate the\ngeneration of misinformation that impacts human well-being.\n  Objectives/Methods: We analyzed compliance to requests to generate misleading\ncontent about medications in settings where models know the request is\nillogical. We investigated whether in-context directions and instruction-tuning\nof LLMs to prioritize logical reasoning over compliance reduced misinformation\nrisk.\n  Results: While all frontier LLMs complied with misinformation requests, both\nprompt-based and parameter-based approaches can improve the detection of logic\nflaws in requests and prevent the dissemination of medical misinformation.\n  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce\nrisks of exploitation for medical misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) are trained to follow directions,\nbut this introduces a vulnerability to blindly comply with user requests even\nif they generate wrong information. In medicine, this could accelerate the\ngeneration of misinformation that impacts human well-being.\n  Objectives/Methods: We analyzed compliance to requests to generate misleading\ncontent about medications in settings where models know the request is\nillogical. We investigated whether in-context directions and instruction-tuning\nof LLMs to prioritize logical reasoning over compliance reduced misinformation\nrisk.\n  Results: While all frontier LLMs complied with misinformation requests, both\nprompt-based and parameter-based approaches can improve the detection of logic\nflaws in requests and prevent the dissemination of medical misinformation.\n  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce\nrisks of exploitation for medical misinformation."
                },
                "authors": [
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Mingye Gao"
                    },
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Brian Anthony"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Hugo Aerts"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Danielle Bitterman"
                    }
                ],
                "author_detail": {
                    "name": "Danielle Bitterman"
                },
                "author": "Danielle Bitterman",
                "arxiv_comment": "Submitted for Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20370v1",
                "updated": "2024-09-30T15:06:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    6,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:06:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    6,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications."
                },
                "authors": [
                    {
                        "name": "Tengyu Xu"
                    },
                    {
                        "name": "Eryk Helenowski"
                    },
                    {
                        "name": "Karthik Abinav Sankararaman"
                    },
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Kaiyan Peng"
                    },
                    {
                        "name": "Eric Han"
                    },
                    {
                        "name": "Shaoliang Nie"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Hejia Zhang"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Zhouhao Zeng"
                    },
                    {
                        "name": "Yun He"
                    },
                    {
                        "name": "Karishma Mandyam"
                    },
                    {
                        "name": "Arya Talabzadeh"
                    },
                    {
                        "name": "Madian Khabsa"
                    },
                    {
                        "name": "Gabriel Cohen"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Sinong Wang"
                    },
                    {
                        "name": "Han Fang"
                    }
                ],
                "author_detail": {
                    "name": "Han Fang"
                },
                "author": "Han Fang",
                "arxiv_comment": "submitted to conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20365v1",
                "updated": "2024-09-30T15:04:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    4,
                    14,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:04:14Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    4,
                    14,
                    0,
                    274,
                    0
                ],
                "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative\n  Spatial-Temporal Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoINSTA: Zero-shot Long Video Understanding via Informative\n  Spatial-Temporal Reasoning with LLMs"
                },
                "summary": "In the video-language domain, recent works in leveraging zero-shot Large\nLanguage Model-based reasoning for video understanding have become competitive\nchallengers to previous end-to-end models. However, long video understanding\npresents unique challenges due to the complexity of reasoning over extended\ntimespans, even for zero-shot LLM-based approaches. The challenge of\ninformation redundancy in long videos prompts the question of what specific\ninformation is essential for large language models (LLMs) and how to leverage\nthem for complex spatial-temporal reasoning in long-form video analysis. We\npropose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for\nzero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot\nframework for long video understanding using LLMs; (2) an event-based temporal\nreasoning and content-based spatial reasoning approach for LLMs to reason over\nspatial-temporal information in videos; (3) a self-reflective information\nreasoning scheme balancing temporal factors based on information sufficiency\nand prediction confidence. Our model significantly improves the\nstate-of-the-art on three long video question-answering benchmarks: EgoSchema,\nNextQA, and IntentQA, and the open question answering dataset ActivityNetQA.\nThe code is released here: https://github.com/mayhugotong/VideoINSTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the video-language domain, recent works in leveraging zero-shot Large\nLanguage Model-based reasoning for video understanding have become competitive\nchallengers to previous end-to-end models. However, long video understanding\npresents unique challenges due to the complexity of reasoning over extended\ntimespans, even for zero-shot LLM-based approaches. The challenge of\ninformation redundancy in long videos prompts the question of what specific\ninformation is essential for large language models (LLMs) and how to leverage\nthem for complex spatial-temporal reasoning in long-form video analysis. We\npropose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for\nzero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot\nframework for long video understanding using LLMs; (2) an event-based temporal\nreasoning and content-based spatial reasoning approach for LLMs to reason over\nspatial-temporal information in videos; (3) a self-reflective information\nreasoning scheme balancing temporal factors based on information sufficiency\nand prediction confidence. Our model significantly improves the\nstate-of-the-art on three long video question-answering benchmarks: EgoSchema,\nNextQA, and IntentQA, and the open question answering dataset ActivityNetQA.\nThe code is released here: https://github.com/mayhugotong/VideoINSTA."
                },
                "authors": [
                    {
                        "name": "Ruotong Liao"
                    },
                    {
                        "name": "Max Erler"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "EMNLP 2024 Findings; 22 pages; Code:\n  https://github.com/mayhugotong/VideoINSTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20364v1",
                "updated": "2024-09-30T15:03:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    3,
                    55,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:03:55Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    3,
                    55,
                    0,
                    274,
                    0
                ],
                "title": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using\n  Large Language Models"
                },
                "summary": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks."
                },
                "authors": [
                    {
                        "name": "Yizhou Huang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kezhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Wang"
                },
                "author": "Kezhi Wang",
                "arxiv_comment": "Submitted for possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20361v1",
                "updated": "2024-09-30T14:59:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    59,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T14:59:22Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    59,
                    22,
                    0,
                    274,
                    0
                ],
                "title": "Rotated Runtime Smooth: Training-Free Activation Smoother for accurate\n  INT4 inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotated Runtime Smooth: Training-Free Activation Smoother for accurate\n  INT4 inference"
                },
                "summary": "Large language models have demonstrated promising capabilities upon scaling\nup parameters. However, serving large language models incurs substantial\ncomputation and memory movement costs due to their large scale. Quantization\nmethods have been employed to reduce service costs and latency. Nevertheless,\noutliers in activations hinder the development of INT4 weight-activation\nquantization. Existing approaches separate outliers and normal values into two\nmatrices or migrate outliers from activations to weights, suffering from high\nlatency or accuracy degradation. Based on observing activations from large\nlanguage models, outliers can be classified into channel-wise and spike\noutliers. In this work, we propose Rotated Runtime Smooth (RRS), a\nplug-and-play activation smoother for quantization, consisting of Runtime\nSmooth and the Rotation operation. Runtime Smooth (RS) is introduced to\neliminate channel-wise outliers by smoothing activations with channel-wise\nmaximums during runtime. The rotation operation can narrow the gap between\nspike outliers and normal values, alleviating the effect of victims caused by\nchannel-wise smoothing. The proposed method outperforms the state-of-the-art\nmethod in the LLaMA and Qwen families and improves WikiText-2 perplexity from\n57.33 to 6.66 for INT4 inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated promising capabilities upon scaling\nup parameters. However, serving large language models incurs substantial\ncomputation and memory movement costs due to their large scale. Quantization\nmethods have been employed to reduce service costs and latency. Nevertheless,\noutliers in activations hinder the development of INT4 weight-activation\nquantization. Existing approaches separate outliers and normal values into two\nmatrices or migrate outliers from activations to weights, suffering from high\nlatency or accuracy degradation. Based on observing activations from large\nlanguage models, outliers can be classified into channel-wise and spike\noutliers. In this work, we propose Rotated Runtime Smooth (RRS), a\nplug-and-play activation smoother for quantization, consisting of Runtime\nSmooth and the Rotation operation. Runtime Smooth (RS) is introduced to\neliminate channel-wise outliers by smoothing activations with channel-wise\nmaximums during runtime. The rotation operation can narrow the gap between\nspike outliers and normal values, alleviating the effect of victims caused by\nchannel-wise smoothing. The proposed method outperforms the state-of-the-art\nmethod in the LLaMA and Qwen families and improves WikiText-2 perplexity from\n57.33 to 6.66 for INT4 inference."
                },
                "authors": [
                    {
                        "name": "Ke Yi"
                    },
                    {
                        "name": "Zengke Liu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Chengyuan Li"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10957v3",
                "updated": "2024-09-30T14:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    54,
                    17,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-16T14:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    14,
                    24,
                    30,
                    6,
                    168,
                    0
                ],
                "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "EMNLP 2024 Main, Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18452v2",
                "updated": "2024-09-30T14:48:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    48,
                    21,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-26T01:18:46Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    1,
                    18,
                    46,
                    4,
                    208,
                    0
                ],
                "title": "Nambu-Jona-Lasinio description of hadronic matter from a Bayesian\n  approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nambu-Jona-Lasinio description of hadronic matter from a Bayesian\n  approach"
                },
                "summary": "A microscopic nuclear matter formalism with explicit chiral symmetry based on\nthe Nambu Jona-Lasinio model is considered to describe nuclear matter. To\nreproduce nuclear matter properties adequately at the saturation density,\nfour-point and eight-point interactions are introduced. Within a Bayesian\ninference approach, the parameters of the model are determined by imposing\nnuclear matter, both experimental and from {\\it ab-initio} calculations, and\nneutron star observational constraints. Nuclear matter properties are well\nreproduced with an effective mass of 0.75 to 0.8 nucleon mass at the saturation\ndensity. At 90% confidence level, the radius of a $1.4 ~\\rm M_\\odot$ star\nvaries between 11.48 km and 13.20 km, masses as large as $\\sim 2.2 ~\\rm\nM_\\odot$ are predicted and the radius of a 2 M$_\\odot$ star is above 10.5 km.\nHigh-density perturbative QCD (pQCD) results exclude equations of state that\npredict larger maximum masses and radii. The speed of sound increases\nmonotonically with density and reaches values as large as\n$\\sqrt{0.7}c$-$\\sqrt{0.8}c$ in the center of massive stars. Several properties\nsuch as the polytropic index or the renormalized trace anomaly, that have been\nproposed to identify the deconfined phase transition, are analyzed.\nInterestingly, the radius of the obtained posterior that also meets pQCD\nconstraints aligns closely with the mass-radius measurement of the recent PSR\nJ0437-4715, which contrasts with other relativistic mean field model results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic nuclear matter formalism with explicit chiral symmetry based on\nthe Nambu Jona-Lasinio model is considered to describe nuclear matter. To\nreproduce nuclear matter properties adequately at the saturation density,\nfour-point and eight-point interactions are introduced. Within a Bayesian\ninference approach, the parameters of the model are determined by imposing\nnuclear matter, both experimental and from {\\it ab-initio} calculations, and\nneutron star observational constraints. Nuclear matter properties are well\nreproduced with an effective mass of 0.75 to 0.8 nucleon mass at the saturation\ndensity. At 90% confidence level, the radius of a $1.4 ~\\rm M_\\odot$ star\nvaries between 11.48 km and 13.20 km, masses as large as $\\sim 2.2 ~\\rm\nM_\\odot$ are predicted and the radius of a 2 M$_\\odot$ star is above 10.5 km.\nHigh-density perturbative QCD (pQCD) results exclude equations of state that\npredict larger maximum masses and radii. The speed of sound increases\nmonotonically with density and reaches values as large as\n$\\sqrt{0.7}c$-$\\sqrt{0.8}c$ in the center of massive stars. Several properties\nsuch as the polytropic index or the renormalized trace anomaly, that have been\nproposed to identify the deconfined phase transition, are analyzed.\nInterestingly, the radius of the obtained posterior that also meets pQCD\nconstraints aligns closely with the mass-radius measurement of the recent PSR\nJ0437-4715, which contrasts with other relativistic mean field model results."
                },
                "authors": [
                    {
                        "name": "K. D. Marquez"
                    },
                    {
                        "name": "Tuhin Malik"
                    },
                    {
                        "name": "Helena Pais"
                    },
                    {
                        "name": "Débora P. Menezes"
                    },
                    {
                        "name": "Constança Providência"
                    }
                ],
                "author_detail": {
                    "name": "Constança Providência"
                },
                "author": "Constança Providência",
                "arxiv_doi": "10.1103/PhysRevD.110.063040",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.063040",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.18452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures",
                "arxiv_journal_ref": "Phys. Rev. D 110, 063040 (2024)",
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08424v2",
                "updated": "2024-09-30T14:25:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    25,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-13T11:16:43Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    11,
                    16,
                    43,
                    2,
                    73,
                    0
                ],
                "title": "Distract Large Language Models for Automatic Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distract Large Language Models for Automatic Jailbreak Attack"
                },
                "summary": "Extensive efforts have been made before the public release of Large language\nmodels (LLMs) to align their behaviors with human values. However, even\nmeticulously aligned LLMs remain vulnerable to malicious manipulations such as\njailbreaking, leading to unintended behaviors. In this work, we propose a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive efforts have been made before the public release of Large language\nmodels (LLMs) to align their behaviors with human values. However, even\nmeticulously aligned LLMs remain vulnerable to malicious manipulations such as\njailbreaking, leading to unintended behaviors. In this work, we propose a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies."
                },
                "authors": [
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun Chen"
                },
                "author": "Yun Chen",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09275v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09275v3",
                "updated": "2024-09-30T14:18:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    18,
                    16,
                    0,
                    274,
                    0
                ],
                "published": "2023-08-18T03:35:41Z",
                "published_parsed": [
                    2023,
                    8,
                    18,
                    3,
                    35,
                    41,
                    4,
                    230,
                    0
                ],
                "title": "Stochastic Opinion Dynamics under Social Pressure in Arbitrary Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Opinion Dynamics under Social Pressure in Arbitrary Networks"
                },
                "summary": "Social pressure is a key factor affecting the evolution of opinions on\nnetworks in many types of settings, pushing people to conform to their\nneighbors' opinions. To study this, the interacting Polya urn model was\nintroduced by Jadbabaie et al., in which each agent has two kinds of opinion:\ninherent beliefs, which are hidden from the other agents and fixed; and\ndeclared opinions, which are randomly sampled at each step from a distribution\nwhich depends on the agent's inherent belief and her neighbors' past declared\nopinions (the social pressure component), and which is then communicated to her\nneighbors. Each agent also has a bias parameter denoting her level of\nresistance to social pressure. At every step, each agent updates her declared\nopinion (simultaneously with all other agents) according to her neighbors'\naggregate past declared opinions, her inherent belief, and her bias parameter.\nWe study the asymptotic behavior of this opinion dynamics model and show that\nthe agents' declaration probabilities approaches a set of equilibrium points of\nthe expected dynamics using Lyapunov theory and stochastic approximation\ntechniques. We also derive necessary and sufficient conditions for the agents\nto approach consensus on their declared opinions. Our work provides further\ninsight into the difficulty of inferring the inherent beliefs of agents when\nthey are under social pressure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social pressure is a key factor affecting the evolution of opinions on\nnetworks in many types of settings, pushing people to conform to their\nneighbors' opinions. To study this, the interacting Polya urn model was\nintroduced by Jadbabaie et al., in which each agent has two kinds of opinion:\ninherent beliefs, which are hidden from the other agents and fixed; and\ndeclared opinions, which are randomly sampled at each step from a distribution\nwhich depends on the agent's inherent belief and her neighbors' past declared\nopinions (the social pressure component), and which is then communicated to her\nneighbors. Each agent also has a bias parameter denoting her level of\nresistance to social pressure. At every step, each agent updates her declared\nopinion (simultaneously with all other agents) according to her neighbors'\naggregate past declared opinions, her inherent belief, and her bias parameter.\nWe study the asymptotic behavior of this opinion dynamics model and show that\nthe agents' declaration probabilities approaches a set of equilibrium points of\nthe expected dynamics using Lyapunov theory and stochastic approximation\ntechniques. We also derive necessary and sufficient conditions for the agents\nto approach consensus on their declared opinions. Our work provides further\ninsight into the difficulty of inferring the inherent beliefs of agents when\nthey are under social pressure."
                },
                "authors": [
                    {
                        "name": "Jennifer Tang"
                    },
                    {
                        "name": "Aviv Adler"
                    },
                    {
                        "name": "Amir Ajorlou"
                    },
                    {
                        "name": "Ali Jadbabaie"
                    }
                ],
                "author_detail": {
                    "name": "Ali Jadbabaie"
                },
                "author": "Ali Jadbabaie",
                "arxiv_comment": "Updated cited theorems (and proofs included)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09275v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09275v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09794v2",
                "updated": "2024-09-30T14:02:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    2,
                    59,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-19T08:41:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    41,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "AutoML-guided Fusion of Entity and LLM-based Representations for\n  Document Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoML-guided Fusion of Entity and LLM-based Representations for\n  Document Classification"
                },
                "summary": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (i.e. embeddings) do not\nefficiently exploit these resources. Dense and robust representations of\ndocuments are essential for effectively solving downstream classification and\nretrieval tasks. This work demonstrates that injecting embedded information\nfrom knowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets. The code is freely available at\n\\url{https://github.com/bkolosk1/bablfusion.git}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (i.e. embeddings) do not\nefficiently exploit these resources. Dense and robust representations of\ndocuments are essential for effectively solving downstream classification and\nretrieval tasks. This work demonstrates that injecting embedded information\nfrom knowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets. The code is freely available at\n\\url{https://github.com/bkolosk1/bablfusion.git}."
                },
                "authors": [
                    {
                        "name": "Boshko Koloski"
                    },
                    {
                        "name": "Senja Pollak"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Blaž Škrlj"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Škrlj"
                },
                "author": "Blaž Škrlj",
                "arxiv_comment": "Accepted at the 2024 Discovery Science Conference, oral presentation\n  track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20303v1",
                "updated": "2024-09-30T14:00:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T14:00:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "A Looming Replication Crisis in Evaluating Behavior in Language Models?\n  Evidence and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Looming Replication Crisis in Evaluating Behavior in Language Models?\n  Evidence and Solutions"
                },
                "summary": "In an era where large language models (LLMs) are increasingly integrated into\na wide range of everyday applications, research into these models' behavior has\nsurged. However, due to the novelty of the field, clear methodological\nguidelines are lacking. This raises concerns about the replicability and\ngeneralizability of insights gained from research on LLM behavior. In this\nstudy, we discuss the potential risk of a replication crisis and support our\nconcerns with a series of replication experiments focused on prompt engineering\ntechniques purported to influence reasoning abilities in LLMs. We tested\nGPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on\nthe chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well\nas Re-Reading prompt engineering techniques, using manually double-checked\nsubsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE,\nScienceQA, and StrategyQA. Our findings reveal a general lack of statistically\nsignificant differences across nearly all techniques tested, highlighting,\namong others, several methodological weaknesses in previous research. We\npropose a forward-looking approach that includes developing robust\nmethodologies for evaluating LLMs, establishing sound benchmarks, and designing\nrigorous experimental frameworks to ensure accurate and reliable assessments of\nmodel outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where large language models (LLMs) are increasingly integrated into\na wide range of everyday applications, research into these models' behavior has\nsurged. However, due to the novelty of the field, clear methodological\nguidelines are lacking. This raises concerns about the replicability and\ngeneralizability of insights gained from research on LLM behavior. In this\nstudy, we discuss the potential risk of a replication crisis and support our\nconcerns with a series of replication experiments focused on prompt engineering\ntechniques purported to influence reasoning abilities in LLMs. We tested\nGPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on\nthe chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well\nas Re-Reading prompt engineering techniques, using manually double-checked\nsubsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE,\nScienceQA, and StrategyQA. Our findings reveal a general lack of statistically\nsignificant differences across nearly all techniques tested, highlighting,\namong others, several methodological weaknesses in previous research. We\npropose a forward-looking approach that includes developing robust\nmethodologies for evaluating LLMs, establishing sound benchmarks, and designing\nrigorous experimental frameworks to ensure accurate and reliable assessments of\nmodel outputs."
                },
                "authors": [
                    {
                        "name": "Laurène Vaugrante"
                    },
                    {
                        "name": "Mathias Niepert"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20296v1",
                "updated": "2024-09-30T13:55:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    55,
                    42,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T13:55:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    55,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "PersonalLLM: Tailoring LLMs to Individual Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalLLM: Tailoring LLMs to Individual Preferences"
                },
                "summary": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM"
                },
                "authors": [
                    {
                        "name": "Thomas P. Zollo"
                    },
                    {
                        "name": "Andrew Wei Tung Siah"
                    },
                    {
                        "name": "Naimeng Ye"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20288v1",
                "updated": "2024-09-30T13:44:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T13:44:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "You Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02647v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02647v4",
                "updated": "2024-09-30T13:41:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "published": "2023-12-05T10:39:37Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    10,
                    39,
                    37,
                    1,
                    339,
                    0
                ],
                "title": "TPA3D: Triplane Attention for Fast Text-to-3D Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPA3D: Triplane Attention for Fast Text-to-3D Generation"
                },
                "summary": "Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed."
                },
                "authors": [
                    {
                        "name": "Bin-Shih Wu"
                    },
                    {
                        "name": "Hong-En Chen"
                    },
                    {
                        "name": "Sheng-Yu Huang"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Chiang Frank Wang"
                },
                "author": "Yu-Chiang Frank Wang",
                "arxiv_doi": "10.1007/978-3-031-72649-1_25",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-72649-1_25",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.02647v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02647v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ECCV2024, Project Page: https://redxouls.github.io/TPA3D/",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12575v2",
                "updated": "2024-09-30T13:30:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    30,
                    54,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-22T17:42:16Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    42,
                    16,
                    3,
                    235,
                    0
                ],
                "title": "Enhanced Parking Perception by Multi-Task Fisheye Cross-view\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Parking Perception by Multi-Task Fisheye Cross-view\n  Transformers"
                },
                "summary": "Current parking area perception algorithms primarily focus on detecting\nvacant slots within a limited range, relying on error-prone homographic\nprojection for both labeling and inference. However, recent advancements in\nAdvanced Driver Assistance System (ADAS) require interaction with end-users\nthrough comprehensive and intelligent Human-Machine Interfaces (HMIs). These\ninterfaces should present a complete perception of the parking area going from\ndistinguishing vacant slots' entry lines to the orientation of other parked\nvehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT\nF-CVT), which leverages features from a four-camera fisheye Surround-view\nCamera System (SVCS) with multihead attentions to create a detailed Bird-Eye\nView (BEV) grid feature map. Features are processed by both a segmentation\ndecoder and a Polygon-Yolo based object detection decoder for parking slots and\nvehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects\nwithin a 25m x 25m real open-road scenes with an average error of only 20 cm.\nOur larger model achieves an F-1 score of 0.89. Moreover the smaller model\noperates at 16 fps on an Nvidia Jetson Orin embedded board, with similar\ndetection results to the larger one. MT F-CVT demonstrates robust\ngeneralization capability across different vehicles and camera rig\nconfigurations. A demo video from an unseen vehicle and camera rig is available\nat: https://streamable.com/jjw54x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current parking area perception algorithms primarily focus on detecting\nvacant slots within a limited range, relying on error-prone homographic\nprojection for both labeling and inference. However, recent advancements in\nAdvanced Driver Assistance System (ADAS) require interaction with end-users\nthrough comprehensive and intelligent Human-Machine Interfaces (HMIs). These\ninterfaces should present a complete perception of the parking area going from\ndistinguishing vacant slots' entry lines to the orientation of other parked\nvehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT\nF-CVT), which leverages features from a four-camera fisheye Surround-view\nCamera System (SVCS) with multihead attentions to create a detailed Bird-Eye\nView (BEV) grid feature map. Features are processed by both a segmentation\ndecoder and a Polygon-Yolo based object detection decoder for parking slots and\nvehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects\nwithin a 25m x 25m real open-road scenes with an average error of only 20 cm.\nOur larger model achieves an F-1 score of 0.89. Moreover the smaller model\noperates at 16 fps on an Nvidia Jetson Orin embedded board, with similar\ndetection results to the larger one. MT F-CVT demonstrates robust\ngeneralization capability across different vehicles and camera rig\nconfigurations. A demo video from an unseen vehicle and camera rig is available\nat: https://streamable.com/jjw54x."
                },
                "authors": [
                    {
                        "name": "Antonyo Musabini"
                    },
                    {
                        "name": "Ivan Novikov"
                    },
                    {
                        "name": "Sana Soula"
                    },
                    {
                        "name": "Christel Leonet"
                    },
                    {
                        "name": "Lihao Wang"
                    },
                    {
                        "name": "Rachid Benmokhtar"
                    },
                    {
                        "name": "Fabian Burger"
                    },
                    {
                        "name": "Thomas Boulay"
                    },
                    {
                        "name": "Xavier Perrotton"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Perrotton"
                },
                "author": "Xavier Perrotton",
                "arxiv_comment": "This paper is a preprint of a paper submitted to the 26th Irish\n  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the\n  copy of record will be available at IET Digital Library",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02365v3",
                "updated": "2024-09-30T13:30:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    30,
                    49,
                    0,
                    274,
                    0
                ],
                "published": "2024-05-03T06:41:48Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    6,
                    41,
                    48,
                    4,
                    124,
                    0
                ],
                "title": "ModelShield: Adaptive and Robust Watermark against Model Extraction\n  Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModelShield: Adaptive and Robust Watermark against Model Extraction\n  Attack"
                },
                "summary": "Large language models (LLMs) demonstrate general intelligence across a\nvariety of machine learning tasks, thereby enhancing the commercial value of\ntheir intellectual property (IP). To protect this IP, model owners typically\nallow user access only in a black-box manner, however, adversaries can still\nutilize model extraction attacks to steal the model intelligence encoded in\nmodel generation. Watermarking technology offers a promising solution for\ndefending against such attacks by embedding unique identifiers into the\nmodel-generated content. However, existing watermarking methods often\ncompromise the quality of generated content due to heuristic alterations and\nlack robust mechanisms to counteract adversarial strategies, thus limiting\ntheir practicality in real-world scenarios. In this paper, we introduce an\nadaptive and robust watermarking method (named ModelShield) to protect the IP\nof LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs\nto autonomously insert watermarks into their generated content to avoid the\ndegradation of model content. We also propose a robust watermark detection\nmechanism capable of effectively identifying watermark signals under the\ninterference of varying adversarial strategies. Besides, ModelShield is a\nplug-and-play method that does not require additional model training, enhancing\nits applicability in LLM deployments. Extensive evaluations on two real-world\ndatasets and three LLMs demonstrate that our method surpasses existing methods\nin terms of defense effectiveness and robustness while significantly reducing\nthe degradation of watermarking on the model-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate general intelligence across a\nvariety of machine learning tasks, thereby enhancing the commercial value of\ntheir intellectual property (IP). To protect this IP, model owners typically\nallow user access only in a black-box manner, however, adversaries can still\nutilize model extraction attacks to steal the model intelligence encoded in\nmodel generation. Watermarking technology offers a promising solution for\ndefending against such attacks by embedding unique identifiers into the\nmodel-generated content. However, existing watermarking methods often\ncompromise the quality of generated content due to heuristic alterations and\nlack robust mechanisms to counteract adversarial strategies, thus limiting\ntheir practicality in real-world scenarios. In this paper, we introduce an\nadaptive and robust watermarking method (named ModelShield) to protect the IP\nof LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs\nto autonomously insert watermarks into their generated content to avoid the\ndegradation of model content. We also propose a robust watermark detection\nmechanism capable of effectively identifying watermark signals under the\ninterference of varying adversarial strategies. Besides, ModelShield is a\nplug-and-play method that does not require additional model training, enhancing\nits applicability in LLM deployments. Extensive evaluations on two real-world\ndatasets and three LLMs demonstrate that our method surpasses existing methods\nin terms of defense effectiveness and robustness while significantly reducing\nthe degradation of watermarking on the model-generated content."
                },
                "authors": [
                    {
                        "name": "Kaiyi Pang"
                    },
                    {
                        "name": "Tao Qi"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Minhao Bai"
                    },
                    {
                        "name": "Minghu Jiang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Huang"
                },
                "author": "Yongfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.05328v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.05328v5",
                "updated": "2024-09-30T13:29:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    29,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2023-03-09T15:19:31Z",
                "published_parsed": [
                    2023,
                    3,
                    9,
                    15,
                    19,
                    31,
                    3,
                    68,
                    0
                ],
                "title": "Simulation-based, Finite-sample Inference for Privatized Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based, Finite-sample Inference for Privatized Data"
                },
                "summary": "Privacy protection methods, such as differentially private mechanisms,\nintroduce noise into resulting statistics which often produces complex and\nintractable sampling distributions. In this paper, we propose a\nsimulation-based \"repro sample\" approach to produce statistically valid\nconfidence intervals and hypothesis tests, which builds on the work of Xie and\nWang (2022). We show that this methodology is applicable to a wide variety of\nprivate inference problems, appropriately accounts for biases introduced by\nprivacy mechanisms (such as by clamping), and improves over other\nstate-of-the-art inference methods such as the parametric bootstrap in terms of\nthe coverage and type I error of the private inference. We also develop\nsignificant improvements and extensions for the repro sample methodology for\ngeneral models (not necessarily related to privacy), including 1) modifying the\nprocedure to ensure guaranteed coverage and type I errors, even accounting for\nMonte Carlo error, and 2) proposing efficient numerical algorithms to implement\nthe confidence intervals and $p$-values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy protection methods, such as differentially private mechanisms,\nintroduce noise into resulting statistics which often produces complex and\nintractable sampling distributions. In this paper, we propose a\nsimulation-based \"repro sample\" approach to produce statistically valid\nconfidence intervals and hypothesis tests, which builds on the work of Xie and\nWang (2022). We show that this methodology is applicable to a wide variety of\nprivate inference problems, appropriately accounts for biases introduced by\nprivacy mechanisms (such as by clamping), and improves over other\nstate-of-the-art inference methods such as the parametric bootstrap in terms of\nthe coverage and type I error of the private inference. We also develop\nsignificant improvements and extensions for the repro sample methodology for\ngeneral models (not necessarily related to privacy), including 1) modifying the\nprocedure to ensure guaranteed coverage and type I errors, even accounting for\nMonte Carlo error, and 2) proposing efficient numerical algorithms to implement\nthe confidence intervals and $p$-values."
                },
                "authors": [
                    {
                        "name": "Jordan Awan"
                    },
                    {
                        "name": "Zhanyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Wang"
                },
                "author": "Zhanyu Wang",
                "arxiv_comment": "25 pages before references and appendices, 42 pages total, 10\n  figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.05328v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.05328v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20274v1",
                "updated": "2024-09-30T13:24:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    24,
                    42,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T13:24:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    24,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "Probabilistic Answer Set Programming with Discrete and Continuous Random\n  Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Answer Set Programming with Discrete and Continuous Random\n  Variables"
                },
                "summary": "Probabilistic Answer Set Programming under the credal semantics (PASP)\nextends Answer Set Programming with probabilistic facts that represent\nuncertain information. The probabilistic facts are discrete with Bernoulli\ndistributions. However, several real-world scenarios require a combination of\nboth discrete and continuous random variables. In this paper, we extend the\nPASP framework to support continuous random variables and propose Hybrid\nProbabilistic Answer Set Programming (HPASP). Moreover, we discuss, implement,\nand assess the performance of two exact algorithms based on projected answer\nset enumeration and knowledge compilation and two approximate algorithms based\non sampling. Empirical results, also in line with known theoretical results,\nshow that exact inference is feasible only for small instances, but knowledge\ncompilation has a huge positive impact on the performance. Sampling allows\nhandling larger instances, but sometimes requires an increasing amount of\nmemory. Under consideration in Theory and Practice of Logic Programming (TPLP).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Answer Set Programming under the credal semantics (PASP)\nextends Answer Set Programming with probabilistic facts that represent\nuncertain information. The probabilistic facts are discrete with Bernoulli\ndistributions. However, several real-world scenarios require a combination of\nboth discrete and continuous random variables. In this paper, we extend the\nPASP framework to support continuous random variables and propose Hybrid\nProbabilistic Answer Set Programming (HPASP). Moreover, we discuss, implement,\nand assess the performance of two exact algorithms based on projected answer\nset enumeration and knowledge compilation and two approximate algorithms based\non sampling. Empirical results, also in line with known theoretical results,\nshow that exact inference is feasible only for small instances, but knowledge\ncompilation has a huge positive impact on the performance. Sampling allows\nhandling larger instances, but sometimes requires an increasing amount of\nmemory. Under consideration in Theory and Practice of Logic Programming (TPLP)."
                },
                "authors": [
                    {
                        "name": "Damiano Azzolini"
                    },
                    {
                        "name": "Fabrizio Riguzzi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Riguzzi"
                },
                "author": "Fabrizio Riguzzi",
                "arxiv_comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14511v2",
                "updated": "2024-09-30T13:24:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    24,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-06T00:00:18Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    0,
                    18,
                    4,
                    250,
                    0
                ],
                "title": "Evaluation of Task Specific Productivity Improvements Using a Generative\n  Artificial Intelligence Personal Assistant Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Task Specific Productivity Improvements Using a Generative\n  Artificial Intelligence Personal Assistant Tool"
                },
                "summary": "This study evaluates the productivity improvements achieved using a\ngenerative artificial intelligence personal assistant tool (PAT) developed by\nTrane Technologies. The PAT, based on OpenAI's GPT 3.5 model, was deployed on\nMicrosoft Azure to ensure secure access and protection of intellectual\nproperty. To assess the tool's productivity effectiveness, an experiment was\nconducted comparing the completion times and content quality of four common\noffice tasks: writing an email, summarizing an article, creating instructions\nfor a simple task, and preparing a presentation outline. Sixty-three (63)\nparticipants were randomly divided into a test group using the PAT and a\ncontrol group performing the tasks manually. Results indicated significant\nproductivity enhancements, particularly for tasks involving summarization and\ninstruction creation, with improvements ranging from 3.3% to 69%. The study\nfurther analyzed factors such as the age of users, response word counts, and\nquality of responses, revealing that the PAT users generated more verbose and\nhigher-quality content. An 'LLM-as-a-judge' method employing GPT-4 was used to\ngrade the quality of responses, which effectively distinguished between high\nand low-quality outputs. The findings underscore the potential of PATs in\nenhancing workplace productivity and highlight areas for further research and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the productivity improvements achieved using a\ngenerative artificial intelligence personal assistant tool (PAT) developed by\nTrane Technologies. The PAT, based on OpenAI's GPT 3.5 model, was deployed on\nMicrosoft Azure to ensure secure access and protection of intellectual\nproperty. To assess the tool's productivity effectiveness, an experiment was\nconducted comparing the completion times and content quality of four common\noffice tasks: writing an email, summarizing an article, creating instructions\nfor a simple task, and preparing a presentation outline. Sixty-three (63)\nparticipants were randomly divided into a test group using the PAT and a\ncontrol group performing the tasks manually. Results indicated significant\nproductivity enhancements, particularly for tasks involving summarization and\ninstruction creation, with improvements ranging from 3.3% to 69%. The study\nfurther analyzed factors such as the age of users, response word counts, and\nquality of responses, revealing that the PAT users generated more verbose and\nhigher-quality content. An 'LLM-as-a-judge' method employing GPT-4 was used to\ngrade the quality of responses, which effectively distinguished between high\nand low-quality outputs. The findings underscore the potential of PATs in\nenhancing workplace productivity and highlight areas for further research and\noptimization."
                },
                "authors": [
                    {
                        "name": "Brian S. Freeman"
                    },
                    {
                        "name": "Kendall Arriola"
                    },
                    {
                        "name": "Dan Cottell"
                    },
                    {
                        "name": "Emmett Lawlor"
                    },
                    {
                        "name": "Matt Erdman"
                    },
                    {
                        "name": "Trevor Sutherland"
                    },
                    {
                        "name": "Brian Wells"
                    }
                ],
                "author_detail": {
                    "name": "Brian Wells"
                },
                "author": "Brian Wells",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13993v2",
                "updated": "2024-09-30T13:03:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    3,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-19T02:48:54Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    48,
                    54,
                    4,
                    201,
                    0
                ],
                "title": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models"
                },
                "summary": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently."
                },
                "authors": [
                    {
                        "name": "Christoforus Yoga Haryanto"
                    }
                ],
                "author_detail": {
                    "name": "Christoforus Yoga Haryanto"
                },
                "author": "Christoforus Yoga Haryanto",
                "arxiv_comment": "10 pages, 3 figures, 1 table, accepted to the 51st International\n  Conference on Computers and Industrial Engineering (CIE51)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13707v2",
                "updated": "2024-09-30T12:58:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    58,
                    34,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-18T17:06:15Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    6,
                    15,
                    3,
                    200,
                    0
                ],
                "title": "Dissipation at limited resolutions: Power law and detection of hidden\n  dissipative scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissipation at limited resolutions: Power law and detection of hidden\n  dissipative scales"
                },
                "summary": "Nonequilibrium systems, in particular living organisms, are maintained by\nirreversible transformations of energy that drive diverse functions.\nQuantifying their irreversibility, as measured by energy dissipation, is\nessential for understanding the underlying mechanisms. However, existing\ntechniques usually overlook experimental limitations, either by assuming full\ninformation or by employing a coarse-graining method that requires knowledge of\nthe structure behind hidden degrees of freedom. Here, we study the inference of\ndissipation from finite-resolution measurements by employing a recently\ndeveloped model-free estimator that considers both the sequence of\ncoarse-grained transitions and the waiting time distributions:\n$\\sigma_2=\\sigma_2^\\ell + \\sigma_2^t$. The dominant term $\\sigma_2^\\ell$\noriginates from the sequence of observed transitions; we find that it scales\nwith resolution following a power law. Comparing the scaling exponent with a\nprevious estimator highlights the importance of accounting for flux\ncorrelations at lower resolutions. $\\sigma_2^t$ comes from asymmetries in\nwaiting time distributions. It is non-monotonic in resolution, with its peak\nposition revealing characteristic scales of the underlying dissipative process,\nconsistent with observations in the actomyosin cortex of starfish oocytes.\nAlternatively, the characteristic scale can be detected in a crossover of the\nscaling of $\\sigma_2^\\ell$. This provides a novel perspective for extracting\notherwise hidden characteristic dissipative scales directly from dissipation\nmeasurements. We illustrate these results in biochemical models as well as\ncomplex networks. Overall, this study highlights the significance of resolution\nconsiderations in nonequilibrium systems, providing insights into the interplay\nbetween experimental resolution, entropy production, and underlying complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilibrium systems, in particular living organisms, are maintained by\nirreversible transformations of energy that drive diverse functions.\nQuantifying their irreversibility, as measured by energy dissipation, is\nessential for understanding the underlying mechanisms. However, existing\ntechniques usually overlook experimental limitations, either by assuming full\ninformation or by employing a coarse-graining method that requires knowledge of\nthe structure behind hidden degrees of freedom. Here, we study the inference of\ndissipation from finite-resolution measurements by employing a recently\ndeveloped model-free estimator that considers both the sequence of\ncoarse-grained transitions and the waiting time distributions:\n$\\sigma_2=\\sigma_2^\\ell + \\sigma_2^t$. The dominant term $\\sigma_2^\\ell$\noriginates from the sequence of observed transitions; we find that it scales\nwith resolution following a power law. Comparing the scaling exponent with a\nprevious estimator highlights the importance of accounting for flux\ncorrelations at lower resolutions. $\\sigma_2^t$ comes from asymmetries in\nwaiting time distributions. It is non-monotonic in resolution, with its peak\nposition revealing characteristic scales of the underlying dissipative process,\nconsistent with observations in the actomyosin cortex of starfish oocytes.\nAlternatively, the characteristic scale can be detected in a crossover of the\nscaling of $\\sigma_2^\\ell$. This provides a novel perspective for extracting\notherwise hidden characteristic dissipative scales directly from dissipation\nmeasurements. We illustrate these results in biochemical models as well as\ncomplex networks. Overall, this study highlights the significance of resolution\nconsiderations in nonequilibrium systems, providing insights into the interplay\nbetween experimental resolution, entropy production, and underlying complexity."
                },
                "authors": [
                    {
                        "name": "Qiwei Yu"
                    },
                    {
                        "name": "Pedro E. Harunari"
                    }
                ],
                "author_detail": {
                    "name": "Pedro E. Harunari"
                },
                "author": "Pedro E. Harunari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16694v2",
                "updated": "2024-09-30T12:55:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    55,
                    3,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-25T07:38:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms"
                },
                "summary": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization."
                },
                "authors": [
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Chengtao Lv"
                    },
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Jinyang Du"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_comment": "Ruihao Gong leads the overall organization of the survey, with Yifu\n  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is\n  responsible for authoring Section 4, while Chengtao Lv and Zining Wang\n  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and\n  Xianglong Liu provide guidance during the whole process and assist in\n  refining the final manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20258v1",
                "updated": "2024-09-30T12:49:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    49,
                    10,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:49:10Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    49,
                    10,
                    0,
                    274,
                    0
                ],
                "title": "Inferring Preferences from Demonstrations in Multi-objective\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Preferences from Demonstrations in Multi-objective\n  Reinforcement Learning"
                },
                "summary": "Many decision-making problems feature multiple objectives where it is not\nalways possible to know the preferences of a human or agent decision-maker for\ndifferent objectives. However, demonstrated behaviors from the decision-maker\nare often available. This research proposes a dynamic weight-based preference\ninference (DWPI) algorithm that can infer the preferences of agents acting in\nmulti-objective decision-making problems from demonstrations. The proposed\nalgorithm is evaluated on three multi-objective Markov decision processes: Deep\nSea Treasure, Traffic, and Item Gathering, and is compared to two existing\npreference inference algorithms. Empirical results demonstrate significant\nimprovements compared to the baseline algorithms, in terms of both time\nefficiency and inference accuracy. The DWPI algorithm maintains its performance\nwhen inferring preferences for sub-optimal demonstrations. Moreover, the DWPI\nalgorithm does not necessitate any interactions with the user during inference\n- only demonstrations are required. We provide a correctness proof and\ncomplexity analysis of the algorithm and statistically evaluate the performance\nunder different representation of demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many decision-making problems feature multiple objectives where it is not\nalways possible to know the preferences of a human or agent decision-maker for\ndifferent objectives. However, demonstrated behaviors from the decision-maker\nare often available. This research proposes a dynamic weight-based preference\ninference (DWPI) algorithm that can infer the preferences of agents acting in\nmulti-objective decision-making problems from demonstrations. The proposed\nalgorithm is evaluated on three multi-objective Markov decision processes: Deep\nSea Treasure, Traffic, and Item Gathering, and is compared to two existing\npreference inference algorithms. Empirical results demonstrate significant\nimprovements compared to the baseline algorithms, in terms of both time\nefficiency and inference accuracy. The DWPI algorithm maintains its performance\nwhen inferring preferences for sub-optimal demonstrations. Moreover, the DWPI\nalgorithm does not necessitate any interactions with the user during inference\n- only demonstrations are required. We provide a correctness proof and\ncomplexity analysis of the algorithm and statistically evaluate the performance\nunder different representation of demonstrations."
                },
                "authors": [
                    {
                        "name": "Junlin Lu"
                    },
                    {
                        "name": "Patrick Mannion"
                    },
                    {
                        "name": "Karl Mason"
                    }
                ],
                "author_detail": {
                    "name": "Karl Mason"
                },
                "author": "Karl Mason",
                "arxiv_doi": "10.1007/s00521-024-10412-x",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00521-024-10412-x",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.20258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Neural Comput & Applic (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20252v1",
                "updated": "2024-09-30T12:42:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    42,
                    25,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:42:25Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    42,
                    25,
                    0,
                    274,
                    0
                ],
                "title": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?"
                },
                "summary": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry."
                },
                "authors": [
                    {
                        "name": "Morgan Fouesneau"
                    },
                    {
                        "name": "Ivelina G. Momcheva"
                    },
                    {
                        "name": "Urmila Chadayammuri"
                    },
                    {
                        "name": "Mariia Demianenko"
                    },
                    {
                        "name": "Antoine Dumont"
                    },
                    {
                        "name": "Raphael E. Hviding"
                    },
                    {
                        "name": "K. Angelique Kahle"
                    },
                    {
                        "name": "Nadiia Pulatova"
                    },
                    {
                        "name": "Bhavesh Rajpoot"
                    },
                    {
                        "name": "Marten B. Scheuck"
                    },
                    {
                        "name": "Rhys Seeburger"
                    },
                    {
                        "name": "Dmitry Semenov"
                    },
                    {
                        "name": "Jaime I. Villaseñor"
                    }
                ],
                "author_detail": {
                    "name": "Jaime I. Villaseñor"
                },
                "author": "Jaime I. Villaseñor",
                "arxiv_comment": "Paper submitted to RASTI. We share our experience, ethical and legal\n  concerns (5.3), and recommendations for individuals and journals (6.). We\n  welcome feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20247v1",
                "updated": "2024-09-30T12:36:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    36,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:36:27Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    36,
                    27,
                    0,
                    274,
                    0
                ],
                "title": "Resource Allocation for Stable LLM Training in Mobile Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Stable LLM Training in Mobile Edge Computing"
                },
                "summary": "As mobile devices increasingly become focal points for advanced applications,\nedge computing presents a viable solution to their inherent computational\nlimitations, particularly in deploying large language models (LLMs). However,\ndespite the advancements in edge computing, significant challenges remain in\nefficient training and deploying LLMs due to the computational demands and data\nprivacy concerns associated with these models. This paper explores a\ncollaborative training framework that integrates mobile users with edge servers\nto optimize resource allocation, thereby enhancing both performance and\nefficiency. Our approach leverages parameter-efficient fine-tuning (PEFT)\nmethods, allowing mobile users to adjust the initial layers of the LLM while\nedge servers handle the more demanding latter layers. Specifically, we\nformulate a multi-objective optimization problem to minimize the total energy\nconsumption and delay during training. We also address the common issue of\ninstability in model performance by incorporating stability enhancements into\nour objective function. Through novel fractional programming technique, we\nachieve a stationary point for the formulated problem. Simulations demonstrate\nthat our method reduces the energy consumption as well as the latency, and\nincreases the reliability of LLMs across various mobile settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As mobile devices increasingly become focal points for advanced applications,\nedge computing presents a viable solution to their inherent computational\nlimitations, particularly in deploying large language models (LLMs). However,\ndespite the advancements in edge computing, significant challenges remain in\nefficient training and deploying LLMs due to the computational demands and data\nprivacy concerns associated with these models. This paper explores a\ncollaborative training framework that integrates mobile users with edge servers\nto optimize resource allocation, thereby enhancing both performance and\nefficiency. Our approach leverages parameter-efficient fine-tuning (PEFT)\nmethods, allowing mobile users to adjust the initial layers of the LLM while\nedge servers handle the more demanding latter layers. Specifically, we\nformulate a multi-objective optimization problem to minimize the total energy\nconsumption and delay during training. We also address the common issue of\ninstability in model performance by incorporating stability enhancements into\nour objective function. Through novel fractional programming technique, we\nachieve a stationary point for the formulated problem. Simulations demonstrate\nthat our method reduces the energy consumption as well as the latency, and\nincreases the reliability of LLMs across various mobile settings."
                },
                "authors": [
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "This paper appears in the 2024 International Symposium on Theory,\n  Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile\n  Computing (MobiHoc)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17717v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17717v3",
                "updated": "2024-09-30T12:02:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    2,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-02-27T17:52:33Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    17,
                    52,
                    33,
                    1,
                    58,
                    0
                ],
                "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG"
                },
                "summary": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification."
                },
                "authors": [
                    {
                        "name": "Ayana Niwa"
                    },
                    {
                        "name": "Hayate Iso"
                    }
                ],
                "author_detail": {
                    "name": "Hayate Iso"
                },
                "author": "Hayate Iso",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17717v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17717v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20222v1",
                "updated": "2024-09-30T12:01:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    1,
                    29,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:01:29Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    1,
                    29,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models"
                },
                "summary": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture."
                },
                "authors": [
                    {
                        "name": "David Castillo-Bolado"
                    },
                    {
                        "name": "Joseph Davidson"
                    },
                    {
                        "name": "Finlay Gray"
                    },
                    {
                        "name": "Marek Rosa"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rosa"
                },
                "author": "Marek Rosa",
                "arxiv_comment": "Accepted as a poster at NeurIPS D&B Track 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10383v2",
                "updated": "2024-09-30T11:58:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    58,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-16T13:23:54Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    13,
                    23,
                    54,
                    0,
                    289,
                    0
                ],
                "title": "Privacy in Large Language Models: Attacks, Defenses and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy in Large Language Models: Attacks, Defenses and Future\n  Directions"
                },
                "summary": "The advancement of large language models (LLMs) has significantly enhanced\nthe ability to effectively tackle various downstream NLP tasks and unify these\ntasks into generative pipelines. On the one hand, powerful language models,\ntrained on massive textual data, have brought unparalleled accessibility and\nusability for both models and users. On the other hand, unrestricted access to\nthese models can also introduce potential malicious and unintentional privacy\nrisks. Despite ongoing efforts to address the safety and privacy concerns\nassociated with LLMs, the problem remains unresolved. In this paper, we provide\na comprehensive analysis of the current privacy attacks targeting LLMs and\ncategorize them according to the adversary's assumed capabilities to shed light\non the potential vulnerabilities present in LLMs. Then, we present a detailed\noverview of prominent defense strategies that have been developed to counter\nthese privacy attacks. Beyond existing works, we identify upcoming privacy\nconcerns as LLMs evolve. Lastly, we point out several potential avenues for\nfuture exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has significantly enhanced\nthe ability to effectively tackle various downstream NLP tasks and unify these\ntasks into generative pipelines. On the one hand, powerful language models,\ntrained on massive textual data, have brought unparalleled accessibility and\nusability for both models and users. On the other hand, unrestricted access to\nthese models can also introduce potential malicious and unintentional privacy\nrisks. Despite ongoing efforts to address the safety and privacy concerns\nassociated with LLMs, the problem remains unresolved. In this paper, we provide\na comprehensive analysis of the current privacy attacks targeting LLMs and\ncategorize them according to the adversary's assumed capabilities to shed light\non the potential vulnerabilities present in LLMs. Then, we present a detailed\noverview of prominent defense strategies that have been developed to counter\nthese privacy attacks. Beyond existing works, we identify upcoming privacy\nconcerns as LLMs evolve. Lastly, we point out several potential avenues for\nfuture exploration."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Jinglong Luo"
                    },
                    {
                        "name": "Jiecong Wang"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "We upload the survey to cover more recent papers and inlcude privacy\n  resaearch on multi-modality",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20221v1",
                "updated": "2024-09-30T11:58:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    58,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T11:58:09Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    58,
                    9,
                    0,
                    274,
                    0
                ],
                "title": "Linear Magnetoresistance and Type-I Superconductivity in\n  $β$-IrSn$_4$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Magnetoresistance and Type-I Superconductivity in\n  $β$-IrSn$_4$"
                },
                "summary": "Layered material $\\beta$-IrSn$_4$ ($I4_1/acd$, $D^{20}_{4h}$, #142), whose\nelectron bands have symmetry-enforced Dirac points, was investigated using\nhigh-quality single crystals. It exhibits a pronounced linear field-dependence\nof magnetoresistance (LMR), which cannot be explained by currently existing\nmodels. Structures in the field-angle dependence of magnetoresistance and Hall\nresistivity are attributable to the Fermi surface topology; the presence of\nopen orbits is inferred. At the superconducting (SC) transition, the\nspecific-heat jump exhibits a significant increase in applied fields, revealing\nthe type-I SC nature. This feature is attributable to the high Fermi velocity\nof linearly dispersive multibands. To clarify the mechanism of the puzzling\nLMR, investigations into the topological nature of those multibands in applied\nfields are highly desired.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered material $\\beta$-IrSn$_4$ ($I4_1/acd$, $D^{20}_{4h}$, #142), whose\nelectron bands have symmetry-enforced Dirac points, was investigated using\nhigh-quality single crystals. It exhibits a pronounced linear field-dependence\nof magnetoresistance (LMR), which cannot be explained by currently existing\nmodels. Structures in the field-angle dependence of magnetoresistance and Hall\nresistivity are attributable to the Fermi surface topology; the presence of\nopen orbits is inferred. At the superconducting (SC) transition, the\nspecific-heat jump exhibits a significant increase in applied fields, revealing\nthe type-I SC nature. This feature is attributable to the high Fermi velocity\nof linearly dispersive multibands. To clarify the mechanism of the puzzling\nLMR, investigations into the topological nature of those multibands in applied\nfields are highly desired."
                },
                "authors": [
                    {
                        "name": "Nazir Ahmad"
                    },
                    {
                        "name": "Shunsuke Shimada"
                    },
                    {
                        "name": "Takumi Hasegawa"
                    },
                    {
                        "name": "Hiroto Suzuki"
                    },
                    {
                        "name": "Md Asif Afzal"
                    },
                    {
                        "name": "Naoki Nakamura"
                    },
                    {
                        "name": "Ryuji Higashinaka"
                    },
                    {
                        "name": "Tatsuma D. Matsuda"
                    },
                    {
                        "name": "Yuji Aoki"
                    }
                ],
                "author_detail": {
                    "name": "Yuji Aoki"
                },
                "arxiv_affiliation": "Tokyo Metropolitan University",
                "author": "Yuji Aoki",
                "arxiv_doi": "10.7566/JPSJ.93.044706",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.7566/JPSJ.93.044706",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J. Phys. Soc. Jpn. 93, 044706 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20209v1",
                "updated": "2024-09-30T11:43:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    43,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T11:43:58Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    43,
                    58,
                    0,
                    274,
                    0
                ],
                "title": "Development of a Machine Learning based Radio source localisation\n  algorithm for Tri-axial antenna configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Machine Learning based Radio source localisation\n  algorithm for Tri-axial antenna configuration"
                },
                "summary": "Accurately determining the origin of radio emissions is essential for\nnumerous scientific experiments, particularly in radio astronomy. Conventional\ntechniques, such as the use of antenna arrays encounter significant challenges,\nspecially at very low frequencies, due to factors like the substantial size of\nthe antennas and ionospheric interference. To address these challenges, we\nemploy a space-based single-telescope that utilizes co-located antennas,\ncomplemented by goniopolarimetric techniques for precise source localization.\nThis study explores a novel and elementary machine learning (ML) technique as a\nway to improve and estimate Direction of Arrival (DoA), leveraging a tri-axial\nantenna arrangement for radio source localization. Employing a simplistic\nemission and receiving antenna model, our study involves training an artificial\nneural network (ANN) using synthetic radio signals. These synthetic signals can\noriginate from any location in the sky and cover an incoherent frequency range\nof 0.3 to 30 MHz, with a signal-to-noise ratio (SNR) between 0 and 60 dB. Then,\na large data set was generated to train the ANN model catering to the possible\nsignal configurations and variations. After training, the developed ANN model\ndemonstrated exceptional performance, achieving loss levels in the training\n($\\sim0.02$), validation ($\\sim0.23\\%$), and testing ($\\sim0.21\\%$) phases. The\nmachine learning-based approach remarkably, exhibits substantially quicker\ninference times ($\\sim5$ ms) in contrast to analytically derived Direction of\nArrival (DoA) methods, which typically range from 100 ms to a few seconds. This\nunderscores its practicality for real-time applications in radio source\nlocalization, particularly in scenarios with limited number of sensors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately determining the origin of radio emissions is essential for\nnumerous scientific experiments, particularly in radio astronomy. Conventional\ntechniques, such as the use of antenna arrays encounter significant challenges,\nspecially at very low frequencies, due to factors like the substantial size of\nthe antennas and ionospheric interference. To address these challenges, we\nemploy a space-based single-telescope that utilizes co-located antennas,\ncomplemented by goniopolarimetric techniques for precise source localization.\nThis study explores a novel and elementary machine learning (ML) technique as a\nway to improve and estimate Direction of Arrival (DoA), leveraging a tri-axial\nantenna arrangement for radio source localization. Employing a simplistic\nemission and receiving antenna model, our study involves training an artificial\nneural network (ANN) using synthetic radio signals. These synthetic signals can\noriginate from any location in the sky and cover an incoherent frequency range\nof 0.3 to 30 MHz, with a signal-to-noise ratio (SNR) between 0 and 60 dB. Then,\na large data set was generated to train the ANN model catering to the possible\nsignal configurations and variations. After training, the developed ANN model\ndemonstrated exceptional performance, achieving loss levels in the training\n($\\sim0.02$), validation ($\\sim0.23\\%$), and testing ($\\sim0.21\\%$) phases. The\nmachine learning-based approach remarkably, exhibits substantially quicker\ninference times ($\\sim5$ ms) in contrast to analytically derived Direction of\nArrival (DoA) methods, which typically range from 100 ms to a few seconds. This\nunderscores its practicality for real-time applications in radio source\nlocalization, particularly in scenarios with limited number of sensors."
                },
                "authors": [
                    {
                        "name": "Harsha Aviansh Tanti"
                    },
                    {
                        "name": "Abhirup Datta"
                    },
                    {
                        "name": "Tiasha Biswas"
                    },
                    {
                        "name": "Anshuman Tripathi"
                    }
                ],
                "author_detail": {
                    "name": "Anshuman Tripathi"
                },
                "author": "Anshuman Tripathi",
                "arxiv_comment": "Accepted for publication in JoAA, 11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15484v2",
                "updated": "2024-09-30T11:25:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    25,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-17T09:15:57Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    15,
                    57,
                    0,
                    169,
                    0
                ],
                "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large\n  Language Models"
                },
                "summary": "The use of Large Language Models (LLMs) in hiring has led to legislative\nactions to protect vulnerable demographic groups. This paper presents a novel\nframework for benchmarking hierarchical gender hiring bias in Large Language\nModels (LLMs) for resume scoring, revealing significant issues of reverse\ngender hiring bias and overdebiasing. Our contributions are fourfold: Firstly,\nwe introduce a new construct grounded in labour economics, legal principles,\nand critiques of current bias benchmarks: hiring bias can be categorized into\ntwo types: Level bias (difference in the average outcomes between demographic\ncounterfactual groups) and Spread bias (difference in the variance of outcomes\nbetween demographic counterfactual groups); Level bias can be further\nsubdivided into statistical bias (i.e. changing with non-demographic content)\nand taste-based bias (i.e. consistent regardless of non-demographic content).\nSecondly, the framework includes rigorous statistical and computational hiring\nbias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio,\nPermutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring\nbiases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant\nbiases against males in at least one industry. An industry-effect regression\nreveals that the healthcare industry is the most biased against males.\nMoreover, we found that the bias performance remains invariant with resume\ncontent for eight out of ten LLMs. This indicates that the bias performance\nmeasured in this paper might apply to other resume datasets with different\nresume qualities. Fourthly, we provide a user-friendly demo and resume dataset\nto support the adoption and practical use of the framework, which can be\ngeneralized to other social traits and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in hiring has led to legislative\nactions to protect vulnerable demographic groups. This paper presents a novel\nframework for benchmarking hierarchical gender hiring bias in Large Language\nModels (LLMs) for resume scoring, revealing significant issues of reverse\ngender hiring bias and overdebiasing. Our contributions are fourfold: Firstly,\nwe introduce a new construct grounded in labour economics, legal principles,\nand critiques of current bias benchmarks: hiring bias can be categorized into\ntwo types: Level bias (difference in the average outcomes between demographic\ncounterfactual groups) and Spread bias (difference in the variance of outcomes\nbetween demographic counterfactual groups); Level bias can be further\nsubdivided into statistical bias (i.e. changing with non-demographic content)\nand taste-based bias (i.e. consistent regardless of non-demographic content).\nSecondly, the framework includes rigorous statistical and computational hiring\nbias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio,\nPermutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring\nbiases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant\nbiases against males in at least one industry. An industry-effect regression\nreveals that the healthcare industry is the most biased against males.\nMoreover, we found that the bias performance remains invariant with resume\ncontent for eight out of ten LLMs. This indicates that the bias performance\nmeasured in this paper might apply to other resume datasets with different\nresume qualities. Fourthly, we provide a user-friendly demo and resume dataset\nto support the adoption and practical use of the framework, which can be\ngeneralized to other social traits and tasks."
                },
                "authors": [
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Michael Thaler"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Skylar Lu"
                    },
                    {
                        "name": "Sachin Beepath"
                    },
                    {
                        "name": "Ediz Ertekin Jr."
                    },
                    {
                        "name": "Maria Perez-Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "Maria Perez-Ortiz"
                },
                "author": "Maria Perez-Ortiz",
                "arxiv_comment": "EMNLP 2024 Findings Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18035v3",
                "updated": "2024-09-30T11:18:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    18,
                    38,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-26T18:40:36Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    18,
                    40,
                    36,
                    1,
                    86,
                    0
                ],
                "title": "Bidirectional Consistency Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Consistency Models"
                },
                "summary": "Diffusion models (DMs) are capable of generating remarkably high-quality\nsamples by iteratively denoising a random vector, a process that corresponds to\nmoving along the probability flow ordinary differential equation (PF ODE).\nInterestingly, DMs can also invert an input image to noise by moving backward\nalong the PF ODE, a key operation for downstream tasks such as interpolation\nand image editing. However, the iterative nature of this process restricts its\nspeed, hindering its broader application. Recently, Consistency Models (CMs)\nhave emerged to address this challenge by approximating the integral of the PF\nODE, largely reducing the number of iterations. Yet, the absence of an explicit\nODE solver complicates the inversion process. To resolve this, we introduce\nBidirectional Consistency Model (BCM), which learns a single neural network\nthat enables both forward and backward traversal along the PF ODE, efficiently\nunifying generation and inversion tasks within one framework. We can train BCM\nfrom scratch or tune it using a pretrained consistency model, wh ich reduces\nthe training cost and increases scalability. We demonstrate that BCM enables\none-step generation and inversion while also allowing the use of additional\nsteps to enhance generation quality or reduce reconstruction error. We further\nshowcase BCM's capability in downstream tasks, such as interpolation,\ninpainting, and blind restoration of compressed images. Notably, when the\nnumber of function evaluations (NFE) is constrained, BCM surpasses\ndomain-specific restoration methods, such as I$^2$SB and Palette, in a fully\nzero-shot manner, offering an efficient alternative for inversion problems. Our\ncode and weights are available at\nhttps://github.com/Mosasaur5526/BCM-iCT-torch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are capable of generating remarkably high-quality\nsamples by iteratively denoising a random vector, a process that corresponds to\nmoving along the probability flow ordinary differential equation (PF ODE).\nInterestingly, DMs can also invert an input image to noise by moving backward\nalong the PF ODE, a key operation for downstream tasks such as interpolation\nand image editing. However, the iterative nature of this process restricts its\nspeed, hindering its broader application. Recently, Consistency Models (CMs)\nhave emerged to address this challenge by approximating the integral of the PF\nODE, largely reducing the number of iterations. Yet, the absence of an explicit\nODE solver complicates the inversion process. To resolve this, we introduce\nBidirectional Consistency Model (BCM), which learns a single neural network\nthat enables both forward and backward traversal along the PF ODE, efficiently\nunifying generation and inversion tasks within one framework. We can train BCM\nfrom scratch or tune it using a pretrained consistency model, wh ich reduces\nthe training cost and increases scalability. We demonstrate that BCM enables\none-step generation and inversion while also allowing the use of additional\nsteps to enhance generation quality or reduce reconstruction error. We further\nshowcase BCM's capability in downstream tasks, such as interpolation,\ninpainting, and blind restoration of compressed images. Notably, when the\nnumber of function evaluations (NFE) is constrained, BCM surpasses\ndomain-specific restoration methods, such as I$^2$SB and Palette, in a fully\nzero-shot manner, offering an efficient alternative for inversion problems. Our\ncode and weights are available at\nhttps://github.com/Mosasaur5526/BCM-iCT-torch."
                },
                "authors": [
                    {
                        "name": "Liangchen Li"
                    },
                    {
                        "name": "Jiajun He"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun He"
                },
                "author": "Jiajun He",
                "arxiv_comment": "39 pages, 27 figures; a shorter version of this paper also appeared\n  in the ICML 2024 Workshop on Structured Probabilistic Inference & Generative\n  Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12830v3",
                "updated": "2024-09-30T11:15:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    15,
                    24,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-18T17:51:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    17,
                    51,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "What Are the Odds? Language Models Are Capable of Probabilistic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Are the Odds? Language Models Are Capable of Probabilistic\n  Reasoning"
                },
                "summary": "Language models (LM) are capable of remarkably complex linguistic tasks;\nhowever, numerical reasoning is an area in which they frequently struggle. An\nimportant but rarely evaluated form of reasoning is understanding probability\ndistributions. In this paper, we focus on evaluating the probabilistic\nreasoning capabilities of LMs using idealized and real-world statistical\ndistributions. We perform a systematic evaluation of state-of-the-art LMs on\nthree tasks: estimating percentiles, drawing samples, and calculating\nprobabilities. We evaluate three ways to provide context to LMs 1) anchoring\nexamples from within a distribution or family of distributions, 2) real-world\ncontext, 3) summary statistics on which to base a Normal approximation. Models\ncan make inferences about distributions, and can be further aided by the\nincorporation of real-world context, example shots and simplified assumptions,\neven if these assumptions are incorrect or misspecified. To conduct this work,\nwe developed a comprehensive benchmark distribution dataset with associated\nquestion-answer pairs that we have released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LM) are capable of remarkably complex linguistic tasks;\nhowever, numerical reasoning is an area in which they frequently struggle. An\nimportant but rarely evaluated form of reasoning is understanding probability\ndistributions. In this paper, we focus on evaluating the probabilistic\nreasoning capabilities of LMs using idealized and real-world statistical\ndistributions. We perform a systematic evaluation of state-of-the-art LMs on\nthree tasks: estimating percentiles, drawing samples, and calculating\nprobabilities. We evaluate three ways to provide context to LMs 1) anchoring\nexamples from within a distribution or family of distributions, 2) real-world\ncontext, 3) summary statistics on which to base a Normal approximation. Models\ncan make inferences about distributions, and can be further aided by the\nincorporation of real-world context, example shots and simplified assumptions,\neven if these assumptions are incorrect or misspecified. To conduct this work,\nwe developed a comprehensive benchmark distribution dataset with associated\nquestion-answer pairs that we have released publicly."
                },
                "authors": [
                    {
                        "name": "Akshay Paruchuri"
                    },
                    {
                        "name": "Jake Garrison"
                    },
                    {
                        "name": "Shun Liao"
                    },
                    {
                        "name": "John Hernandez"
                    },
                    {
                        "name": "Jacob Sunshine"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Daniel McDuff"
                    }
                ],
                "author_detail": {
                    "name": "Daniel McDuff"
                },
                "author": "Daniel McDuff",
                "arxiv_comment": "EMNLP 2024 (Main), 21 pages, 9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20192v1",
                "updated": "2024-09-30T11:08:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    8,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T11:08:27Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    8,
                    27,
                    0,
                    274,
                    0
                ],
                "title": "Factory Operators' Perspectives on Cognitive Assistants for Knowledge\n  Sharing: Challenges, Risks, and Impact on Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factory Operators' Perspectives on Cognitive Assistants for Knowledge\n  Sharing: Challenges, Risks, and Impact on Work"
                },
                "summary": "In the shift towards human-centered manufacturing, our two-year longitudinal\nstudy investigates the real-world impact of deploying Cognitive Assistants\n(CAs) in factories. The CAs were designed to facilitate knowledge sharing among\nfactory operators. Our investigation focused on smartphone-based voice\nassistants and LLM-powered chatbots, examining their usability and utility in a\nreal-world factory setting. Based on the qualitative feedback we collected\nduring the deployments of CAs at the factories, we conducted a thematic\nanalysis to investigate the perceptions, challenges, and overall impact on\nworkflow and knowledge sharing.\n  Our results indicate that while CAs have the potential to significantly\nimprove efficiency through knowledge sharing and quicker resolution of\nproduction issues, they also introduce concerns around workplace surveillance,\nthe types of knowledge that can be shared, and shortcomings compared to\nhuman-to-human knowledge sharing. Additionally, our findings stress the\nimportance of addressing privacy, knowledge contribution burdens, and tensions\nbetween factory operators and their managers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the shift towards human-centered manufacturing, our two-year longitudinal\nstudy investigates the real-world impact of deploying Cognitive Assistants\n(CAs) in factories. The CAs were designed to facilitate knowledge sharing among\nfactory operators. Our investigation focused on smartphone-based voice\nassistants and LLM-powered chatbots, examining their usability and utility in a\nreal-world factory setting. Based on the qualitative feedback we collected\nduring the deployments of CAs at the factories, we conducted a thematic\nanalysis to investigate the perceptions, challenges, and overall impact on\nworkflow and knowledge sharing.\n  Our results indicate that while CAs have the potential to significantly\nimprove efficiency through knowledge sharing and quicker resolution of\nproduction issues, they also introduce concerns around workplace surveillance,\nthe types of knowledge that can be shared, and shortcomings compared to\nhuman-to-human knowledge sharing. Additionally, our findings stress the\nimportance of addressing privacy, knowledge contribution burdens, and tensions\nbetween factory operators and their managers."
                },
                "authors": [
                    {
                        "name": "Samuel Kernan Freire"
                    },
                    {
                        "name": "Tianhao He"
                    },
                    {
                        "name": "Chaofan Wang"
                    },
                    {
                        "name": "Evangelos Niforatos"
                    },
                    {
                        "name": "Alessandro Bozzon"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Bozzon"
                },
                "author": "Alessandro Bozzon",
                "arxiv_comment": "32 pages, 6 figures, 2 tables, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06790v2",
                "updated": "2024-09-30T11:05:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    5,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-09T12:00:44Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    12,
                    0,
                    44,
                    1,
                    191,
                    0
                ],
                "title": "Constraining the dense matter equation of state with new NICER\n  mass-radius measurements and new chiral effective field theory inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the dense matter equation of state with new NICER\n  mass-radius measurements and new chiral effective field theory inputs"
                },
                "summary": "Pulse profile modeling of X-ray data from NICER is now enabling precision\ninference of neutron star mass and radius. Combined with nuclear physics\nconstraints from chiral effective field theory ($\\chi$EFT), and masses and\ntidal deformabilities inferred from gravitational wave detections of binary\nneutron star mergers, this has lead to a steady improvement in our\nunderstanding of the dense matter equation of state (EOS). Here we consider the\nimpact of several new results: the radius measurement for the 1.42$\\,M_\\odot$\npulsar PSR J0437$-$4715 presented by Choudhury et al. (2024), updates to the\nmasses and radii of PSR J0740$+$6620 and PSR J0030$+$0451, and new $\\chi$EFT\nresults for neutron star matter up to 1.5 times nuclear saturation density.\nUsing two different high-density EOS extensions -- a piecewise-polytropic (PP)\nmodel and a model based on the speed of sound in a neutron star (CS) -- we find\nthe radius of a 1.4$\\,M_\\odot$ (2.0$\\,M_\\odot$) neutron star to be constrained\nto the 95% credible ranges $12.28^{+0.50}_{-0.76}\\,$km\n($12.33^{+0.70}_{-1.34}\\,$km) for the PP model and $12.01^{+0.56}_{-0.75}\\,$km\n($11.55^{+0.94}_{-1.09}\\,$km) for the CS model. The maximum neutron star mass\nis predicted to be $2.15^{+0.14}_{-0.16}\\,$$M_\\odot$ and\n$2.08^{+0.28}_{-0.16}\\,$$M_\\odot$ for the PP and CS model, respectively. We\nexplore the sensitivity of our results to different orders and different\ndensities up to which $\\chi$EFT is used, and show how the astrophysical\nobservations provide constraints for the pressure at intermediate densities.\nMoreover, we investigate the difference $R_{2.0} - R_{1.4}$ of the radius of\n2$\\,M_\\odot$ and 1.4$\\,M_\\odot$ neutron stars within our EOS inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulse profile modeling of X-ray data from NICER is now enabling precision\ninference of neutron star mass and radius. Combined with nuclear physics\nconstraints from chiral effective field theory ($\\chi$EFT), and masses and\ntidal deformabilities inferred from gravitational wave detections of binary\nneutron star mergers, this has lead to a steady improvement in our\nunderstanding of the dense matter equation of state (EOS). Here we consider the\nimpact of several new results: the radius measurement for the 1.42$\\,M_\\odot$\npulsar PSR J0437$-$4715 presented by Choudhury et al. (2024), updates to the\nmasses and radii of PSR J0740$+$6620 and PSR J0030$+$0451, and new $\\chi$EFT\nresults for neutron star matter up to 1.5 times nuclear saturation density.\nUsing two different high-density EOS extensions -- a piecewise-polytropic (PP)\nmodel and a model based on the speed of sound in a neutron star (CS) -- we find\nthe radius of a 1.4$\\,M_\\odot$ (2.0$\\,M_\\odot$) neutron star to be constrained\nto the 95% credible ranges $12.28^{+0.50}_{-0.76}\\,$km\n($12.33^{+0.70}_{-1.34}\\,$km) for the PP model and $12.01^{+0.56}_{-0.75}\\,$km\n($11.55^{+0.94}_{-1.09}\\,$km) for the CS model. The maximum neutron star mass\nis predicted to be $2.15^{+0.14}_{-0.16}\\,$$M_\\odot$ and\n$2.08^{+0.28}_{-0.16}\\,$$M_\\odot$ for the PP and CS model, respectively. We\nexplore the sensitivity of our results to different orders and different\ndensities up to which $\\chi$EFT is used, and show how the astrophysical\nobservations provide constraints for the pressure at intermediate densities.\nMoreover, we investigate the difference $R_{2.0} - R_{1.4}$ of the radius of\n2$\\,M_\\odot$ and 1.4$\\,M_\\odot$ neutron stars within our EOS inference."
                },
                "authors": [
                    {
                        "name": "Nathan Rutherford"
                    },
                    {
                        "name": "Melissa Mendes"
                    },
                    {
                        "name": "Isak Svensson"
                    },
                    {
                        "name": "Achim Schwenk"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Kai Hebeler"
                    },
                    {
                        "name": "Jonas Keller"
                    },
                    {
                        "name": "Chanda Prescod-Weinstein"
                    },
                    {
                        "name": "Devarshi Choudhury"
                    },
                    {
                        "name": "Geert Raaijmakers"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Patrick Timmerman"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "Sebastien Guillot"
                    },
                    {
                        "name": "James M. Lattimer"
                    }
                ],
                "author_detail": {
                    "name": "James M. Lattimer"
                },
                "author": "James M. Lattimer",
                "arxiv_doi": "10.3847/2041-8213/ad5f02",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad5f02",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.06790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 15 figures; published version",
                "arxiv_journal_ref": "ApJ Letters, 971, L19 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20181v1",
                "updated": "2024-09-30T10:48:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    48,
                    20,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:48:20Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    48,
                    20,
                    0,
                    274,
                    0
                ],
                "title": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v3",
                "updated": "2024-09-30T10:43:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    43,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20175v1",
                "updated": "2024-09-30T10:36:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    36,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:36:41Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    36,
                    41,
                    0,
                    274,
                    0
                ],
                "title": "Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse\n  Problems"
                },
                "summary": "When solving inverse problems, it is increasingly popular to use pre-trained\ndiffusion models as plug-and-play priors. This framework can accommodate\ndifferent forward models without re-training while preserving the generative\ncapability of diffusion models. Despite their success in many imaging inverse\nproblems, most existing methods rely on privileged information such as\nderivative, pseudo-inverse, or full knowledge about the forward model. This\nreliance poses a substantial limitation that restricts their use in a wide\nrange of problems where such information is unavailable, such as in many\nscientific applications. To address this issue, we propose Ensemble Kalman\nDiffusion Guidance (EnKG) for diffusion models, a derivative-free approach that\ncan solve inverse problems by only accessing forward model evaluations and a\npre-trained diffusion model prior. We study the empirical effectiveness of our\nmethod across various inverse problems, including scientific settings such as\ninferring fluid flows and astronomical objects, which are highly non-linear\ninverse problems that often only permit black-box access to the forward model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When solving inverse problems, it is increasingly popular to use pre-trained\ndiffusion models as plug-and-play priors. This framework can accommodate\ndifferent forward models without re-training while preserving the generative\ncapability of diffusion models. Despite their success in many imaging inverse\nproblems, most existing methods rely on privileged information such as\nderivative, pseudo-inverse, or full knowledge about the forward model. This\nreliance poses a substantial limitation that restricts their use in a wide\nrange of problems where such information is unavailable, such as in many\nscientific applications. To address this issue, we propose Ensemble Kalman\nDiffusion Guidance (EnKG) for diffusion models, a derivative-free approach that\ncan solve inverse problems by only accessing forward model evaluations and a\npre-trained diffusion model prior. We study the empirical effectiveness of our\nmethod across various inverse problems, including scientific settings such as\ninferring fluid flows and astronomical objects, which are highly non-linear\ninverse problems that often only permit black-box access to the forward model."
                },
                "authors": [
                    {
                        "name": "Hongkai Zheng"
                    },
                    {
                        "name": "Wenda Chu"
                    },
                    {
                        "name": "Austin Wang"
                    },
                    {
                        "name": "Nikola Kovachki"
                    },
                    {
                        "name": "Ricardo Baptista"
                    },
                    {
                        "name": "Yisong Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yisong Yue"
                },
                "author": "Yisong Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00341v2",
                "updated": "2024-09-30T10:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-29T07:00:37Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    7,
                    0,
                    37,
                    5,
                    181,
                    0
                ],
                "title": "Iterative Data Generation with Large Language Models for Aspect-based\n  Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Data Generation with Large Language Models for Aspect-based\n  Sentiment Analysis"
                },
                "summary": "Aspect-based Sentiment Analysis (ABSA) is an important sentiment analysis\ntask, which aims to determine the sentiment polarity towards an aspect in a\nsentence. Due to the expensive and limited labeled data, data generation (DG)\nhas become the standard for improving the performance of ABSA. However, current\nDG methods usually have some shortcomings: 1) poor fluency and coherence, 2)\nlack of diversity of generated data, and 3) reliance on some existing labeled\ndata, hindering its applications in real-world scenarios. With the advancement\nof large language models (LLMs), LLM-based DG has the potential to solve the\nabove issues. Unfortunately, directly prompting LLMs struggles to generate the\ndesired pseudo-label ABSA data, as LLMs are prone to hallucinations, leading to\nundesired data generation. To this end, we propose a systematic Iterative Data\nGeneration framework, namely IDG, to boost the performance of ABSA. The core of\nIDG is to make full use of the powerful abilities (i.e., instruction-following,\nin-context learning and self-reflection) of LLMs to iteratively generate more\nfluent and diverse pseudo-label data, starting from an unsupervised sentence\ncorpus. Specifically, IDG designs a novel iterative data generation mechanism\nand a self-reflection data filtering module to tackle the challenges of\nunexpected data generation caused by hallucinations. Extensive experiments on\nfour widely-used ABSA benchmarks show that IDG brings consistent and\nsignificant performance gains among five baseline ABSA models. More\nencouragingly, the synthetic data generated by IDG can achieve comparable or\neven better performance against the manually annotated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based Sentiment Analysis (ABSA) is an important sentiment analysis\ntask, which aims to determine the sentiment polarity towards an aspect in a\nsentence. Due to the expensive and limited labeled data, data generation (DG)\nhas become the standard for improving the performance of ABSA. However, current\nDG methods usually have some shortcomings: 1) poor fluency and coherence, 2)\nlack of diversity of generated data, and 3) reliance on some existing labeled\ndata, hindering its applications in real-world scenarios. With the advancement\nof large language models (LLMs), LLM-based DG has the potential to solve the\nabove issues. Unfortunately, directly prompting LLMs struggles to generate the\ndesired pseudo-label ABSA data, as LLMs are prone to hallucinations, leading to\nundesired data generation. To this end, we propose a systematic Iterative Data\nGeneration framework, namely IDG, to boost the performance of ABSA. The core of\nIDG is to make full use of the powerful abilities (i.e., instruction-following,\nin-context learning and self-reflection) of LLMs to iteratively generate more\nfluent and diverse pseudo-label data, starting from an unsupervised sentence\ncorpus. Specifically, IDG designs a novel iterative data generation mechanism\nand a self-reflection data filtering module to tackle the challenges of\nunexpected data generation caused by hallucinations. Extensive experiments on\nfour widely-used ABSA benchmarks show that IDG brings consistent and\nsignificant performance gains among five baseline ABSA models. More\nencouragingly, the synthetic data generated by IDG can achieve comparable or\neven better performance against the manually annotated data."
                },
                "authors": [
                    {
                        "name": "Qihuang Zhong"
                    },
                    {
                        "name": "Haiyun Li"
                    },
                    {
                        "name": "Luyao Zhuang"
                    },
                    {
                        "name": "Juhua Liu"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20165v1",
                "updated": "2024-09-30T10:23:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    23,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:23:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    23,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "How Entangled is Factuality and Deception in German?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Entangled is Factuality and Deception in German?"
                },
                "summary": "The statement \"The earth is flat\" is factually inaccurate, but if someone\ntruly believes and argues in its favor, it is not deceptive. Research on\ndeception detection and fact checking often conflates factual accuracy with the\ntruthfulness of statements. This assumption makes it difficult to (a) study\nsubtle distinctions and interactions between the two and (b) gauge their\neffects on downstream tasks. The belief-based deception framework disentangles\nthese properties by defining texts as deceptive when there is a mismatch\nbetween what people say and what they truly believe. In this study, we assess\nif presumed patterns of deception generalize to German language texts. We test\nthe effectiveness of computational models in detecting deception using an\nestablished corpus of belief-based argumentation. Finally, we gauge the impact\nof deception on the downstream task of fact checking and explore if this\nproperty confounds verification models. Surprisingly, our analysis finds no\ncorrelation with established cues of deception. Previous work claimed that\ncomputational models can outperform humans in deception detection accuracy,\nhowever, our experiments show that both traditional and state-of-the-art models\nstruggle with the task, performing no better than random guessing. For fact\nchecking, we find that Natural Language Inference-based verification performs\nworse on non-factual and deceptive content, while prompting Large Language\nModels for the same task is less sensitive to these properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The statement \"The earth is flat\" is factually inaccurate, but if someone\ntruly believes and argues in its favor, it is not deceptive. Research on\ndeception detection and fact checking often conflates factual accuracy with the\ntruthfulness of statements. This assumption makes it difficult to (a) study\nsubtle distinctions and interactions between the two and (b) gauge their\neffects on downstream tasks. The belief-based deception framework disentangles\nthese properties by defining texts as deceptive when there is a mismatch\nbetween what people say and what they truly believe. In this study, we assess\nif presumed patterns of deception generalize to German language texts. We test\nthe effectiveness of computational models in detecting deception using an\nestablished corpus of belief-based argumentation. Finally, we gauge the impact\nof deception on the downstream task of fact checking and explore if this\nproperty confounds verification models. Surprisingly, our analysis finds no\ncorrelation with established cues of deception. Previous work claimed that\ncomputational models can outperform humans in deception detection accuracy,\nhowever, our experiments show that both traditional and state-of-the-art models\nstruggle with the task, performing no better than random guessing. For fact\nchecking, we find that Natural Language Inference-based verification performs\nworse on non-factual and deceptive content, while prompting Large Language\nModels for the same task is less sensitive to these properties."
                },
                "authors": [
                    {
                        "name": "Aswathy Velutharambath"
                    },
                    {
                        "name": "Amelie Wührl"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "arxiv_comment": "Findings of EMNLP 2024 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20163v1",
                "updated": "2024-09-30T10:19:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    19,
                    4,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:19:04Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    19,
                    4,
                    0,
                    274,
                    0
                ],
                "title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal\n  Assistants"
                },
                "summary": "LLM-based agents have been widely applied as personal assistants, capable of\nmemorizing information from user messages and responding to personal queries.\nHowever, there still lacks an objective and automatic evaluation on their\nmemory capability, largely due to the challenges in constructing reliable\nquestions and answers (QAs) according to user messages. In this paper, we\npropose MemSim, a Bayesian simulator designed to automatically construct\nreliable QAs from generated user messages, simultaneously keeping their\ndiversity and scalability. Specifically, we introduce the Bayesian Relation\nNetwork (BRNet) and a causal generation mechanism to mitigate the impact of LLM\nhallucinations on factual information, facilitating the automatic creation of\nan evaluation dataset. Based on MemSim, we generate a dataset in the daily-life\nscenario, named MemDaily, and conduct extensive experiments to assess the\neffectiveness of our approach. We also provide a benchmark for evaluating\ndifferent memory mechanisms in LLM-based agents with the MemDaily dataset. To\nbenefit the research community, we have released our project at\nhttps://github.com/nuster1128/MemSim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have been widely applied as personal assistants, capable of\nmemorizing information from user messages and responding to personal queries.\nHowever, there still lacks an objective and automatic evaluation on their\nmemory capability, largely due to the challenges in constructing reliable\nquestions and answers (QAs) according to user messages. In this paper, we\npropose MemSim, a Bayesian simulator designed to automatically construct\nreliable QAs from generated user messages, simultaneously keeping their\ndiversity and scalability. Specifically, we introduce the Bayesian Relation\nNetwork (BRNet) and a causal generation mechanism to mitigate the impact of LLM\nhallucinations on factual information, facilitating the automatic creation of\nan evaluation dataset. Based on MemSim, we generate a dataset in the daily-life\nscenario, named MemDaily, and conduct extensive experiments to assess the\neffectiveness of our approach. We also provide a benchmark for evaluating\ndifferent memory mechanisms in LLM-based agents with the MemDaily dataset. To\nbenefit the research community, we have released our project at\nhttps://github.com/nuster1128/MemSim."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Luyu Chen"
                    },
                    {
                        "name": "Zeren Jiang"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "26 pages, 25 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05561v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05561v6",
                "updated": "2024-09-30T10:17:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    17,
                    12,
                    0,
                    274,
                    0
                ],
                "published": "2024-01-10T22:07:21Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    22,
                    7,
                    21,
                    2,
                    10,
                    0
                ],
                "title": "TrustLLM: Trustworthiness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustLLM: Trustworthiness in Large Language Models"
                },
                "summary": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Yixin Huang"
                    },
                    {
                        "name": "Wenhan Lyu"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yijue Wang"
                    },
                    {
                        "name": "Zhikun Zhang"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Chunyuan Li"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Manolis Kellis"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jian Pei"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Joaquin Vanschoren"
                    },
                    {
                        "name": "John Mitchell"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Lifang He"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Suman Jana"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "William Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Xuyu Wang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Yinzhi Cao"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "This work is still under work and we welcome your contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05561v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05561v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20156v1",
                "updated": "2024-09-30T10:07:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    7,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:07:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    7,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "ASTRA: Accurate and Scalable ANNS-based Training of Extreme Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRA: Accurate and Scalable ANNS-based Training of Extreme Classifiers"
                },
                "summary": "`Extreme Classification'' (or XC) is the task of annotating data points\n(queries) with relevant labels (documents), from an extremely large set of $L$\npossible labels, arising in search and recommendations. The most successful\ndeep learning paradigm that has emerged over the last decade or so for XC is to\nembed the queries (and labels) using a deep encoder (e.g. DistilBERT), and use\nlinear classifiers on top of the query embeddings. This architecture is of\nappeal because it enables millisecond-time inference using approximate nearest\nneighbor search (ANNS). The key question is how do we design training\nalgorithms that are accurate as well as scale to $O(100M)$ labels on a limited\nnumber of GPUs.\n  State-of-the-art XC techniques that demonstrate high accuracies (e.g., DEXML,\nRen\\'ee, DEXA) on standard datasets have per-epoch training time that scales as\n$O(L)$ or employ expensive negative sampling strategies, which are prohibitive\nin XC scenarios. In this work, we develop an accurate and scalable XC algorithm\nASTRA with two key observations: (a) building ANNS index on the classifier\nvectors and retrieving hard negatives using the classifiers aligns the negative\nsampling strategy to the loss function optimized; (b) keeping the ANNS indices\ncurrent as the classifiers change through the epochs is prohibitively expensive\nwhile using stale negatives (refreshed periodically) results in poor accuracy;\nto remedy this, we propose a negative sampling strategy that uses a mixture of\nimportance sampling and uniform sampling. By extensive evaluation on standard\nXC as well as proprietary datasets with 120M labels, we demonstrate that ASTRA\nachieves SOTA precision, while reducing training time by 4x-15x relative to the\nsecond best.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "`Extreme Classification'' (or XC) is the task of annotating data points\n(queries) with relevant labels (documents), from an extremely large set of $L$\npossible labels, arising in search and recommendations. The most successful\ndeep learning paradigm that has emerged over the last decade or so for XC is to\nembed the queries (and labels) using a deep encoder (e.g. DistilBERT), and use\nlinear classifiers on top of the query embeddings. This architecture is of\nappeal because it enables millisecond-time inference using approximate nearest\nneighbor search (ANNS). The key question is how do we design training\nalgorithms that are accurate as well as scale to $O(100M)$ labels on a limited\nnumber of GPUs.\n  State-of-the-art XC techniques that demonstrate high accuracies (e.g., DEXML,\nRen\\'ee, DEXA) on standard datasets have per-epoch training time that scales as\n$O(L)$ or employ expensive negative sampling strategies, which are prohibitive\nin XC scenarios. In this work, we develop an accurate and scalable XC algorithm\nASTRA with two key observations: (a) building ANNS index on the classifier\nvectors and retrieving hard negatives using the classifiers aligns the negative\nsampling strategy to the loss function optimized; (b) keeping the ANNS indices\ncurrent as the classifiers change through the epochs is prohibitively expensive\nwhile using stale negatives (refreshed periodically) results in poor accuracy;\nto remedy this, we propose a negative sampling strategy that uses a mixture of\nimportance sampling and uniform sampling. By extensive evaluation on standard\nXC as well as proprietary datasets with 120M labels, we demonstrate that ASTRA\nachieves SOTA precision, while reducing training time by 4x-15x relative to the\nsecond best."
                },
                "authors": [
                    {
                        "name": "Sonu Mehta"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Nagarajan Natarajan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Manik Varma"
                    }
                ],
                "author_detail": {
                    "name": "Manik Varma"
                },
                "author": "Manik Varma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20154v1",
                "updated": "2024-09-30T10:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    2,
                    42,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    2,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for\n  Generalized 3D Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for\n  Generalized 3D Manipulation"
                },
                "summary": "Robots' ability to follow language instructions and execute diverse 3D tasks\nis vital in robot learning. Traditional imitation learning-based methods\nperform well on seen tasks but struggle with novel, unseen ones due to\nvariability. Recent approaches leverage large foundation models to assist in\nunderstanding novel tasks, thereby mitigating this issue. However, these\nmethods lack a task-specific learning process, which is essential for an\naccurate understanding of 3D environments, often leading to execution failures.\nIn this paper, we introduce GravMAD, a sub-goal-driven, language-conditioned\naction diffusion framework that combines the strengths of imitation learning\nand foundation models. Our approach breaks tasks into sub-goals based on\nlanguage instructions, allowing auxiliary guidance during both training and\ninference. During training, we introduce Sub-goal Keypose Discovery to identify\nkey sub-goals from demonstrations. Inference differs from training, as there\nare no demonstrations available, so we use pre-trained foundation models to\nbridge the gap and identify sub-goals for the current task. In both phases,\nGravMaps are generated from sub-goals, providing flexible 3D spatial guidance\ncompared to fixed 3D positions. Empirical evaluations on RLBench show that\nGravMAD significantly outperforms state-of-the-art methods, with a 28.63%\nimprovement on novel tasks and a 13.36% gain on tasks encountered during\ntraining. These results demonstrate GravMAD's strong multi-task learning and\ngeneralization in 3D manipulation. Video demonstrations are available at:\nhttps://gravmad.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots' ability to follow language instructions and execute diverse 3D tasks\nis vital in robot learning. Traditional imitation learning-based methods\nperform well on seen tasks but struggle with novel, unseen ones due to\nvariability. Recent approaches leverage large foundation models to assist in\nunderstanding novel tasks, thereby mitigating this issue. However, these\nmethods lack a task-specific learning process, which is essential for an\naccurate understanding of 3D environments, often leading to execution failures.\nIn this paper, we introduce GravMAD, a sub-goal-driven, language-conditioned\naction diffusion framework that combines the strengths of imitation learning\nand foundation models. Our approach breaks tasks into sub-goals based on\nlanguage instructions, allowing auxiliary guidance during both training and\ninference. During training, we introduce Sub-goal Keypose Discovery to identify\nkey sub-goals from demonstrations. Inference differs from training, as there\nare no demonstrations available, so we use pre-trained foundation models to\nbridge the gap and identify sub-goals for the current task. In both phases,\nGravMaps are generated from sub-goals, providing flexible 3D spatial guidance\ncompared to fixed 3D positions. Empirical evaluations on RLBench show that\nGravMAD significantly outperforms state-of-the-art methods, with a 28.63%\nimprovement on novel tasks and a 13.36% gain on tasks encountered during\ntraining. These results demonstrate GravMAD's strong multi-task learning and\ngeneralization in 3D manipulation. Video demonstrations are available at:\nhttps://gravmad.github.io."
                },
                "authors": [
                    {
                        "name": "Yangtao Chen"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Junhui Yin"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Pinzhuo Tian"
                    },
                    {
                        "name": "Jieqi Shi"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20149v1",
                "updated": "2024-09-30T09:55:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    55,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:55:39Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    55,
                    39,
                    0,
                    274,
                    0
                ],
                "title": "1 Trillion Token (1TT) Platform: A Novel Framework for Efficient Data\n  Sharing and Compensation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1 Trillion Token (1TT) Platform: A Novel Framework for Efficient Data\n  Sharing and Compensation in Large Language Models"
                },
                "summary": "In this paper, we propose the 1 Trillion Token Platform (1TT Platform), a\nnovel framework designed to facilitate efficient data sharing with a\ntransparent and equitable profit-sharing mechanism. The platform fosters\ncollaboration between data contributors, who provide otherwise non-disclosed\ndatasets, and a data consumer, who utilizes these datasets to enhance their own\nservices. Data contributors are compensated in monetary terms, receiving a\nshare of the revenue generated by the services of the data consumer. The data\nconsumer is committed to sharing a portion of the revenue with contributors,\naccording to predefined profit-sharing arrangements. By incorporating a\ntransparent profit-sharing paradigm to incentivize large-scale data sharing,\nthe 1TT Platform creates a collaborative environment to drive the advancement\nof NLP and LLM technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose the 1 Trillion Token Platform (1TT Platform), a\nnovel framework designed to facilitate efficient data sharing with a\ntransparent and equitable profit-sharing mechanism. The platform fosters\ncollaboration between data contributors, who provide otherwise non-disclosed\ndatasets, and a data consumer, who utilizes these datasets to enhance their own\nservices. Data contributors are compensated in monetary terms, receiving a\nshare of the revenue generated by the services of the data consumer. The data\nconsumer is committed to sharing a portion of the revenue with contributors,\naccording to predefined profit-sharing arrangements. By incorporating a\ntransparent profit-sharing paradigm to incentivize large-scale data sharing,\nthe 1TT Platform creates a collaborative environment to drive the advancement\nof NLP and LLM technologies."
                },
                "authors": [
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Hyunsoo Ha"
                    },
                    {
                        "name": "Jihoo Kim"
                    },
                    {
                        "name": "Yungi Kim"
                    },
                    {
                        "name": "Dahyun Kim"
                    },
                    {
                        "name": "Sukyung Lee"
                    },
                    {
                        "name": "Seonghoon Yang"
                    }
                ],
                "author_detail": {
                    "name": "Seonghoon Yang"
                },
                "author": "Seonghoon Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20147v1",
                "updated": "2024-09-30T09:52:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    52,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:52:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    52,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "Classification of Radiological Text in Small and Imbalanced Datasets in\n  a Non-English Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification of Radiological Text in Small and Imbalanced Datasets in\n  a Non-English Language"
                },
                "summary": "Natural language processing (NLP) in the medical domain can underperform in\nreal-world applications involving small datasets in a non-English language with\nfew labeled samples and imbalanced classes. There is yet no consensus on how to\napproach this problem. We evaluated a set of NLP models including BERT-like\ntransformers, few-shot learning with sentence transformers (SetFit), and\nprompted large language models (LLM), using three datasets of radiology reports\non magnetic resonance images of epilepsy patients in Danish, a low-resource\nlanguage. Our results indicate that BERT-like models pretrained in the target\ndomain of radiology reports currently offer the optimal performances for this\nscenario. Notably, the SetFit and LLM models underperformed compared to\nBERT-like models, with LLM performing the worst. Importantly, none of the\nmodels investigated was sufficiently accurate to allow for text classification\nwithout any supervision. However, they show potential for data filtering, which\ncould reduce the amount of manual labeling required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) in the medical domain can underperform in\nreal-world applications involving small datasets in a non-English language with\nfew labeled samples and imbalanced classes. There is yet no consensus on how to\napproach this problem. We evaluated a set of NLP models including BERT-like\ntransformers, few-shot learning with sentence transformers (SetFit), and\nprompted large language models (LLM), using three datasets of radiology reports\non magnetic resonance images of epilepsy patients in Danish, a low-resource\nlanguage. Our results indicate that BERT-like models pretrained in the target\ndomain of radiology reports currently offer the optimal performances for this\nscenario. Notably, the SetFit and LLM models underperformed compared to\nBERT-like models, with LLM performing the worst. Importantly, none of the\nmodels investigated was sufficiently accurate to allow for text classification\nwithout any supervision. However, they show potential for data filtering, which\ncould reduce the amount of manual labeling required."
                },
                "authors": [
                    {
                        "name": "Vincent Beliveau"
                    },
                    {
                        "name": "Helene Kaas"
                    },
                    {
                        "name": "Martin Prener"
                    },
                    {
                        "name": "Claes N. Ladefoged"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Gitte M. Knudsen"
                    },
                    {
                        "name": "Lars H. Pinborg"
                    },
                    {
                        "name": "Melanie Ganz"
                    }
                ],
                "author_detail": {
                    "name": "Melanie Ganz"
                },
                "author": "Melanie Ganz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07736v3",
                "updated": "2024-09-30T09:49:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    49,
                    8,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-11T21:46:03Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    21,
                    46,
                    3,
                    1,
                    163,
                    0
                ],
                "title": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models"
                },
                "summary": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems."
                },
                "authors": [
                    {
                        "name": "Dojun Park"
                    },
                    {
                        "name": "Jiwoo Lee"
                    },
                    {
                        "name": "Seohyun Park"
                    },
                    {
                        "name": "Hyeyun Jeong"
                    },
                    {
                        "name": "Youngeun Koo"
                    },
                    {
                        "name": "Soonha Hwang"
                    },
                    {
                        "name": "Seonwoo Park"
                    },
                    {
                        "name": "Sungeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Lee"
                },
                "author": "Sungeun Lee",
                "arxiv_comment": "The 2nd GenBench workshop on generalisation (benchmarking) in NLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04808v2",
                "updated": "2024-09-30T09:41:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    41,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-06T10:55:30Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    10,
                    55,
                    30,
                    2,
                    66,
                    0
                ],
                "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off"
                },
                "summary": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax."
                },
                "authors": [
                    {
                        "name": "Eva Giboulot"
                    },
                    {
                        "name": "Furon Teddy"
                    }
                ],
                "author_detail": {
                    "name": "Furon Teddy"
                },
                "author": "Furon Teddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v2",
                "updated": "2024-10-01T05:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    37,
                    7,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14351v2",
                "updated": "2024-09-30T09:19:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    19,
                    49,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-26T15:29:09Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    9,
                    0,
                    239,
                    0
                ],
                "title": "Can supernova from runaway stars mimic the signs of absorbing\n  `super-virial' gas?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can supernova from runaway stars mimic the signs of absorbing\n  `super-virial' gas?"
                },
                "summary": "The recent detection of large column density absorption lines from highly\nionized gas in a few directions through the circumgalactic medium (CGM) of the\nMilky Way (MW) has been puzzling. The inferred temperature from these\nabsorption lines far exceeds the virial temperature of the MW, and the column\ndensities are also too large to be easily explained. In this paper, we propose\na novel idea to explain these observations and claim that they may not have\noriginated from the CGM, but from a totally different type of source, namely,\nstellar ejecta from supernovae (SNe) above the Galactic disk that happen to lie\nin the line of sight to the background quasars. About $\\sim 20\\%$ of massive OB\nstars (progenitors of core-collapse supernovae) are known to be runaway stars\nthat have high ejection velocities near the Galactic plane and can end up\nexploding as SNe above the Galactic disk. We show that the associated reverse\nshock in the supernova remnant in the early non-radiative phase can heat the\nejecta to temperatures of $\\gtrsim 10^7\\,{\\rm K}$ and can naturally explain the\nobserved high column density of ions in the observed `super-virial' phase along\nwith $\\alpha$-enriched super-solar abundance that is typical of core-collapse\nsupernovae. However, SNe from runaway stars has a covering fraction of\n$\\lesssim 0.7 \\%$ and thus can only explain the observations along limited\nsightlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent detection of large column density absorption lines from highly\nionized gas in a few directions through the circumgalactic medium (CGM) of the\nMilky Way (MW) has been puzzling. The inferred temperature from these\nabsorption lines far exceeds the virial temperature of the MW, and the column\ndensities are also too large to be easily explained. In this paper, we propose\na novel idea to explain these observations and claim that they may not have\noriginated from the CGM, but from a totally different type of source, namely,\nstellar ejecta from supernovae (SNe) above the Galactic disk that happen to lie\nin the line of sight to the background quasars. About $\\sim 20\\%$ of massive OB\nstars (progenitors of core-collapse supernovae) are known to be runaway stars\nthat have high ejection velocities near the Galactic plane and can end up\nexploding as SNe above the Galactic disk. We show that the associated reverse\nshock in the supernova remnant in the early non-radiative phase can heat the\nejecta to temperatures of $\\gtrsim 10^7\\,{\\rm K}$ and can naturally explain the\nobserved high column density of ions in the observed `super-virial' phase along\nwith $\\alpha$-enriched super-solar abundance that is typical of core-collapse\nsupernovae. However, SNe from runaway stars has a covering fraction of\n$\\lesssim 0.7 \\%$ and thus can only explain the observations along limited\nsightlines."
                },
                "authors": [
                    {
                        "name": "Mukesh Singh Bisht"
                    },
                    {
                        "name": "Projjwal Banerjee"
                    },
                    {
                        "name": "Biman B. Nath"
                    },
                    {
                        "name": "Yuri Shchekinov"
                    }
                ],
                "author_detail": {
                    "name": "Yuri Shchekinov"
                },
                "author": "Yuri Shchekinov",
                "arxiv_comment": "14 pages, accepted for publication in the Astrophysical Journal (ApJ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09299v3",
                "updated": "2024-09-30T09:16:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    16,
                    47,
                    0,
                    274,
                    0
                ],
                "published": "2024-02-14T16:41:35Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    16,
                    41,
                    35,
                    2,
                    45,
                    0
                ],
                "title": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code"
                },
                "summary": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets."
                },
                "authors": [
                    {
                        "name": "Vahid Majdinasab"
                    },
                    {
                        "name": "Amin Nikanjam"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "Accepted for publication in TOSEM (ACM Transactions on Software\n  Engineering and Methodology)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05014v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05014v2",
                "updated": "2024-09-30T09:13:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    13,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-08T05:13:25Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    5,
                    13,
                    25,
                    6,
                    281,
                    0
                ],
                "title": "Congruence Closure Modulo Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Congruence Closure Modulo Groups"
                },
                "summary": "This paper presents a new framework for constructing congruence closure of a\nfinite set of ground equations over uninterpreted symbols and interpreted\nsymbols for the group axioms. In this framework, ground equations are flattened\ninto certain forms by introducing new constants, and a completion procedure is\nperformed on ground flat equations. The proposed completion procedure uses\nequational inference rules and constructs a ground convergent rewrite system\nfor congruence closure with such interpreted symbols. If the completion\nprocedure terminates, then it yields a decision procedure for the word problem\nfor a finite set of ground equations with respect to the group axioms. This\npaper also provides a sufficient terminating condition of the completion\nprocedure for constructing a ground convergent rewrite system from ground flat\nequations containing interpreted symbols for the group axioms. In addition,\nthis paper presents a new method for constructing congruence closure of a\nfinite set of ground equations containing interpreted symbols for the\nsemigroup, monoid, and the multiple disjoint sets of group axioms,\nrespectively, using the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new framework for constructing congruence closure of a\nfinite set of ground equations over uninterpreted symbols and interpreted\nsymbols for the group axioms. In this framework, ground equations are flattened\ninto certain forms by introducing new constants, and a completion procedure is\nperformed on ground flat equations. The proposed completion procedure uses\nequational inference rules and constructs a ground convergent rewrite system\nfor congruence closure with such interpreted symbols. If the completion\nprocedure terminates, then it yields a decision procedure for the word problem\nfor a finite set of ground equations with respect to the group axioms. This\npaper also provides a sufficient terminating condition of the completion\nprocedure for constructing a ground convergent rewrite system from ground flat\nequations containing interpreted symbols for the group axioms. In addition,\nthis paper presents a new method for constructing congruence closure of a\nfinite set of ground equations containing interpreted symbols for the\nsemigroup, monoid, and the multiple disjoint sets of group axioms,\nrespectively, using the proposed framework."
                },
                "authors": [
                    {
                        "name": "Dohan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dohan Kim"
                },
                "author": "Dohan Kim",
                "arxiv_comment": "29 pages, 1 figure, submitted to LMCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05014v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05014v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11030v2",
                "updated": "2024-09-30T09:03:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    3,
                    50,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    17,
                    59,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese\n  Food Culture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese\n  Food Culture"
                },
                "summary": "Food is a rich and varied dimension of cultural heritage, crucial to both\nindividuals and social groups. To bridge the gap in the literature on the\noften-overlooked regional diversity in this domain, we introduce FoodieQA, a\nmanually curated, fine-grained image-text dataset capturing the intricate\nfeatures of food cultures across various regions in China. We evaluate\nvision-language Models (VLMs) and large language models (LLMs) on newly\ncollected, unseen food images and corresponding questions. FoodieQA comprises\nthree multiple-choice question-answering tasks where models need to answer\nquestions based on multiple images, a single image, and text-only descriptions,\nrespectively. While LLMs excel at text-based question answering, surpassing\nhuman accuracy, the open-sourced VLMs still fall short by 41% on multi-image\nand 21% on single-image VQA tasks, although closed-weights models perform\ncloser to human levels (within 10%). Our findings highlight that understanding\nfood and its cultural implications remains a challenging and under-explored\ndirection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Food is a rich and varied dimension of cultural heritage, crucial to both\nindividuals and social groups. To bridge the gap in the literature on the\noften-overlooked regional diversity in this domain, we introduce FoodieQA, a\nmanually curated, fine-grained image-text dataset capturing the intricate\nfeatures of food cultures across various regions in China. We evaluate\nvision-language Models (VLMs) and large language models (LLMs) on newly\ncollected, unseen food images and corresponding questions. FoodieQA comprises\nthree multiple-choice question-answering tasks where models need to answer\nquestions based on multiple images, a single image, and text-only descriptions,\nrespectively. While LLMs excel at text-based question answering, surpassing\nhuman accuracy, the open-sourced VLMs still fall short by 41% on multi-image\nand 21% on single-image VQA tasks, although closed-weights models perform\ncloser to human levels (within 10%). Our findings highlight that understanding\nfood and its cultural implications remains a challenging and under-explored\ndirection."
                },
                "authors": [
                    {
                        "name": "Wenyan Li"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Jiaang Li"
                    },
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Raphael Tang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Anders Søgaard"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    },
                    {
                        "name": "Desmond Elliott"
                    }
                ],
                "author_detail": {
                    "name": "Desmond Elliott"
                },
                "author": "Desmond Elliott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03937v2",
                "updated": "2024-09-30T08:54:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    54,
                    45,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-04T13:52:23Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    13,
                    52,
                    23,
                    3,
                    186,
                    0
                ],
                "title": "TongGu: Mastering Classical Chinese Understanding with\n  Knowledge-Grounded Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TongGu: Mastering Classical Chinese Understanding with\n  Knowledge-Grounded Large Language Models"
                },
                "summary": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient\nChina, yet its complexities pose formidable comprehension barriers for most\nmodern people without specialized knowledge. While Large Language Models (LLMs)\nhave shown remarkable capabilities in Natural Language Processing (NLP), they\nstruggle with Classical Chinese Understanding (CCU), especially in\ndata-demanding and knowledge-intensive tasks. In response to this dilemma, we\npropose \\textbf{TongGu} (mean understanding ancient and modern), the first\nCCU-specific LLM, underpinned by three core contributions. First, we construct\na two-stage instruction-tuning dataset ACCN-INS derived from rich classical\nChinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we\npropose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,\nenabling TongGu to acquire new capabilities while preserving its foundational\nknowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)\ntechnique to reduce hallucinations based on knowledge-grounding. Extensive\nexperiments across 24 diverse CCU tasks validate TongGu's superior ability,\nunderscoring the effectiveness of RAT and CCU-RAG. The model and dataset are\navailable at \\url{https://github.com/SCUT-DLVCLab/TongGu-LLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient\nChina, yet its complexities pose formidable comprehension barriers for most\nmodern people without specialized knowledge. While Large Language Models (LLMs)\nhave shown remarkable capabilities in Natural Language Processing (NLP), they\nstruggle with Classical Chinese Understanding (CCU), especially in\ndata-demanding and knowledge-intensive tasks. In response to this dilemma, we\npropose \\textbf{TongGu} (mean understanding ancient and modern), the first\nCCU-specific LLM, underpinned by three core contributions. First, we construct\na two-stage instruction-tuning dataset ACCN-INS derived from rich classical\nChinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we\npropose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,\nenabling TongGu to acquire new capabilities while preserving its foundational\nknowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)\ntechnique to reduce hallucinations based on knowledge-grounding. Extensive\nexperiments across 24 diverse CCU tasks validate TongGu's superior ability,\nunderscoring the effectiveness of RAT and CCU-RAG. The model and dataset are\navailable at \\url{https://github.com/SCUT-DLVCLab/TongGu-LLM}."
                },
                "authors": [
                    {
                        "name": "Jiahuan Cao"
                    },
                    {
                        "name": "Dezhi Peng"
                    },
                    {
                        "name": "Peirong Zhang"
                    },
                    {
                        "name": "Yongxin Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Kai Ding"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20094v1",
                "updated": "2024-09-30T08:47:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    47,
                    17,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:47:17Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    47,
                    17,
                    0,
                    274,
                    0
                ],
                "title": "Aggressive Post-Training Compression on Extremely Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggressive Post-Training Compression on Extremely Large Language Models"
                },
                "summary": "The increasing size and complexity of Large Language Models (LLMs) pose\nchallenges for their deployment on personal computers and mobile devices.\nAggressive post-training model compression is necessary to reduce the models'\nsize, but it often results in significant accuracy loss. To address this\nchallenge, we propose a novel network pruning technology that utilizes over 0.7\nsparsity and less than 8 bits of quantization. Our approach enables the\ncompression of prevailing LLMs within a couple of hours while maintaining a\nrelatively small accuracy loss. In experimental evaluations, our method\ndemonstrates effectiveness and potential for practical deployment. By making\nLLMs available on domestic devices, our work can facilitate a new era of\nnatural language processing applications with wide-ranging impacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size and complexity of Large Language Models (LLMs) pose\nchallenges for their deployment on personal computers and mobile devices.\nAggressive post-training model compression is necessary to reduce the models'\nsize, but it often results in significant accuracy loss. To address this\nchallenge, we propose a novel network pruning technology that utilizes over 0.7\nsparsity and less than 8 bits of quantization. Our approach enables the\ncompression of prevailing LLMs within a couple of hours while maintaining a\nrelatively small accuracy loss. In experimental evaluations, our method\ndemonstrates effectiveness and potential for practical deployment. By making\nLLMs available on domestic devices, our work can facilitate a new era of\nnatural language processing applications with wide-ranging impacts."
                },
                "authors": [
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Yao Chen"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Zhenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenjie Zhang"
                },
                "author": "Zhenjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04235v2",
                "updated": "2024-09-30T08:42:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    42,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-05-07T11:54:22Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    11,
                    54,
                    22,
                    1,
                    128,
                    0
                ],
                "title": "LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe\n  Diffusion-based Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe\n  Diffusion-based Planning"
                },
                "summary": "Operating effectively in complex environments while complying with specified\nconstraints is crucial for the safe and successful deployment of robots that\ninteract with and operate around people. In this work, we focus on generating\nlong-horizon trajectories that adhere to novel static and temporally-extended\nconstraints/instructions at test time. We propose a data-driven diffusion-based\nframework, LTLDoG, that modifies the inference steps of the reverse process\ngiven an instruction specified using finite linear temporal logic\n($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on\n$\\text{LTL}_f$ and guides the sampling steps using its gradient field. This\nvalue function can also be trained to generalize to new instructions not\nobserved during training, enabling flexible test-time adaptability. Experiments\nin robot navigation and manipulation illustrate that the method is able to\ngenerate trajectories that satisfy formulae that specify obstacle avoidance and\nvisitation sequences. Code and supplementary material are available online at\nhttps://github.com/clear-nus/ltldog.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating effectively in complex environments while complying with specified\nconstraints is crucial for the safe and successful deployment of robots that\ninteract with and operate around people. In this work, we focus on generating\nlong-horizon trajectories that adhere to novel static and temporally-extended\nconstraints/instructions at test time. We propose a data-driven diffusion-based\nframework, LTLDoG, that modifies the inference steps of the reverse process\ngiven an instruction specified using finite linear temporal logic\n($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on\n$\\text{LTL}_f$ and guides the sampling steps using its gradient field. This\nvalue function can also be trained to generalize to new instructions not\nobserved during training, enabling flexible test-time adaptability. Experiments\nin robot navigation and manipulation illustrate that the method is able to\ngenerate trajectories that satisfy formulae that specify obstacle avoidance and\nvisitation sequences. Code and supplementary material are available online at\nhttps://github.com/clear-nus/ltldog."
                },
                "authors": [
                    {
                        "name": "Zeyu Feng"
                    },
                    {
                        "name": "Hao Luan"
                    },
                    {
                        "name": "Pranav Goyal"
                    },
                    {
                        "name": "Harold Soh"
                    }
                ],
                "author_detail": {
                    "name": "Harold Soh"
                },
                "author": "Harold Soh",
                "arxiv_doi": "10.1109/LRA.2024.3443501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3443501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "in IEEE Robotics and Automation Letters, vol. 9, no. 10, pp.\n  8571-8578, Oct. 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20089v1",
                "updated": "2024-09-30T08:41:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    41,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:41:39Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    41,
                    39,
                    0,
                    274,
                    0
                ],
                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM safeguarding via refusal feature adversarial training"
                },
                "summary": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods."
                },
                "authors": [
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Virginie Do"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Nicola Cancedda"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Cancedda"
                },
                "author": "Nicola Cancedda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20087v1",
                "updated": "2024-09-30T08:40:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    40,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:40:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    40,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "Inferring Thunderstorm Occurrence from Vertical Profiles of\n  Convection-Permitting Simulations: Physical Insights from a Physical Deep\n  Learning Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Thunderstorm Occurrence from Vertical Profiles of\n  Convection-Permitting Simulations: Physical Insights from a Physical Deep\n  Learning Model"
                },
                "summary": "Thunderstorms have significant social and economic impacts due to heavy\nprecipitation, hail, lightning, and strong winds, necessitating reliable\nforecasts. Thunderstorm forecasts based on numerical weather prediction (NWP)\noften rely on single-level surrogate predictors, like convective available\npotential energy and precipitation rate, derived from vertical profiles of\nthree-dimensional atmospheric variables. In this study, we develop SALAMA 1D, a\ndeep neural network that directly infers the probability of thunderstorm\noccurrence from vertical profiles of ten atmospheric variables, bypassing\nsingle-level predictors. By training the model on convection-permitting NWP\nforecasts, we allow SALAMA 1D to flexibly identify convective patterns, with\nthe goal of enhancing forecast accuracy. The model's architecture is physically\nmotivated: sparse connections encourage interactions at similar height levels,\nwhile a shuffling mechanism prevents the model from learning non-physical\npatterns tied to the vertical grid. SALAMA 1D is trained over Central Europe\nwith lightning observations as the ground truth. Comparative analysis against a\nbaseline machine learning model that uses single-level predictors shows SALAMA\n1D's superior skill across various metrics and lead times of up to at least 11\nhours. Moreover, increasing the number of forecasts used to compile the\ntraining set improves skill, even when training set size is kept constant.\nSensitivity analysis using saliency maps indicates that the model reconstructs\nenvironmental lapse rates and rediscovers patterns consistent with established\ntheoretical understandings, such as positive buoyancy, convective inhibition,\nand ice particle formation near the tropopause, while ruling out thunderstorm\noccurrence based on the absence of mid-level graupel and cloud cover.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thunderstorms have significant social and economic impacts due to heavy\nprecipitation, hail, lightning, and strong winds, necessitating reliable\nforecasts. Thunderstorm forecasts based on numerical weather prediction (NWP)\noften rely on single-level surrogate predictors, like convective available\npotential energy and precipitation rate, derived from vertical profiles of\nthree-dimensional atmospheric variables. In this study, we develop SALAMA 1D, a\ndeep neural network that directly infers the probability of thunderstorm\noccurrence from vertical profiles of ten atmospheric variables, bypassing\nsingle-level predictors. By training the model on convection-permitting NWP\nforecasts, we allow SALAMA 1D to flexibly identify convective patterns, with\nthe goal of enhancing forecast accuracy. The model's architecture is physically\nmotivated: sparse connections encourage interactions at similar height levels,\nwhile a shuffling mechanism prevents the model from learning non-physical\npatterns tied to the vertical grid. SALAMA 1D is trained over Central Europe\nwith lightning observations as the ground truth. Comparative analysis against a\nbaseline machine learning model that uses single-level predictors shows SALAMA\n1D's superior skill across various metrics and lead times of up to at least 11\nhours. Moreover, increasing the number of forecasts used to compile the\ntraining set improves skill, even when training set size is kept constant.\nSensitivity analysis using saliency maps indicates that the model reconstructs\nenvironmental lapse rates and rediscovers patterns consistent with established\ntheoretical understandings, such as positive buoyancy, convective inhibition,\nand ice particle formation near the tropopause, while ruling out thunderstorm\noccurrence based on the absence of mid-level graupel and cloud cover."
                },
                "authors": [
                    {
                        "name": "Kianusch Vahid Yousefnia"
                    },
                    {
                        "name": "Tobias Bölle"
                    },
                    {
                        "name": "Christoph Metzl"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Metzl"
                },
                "author": "Christoph Metzl",
                "arxiv_comment": "14 pages, 8 figures, 2 tables. This work has been submitted to\n  Artificial Intelligence for the Earth Systems. Copyright in this work may be\n  transferred without further notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.15182v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.15182v3",
                "updated": "2024-09-30T08:16:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    16,
                    1,
                    0,
                    274,
                    0
                ],
                "published": "2022-10-27T05:19:55Z",
                "published_parsed": [
                    2022,
                    10,
                    27,
                    5,
                    19,
                    55,
                    3,
                    300,
                    0
                ],
                "title": "Text2Model: Text-based Model Induction for Zero-shot Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Model: Text-based Model Induction for Zero-shot Image\n  Classification"
                },
                "summary": "We address the challenge of building task-agnostic classifiers using only\ntext descriptions, demonstrating a unified approach to image classification, 3D\npoint cloud classification, and action recognition from scenes. Unlike\napproaches that learn a fixed representation of the output classes, we generate\nat inference time a model tailored to a query classification task. To generate\ntask-based zero-shot classifiers, we train a hypernetwork that receives class\ndescriptions and outputs a multi-class model. The hypernetwork is designed to\nbe equivariant with respect to the set of descriptions and the classification\nlayer, thus obeying the symmetries of the problem and improving generalization.\nOur approach generates non-linear classifiers, handles rich textual\ndescriptions, and may be adapted to produce lightweight models efficient enough\nfor on-device applications. We evaluate this approach in a series of zero-shot\nclassification tasks, for image, point-cloud, and action recognition, using a\nrange of text descriptions: From single words to rich descriptions. Our results\ndemonstrate strong improvements over previous approaches, showing that\nzero-shot learning can be applied with little training data. Furthermore, we\nconduct an analysis with foundational vision and language models, demonstrating\nthat they struggle to generalize when describing what attributes the class\nlacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of building task-agnostic classifiers using only\ntext descriptions, demonstrating a unified approach to image classification, 3D\npoint cloud classification, and action recognition from scenes. Unlike\napproaches that learn a fixed representation of the output classes, we generate\nat inference time a model tailored to a query classification task. To generate\ntask-based zero-shot classifiers, we train a hypernetwork that receives class\ndescriptions and outputs a multi-class model. The hypernetwork is designed to\nbe equivariant with respect to the set of descriptions and the classification\nlayer, thus obeying the symmetries of the problem and improving generalization.\nOur approach generates non-linear classifiers, handles rich textual\ndescriptions, and may be adapted to produce lightweight models efficient enough\nfor on-device applications. We evaluate this approach in a series of zero-shot\nclassification tasks, for image, point-cloud, and action recognition, using a\nrange of text descriptions: From single words to rich descriptions. Our results\ndemonstrate strong improvements over previous approaches, showing that\nzero-shot learning can be applied with little training data. Furthermore, we\nconduct an analysis with foundational vision and language models, demonstrating\nthat they struggle to generalize when describing what attributes the class\nlacks."
                },
                "authors": [
                    {
                        "name": "Ohad Amosy"
                    },
                    {
                        "name": "Tomer Volk"
                    },
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Eyal Ben-David"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Gal Chechik"
                    }
                ],
                "author_detail": {
                    "name": "Gal Chechik"
                },
                "author": "Gal Chechik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.15182v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.15182v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20059v1",
                "updated": "2024-09-30T08:01:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    1,
                    44,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:01:44Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    1,
                    44,
                    0,
                    274,
                    0
                ],
                "title": "Is Preference Alignment Always the Best Option to Enhance LLM-Based\n  Translation? An Empirical Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Preference Alignment Always the Best Option to Enhance LLM-Based\n  Translation? An Empirical Analysis"
                },
                "summary": "Neural metrics for machine translation (MT) evaluation have become\nincreasingly prominent due to their superior correlation with human judgments\ncompared to traditional lexical metrics. Researchers have therefore utilized\nneural metrics through quality-informed decoding strategies, achieving better\nresults than likelihood-based methods. With the rise of Large Language Models\n(LLMs), preference-based alignment techniques have gained attention for their\npotential to enhance translation quality by optimizing model weights directly\non preferences induced by quality estimators. This study focuses on Contrastive\nPreference Optimization (CPO) and conducts extensive experiments to evaluate\nthe impact of preference-based alignment on translation quality. Our findings\nindicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT)\non high-quality data with regard to the alignment metric, it may lead to\ninstability across downstream evaluation metrics, particularly between neural\nand lexical ones. Additionally, we demonstrate that relying solely on the base\nmodel for generating candidate translations achieves performance comparable to\nusing multiple external systems, while ensuring better consistency across\ndownstream metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural metrics for machine translation (MT) evaluation have become\nincreasingly prominent due to their superior correlation with human judgments\ncompared to traditional lexical metrics. Researchers have therefore utilized\nneural metrics through quality-informed decoding strategies, achieving better\nresults than likelihood-based methods. With the rise of Large Language Models\n(LLMs), preference-based alignment techniques have gained attention for their\npotential to enhance translation quality by optimizing model weights directly\non preferences induced by quality estimators. This study focuses on Contrastive\nPreference Optimization (CPO) and conducts extensive experiments to evaluate\nthe impact of preference-based alignment on translation quality. Our findings\nindicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT)\non high-quality data with regard to the alignment metric, it may lead to\ninstability across downstream evaluation metrics, particularly between neural\nand lexical ones. Additionally, we demonstrate that relying solely on the base\nmodel for generating candidate translations achieves performance comparable to\nusing multiple external systems, while ensuring better consistency across\ndownstream metrics."
                },
                "authors": [
                    {
                        "name": "Hippolyte Gisserot-Boukhlef"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Emmanuel Malherbe"
                    },
                    {
                        "name": "Céline Hudelot"
                    },
                    {
                        "name": "Pierre Colombo"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    }
                ],
                "author_detail": {
                    "name": "Nuno M. Guerreiro"
                },
                "author": "Nuno M. Guerreiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20053v1",
                "updated": "2024-09-30T07:59:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    59,
                    10,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:59:10Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    59,
                    10,
                    0,
                    274,
                    0
                ],
                "title": "GUNDAM: Aligning Large Language Models with Graph Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUNDAM: Aligning Large Language Models with Graph Understanding"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive results in processing\ntext data, which has sparked interest in applying these models beyond textual\ndata, such as graphs. In the field of graph learning, there is a growing\ninterest in harnessing LLMs to comprehend and manipulate graph-structured data.\nExisting research predominantly focuses on graphs with rich textual features,\nsuch as knowledge graphs or text attribute graphs, leveraging LLMs' ability to\nprocess text but inadequately addressing graph structure. This work\nspecifically aims to assess and enhance LLMs' abilities to comprehend and\nutilize the structural knowledge inherent in graph data itself, rather than\nfocusing solely on graphs rich in textual content. To achieve this, we\nintroduce the \\textbf{G}raph \\textbf{U}nderstanding for \\textbf{N}atural\nLanguage \\textbf{D}riven \\textbf{A}nalytical \\textbf{M}odel (\\model). This\nmodel adapts LLMs to better understand and engage with the structure of graph\ndata, enabling them to perform complex reasoning tasks by leveraging the\ngraph's structure itself. Our experimental evaluations on graph reasoning\nbenchmarks not only substantiate that \\model~ outperforms the SOTA baselines\nfor comparisons. But also reveals key factors affecting the graph reasoning\ncapabilities of LLMs. Moreover, we provide a theoretical analysis illustrating\nhow reasoning paths can enhance LLMs' reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results in processing\ntext data, which has sparked interest in applying these models beyond textual\ndata, such as graphs. In the field of graph learning, there is a growing\ninterest in harnessing LLMs to comprehend and manipulate graph-structured data.\nExisting research predominantly focuses on graphs with rich textual features,\nsuch as knowledge graphs or text attribute graphs, leveraging LLMs' ability to\nprocess text but inadequately addressing graph structure. This work\nspecifically aims to assess and enhance LLMs' abilities to comprehend and\nutilize the structural knowledge inherent in graph data itself, rather than\nfocusing solely on graphs rich in textual content. To achieve this, we\nintroduce the \\textbf{G}raph \\textbf{U}nderstanding for \\textbf{N}atural\nLanguage \\textbf{D}riven \\textbf{A}nalytical \\textbf{M}odel (\\model). This\nmodel adapts LLMs to better understand and engage with the structure of graph\ndata, enabling them to perform complex reasoning tasks by leveraging the\ngraph's structure itself. Our experimental evaluations on graph reasoning\nbenchmarks not only substantiate that \\model~ outperforms the SOTA baselines\nfor comparisons. But also reveals key factors affecting the graph reasoning\ncapabilities of LLMs. Moreover, we provide a theoretical analysis illustrating\nhow reasoning paths can enhance LLMs' reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Sheng Ouyang"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Ge Chen"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20052v1",
                "updated": "2024-09-30T07:57:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    57,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:57:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    57,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems"
                },
                "summary": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models."
                },
                "authors": [
                    {
                        "name": "Guixian Zhang"
                    },
                    {
                        "name": "Guan Yuan"
                    },
                    {
                        "name": "Debo Cheng"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Jiuyong Li"
                    },
                    {
                        "name": "Shichao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichao Zhang"
                },
                "author": "Shichao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20043v1",
                "updated": "2024-09-30T07:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    49,
                    30,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    49,
                    30,
                    0,
                    274,
                    0
                ],
                "title": "OPONeRF: One-Point-One NeRF for Robust Neural Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPONeRF: One-Point-One NeRF for Robust Neural Rendering"
                },
                "summary": "In this paper, we propose a One-Point-One NeRF (OPONeRF) framework for robust\nscene rendering. Existing NeRFs are designed based on a key assumption that the\ntarget scene remains unchanged between the training and test time. However,\nsmall but unpredictable perturbations such as object movements, light changes\nand data contaminations broadly exist in real-life 3D scenes, which lead to\nsignificantly defective or failed rendering results even for the recent\nstate-of-the-art generalizable methods. To address this, we propose a\ndivide-and-conquer framework in OPONeRF that adaptively responds to local scene\nvariations via personalizing appropriate point-wise parameters, instead of\nfitting a single set of NeRF parameters that are inactive to test-time unseen\nchanges. Moreover, to explicitly capture the local uncertainty, we decompose\nthe point representation into deterministic mapping and probabilistic\ninference. In this way, OPONeRF learns the sharable invariance and\nunsupervisedly models the unexpected scene variations between the training and\ntesting scenes. To validate the effectiveness of the proposed method, we\nconstruct benchmarks from both realistic and synthetic data with diverse\ntest-time perturbations including foreground motions, illumination variations\nand multi-modality noises, which are more challenging than conventional\ngeneralization and temporal reconstruction benchmarks. Experimental results\nshow that our OPONeRF outperforms state-of-the-art NeRFs on various evaluation\nmetrics through benchmark experiments and cross-scene evaluations. We further\nshow the efficacy of the proposed method via experimenting on other existing\ngeneralization-based benchmarks and incorporating the idea of One-Point-One\nNeRF into other advanced baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a One-Point-One NeRF (OPONeRF) framework for robust\nscene rendering. Existing NeRFs are designed based on a key assumption that the\ntarget scene remains unchanged between the training and test time. However,\nsmall but unpredictable perturbations such as object movements, light changes\nand data contaminations broadly exist in real-life 3D scenes, which lead to\nsignificantly defective or failed rendering results even for the recent\nstate-of-the-art generalizable methods. To address this, we propose a\ndivide-and-conquer framework in OPONeRF that adaptively responds to local scene\nvariations via personalizing appropriate point-wise parameters, instead of\nfitting a single set of NeRF parameters that are inactive to test-time unseen\nchanges. Moreover, to explicitly capture the local uncertainty, we decompose\nthe point representation into deterministic mapping and probabilistic\ninference. In this way, OPONeRF learns the sharable invariance and\nunsupervisedly models the unexpected scene variations between the training and\ntesting scenes. To validate the effectiveness of the proposed method, we\nconstruct benchmarks from both realistic and synthetic data with diverse\ntest-time perturbations including foreground motions, illumination variations\nand multi-modality noises, which are more challenging than conventional\ngeneralization and temporal reconstruction benchmarks. Experimental results\nshow that our OPONeRF outperforms state-of-the-art NeRFs on various evaluation\nmetrics through benchmark experiments and cross-scene evaluations. We further\nshow the efficacy of the proposed method via experimenting on other existing\ngeneralization-based benchmarks and incorporating the idea of One-Point-One\nNeRF into other advanced baseline methods."
                },
                "authors": [
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Yueqi Duan"
                    },
                    {
                        "name": "Kangfu Zheng"
                    },
                    {
                        "name": "Hongru Yan"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20042v1",
                "updated": "2024-09-30T07:48:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    48,
                    55,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:48:55Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    48,
                    55,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Scores: A Modular RAG-Based System for Automatic Short Answer\n  Scoring with Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Scores: A Modular RAG-Based System for Automatic Short Answer\n  Scoring with Feedback"
                },
                "summary": "Automatic short answer scoring (ASAS) helps reduce the grading burden on\neducators but often lacks detailed, explainable feedback. Existing methods in\nASAS with feedback (ASAS-F) rely on fine-tuning language models with limited\ndatasets, which is resource-intensive and struggles to generalize across\ncontexts. Recent approaches using large language models (LLMs) have focused on\nscoring without extensive fine-tuning. However, they often rely heavily on\nprompt engineering and either fail to generate elaborated feedback or do not\nadequately evaluate it. In this paper, we propose a modular retrieval augmented\ngeneration based ASAS-F system that scores answers and generates feedback in\nstrict zero-shot and few-shot learning scenarios. We design our system to be\nadaptable to various educational tasks without extensive prompt engineering\nusing an automatic prompt generation framework. Results show an improvement in\nscoring accuracy by 9\\% on unseen questions compared to fine-tuning, offering a\nscalable and cost-effective solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic short answer scoring (ASAS) helps reduce the grading burden on\neducators but often lacks detailed, explainable feedback. Existing methods in\nASAS with feedback (ASAS-F) rely on fine-tuning language models with limited\ndatasets, which is resource-intensive and struggles to generalize across\ncontexts. Recent approaches using large language models (LLMs) have focused on\nscoring without extensive fine-tuning. However, they often rely heavily on\nprompt engineering and either fail to generate elaborated feedback or do not\nadequately evaluate it. In this paper, we propose a modular retrieval augmented\ngeneration based ASAS-F system that scores answers and generates feedback in\nstrict zero-shot and few-shot learning scenarios. We design our system to be\nadaptable to various educational tasks without extensive prompt engineering\nusing an automatic prompt generation framework. Results show an improvement in\nscoring accuracy by 9\\% on unseen questions compared to fine-tuning, offering a\nscalable and cost-effective solution."
                },
                "authors": [
                    {
                        "name": "Menna Fateen"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Tsunenori Mine"
                    }
                ],
                "author_detail": {
                    "name": "Tsunenori Mine"
                },
                "author": "Tsunenori Mine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07678v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07678v3",
                "updated": "2024-09-30T07:26:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    26,
                    47,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-10T14:06:15Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    14,
                    6,
                    15,
                    2,
                    192,
                    0
                ],
                "title": "The ESO SupJup Survey II: The 12C/13C ratios of three young brown dwarfs\n  with CRIRES$^+$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ESO SupJup Survey II: The 12C/13C ratios of three young brown dwarfs\n  with CRIRES$^+$"
                },
                "summary": "Young brown dwarfs exhibit atmospheric characteristics similar to those of\nsuper-Jupiters, providing a unique opportunity to study planetary atmospheres.\nThe ESO SupJup Survey, utilizing CRIRES$^+$ on the Very Large Telescope, aims\nto assess the role of $^{12}$C/$^{13}$C as a formation tracer. We present\nobservations of three young brown dwarfs: 2MASS J12003792-7845082, TWA 28, and\n2MASS J08561384-1342242, with the goal of constraining their chemical\ncompositions, thermal profiles, surface gravities, spin rotations, and\n$^{12}$C/$^{13}$C. Atmospheric retrievals of CRIRES$^+$ K-band spectra were\nconducted using the radiative transfer code petitRADTRANS coupled with the\nBayesian inference algorithm MultiNest, resulting in a detailed\ncharacterization of the atmospheres of these objects. We report the volume\nmixing ratios of main molecular and atomic species, including the novel\ndetection of hydrogen fluoride (HF) in a brown dwarf's atmosphere, and\ndetermine $^{12}$C/$^{13}$C values of $81^{+28}_{-19}$ and $79^{+20}_{-14}$ in\nthe atmospheres of TWA 28 and J0856, respectively, with strong significance\n($>3\\sigma$). Tentative evidence ($\\sim 2\\sigma$) of $^{13}$C in J1200 was\nfound, with $^{12}$C/$^{13}$C = $114^{+69}_{-33}$, along with $^{18}$O detected\nat moderate significance in J0856 (3.3$\\sigma$) and TWA 28 (2.1$\\sigma$). The\nretrieved thermal profiles indicate hot atmospheres (2300-2600 K) with low\nsurface gravities and slow spins, consistent with young objects. The consistent\ncarbon isotope ratios among the three objects, showing no significant deviation\nfrom the local ISM, suggest a fragmentation-based formation mechanism similar\nto star formation. The tentative detection of $^{18}$O in two objects\nhighlights the potential of high-resolution spectroscopy to probe additional\nisotope ratios, such as $^{16}$O/$^{18}$O, in the atmospheres of brown dwarfs\nand super-Jupiters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young brown dwarfs exhibit atmospheric characteristics similar to those of\nsuper-Jupiters, providing a unique opportunity to study planetary atmospheres.\nThe ESO SupJup Survey, utilizing CRIRES$^+$ on the Very Large Telescope, aims\nto assess the role of $^{12}$C/$^{13}$C as a formation tracer. We present\nobservations of three young brown dwarfs: 2MASS J12003792-7845082, TWA 28, and\n2MASS J08561384-1342242, with the goal of constraining their chemical\ncompositions, thermal profiles, surface gravities, spin rotations, and\n$^{12}$C/$^{13}$C. Atmospheric retrievals of CRIRES$^+$ K-band spectra were\nconducted using the radiative transfer code petitRADTRANS coupled with the\nBayesian inference algorithm MultiNest, resulting in a detailed\ncharacterization of the atmospheres of these objects. We report the volume\nmixing ratios of main molecular and atomic species, including the novel\ndetection of hydrogen fluoride (HF) in a brown dwarf's atmosphere, and\ndetermine $^{12}$C/$^{13}$C values of $81^{+28}_{-19}$ and $79^{+20}_{-14}$ in\nthe atmospheres of TWA 28 and J0856, respectively, with strong significance\n($>3\\sigma$). Tentative evidence ($\\sim 2\\sigma$) of $^{13}$C in J1200 was\nfound, with $^{12}$C/$^{13}$C = $114^{+69}_{-33}$, along with $^{18}$O detected\nat moderate significance in J0856 (3.3$\\sigma$) and TWA 28 (2.1$\\sigma$). The\nretrieved thermal profiles indicate hot atmospheres (2300-2600 K) with low\nsurface gravities and slow spins, consistent with young objects. The consistent\ncarbon isotope ratios among the three objects, showing no significant deviation\nfrom the local ISM, suggest a fragmentation-based formation mechanism similar\nto star formation. The tentative detection of $^{18}$O in two objects\nhighlights the potential of high-resolution spectroscopy to probe additional\nisotope ratios, such as $^{16}$O/$^{18}$O, in the atmospheres of brown dwarfs\nand super-Jupiters."
                },
                "authors": [
                    {
                        "name": "D. González Picos"
                    },
                    {
                        "name": "I. A. G. Snellen"
                    },
                    {
                        "name": "S. de Regt"
                    },
                    {
                        "name": "R. Landman"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "S. Gandhi"
                    },
                    {
                        "name": "C. Ginski"
                    },
                    {
                        "name": "A. Y. Kesseli"
                    },
                    {
                        "name": "P. Mollière"
                    },
                    {
                        "name": "T. Stolker"
                    }
                ],
                "author_detail": {
                    "name": "T. Stolker"
                },
                "author": "T. Stolker",
                "arxiv_doi": "10.1051/0004-6361/202450028",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202450028",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07678v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07678v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in A&A Volume 689, September 2024",
                "arxiv_journal_ref": "A&A 689, A212 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20018v1",
                "updated": "2024-09-30T07:25:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    25,
                    16,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:25:16Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    25,
                    16,
                    0,
                    274,
                    0
                ],
                "title": "Visual Context Window Extension: A New Perspective for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Context Window Extension: A New Perspective for Long Video\n  Understanding"
                },
                "summary": "Large Multimodal Models (LMMs) have demonstrated impressive performance in\nshort video understanding tasks but face great challenges when applied to long\nvideo understanding. In contrast, Large Language Models (LLMs) exhibit\noutstanding capabilities in modeling long texts. Existing work attempts to\naddress this issue by introducing long video-text pairs during training.\nHowever, these approaches require substantial computational and data resources.\nIn this paper, we tackle the challenge of long video understanding from the\nperspective of context windows, aiming to apply LMMs to long video tasks\nwithout retraining on long video datasets. We first conduct an in-depth\nanalysis of why pretrained LMMs struggle to understand lengthy video content,\nidentifying that discrepancies between visual and language modalities lead to\ndifferent context windows for visual and language tokens, making it difficult\nto directly extend the visual tokens to match the language context window.\nBased on this, we propose to adapt LMMs for long video understanding tasks by\nextending the visual context window, eliminating the need for retraining on\nlarge scalelong video datasets. To further mitigate the significant memory\nconsumption caused by long sequences, we introduce a progressive pooling\ninference strategy that selectively adjusts the spatial resolution of frame\nembeddings, reducing the number of visual tokens while retaining important\nspatial information. Across multiple long video understanding benchmarks, our\nmethod consistently improves the performance as the number of video frames\nincreases. On the MLVU benchmark, our method outperforms GPT-4o, even though\nour model size is only 7B. Additionally, in the 256-frame setting, our method\nreduces memory usage by approximately 45% compared to the baseline, without\nintroducing any performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have demonstrated impressive performance in\nshort video understanding tasks but face great challenges when applied to long\nvideo understanding. In contrast, Large Language Models (LLMs) exhibit\noutstanding capabilities in modeling long texts. Existing work attempts to\naddress this issue by introducing long video-text pairs during training.\nHowever, these approaches require substantial computational and data resources.\nIn this paper, we tackle the challenge of long video understanding from the\nperspective of context windows, aiming to apply LMMs to long video tasks\nwithout retraining on long video datasets. We first conduct an in-depth\nanalysis of why pretrained LMMs struggle to understand lengthy video content,\nidentifying that discrepancies between visual and language modalities lead to\ndifferent context windows for visual and language tokens, making it difficult\nto directly extend the visual tokens to match the language context window.\nBased on this, we propose to adapt LMMs for long video understanding tasks by\nextending the visual context window, eliminating the need for retraining on\nlarge scalelong video datasets. To further mitigate the significant memory\nconsumption caused by long sequences, we introduce a progressive pooling\ninference strategy that selectively adjusts the spatial resolution of frame\nembeddings, reducing the number of visual tokens while retaining important\nspatial information. Across multiple long video understanding benchmarks, our\nmethod consistently improves the performance as the number of video frames\nincreases. On the MLVU benchmark, our method outperforms GPT-4o, even though\nour model size is only 7B. Additionally, in the 256-frame setting, our method\nreduces memory usage by approximately 45% compared to the baseline, without\nintroducing any performance loss."
                },
                "authors": [
                    {
                        "name": "Hongchen Wei"
                    },
                    {
                        "name": "Zhenzhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Chen"
                },
                "author": "Zhenzhong Chen",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20016v1",
                "updated": "2024-09-30T07:23:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    23,
                    47,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:23:47Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    23,
                    47,
                    0,
                    274,
                    0
                ],
                "title": "Personalisation via Dynamic Policy Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalisation via Dynamic Policy Fusion"
                },
                "summary": "Deep reinforcement learning (RL) policies, although optimal in terms of task\nrewards, may not align with the personal preferences of human users. To ensure\nthis alignment, a naive solution would be to retrain the agent using a reward\nfunction that encodes the user's specific preferences. However, such a reward\nfunction is typically not readily available, and as such, retraining the agent\nfrom scratch can be prohibitively expensive. We propose a more practical\napproach - to adapt the already trained policy to user-specific needs with the\nhelp of human feedback. To this end, we infer the user's intent through\ntrajectory-level feedback and combine it with the trained task policy via a\ntheoretically grounded dynamic policy fusion approach. As our approach collects\nhuman feedback on the very same trajectories used to learn the task policy, it\ndoes not require any additional interactions with the environment, making it a\nzero-shot approach. We empirically demonstrate in a number of environments that\nour proposed dynamic policy fusion approach consistently achieves the intended\ntask while simultaneously adhering to user-specific needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (RL) policies, although optimal in terms of task\nrewards, may not align with the personal preferences of human users. To ensure\nthis alignment, a naive solution would be to retrain the agent using a reward\nfunction that encodes the user's specific preferences. However, such a reward\nfunction is typically not readily available, and as such, retraining the agent\nfrom scratch can be prohibitively expensive. We propose a more practical\napproach - to adapt the already trained policy to user-specific needs with the\nhelp of human feedback. To this end, we infer the user's intent through\ntrajectory-level feedback and combine it with the trained task policy via a\ntheoretically grounded dynamic policy fusion approach. As our approach collects\nhuman feedback on the very same trajectories used to learn the task policy, it\ndoes not require any additional interactions with the environment, making it a\nzero-shot approach. We empirically demonstrate in a number of environments that\nour proposed dynamic policy fusion approach consistently achieves the intended\ntask while simultaneously adhering to user-specific needs."
                },
                "authors": [
                    {
                        "name": "Ajsal Shereef Palattuparambil"
                    },
                    {
                        "name": "Thommen George Karimpanal"
                    },
                    {
                        "name": "Santu Rana"
                    }
                ],
                "author_detail": {
                    "name": "Santu Rana"
                },
                "author": "Santu Rana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09414v2",
                "updated": "2024-09-30T07:09:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    9,
                    31,
                    0,
                    274,
                    0
                ],
                "published": "2024-04-15T02:01:42Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    2,
                    1,
                    42,
                    0,
                    106,
                    0
                ],
                "title": "General Bayesian inference for causal effects using covariate balancing\n  procedure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Bayesian inference for causal effects using covariate balancing\n  procedure"
                },
                "summary": "In observational studies, the propensity score plays a central role in\nestimating causal effects of interest. The inverse probability weighting (IPW)\nestimator is commonly used for this purpose. However, if the propensity score\nmodel is misspecified, the IPW estimator may produce biased estimates of causal\neffects. Previous studies have proposed some robust propensity score estimation\nprocedures. However, these methods require considering parameters that dominate\nthe uncertainty of sampling and treatment allocation. This study proposes a\nnovel Bayesian estimating procedure that necessitates probabilistically\ndeciding the parameter, rather than deterministically. Since the IPW estimator\nand propensity score estimator can be derived as solutions to certain loss\nfunctions, the general Bayesian paradigm, which does not require the\nconsidering the full likelihood, can be applied. Therefore, our proposed method\nonly requires the same level of assumptions as ordinary causal inference\ncontexts. The proposed Bayesian method demonstrates equal or superior results\ncompared to some previous methods in simulation experimentss, and is also\napplied to real data, namely the Whitehall dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In observational studies, the propensity score plays a central role in\nestimating causal effects of interest. The inverse probability weighting (IPW)\nestimator is commonly used for this purpose. However, if the propensity score\nmodel is misspecified, the IPW estimator may produce biased estimates of causal\neffects. Previous studies have proposed some robust propensity score estimation\nprocedures. However, these methods require considering parameters that dominate\nthe uncertainty of sampling and treatment allocation. This study proposes a\nnovel Bayesian estimating procedure that necessitates probabilistically\ndeciding the parameter, rather than deterministically. Since the IPW estimator\nand propensity score estimator can be derived as solutions to certain loss\nfunctions, the general Bayesian paradigm, which does not require the\nconsidering the full likelihood, can be applied. Therefore, our proposed method\nonly requires the same level of assumptions as ordinary causal inference\ncontexts. The proposed Bayesian method demonstrates equal or superior results\ncompared to some previous methods in simulation experimentss, and is also\napplied to real data, namely the Whitehall dataset."
                },
                "authors": [
                    {
                        "name": "Shunichiro Orihara"
                    },
                    {
                        "name": "Tomotaka Momozaki"
                    },
                    {
                        "name": "Tomoyuki Nakagawa"
                    }
                ],
                "author_detail": {
                    "name": "Tomoyuki Nakagawa"
                },
                "author": "Tomoyuki Nakagawa",
                "arxiv_comment": "covariate balancing, general Bayes, inverse probability weighting,\n  M-estimator, propensity score",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.20566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20566v1",
                "updated": "2024-09-30T17:59:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    34,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:59:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning"
                },
                "summary": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development."
                },
                "authors": [
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Aleksei Timofeev"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Jean-Philippe Fauconnier"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Yinfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Yang"
                },
                "author": "Yinfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20565v1",
                "updated": "2024-09-30T17:59:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    33,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:59:33Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    33,
                    0,
                    274,
                    0
                ],
                "title": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation\n  of LLM-Generated Medical Explanatory Arguments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation\n  of LLM-Generated Medical Explanatory Arguments"
                },
                "summary": "Evaluating LLM-generated text has become a key challenge, especially in\ndomain-specific contexts like the medical field. This work introduces a novel\nevaluation methodology for LLM-generated medical explanatory arguments, relying\non Proxy Tasks and rankings to closely align results with human evaluation\ncriteria, overcoming the biases typically seen in LLMs used as judges. We\ndemonstrate that the proposed evaluators are robust against adversarial\nattacks, including the assessment of non-argumentative text. Additionally, the\nhuman-crafted arguments needed to train the evaluators are minimized to just\none example per Proxy Task. By examining multiple LLM-generated arguments, we\nestablish a methodology for determining whether a Proxy Task is suitable for\nevaluating LLM-generated medical explanatory arguments, requiring only five\nexamples and two human experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-generated text has become a key challenge, especially in\ndomain-specific contexts like the medical field. This work introduces a novel\nevaluation methodology for LLM-generated medical explanatory arguments, relying\non Proxy Tasks and rankings to closely align results with human evaluation\ncriteria, overcoming the biases typically seen in LLMs used as judges. We\ndemonstrate that the proposed evaluators are robust against adversarial\nattacks, including the assessment of non-argumentative text. Additionally, the\nhuman-crafted arguments needed to train the evaluators are minimized to just\none example per Proxy Task. By examining multiple LLM-generated arguments, we\nestablish a methodology for determining whether a Proxy Task is suitable for\nevaluating LLM-generated medical explanatory arguments, requiring only five\nexamples and two human experts."
                },
                "authors": [
                    {
                        "name": "Iker De la Iglesia"
                    },
                    {
                        "name": "Iakes Goenaga"
                    },
                    {
                        "name": "Johanna Ramirez-Romero"
                    },
                    {
                        "name": "Jose Maria Villa-Gonzalez"
                    },
                    {
                        "name": "Josu Goikoetxea"
                    },
                    {
                        "name": "Ander Barrena"
                    }
                ],
                "author_detail": {
                    "name": "Ander Barrena"
                },
                "author": "Ander Barrena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20557v1",
                "updated": "2024-09-30T17:57:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    57,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:57:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    57,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in\n  Instructional Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in\n  Instructional Videos"
                },
                "summary": "Goal-oriented planning, or anticipating a series of actions that transition\nan agent from its current state to a predefined objective, is crucial for\ndeveloping intelligent assistants aiding users in daily procedural tasks. The\nproblem presents significant challenges due to the need for comprehensive\nknowledge of temporal and hierarchical task structures, as well as strong\ncapabilities in reasoning and planning. To achieve this, prior work typically\nrelies on extensive training on the target dataset, which often results in\nsignificant dataset bias and a lack of generalization to unseen tasks. In this\nwork, we introduce VidAssist, an integrated framework designed for\nzero/few-shot goal-oriented planning in instructional videos. VidAssist\nleverages large language models (LLMs) as both the knowledge base and the\nassessment tool for generating and evaluating action plans, thus overcoming the\nchallenges of acquiring procedural knowledge from small-scale, low-diversity\ndatasets. Moreover, VidAssist employs a breadth-first search algorithm for\noptimal plan generation, in which a composite of value functions designed for\ngoal-oriented planning is utilized to assess the predicted actions at each\nstep. Extensive experiments demonstrate that VidAssist offers a unified\nframework for different goal-oriented planning setups, e.g., visual planning\nfor assistance (VPA) and procedural planning (PP), and achieves remarkable\nperformance in zero-shot and few-shot setups. Specifically, our few-shot model\noutperforms the prior fully supervised state-of-the-art method by +7.7% in VPA\nand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,\nand models are publicly available at https://sites.google.com/view/vidassist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented planning, or anticipating a series of actions that transition\nan agent from its current state to a predefined objective, is crucial for\ndeveloping intelligent assistants aiding users in daily procedural tasks. The\nproblem presents significant challenges due to the need for comprehensive\nknowledge of temporal and hierarchical task structures, as well as strong\ncapabilities in reasoning and planning. To achieve this, prior work typically\nrelies on extensive training on the target dataset, which often results in\nsignificant dataset bias and a lack of generalization to unseen tasks. In this\nwork, we introduce VidAssist, an integrated framework designed for\nzero/few-shot goal-oriented planning in instructional videos. VidAssist\nleverages large language models (LLMs) as both the knowledge base and the\nassessment tool for generating and evaluating action plans, thus overcoming the\nchallenges of acquiring procedural knowledge from small-scale, low-diversity\ndatasets. Moreover, VidAssist employs a breadth-first search algorithm for\noptimal plan generation, in which a composite of value functions designed for\ngoal-oriented planning is utilized to assess the predicted actions at each\nstep. Extensive experiments demonstrate that VidAssist offers a unified\nframework for different goal-oriented planning setups, e.g., visual planning\nfor assistance (VPA) and procedural planning (PP), and achieves remarkable\nperformance in zero-shot and few-shot setups. Specifically, our few-shot model\noutperforms the prior fully supervised state-of-the-art method by +7.7% in VPA\nand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,\nand models are publicly available at https://sites.google.com/view/vidassist."
                },
                "authors": [
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Fu-Jen Chu"
                    },
                    {
                        "name": "Kris Kitani"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Xitong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Yang"
                },
                "author": "Xitong Yang",
                "arxiv_comment": "Accepted by ECCV 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20550v1",
                "updated": "2024-09-30T17:51:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    15,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    15,
                    0,
                    274,
                    0
                ],
                "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation"
                },
                "summary": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination"
                },
                "authors": [
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "11 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11634v2",
                "updated": "2024-09-30T17:51:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    11,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-17T15:14:10Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    14,
                    10,
                    0,
                    169,
                    0
                ],
                "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating\n  Test-Taking Strategies from Benchmark Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating\n  Test-Taking Strategies from Benchmark Performance"
                },
                "summary": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter."
                },
                "authors": [
                    {
                        "name": "Kyle Moore"
                    },
                    {
                        "name": "Jesse Roberts"
                    },
                    {
                        "name": "Thao Pham"
                    },
                    {
                        "name": "Oseremhen Ewaleifoh"
                    },
                    {
                        "name": "Doug Fisher"
                    }
                ],
                "author_detail": {
                    "name": "Doug Fisher"
                },
                "author": "Doug Fisher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20548v1",
                "updated": "2024-09-30T17:49:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    49,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:49:09Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    49,
                    9,
                    0,
                    274,
                    0
                ],
                "title": "Robi Butler: Remote Multimodal Interactions with Household Robot\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robi Butler: Remote Multimodal Interactions with Household Robot\n  Assistant"
                },
                "summary": "In this paper, we introduce Robi Butler, a novel household robotic system\nthat enables multimodal interactions with remote users. Building on the\nadvanced communication interfaces, Robi Butler allows users to monitor the\nrobot's status, send text or voice instructions, and select target objects by\nhand pointing. At the core of our system is a high-level behavior module,\npowered by Large Language Models (LLMs), that interprets multimodal\ninstructions to generate action plans. These plans are composed of a set of\nopen vocabulary primitives supported by Vision Language Models (VLMs) that\nhandle both text and pointing queries. The integration of the above components\nallows Robi Butler to ground remote multimodal instructions in the real-world\nhome environment in a zero-shot manner. We demonstrate the effectiveness and\nefficiency of this system using a variety of daily household tasks that involve\nremote users giving multimodal instructions. Additionally, we conducted a user\nstudy to analyze how multimodal interactions affect efficiency and user\nexperience during remote human-robot interaction and discuss the potential\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Robi Butler, a novel household robotic system\nthat enables multimodal interactions with remote users. Building on the\nadvanced communication interfaces, Robi Butler allows users to monitor the\nrobot's status, send text or voice instructions, and select target objects by\nhand pointing. At the core of our system is a high-level behavior module,\npowered by Large Language Models (LLMs), that interprets multimodal\ninstructions to generate action plans. These plans are composed of a set of\nopen vocabulary primitives supported by Vision Language Models (VLMs) that\nhandle both text and pointing queries. The integration of the above components\nallows Robi Butler to ground remote multimodal instructions in the real-world\nhome environment in a zero-shot manner. We demonstrate the effectiveness and\nefficiency of this system using a variety of daily household tasks that involve\nremote users giving multimodal instructions. Additionally, we conducted a user\nstudy to analyze how multimodal interactions affect efficiency and user\nexperience during remote human-robot interaction and discuss the potential\nimprovements."
                },
                "authors": [
                    {
                        "name": "Anxing Xiao"
                    },
                    {
                        "name": "Nuwan Janaka"
                    },
                    {
                        "name": "Tianrun Hu"
                    },
                    {
                        "name": "Anshul Gupta"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Cunjun Yu"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01261v2",
                "updated": "2024-09-30T17:39:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    39,
                    59,
                    0,
                    274,
                    0
                ],
                "published": "2024-04-01T17:33:38Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    33,
                    38,
                    0,
                    92,
                    0
                ],
                "title": "FABLES: Evaluating faithfulness and content selection in book-length\n  summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FABLES: Evaluating faithfulness and content selection in book-length\n  summarization"
                },
                "summary": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook."
                },
                "authors": [
                    {
                        "name": "Yekyung Kim"
                    },
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Aparna Garimella"
                    },
                    {
                        "name": "Varun Manjunatha"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "preprint - 39 pages",
                "arxiv_journal_ref": "1st Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00222v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00222v4",
                "updated": "2024-09-30T17:37:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    37,
                    16,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-30T19:26:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    26,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Can Large Language Models Address Open-Target Stance Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Address Open-Target Stance Detection?"
                },
                "summary": "Stance detection (SD) identifies a text's position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs)\nGPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the only\nexisting work, Target-Stance Extraction (TSE), which benefits from predefined\ntargets. Unlike TSE, OTSD removes the dependency of a predefined list, making\ntarget generation and evaluation more challenging. We also provide a metric for\nevaluating target quality that correlates well with human judgment. Our\nexperiments reveal that LLMs outperform TSE in target generation when the real\ntarget is explicitly and not explicitly mentioned in the text. Likewise, for\nstance detection, LLMs excel in explicit cases with comparable performance in\nnon-explicit in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection (SD) identifies a text's position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs)\nGPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the only\nexisting work, Target-Stance Extraction (TSE), which benefits from predefined\ntargets. Unlike TSE, OTSD removes the dependency of a predefined list, making\ntarget generation and evaluation more challenging. We also provide a metric for\nevaluating target quality that correlates well with human judgment. Our\nexperiments reveal that LLMs outperform TSE in target generation when the real\ntarget is explicitly and not explicitly mentioned in the text. Likewise, for\nstance detection, LLMs excel in explicit cases with comparable performance in\nnon-explicit in general."
                },
                "authors": [
                    {
                        "name": "Abu Ubaida Akash"
                    },
                    {
                        "name": "Ahmed Fahmy"
                    },
                    {
                        "name": "Amine Trabelsi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Trabelsi"
                },
                "author": "Amine Trabelsi",
                "arxiv_comment": "14 pages; currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00222v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00222v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00746v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00746v7",
                "updated": "2024-09-30T17:22:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    22,
                    1,
                    0,
                    274,
                    0
                ],
                "published": "2024-02-01T16:40:32Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    16,
                    40,
                    32,
                    3,
                    32,
                    0
                ],
                "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System"
                },
                "summary": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00746v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00746v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20521v1",
                "updated": "2024-09-30T17:21:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    21,
                    15,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:21:15Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    21,
                    15,
                    0,
                    274,
                    0
                ],
                "title": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics\n  Reinforcement Learning"
                },
                "summary": "We study off-dynamics Reinforcement Learning (RL), where the policy training\nand deployment environments are different. To deal with this environmental\nperturbation, we focus on learning policies robust to uncertainties in\ntransition dynamics under the framework of distributionally robust Markov\ndecision processes (DRMDPs), where the nominal and perturbed dynamics are\nlinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that\nenjoys an average suboptimality $\\widetilde{\\mathcal{O}}\\big({d H \\cdot \\min\n\\{1/{\\rho}, H\\}/\\sqrt{K} }\\big)$, where $K$ is the number of episodes, $H$ is\nthe horizon length, $d$ is the feature dimension and $\\rho$ is the uncertainty\nlevel. This result improves the state-of-the-art by\n$\\mathcal{O}(dH/\\min\\{1/\\rho,H\\})$. We also construct a novel hard instance and\nderive the first information-theoretic lower bound in this setting, which\nindicates our algorithm is near-optimal up to $\\mathcal{O}(\\sqrt{H})$ for any\nuncertainty level $\\rho\\in(0,1]$. Our algorithm also enjoys a 'rare-switching'\ndesign, and thus only requires $\\mathcal{O}(dH\\log(1+H^2K))$ policy switches\nand $\\mathcal{O}(d^2H\\log(1+H^2K))$ calls for oracle to solve dual optimization\nproblems, which significantly improves the computational efficiency of existing\nalgorithms for DRMDPs, whose policy switch and oracle complexities are both\n$\\mathcal{O}(K)$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study off-dynamics Reinforcement Learning (RL), where the policy training\nand deployment environments are different. To deal with this environmental\nperturbation, we focus on learning policies robust to uncertainties in\ntransition dynamics under the framework of distributionally robust Markov\ndecision processes (DRMDPs), where the nominal and perturbed dynamics are\nlinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that\nenjoys an average suboptimality $\\widetilde{\\mathcal{O}}\\big({d H \\cdot \\min\n\\{1/{\\rho}, H\\}/\\sqrt{K} }\\big)$, where $K$ is the number of episodes, $H$ is\nthe horizon length, $d$ is the feature dimension and $\\rho$ is the uncertainty\nlevel. This result improves the state-of-the-art by\n$\\mathcal{O}(dH/\\min\\{1/\\rho,H\\})$. We also construct a novel hard instance and\nderive the first information-theoretic lower bound in this setting, which\nindicates our algorithm is near-optimal up to $\\mathcal{O}(\\sqrt{H})$ for any\nuncertainty level $\\rho\\in(0,1]$. Our algorithm also enjoys a 'rare-switching'\ndesign, and thus only requires $\\mathcal{O}(dH\\log(1+H^2K))$ policy switches\nand $\\mathcal{O}(d^2H\\log(1+H^2K))$ calls for oracle to solve dual optimization\nproblems, which significantly improves the computational efficiency of existing\nalgorithms for DRMDPs, whose policy switch and oracle complexities are both\n$\\mathcal{O}(K)$."
                },
                "authors": [
                    {
                        "name": "Zhishuai Liu"
                    },
                    {
                        "name": "Weixin Wang"
                    },
                    {
                        "name": "Pan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Pan Xu"
                },
                "author": "Pan Xu",
                "arxiv_comment": "48 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v3",
                "updated": "2024-10-01T08:50:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    50,
                    1,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20512v1",
                "updated": "2024-09-30T17:13:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    13,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:13:40Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    13,
                    40,
                    0,
                    274,
                    0
                ],
                "title": "Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis\n  of Perovskite via Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis\n  of Perovskite via Language Models"
                },
                "summary": "The challenge of accurately predicting toxicity of industrial solvents used\nin perovskite synthesis is a necessary undertaking but is limited by a lack of\na targeted and structured toxicity data. This paper presents a novel framework\nthat combines an automated data extraction using language models, and an\nuncertainty-informed prediction model to fill data gaps and improve prediction\nconfidence. First, we have utilized and compared two approaches to\nautomatically extract relevant data from a corpus of scientific literature on\nsolvents used in perovskite synthesis: smaller bidirectional language models\nlike BERT and ELMo are used for their repeatability and deterministic outputs,\nwhile autoregressive large language model (LLM) such as GPT-3.5 is used to\nleverage its larger training corpus and better response generation. Our novel\n'prompting and verification' technique integrated with an LLM aims at targeted\nextraction and refinement, thereby reducing hallucination and improving the\nquality of the extracted data using the LLM. Next, the extracted data is fed\ninto our pre-trained multi-task binary classification deep learning to predict\nthe ED nature of extracted solvents. We have used a Shannon entropy-based\nuncertainty quantification utilizing the class probabilities obtained from the\nclassification model to quantify uncertainty and identify data gaps in our\npredictions. This approach leads to the curation of a structured dataset for\nsolvents used in perovskite synthesis and their uncertainty-informed virtual\ntoxicity assessment. Additionally, chord diagrams have been used to visualize\nsolvent interactions and prioritize those with potential hazards, revealing\nthat 70% of the solvent interactions were primarily associated with two\nspecific perovskites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of accurately predicting toxicity of industrial solvents used\nin perovskite synthesis is a necessary undertaking but is limited by a lack of\na targeted and structured toxicity data. This paper presents a novel framework\nthat combines an automated data extraction using language models, and an\nuncertainty-informed prediction model to fill data gaps and improve prediction\nconfidence. First, we have utilized and compared two approaches to\nautomatically extract relevant data from a corpus of scientific literature on\nsolvents used in perovskite synthesis: smaller bidirectional language models\nlike BERT and ELMo are used for their repeatability and deterministic outputs,\nwhile autoregressive large language model (LLM) such as GPT-3.5 is used to\nleverage its larger training corpus and better response generation. Our novel\n'prompting and verification' technique integrated with an LLM aims at targeted\nextraction and refinement, thereby reducing hallucination and improving the\nquality of the extracted data using the LLM. Next, the extracted data is fed\ninto our pre-trained multi-task binary classification deep learning to predict\nthe ED nature of extracted solvents. We have used a Shannon entropy-based\nuncertainty quantification utilizing the class probabilities obtained from the\nclassification model to quantify uncertainty and identify data gaps in our\npredictions. This approach leads to the curation of a structured dataset for\nsolvents used in perovskite synthesis and their uncertainty-informed virtual\ntoxicity assessment. Additionally, chord diagrams have been used to visualize\nsolvent interactions and prioritize those with potential hazards, revealing\nthat 70% of the solvent interactions were primarily associated with two\nspecific perovskites."
                },
                "authors": [
                    {
                        "name": "Arpan Mukherjee"
                    },
                    {
                        "name": "Deepesh Giri"
                    },
                    {
                        "name": "Krishna Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Krishna Rajan"
                },
                "author": "Krishna Rajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17041v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17041v3",
                "updated": "2024-09-30T17:12:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    12,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2023-11-28T18:53:06Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    18,
                    53,
                    6,
                    1,
                    332,
                    0
                ],
                "title": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties"
                },
                "summary": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}."
                },
                "authors": [
                    {
                        "name": "Keunwoo Peter Yu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Fengyuan Hu"
                    },
                    {
                        "name": "Shane Storks"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "16 pages, LaTeX; Accepted to EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17041v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17041v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20502v1",
                "updated": "2024-09-30T17:02:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    2,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:02:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    2,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "COLLAGE: Collaborative Human-Agent Interaction Generation using\n  Hierarchical Latent Diffusion and Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLLAGE: Collaborative Human-Agent Interaction Generation using\n  Hierarchical Latent Diffusion and Language Models"
                },
                "summary": "We propose a novel framework COLLAGE for generating collaborative\nagent-object-agent interactions by leveraging large language models (LLMs) and\nhierarchical motion-specific vector-quantized variational autoencoders\n(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by\nincorporating the knowledge and reasoning abilities of LLMs to guide a\ngenerative diffusion model. The hierarchical VQ-VAE architecture captures\ndifferent motion-specific characteristics at multiple levels of abstraction,\navoiding redundant concepts and enabling efficient multi-resolution\nrepresentation. We introduce a diffusion model that operates in the latent\nspace and incorporates LLM-generated motion planning cues to guide the\ndenoising process, resulting in prompt-specific motion generation with greater\ncontrol and diversity. Experimental results on the CORE-4D, and InterHuman\ndatasets demonstrate the effectiveness of our approach in generating realistic\nand diverse collaborative human-object-human interactions, outperforming\nstate-of-the-art methods. Our work opens up new possibilities for modeling\ncomplex interactions in various domains, such as robotics, graphics and\ncomputer vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework COLLAGE for generating collaborative\nagent-object-agent interactions by leveraging large language models (LLMs) and\nhierarchical motion-specific vector-quantized variational autoencoders\n(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by\nincorporating the knowledge and reasoning abilities of LLMs to guide a\ngenerative diffusion model. The hierarchical VQ-VAE architecture captures\ndifferent motion-specific characteristics at multiple levels of abstraction,\navoiding redundant concepts and enabling efficient multi-resolution\nrepresentation. We introduce a diffusion model that operates in the latent\nspace and incorporates LLM-generated motion planning cues to guide the\ndenoising process, resulting in prompt-specific motion generation with greater\ncontrol and diversity. Experimental results on the CORE-4D, and InterHuman\ndatasets demonstrate the effectiveness of our approach in generating realistic\nand diverse collaborative human-object-human interactions, outperforming\nstate-of-the-art methods. Our work opens up new possibilities for modeling\ncomplex interactions in various domains, such as robotics, graphics and\ncomputer vision."
                },
                "authors": [
                    {
                        "name": "Divyanshu Daiya"
                    },
                    {
                        "name": "Damon Conover"
                    },
                    {
                        "name": "Aniket Bera"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Bera"
                },
                "author": "Aniket Bera",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01270v2",
                "updated": "2024-09-30T16:39:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    39,
                    51,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-01T13:21:33Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    21,
                    33,
                    0,
                    183,
                    0
                ],
                "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit\n  Biases in LLM Open-ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit\n  Biases in LLM Open-ended Text Generation"
                },
                "summary": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured."
                },
                "authors": [
                    {
                        "name": "Serene Lim"
                    },
                    {
                        "name": "María Pérez-Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "María Pérez-Ortiz"
                },
                "author": "María Pérez-Ortiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03843v3",
                "updated": "2024-09-30T16:16:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    16,
                    4,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-06T08:21:30Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    8,
                    21,
                    30,
                    3,
                    158,
                    0
                ],
                "title": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning\n  of Large Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts."
                },
                "authors": [
                    {
                        "name": "Jianben He"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Shiyi Liu"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Claudio Silva"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08648v2",
                "updated": "2024-09-30T16:15:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    15,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-12T21:17:02Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    21,
                    17,
                    2,
                    2,
                    164,
                    0
                ],
                "title": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large\n  Language Models"
                },
                "summary": "When humans create sculptures, we are able to reason about how geometrically\nwe need to alter the clay state to reach our target goal. We are not computing\npoint-wise similarity metrics, or reasoning about low-level positioning of our\ntools, but instead determining the higher-level changes that need to be made.\nIn this work, we propose LLM-Craft, a novel pipeline that leverages large\nlanguage models (LLMs) to iteratively reason about and generate\ndeformation-based crafting action sequences. We simplify and couple the state\nand action representations to further encourage shape-based reasoning. To the\nbest of our knowledge, LLM-Craft is the first system successfully leveraging\nLLMs for complex deformable object interactions. Through our experiments, we\ndemonstrate that with the LLM-Craft framework, LLMs are able to successfully\nreason about the deformation behavior of elasto-plastic objects. Furthermore,\nwe find that LLM-Craft is able to successfully create a set of simple letter\nshapes. Finally, we explore extending the framework to reaching more ambiguous\nsemantic goals, such as \"thinner\" or \"bumpy\". For videos please see our\nwebsite: https://sites.google.com/andrew.cmu.edu/llmcraft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When humans create sculptures, we are able to reason about how geometrically\nwe need to alter the clay state to reach our target goal. We are not computing\npoint-wise similarity metrics, or reasoning about low-level positioning of our\ntools, but instead determining the higher-level changes that need to be made.\nIn this work, we propose LLM-Craft, a novel pipeline that leverages large\nlanguage models (LLMs) to iteratively reason about and generate\ndeformation-based crafting action sequences. We simplify and couple the state\nand action representations to further encourage shape-based reasoning. To the\nbest of our knowledge, LLM-Craft is the first system successfully leveraging\nLLMs for complex deformable object interactions. Through our experiments, we\ndemonstrate that with the LLM-Craft framework, LLMs are able to successfully\nreason about the deformation behavior of elasto-plastic objects. Furthermore,\nwe find that LLM-Craft is able to successfully create a set of simple letter\nshapes. Finally, we explore extending the framework to reaching more ambiguous\nsemantic goals, such as \"thinner\" or \"bumpy\". For videos please see our\nwebsite: https://sites.google.com/andrew.cmu.edu/llmcraft."
                },
                "authors": [
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20441v2",
                "updated": "2024-10-01T06:03:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    3,
                    22,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T16:00:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Instance-adaptive Zero-shot Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance-adaptive Zero-shot Chain-of-Thought Prompting"
                },
                "summary": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Shaotian Yan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Renchu Guan"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20426v1",
                "updated": "2024-09-30T15:50:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    50,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:50:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    50,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Navigating Threats: A Survey of Physical Adversarial Attacks on LiDAR\n  Perception Systems in Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Threats: A Survey of Physical Adversarial Attacks on LiDAR\n  Perception Systems in Autonomous Vehicles"
                },
                "summary": "Autonomous vehicles (AVs) rely heavily on LiDAR (Light Detection and Ranging)\nsystems for accurate perception and navigation, providing high-resolution 3D\nenvironmental data that is crucial for object detection and classification.\nHowever, LiDAR systems are vulnerable to adversarial attacks, which pose\nsignificant challenges to the safety and robustness of AVs. This survey\npresents a thorough review of the current research landscape on physical\nadversarial attacks targeting LiDAR-based perception systems, covering both\nsingle-modality and multi-modality contexts. We categorize and analyze various\nattack types, including spoofing and physical adversarial object attacks,\ndetailing their methodologies, impacts, and potential real-world implications.\nThrough detailed case studies and analyses, we identify critical challenges and\nhighlight gaps in existing attacks for LiDAR-based systems. Additionally, we\npropose future research directions to enhance the security and resilience of\nthese systems, ultimately contributing to the safer deployment of autonomous\nvehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles (AVs) rely heavily on LiDAR (Light Detection and Ranging)\nsystems for accurate perception and navigation, providing high-resolution 3D\nenvironmental data that is crucial for object detection and classification.\nHowever, LiDAR systems are vulnerable to adversarial attacks, which pose\nsignificant challenges to the safety and robustness of AVs. This survey\npresents a thorough review of the current research landscape on physical\nadversarial attacks targeting LiDAR-based perception systems, covering both\nsingle-modality and multi-modality contexts. We categorize and analyze various\nattack types, including spoofing and physical adversarial object attacks,\ndetailing their methodologies, impacts, and potential real-world implications.\nThrough detailed case studies and analyses, we identify critical challenges and\nhighlight gaps in existing attacks for LiDAR-based systems. Additionally, we\npropose future research directions to enhance the security and resilience of\nthese systems, ultimately contributing to the safer deployment of autonomous\nvehicles."
                },
                "authors": [
                    {
                        "name": "Amira Guesmi"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14744v2",
                "updated": "2024-09-30T15:36:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    36,
                    26,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-23T06:42:21Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    6,
                    42,
                    21,
                    0,
                    267,
                    0
                ],
                "title": "LINKAGE: Listwise Ranking among Varied-Quality References for\n  Non-Factoid QA Evaluation via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINKAGE: Listwise Ranking among Varied-Quality References for\n  Non-Factoid QA Evaluation via LLMs"
                },
                "summary": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to\ndiverse potential answers and no objective criterion. The commonly used\nautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measure\nsemantic similarities or answers from different perspectives. Recently, Large\nLanguage Models (LLMs) have been resorted to for NFQA evaluation due to their\ncompelling performance on various NLP tasks. Common approaches include\npointwise scoring of each candidate answer and pairwise comparisons between\nanswers. Inspired by the evolution from pointwise to pairwise to listwise in\nlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,\nthat utilizes LLMs to rank candidate answers in a list of reference answers\nsorted by descending quality. Moreover, for NF questions that do not have\nmulti-grade or any golden answers, we leverage LLMs to generate the reference\nanswer list of various quality to facilitate the listwise evaluation. Extensive\nexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and\nWebGLM show that our method has significantly higher correlations with human\nannotations compared to automatic scores and common pointwise and pairwise\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to\ndiverse potential answers and no objective criterion. The commonly used\nautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measure\nsemantic similarities or answers from different perspectives. Recently, Large\nLanguage Models (LLMs) have been resorted to for NFQA evaluation due to their\ncompelling performance on various NLP tasks. Common approaches include\npointwise scoring of each candidate answer and pairwise comparisons between\nanswers. Inspired by the evolution from pointwise to pairwise to listwise in\nlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,\nthat utilizes LLMs to rank candidate answers in a list of reference answers\nsorted by descending quality. Moreover, for NF questions that do not have\nmulti-grade or any golden answers, we leverage LLMs to generate the reference\nanswer list of various quality to facilitate the listwise evaluation. Extensive\nexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and\nWebGLM show that our method has significantly higher correlations with human\nannotations compared to automatic scores and common pointwise and pairwise\napproaches."
                },
                "authors": [
                    {
                        "name": "Sihui Yang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Wanqing Cui"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Published as a conference paper at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20385v1",
                "updated": "2024-09-30T15:20:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    20,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:20:58Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    20,
                    58,
                    0,
                    274,
                    0
                ],
                "title": "Wait, but Tylenol is Acetaminophen... Investigating and Improving\n  Language Models' Ability to Resist Requests for Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wait, but Tylenol is Acetaminophen... Investigating and Improving\n  Language Models' Ability to Resist Requests for Misinformation"
                },
                "summary": "Background: Large language models (LLMs) are trained to follow directions,\nbut this introduces a vulnerability to blindly comply with user requests even\nif they generate wrong information. In medicine, this could accelerate the\ngeneration of misinformation that impacts human well-being.\n  Objectives/Methods: We analyzed compliance to requests to generate misleading\ncontent about medications in settings where models know the request is\nillogical. We investigated whether in-context directions and instruction-tuning\nof LLMs to prioritize logical reasoning over compliance reduced misinformation\nrisk.\n  Results: While all frontier LLMs complied with misinformation requests, both\nprompt-based and parameter-based approaches can improve the detection of logic\nflaws in requests and prevent the dissemination of medical misinformation.\n  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce\nrisks of exploitation for medical misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) are trained to follow directions,\nbut this introduces a vulnerability to blindly comply with user requests even\nif they generate wrong information. In medicine, this could accelerate the\ngeneration of misinformation that impacts human well-being.\n  Objectives/Methods: We analyzed compliance to requests to generate misleading\ncontent about medications in settings where models know the request is\nillogical. We investigated whether in-context directions and instruction-tuning\nof LLMs to prioritize logical reasoning over compliance reduced misinformation\nrisk.\n  Results: While all frontier LLMs complied with misinformation requests, both\nprompt-based and parameter-based approaches can improve the detection of logic\nflaws in requests and prevent the dissemination of medical misinformation.\n  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce\nrisks of exploitation for medical misinformation."
                },
                "authors": [
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Mingye Gao"
                    },
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Brian Anthony"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Hugo Aerts"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Danielle Bitterman"
                    }
                ],
                "author_detail": {
                    "name": "Danielle Bitterman"
                },
                "author": "Danielle Bitterman",
                "arxiv_comment": "Submitted for Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20370v1",
                "updated": "2024-09-30T15:06:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    6,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:06:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    6,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications."
                },
                "authors": [
                    {
                        "name": "Tengyu Xu"
                    },
                    {
                        "name": "Eryk Helenowski"
                    },
                    {
                        "name": "Karthik Abinav Sankararaman"
                    },
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Kaiyan Peng"
                    },
                    {
                        "name": "Eric Han"
                    },
                    {
                        "name": "Shaoliang Nie"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Hejia Zhang"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Zhouhao Zeng"
                    },
                    {
                        "name": "Yun He"
                    },
                    {
                        "name": "Karishma Mandyam"
                    },
                    {
                        "name": "Arya Talabzadeh"
                    },
                    {
                        "name": "Madian Khabsa"
                    },
                    {
                        "name": "Gabriel Cohen"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Sinong Wang"
                    },
                    {
                        "name": "Han Fang"
                    }
                ],
                "author_detail": {
                    "name": "Han Fang"
                },
                "author": "Han Fang",
                "arxiv_comment": "submitted to conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20365v1",
                "updated": "2024-09-30T15:04:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    4,
                    14,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:04:14Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    4,
                    14,
                    0,
                    274,
                    0
                ],
                "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative\n  Spatial-Temporal Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoINSTA: Zero-shot Long Video Understanding via Informative\n  Spatial-Temporal Reasoning with LLMs"
                },
                "summary": "In the video-language domain, recent works in leveraging zero-shot Large\nLanguage Model-based reasoning for video understanding have become competitive\nchallengers to previous end-to-end models. However, long video understanding\npresents unique challenges due to the complexity of reasoning over extended\ntimespans, even for zero-shot LLM-based approaches. The challenge of\ninformation redundancy in long videos prompts the question of what specific\ninformation is essential for large language models (LLMs) and how to leverage\nthem for complex spatial-temporal reasoning in long-form video analysis. We\npropose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for\nzero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot\nframework for long video understanding using LLMs; (2) an event-based temporal\nreasoning and content-based spatial reasoning approach for LLMs to reason over\nspatial-temporal information in videos; (3) a self-reflective information\nreasoning scheme balancing temporal factors based on information sufficiency\nand prediction confidence. Our model significantly improves the\nstate-of-the-art on three long video question-answering benchmarks: EgoSchema,\nNextQA, and IntentQA, and the open question answering dataset ActivityNetQA.\nThe code is released here: https://github.com/mayhugotong/VideoINSTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the video-language domain, recent works in leveraging zero-shot Large\nLanguage Model-based reasoning for video understanding have become competitive\nchallengers to previous end-to-end models. However, long video understanding\npresents unique challenges due to the complexity of reasoning over extended\ntimespans, even for zero-shot LLM-based approaches. The challenge of\ninformation redundancy in long videos prompts the question of what specific\ninformation is essential for large language models (LLMs) and how to leverage\nthem for complex spatial-temporal reasoning in long-form video analysis. We\npropose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for\nzero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot\nframework for long video understanding using LLMs; (2) an event-based temporal\nreasoning and content-based spatial reasoning approach for LLMs to reason over\nspatial-temporal information in videos; (3) a self-reflective information\nreasoning scheme balancing temporal factors based on information sufficiency\nand prediction confidence. Our model significantly improves the\nstate-of-the-art on three long video question-answering benchmarks: EgoSchema,\nNextQA, and IntentQA, and the open question answering dataset ActivityNetQA.\nThe code is released here: https://github.com/mayhugotong/VideoINSTA."
                },
                "authors": [
                    {
                        "name": "Ruotong Liao"
                    },
                    {
                        "name": "Max Erler"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "EMNLP 2024 Findings; 22 pages; Code:\n  https://github.com/mayhugotong/VideoINSTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20364v1",
                "updated": "2024-09-30T15:03:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    3,
                    55,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:03:55Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    3,
                    55,
                    0,
                    274,
                    0
                ],
                "title": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using\n  Large Language Models"
                },
                "summary": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks."
                },
                "authors": [
                    {
                        "name": "Yizhou Huang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kezhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Wang"
                },
                "author": "Kezhi Wang",
                "arxiv_comment": "Submitted for possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10957v3",
                "updated": "2024-09-30T14:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    54,
                    17,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-16T14:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    14,
                    24,
                    30,
                    6,
                    168,
                    0
                ],
                "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "EMNLP 2024 Main, Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08424v2",
                "updated": "2024-09-30T14:25:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    25,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-13T11:16:43Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    11,
                    16,
                    43,
                    2,
                    73,
                    0
                ],
                "title": "Distract Large Language Models for Automatic Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distract Large Language Models for Automatic Jailbreak Attack"
                },
                "summary": "Extensive efforts have been made before the public release of Large language\nmodels (LLMs) to align their behaviors with human values. However, even\nmeticulously aligned LLMs remain vulnerable to malicious manipulations such as\njailbreaking, leading to unintended behaviors. In this work, we propose a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive efforts have been made before the public release of Large language\nmodels (LLMs) to align their behaviors with human values. However, even\nmeticulously aligned LLMs remain vulnerable to malicious manipulations such as\njailbreaking, leading to unintended behaviors. In this work, we propose a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies."
                },
                "authors": [
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun Chen"
                },
                "author": "Yun Chen",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09794v2",
                "updated": "2024-09-30T14:02:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    2,
                    59,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-19T08:41:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    41,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "AutoML-guided Fusion of Entity and LLM-based Representations for\n  Document Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoML-guided Fusion of Entity and LLM-based Representations for\n  Document Classification"
                },
                "summary": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (i.e. embeddings) do not\nefficiently exploit these resources. Dense and robust representations of\ndocuments are essential for effectively solving downstream classification and\nretrieval tasks. This work demonstrates that injecting embedded information\nfrom knowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets. The code is freely available at\n\\url{https://github.com/bkolosk1/bablfusion.git}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (i.e. embeddings) do not\nefficiently exploit these resources. Dense and robust representations of\ndocuments are essential for effectively solving downstream classification and\nretrieval tasks. This work demonstrates that injecting embedded information\nfrom knowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets. The code is freely available at\n\\url{https://github.com/bkolosk1/bablfusion.git}."
                },
                "authors": [
                    {
                        "name": "Boshko Koloski"
                    },
                    {
                        "name": "Senja Pollak"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Blaž Škrlj"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Škrlj"
                },
                "author": "Blaž Škrlj",
                "arxiv_comment": "Accepted at the 2024 Discovery Science Conference, oral presentation\n  track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20303v1",
                "updated": "2024-09-30T14:00:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T14:00:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "A Looming Replication Crisis in Evaluating Behavior in Language Models?\n  Evidence and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Looming Replication Crisis in Evaluating Behavior in Language Models?\n  Evidence and Solutions"
                },
                "summary": "In an era where large language models (LLMs) are increasingly integrated into\na wide range of everyday applications, research into these models' behavior has\nsurged. However, due to the novelty of the field, clear methodological\nguidelines are lacking. This raises concerns about the replicability and\ngeneralizability of insights gained from research on LLM behavior. In this\nstudy, we discuss the potential risk of a replication crisis and support our\nconcerns with a series of replication experiments focused on prompt engineering\ntechniques purported to influence reasoning abilities in LLMs. We tested\nGPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on\nthe chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well\nas Re-Reading prompt engineering techniques, using manually double-checked\nsubsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE,\nScienceQA, and StrategyQA. Our findings reveal a general lack of statistically\nsignificant differences across nearly all techniques tested, highlighting,\namong others, several methodological weaknesses in previous research. We\npropose a forward-looking approach that includes developing robust\nmethodologies for evaluating LLMs, establishing sound benchmarks, and designing\nrigorous experimental frameworks to ensure accurate and reliable assessments of\nmodel outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where large language models (LLMs) are increasingly integrated into\na wide range of everyday applications, research into these models' behavior has\nsurged. However, due to the novelty of the field, clear methodological\nguidelines are lacking. This raises concerns about the replicability and\ngeneralizability of insights gained from research on LLM behavior. In this\nstudy, we discuss the potential risk of a replication crisis and support our\nconcerns with a series of replication experiments focused on prompt engineering\ntechniques purported to influence reasoning abilities in LLMs. We tested\nGPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on\nthe chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well\nas Re-Reading prompt engineering techniques, using manually double-checked\nsubsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE,\nScienceQA, and StrategyQA. Our findings reveal a general lack of statistically\nsignificant differences across nearly all techniques tested, highlighting,\namong others, several methodological weaknesses in previous research. We\npropose a forward-looking approach that includes developing robust\nmethodologies for evaluating LLMs, establishing sound benchmarks, and designing\nrigorous experimental frameworks to ensure accurate and reliable assessments of\nmodel outputs."
                },
                "authors": [
                    {
                        "name": "Laurène Vaugrante"
                    },
                    {
                        "name": "Mathias Niepert"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20296v1",
                "updated": "2024-09-30T13:55:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    55,
                    42,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T13:55:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    55,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "PersonalLLM: Tailoring LLMs to Individual Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalLLM: Tailoring LLMs to Individual Preferences"
                },
                "summary": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM"
                },
                "authors": [
                    {
                        "name": "Thomas P. Zollo"
                    },
                    {
                        "name": "Andrew Wei Tung Siah"
                    },
                    {
                        "name": "Naimeng Ye"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20293v1",
                "updated": "2024-09-30T13:53:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    53,
                    1,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T13:53:01Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    53,
                    1,
                    0,
                    274,
                    0
                ],
                "title": "Automating MedSAM by Learning Prompts with Weak Few-Shot Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating MedSAM by Learning Prompts with Weak Few-Shot Supervision"
                },
                "summary": "Foundation models such as the recently introduced Segment Anything Model\n(SAM) have achieved remarkable results in image segmentation tasks. However,\nthese models typically require user interaction through handcrafted prompts\nsuch as bounding boxes, which limits their deployment to downstream tasks.\nAdapting these models to a specific task with fully labeled data also demands\nexpensive prior user interaction to obtain ground-truth annotations. This work\nproposes to replace conditioning on input prompts with a lightweight module\nthat directly learns a prompt embedding from the image embedding, both of which\nare subsequently used by the foundation model to output a segmentation mask.\nOur foundation models with learnable prompts can automatically segment any\nspecific region by 1) modifying the input through a prompt embedding predicted\nby a simple module, and 2) using weak labels (tight bounding boxes) and\nfew-shot supervision (10 samples). Our approach is validated on MedSAM, a\nversion of SAM fine-tuned for medical images, with results on three medical\ndatasets in MR and ultrasound imaging. Our code is available on\nhttps://github.com/Minimel/MedSAMWeakFewShotPromptAutomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models such as the recently introduced Segment Anything Model\n(SAM) have achieved remarkable results in image segmentation tasks. However,\nthese models typically require user interaction through handcrafted prompts\nsuch as bounding boxes, which limits their deployment to downstream tasks.\nAdapting these models to a specific task with fully labeled data also demands\nexpensive prior user interaction to obtain ground-truth annotations. This work\nproposes to replace conditioning on input prompts with a lightweight module\nthat directly learns a prompt embedding from the image embedding, both of which\nare subsequently used by the foundation model to output a segmentation mask.\nOur foundation models with learnable prompts can automatically segment any\nspecific region by 1) modifying the input through a prompt embedding predicted\nby a simple module, and 2) using weak labels (tight bounding boxes) and\nfew-shot supervision (10 samples). Our approach is validated on MedSAM, a\nversion of SAM fine-tuned for medical images, with results on three medical\ndatasets in MR and ultrasound imaging. Our code is available on\nhttps://github.com/Minimel/MedSAMWeakFewShotPromptAutomation."
                },
                "authors": [
                    {
                        "name": "Mélanie Gaillochet"
                    },
                    {
                        "name": "Christian Desrosiers"
                    },
                    {
                        "name": "Hervé Lombaert"
                    }
                ],
                "author_detail": {
                    "name": "Hervé Lombaert"
                },
                "author": "Hervé Lombaert",
                "arxiv_comment": "Accepted to MICCAI-MedAGI 2024 (LNCS Proceedings, Volume 15184), 10\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20288v1",
                "updated": "2024-09-30T13:44:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T13:44:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "You Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02365v3",
                "updated": "2024-09-30T13:30:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    30,
                    49,
                    0,
                    274,
                    0
                ],
                "published": "2024-05-03T06:41:48Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    6,
                    41,
                    48,
                    4,
                    124,
                    0
                ],
                "title": "ModelShield: Adaptive and Robust Watermark against Model Extraction\n  Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModelShield: Adaptive and Robust Watermark against Model Extraction\n  Attack"
                },
                "summary": "Large language models (LLMs) demonstrate general intelligence across a\nvariety of machine learning tasks, thereby enhancing the commercial value of\ntheir intellectual property (IP). To protect this IP, model owners typically\nallow user access only in a black-box manner, however, adversaries can still\nutilize model extraction attacks to steal the model intelligence encoded in\nmodel generation. Watermarking technology offers a promising solution for\ndefending against such attacks by embedding unique identifiers into the\nmodel-generated content. However, existing watermarking methods often\ncompromise the quality of generated content due to heuristic alterations and\nlack robust mechanisms to counteract adversarial strategies, thus limiting\ntheir practicality in real-world scenarios. In this paper, we introduce an\nadaptive and robust watermarking method (named ModelShield) to protect the IP\nof LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs\nto autonomously insert watermarks into their generated content to avoid the\ndegradation of model content. We also propose a robust watermark detection\nmechanism capable of effectively identifying watermark signals under the\ninterference of varying adversarial strategies. Besides, ModelShield is a\nplug-and-play method that does not require additional model training, enhancing\nits applicability in LLM deployments. Extensive evaluations on two real-world\ndatasets and three LLMs demonstrate that our method surpasses existing methods\nin terms of defense effectiveness and robustness while significantly reducing\nthe degradation of watermarking on the model-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate general intelligence across a\nvariety of machine learning tasks, thereby enhancing the commercial value of\ntheir intellectual property (IP). To protect this IP, model owners typically\nallow user access only in a black-box manner, however, adversaries can still\nutilize model extraction attacks to steal the model intelligence encoded in\nmodel generation. Watermarking technology offers a promising solution for\ndefending against such attacks by embedding unique identifiers into the\nmodel-generated content. However, existing watermarking methods often\ncompromise the quality of generated content due to heuristic alterations and\nlack robust mechanisms to counteract adversarial strategies, thus limiting\ntheir practicality in real-world scenarios. In this paper, we introduce an\nadaptive and robust watermarking method (named ModelShield) to protect the IP\nof LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs\nto autonomously insert watermarks into their generated content to avoid the\ndegradation of model content. We also propose a robust watermark detection\nmechanism capable of effectively identifying watermark signals under the\ninterference of varying adversarial strategies. Besides, ModelShield is a\nplug-and-play method that does not require additional model training, enhancing\nits applicability in LLM deployments. Extensive evaluations on two real-world\ndatasets and three LLMs demonstrate that our method surpasses existing methods\nin terms of defense effectiveness and robustness while significantly reducing\nthe degradation of watermarking on the model-generated content."
                },
                "authors": [
                    {
                        "name": "Kaiyi Pang"
                    },
                    {
                        "name": "Tao Qi"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Minhao Bai"
                    },
                    {
                        "name": "Minghu Jiang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Huang"
                },
                "author": "Yongfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14511v2",
                "updated": "2024-09-30T13:24:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    24,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-06T00:00:18Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    0,
                    18,
                    4,
                    250,
                    0
                ],
                "title": "Evaluation of Task Specific Productivity Improvements Using a Generative\n  Artificial Intelligence Personal Assistant Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Task Specific Productivity Improvements Using a Generative\n  Artificial Intelligence Personal Assistant Tool"
                },
                "summary": "This study evaluates the productivity improvements achieved using a\ngenerative artificial intelligence personal assistant tool (PAT) developed by\nTrane Technologies. The PAT, based on OpenAI's GPT 3.5 model, was deployed on\nMicrosoft Azure to ensure secure access and protection of intellectual\nproperty. To assess the tool's productivity effectiveness, an experiment was\nconducted comparing the completion times and content quality of four common\noffice tasks: writing an email, summarizing an article, creating instructions\nfor a simple task, and preparing a presentation outline. Sixty-three (63)\nparticipants were randomly divided into a test group using the PAT and a\ncontrol group performing the tasks manually. Results indicated significant\nproductivity enhancements, particularly for tasks involving summarization and\ninstruction creation, with improvements ranging from 3.3% to 69%. The study\nfurther analyzed factors such as the age of users, response word counts, and\nquality of responses, revealing that the PAT users generated more verbose and\nhigher-quality content. An 'LLM-as-a-judge' method employing GPT-4 was used to\ngrade the quality of responses, which effectively distinguished between high\nand low-quality outputs. The findings underscore the potential of PATs in\nenhancing workplace productivity and highlight areas for further research and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the productivity improvements achieved using a\ngenerative artificial intelligence personal assistant tool (PAT) developed by\nTrane Technologies. The PAT, based on OpenAI's GPT 3.5 model, was deployed on\nMicrosoft Azure to ensure secure access and protection of intellectual\nproperty. To assess the tool's productivity effectiveness, an experiment was\nconducted comparing the completion times and content quality of four common\noffice tasks: writing an email, summarizing an article, creating instructions\nfor a simple task, and preparing a presentation outline. Sixty-three (63)\nparticipants were randomly divided into a test group using the PAT and a\ncontrol group performing the tasks manually. Results indicated significant\nproductivity enhancements, particularly for tasks involving summarization and\ninstruction creation, with improvements ranging from 3.3% to 69%. The study\nfurther analyzed factors such as the age of users, response word counts, and\nquality of responses, revealing that the PAT users generated more verbose and\nhigher-quality content. An 'LLM-as-a-judge' method employing GPT-4 was used to\ngrade the quality of responses, which effectively distinguished between high\nand low-quality outputs. The findings underscore the potential of PATs in\nenhancing workplace productivity and highlight areas for further research and\noptimization."
                },
                "authors": [
                    {
                        "name": "Brian S. Freeman"
                    },
                    {
                        "name": "Kendall Arriola"
                    },
                    {
                        "name": "Dan Cottell"
                    },
                    {
                        "name": "Emmett Lawlor"
                    },
                    {
                        "name": "Matt Erdman"
                    },
                    {
                        "name": "Trevor Sutherland"
                    },
                    {
                        "name": "Brian Wells"
                    }
                ],
                "author_detail": {
                    "name": "Brian Wells"
                },
                "author": "Brian Wells",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09866v2",
                "updated": "2024-09-30T13:10:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    10,
                    12,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-14T09:23:40Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    9,
                    23,
                    40,
                    4,
                    166,
                    0
                ],
                "title": "Globally Optimal GNSS Multi-Antenna Lever Arm Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Globally Optimal GNSS Multi-Antenna Lever Arm Calibration"
                },
                "summary": "Sensor calibration is crucial for autonomous driving, providing the basis for\naccurate localization and consistent data fusion. Enabling the use of\nhigh-accuracy GNSS sensors, this work focuses on the antenna lever arm\ncalibration. We propose a globally optimal multi-antenna lever arm calibration\napproach based on motion measurements. For this, we derive an optimization\nmethod that further allows the integration of a-priori knowledge. Globally\noptimal solutions are obtained by leveraging the Lagrangian dual problem and a\nprimal recovery strategy. Generally, motion-based calibration for autonomous\nvehicles is known to be difficult due to cars' predominantly planar motion.\nTherefore, we first describe the motion requirements for a unique solution and\nthen propose a planar motion extension to overcome this issue and enable a\ncalibration based on the restricted motion of autonomous vehicles. Last we\npresent and discuss the results of our thorough evaluation. Using simulated and\naugmented real-world data, we achieve accurate calibration results and fast run\ntimes that allow online deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensor calibration is crucial for autonomous driving, providing the basis for\naccurate localization and consistent data fusion. Enabling the use of\nhigh-accuracy GNSS sensors, this work focuses on the antenna lever arm\ncalibration. We propose a globally optimal multi-antenna lever arm calibration\napproach based on motion measurements. For this, we derive an optimization\nmethod that further allows the integration of a-priori knowledge. Globally\noptimal solutions are obtained by leveraging the Lagrangian dual problem and a\nprimal recovery strategy. Generally, motion-based calibration for autonomous\nvehicles is known to be difficult due to cars' predominantly planar motion.\nTherefore, we first describe the motion requirements for a unique solution and\nthen propose a planar motion extension to overcome this issue and enable a\ncalibration based on the restricted motion of autonomous vehicles. Last we\npresent and discuss the results of our thorough evaluation. Using simulated and\naugmented real-world data, we achieve accurate calibration results and fast run\ntimes that allow online deployment."
                },
                "authors": [
                    {
                        "name": "Thomas Wodtko"
                    },
                    {
                        "name": "Michael Buchholz"
                    }
                ],
                "author_detail": {
                    "name": "Michael Buchholz"
                },
                "author": "Michael Buchholz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13993v2",
                "updated": "2024-09-30T13:03:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    3,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-19T02:48:54Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    48,
                    54,
                    4,
                    201,
                    0
                ],
                "title": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models"
                },
                "summary": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently."
                },
                "authors": [
                    {
                        "name": "Christoforus Yoga Haryanto"
                    }
                ],
                "author_detail": {
                    "name": "Christoforus Yoga Haryanto"
                },
                "author": "Christoforus Yoga Haryanto",
                "arxiv_comment": "10 pages, 3 figures, 1 table, accepted to the 51st International\n  Conference on Computers and Industrial Engineering (CIE51)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16694v2",
                "updated": "2024-09-30T12:55:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    55,
                    3,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-25T07:38:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms"
                },
                "summary": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization."
                },
                "authors": [
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Chengtao Lv"
                    },
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Jinyang Du"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_comment": "Ruihao Gong leads the overall organization of the survey, with Yifu\n  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is\n  responsible for authoring Section 4, while Chengtao Lv and Zining Wang\n  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and\n  Xianglong Liu provide guidance during the whole process and assist in\n  refining the final manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20252v1",
                "updated": "2024-09-30T12:42:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    42,
                    25,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:42:25Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    42,
                    25,
                    0,
                    274,
                    0
                ],
                "title": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?"
                },
                "summary": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry."
                },
                "authors": [
                    {
                        "name": "Morgan Fouesneau"
                    },
                    {
                        "name": "Ivelina G. Momcheva"
                    },
                    {
                        "name": "Urmila Chadayammuri"
                    },
                    {
                        "name": "Mariia Demianenko"
                    },
                    {
                        "name": "Antoine Dumont"
                    },
                    {
                        "name": "Raphael E. Hviding"
                    },
                    {
                        "name": "K. Angelique Kahle"
                    },
                    {
                        "name": "Nadiia Pulatova"
                    },
                    {
                        "name": "Bhavesh Rajpoot"
                    },
                    {
                        "name": "Marten B. Scheuck"
                    },
                    {
                        "name": "Rhys Seeburger"
                    },
                    {
                        "name": "Dmitry Semenov"
                    },
                    {
                        "name": "Jaime I. Villaseñor"
                    }
                ],
                "author_detail": {
                    "name": "Jaime I. Villaseñor"
                },
                "author": "Jaime I. Villaseñor",
                "arxiv_comment": "Paper submitted to RASTI. We share our experience, ethical and legal\n  concerns (5.3), and recommendations for individuals and journals (6.). We\n  welcome feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20247v1",
                "updated": "2024-09-30T12:36:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    36,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:36:27Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    36,
                    27,
                    0,
                    274,
                    0
                ],
                "title": "Resource Allocation for Stable LLM Training in Mobile Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Stable LLM Training in Mobile Edge Computing"
                },
                "summary": "As mobile devices increasingly become focal points for advanced applications,\nedge computing presents a viable solution to their inherent computational\nlimitations, particularly in deploying large language models (LLMs). However,\ndespite the advancements in edge computing, significant challenges remain in\nefficient training and deploying LLMs due to the computational demands and data\nprivacy concerns associated with these models. This paper explores a\ncollaborative training framework that integrates mobile users with edge servers\nto optimize resource allocation, thereby enhancing both performance and\nefficiency. Our approach leverages parameter-efficient fine-tuning (PEFT)\nmethods, allowing mobile users to adjust the initial layers of the LLM while\nedge servers handle the more demanding latter layers. Specifically, we\nformulate a multi-objective optimization problem to minimize the total energy\nconsumption and delay during training. We also address the common issue of\ninstability in model performance by incorporating stability enhancements into\nour objective function. Through novel fractional programming technique, we\nachieve a stationary point for the formulated problem. Simulations demonstrate\nthat our method reduces the energy consumption as well as the latency, and\nincreases the reliability of LLMs across various mobile settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As mobile devices increasingly become focal points for advanced applications,\nedge computing presents a viable solution to their inherent computational\nlimitations, particularly in deploying large language models (LLMs). However,\ndespite the advancements in edge computing, significant challenges remain in\nefficient training and deploying LLMs due to the computational demands and data\nprivacy concerns associated with these models. This paper explores a\ncollaborative training framework that integrates mobile users with edge servers\nto optimize resource allocation, thereby enhancing both performance and\nefficiency. Our approach leverages parameter-efficient fine-tuning (PEFT)\nmethods, allowing mobile users to adjust the initial layers of the LLM while\nedge servers handle the more demanding latter layers. Specifically, we\nformulate a multi-objective optimization problem to minimize the total energy\nconsumption and delay during training. We also address the common issue of\ninstability in model performance by incorporating stability enhancements into\nour objective function. Through novel fractional programming technique, we\nachieve a stationary point for the formulated problem. Simulations demonstrate\nthat our method reduces the energy consumption as well as the latency, and\nincreases the reliability of LLMs across various mobile settings."
                },
                "authors": [
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "This paper appears in the 2024 International Symposium on Theory,\n  Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile\n  Computing (MobiHoc)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20245v1",
                "updated": "2024-09-30T12:34:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    34,
                    4,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:34:04Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    34,
                    4,
                    0,
                    274,
                    0
                ],
                "title": "Waveform Design of Multi-User-Multi-Target ISAC System based on\n  Kullback-Leibler Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Waveform Design of Multi-User-Multi-Target ISAC System based on\n  Kullback-Leibler Divergence"
                },
                "summary": "This paper presents a novel framework that leverages the Kullback-Leibler\ndivergence (KLD) to analyze and optimize performance trade-offs in integrated\nsensing and communication (ISAC) systems. We consider a\nmultiple-input-multiple-output (MIMO) base station that simultaneously serves\ncommunication user equipments (UEs) and detects multiple targets using shared\nantenna deployment. The proposed KLD-based approach provides a unified\nperformance measure encompassing both UE error rate and target detection\ncapability. We apply this approach on two well-known communication beamforming\ntechniques, maximum ratio transmission (MRT) and zero-forcing (ZF), and\nevaluate the effect on the radar subsystem. Furthermore, two optimization\nproblems are formulated and solved. The first one optimizes the KLD of the\nradar subsystem for given constraints on the communication KLD, whereas the\nsecond one focuses on communication waveform KLD-based optimization and\nconstrained radar KLD. These optimization problems are solved using a projected\ngradient method with an adaptive penalty for the radar waveform and a\ngradient-assisted interior point method for the communication waveform. Through\ntheoretical derivations and extensive simulations, it is demonstrated that our\napproach can be a powerful tool for characterizing and optimizing the\nperformance trade-offs of ISAC under various configurations. The results also\nshow significant improvements in both sensing and communication performance by\nthe KLD-optimized system compared to well-known benchmarks, such as\nconventional MRT and ZF for the communication subsystem, and the conventional\nidentity covariance design for the radar subsystem. These findings support the\nholistic design and optimization of ISAC in next-generation wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework that leverages the Kullback-Leibler\ndivergence (KLD) to analyze and optimize performance trade-offs in integrated\nsensing and communication (ISAC) systems. We consider a\nmultiple-input-multiple-output (MIMO) base station that simultaneously serves\ncommunication user equipments (UEs) and detects multiple targets using shared\nantenna deployment. The proposed KLD-based approach provides a unified\nperformance measure encompassing both UE error rate and target detection\ncapability. We apply this approach on two well-known communication beamforming\ntechniques, maximum ratio transmission (MRT) and zero-forcing (ZF), and\nevaluate the effect on the radar subsystem. Furthermore, two optimization\nproblems are formulated and solved. The first one optimizes the KLD of the\nradar subsystem for given constraints on the communication KLD, whereas the\nsecond one focuses on communication waveform KLD-based optimization and\nconstrained radar KLD. These optimization problems are solved using a projected\ngradient method with an adaptive penalty for the radar waveform and a\ngradient-assisted interior point method for the communication waveform. Through\ntheoretical derivations and extensive simulations, it is demonstrated that our\napproach can be a powerful tool for characterizing and optimizing the\nperformance trade-offs of ISAC under various configurations. The results also\nshow significant improvements in both sensing and communication performance by\nthe KLD-optimized system compared to well-known benchmarks, such as\nconventional MRT and ZF for the communication subsystem, and the conventional\nidentity covariance design for the radar subsystem. These findings support the\nholistic design and optimization of ISAC in next-generation wireless networks."
                },
                "authors": [
                    {
                        "name": "Yousef Kloob"
                    },
                    {
                        "name": "Mohammad Al-Jarrah"
                    },
                    {
                        "name": "Emad Alsusa"
                    }
                ],
                "author_detail": {
                    "name": "Emad Alsusa"
                },
                "author": "Emad Alsusa",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17717v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17717v3",
                "updated": "2024-09-30T12:02:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    2,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-02-27T17:52:33Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    17,
                    52,
                    33,
                    1,
                    58,
                    0
                ],
                "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG"
                },
                "summary": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification."
                },
                "authors": [
                    {
                        "name": "Ayana Niwa"
                    },
                    {
                        "name": "Hayate Iso"
                    }
                ],
                "author_detail": {
                    "name": "Hayate Iso"
                },
                "author": "Hayate Iso",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17717v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17717v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20222v1",
                "updated": "2024-09-30T12:01:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    1,
                    29,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T12:01:29Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    1,
                    29,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language\n  Models"
                },
                "summary": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture."
                },
                "authors": [
                    {
                        "name": "David Castillo-Bolado"
                    },
                    {
                        "name": "Joseph Davidson"
                    },
                    {
                        "name": "Finlay Gray"
                    },
                    {
                        "name": "Marek Rosa"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rosa"
                },
                "author": "Marek Rosa",
                "arxiv_comment": "Accepted as a poster at NeurIPS D&B Track 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10383v2",
                "updated": "2024-09-30T11:58:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    58,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-16T13:23:54Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    13,
                    23,
                    54,
                    0,
                    289,
                    0
                ],
                "title": "Privacy in Large Language Models: Attacks, Defenses and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy in Large Language Models: Attacks, Defenses and Future\n  Directions"
                },
                "summary": "The advancement of large language models (LLMs) has significantly enhanced\nthe ability to effectively tackle various downstream NLP tasks and unify these\ntasks into generative pipelines. On the one hand, powerful language models,\ntrained on massive textual data, have brought unparalleled accessibility and\nusability for both models and users. On the other hand, unrestricted access to\nthese models can also introduce potential malicious and unintentional privacy\nrisks. Despite ongoing efforts to address the safety and privacy concerns\nassociated with LLMs, the problem remains unresolved. In this paper, we provide\na comprehensive analysis of the current privacy attacks targeting LLMs and\ncategorize them according to the adversary's assumed capabilities to shed light\non the potential vulnerabilities present in LLMs. Then, we present a detailed\noverview of prominent defense strategies that have been developed to counter\nthese privacy attacks. Beyond existing works, we identify upcoming privacy\nconcerns as LLMs evolve. Lastly, we point out several potential avenues for\nfuture exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has significantly enhanced\nthe ability to effectively tackle various downstream NLP tasks and unify these\ntasks into generative pipelines. On the one hand, powerful language models,\ntrained on massive textual data, have brought unparalleled accessibility and\nusability for both models and users. On the other hand, unrestricted access to\nthese models can also introduce potential malicious and unintentional privacy\nrisks. Despite ongoing efforts to address the safety and privacy concerns\nassociated with LLMs, the problem remains unresolved. In this paper, we provide\na comprehensive analysis of the current privacy attacks targeting LLMs and\ncategorize them according to the adversary's assumed capabilities to shed light\non the potential vulnerabilities present in LLMs. Then, we present a detailed\noverview of prominent defense strategies that have been developed to counter\nthese privacy attacks. Beyond existing works, we identify upcoming privacy\nconcerns as LLMs evolve. Lastly, we point out several potential avenues for\nfuture exploration."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Jinglong Luo"
                    },
                    {
                        "name": "Jiecong Wang"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "We upload the survey to cover more recent papers and inlcude privacy\n  resaearch on multi-modality",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15484v2",
                "updated": "2024-09-30T11:25:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    25,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-17T09:15:57Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    15,
                    57,
                    0,
                    169,
                    0
                ],
                "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large\n  Language Models"
                },
                "summary": "The use of Large Language Models (LLMs) in hiring has led to legislative\nactions to protect vulnerable demographic groups. This paper presents a novel\nframework for benchmarking hierarchical gender hiring bias in Large Language\nModels (LLMs) for resume scoring, revealing significant issues of reverse\ngender hiring bias and overdebiasing. Our contributions are fourfold: Firstly,\nwe introduce a new construct grounded in labour economics, legal principles,\nand critiques of current bias benchmarks: hiring bias can be categorized into\ntwo types: Level bias (difference in the average outcomes between demographic\ncounterfactual groups) and Spread bias (difference in the variance of outcomes\nbetween demographic counterfactual groups); Level bias can be further\nsubdivided into statistical bias (i.e. changing with non-demographic content)\nand taste-based bias (i.e. consistent regardless of non-demographic content).\nSecondly, the framework includes rigorous statistical and computational hiring\nbias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio,\nPermutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring\nbiases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant\nbiases against males in at least one industry. An industry-effect regression\nreveals that the healthcare industry is the most biased against males.\nMoreover, we found that the bias performance remains invariant with resume\ncontent for eight out of ten LLMs. This indicates that the bias performance\nmeasured in this paper might apply to other resume datasets with different\nresume qualities. Fourthly, we provide a user-friendly demo and resume dataset\nto support the adoption and practical use of the framework, which can be\ngeneralized to other social traits and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in hiring has led to legislative\nactions to protect vulnerable demographic groups. This paper presents a novel\nframework for benchmarking hierarchical gender hiring bias in Large Language\nModels (LLMs) for resume scoring, revealing significant issues of reverse\ngender hiring bias and overdebiasing. Our contributions are fourfold: Firstly,\nwe introduce a new construct grounded in labour economics, legal principles,\nand critiques of current bias benchmarks: hiring bias can be categorized into\ntwo types: Level bias (difference in the average outcomes between demographic\ncounterfactual groups) and Spread bias (difference in the variance of outcomes\nbetween demographic counterfactual groups); Level bias can be further\nsubdivided into statistical bias (i.e. changing with non-demographic content)\nand taste-based bias (i.e. consistent regardless of non-demographic content).\nSecondly, the framework includes rigorous statistical and computational hiring\nbias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio,\nPermutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring\nbiases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant\nbiases against males in at least one industry. An industry-effect regression\nreveals that the healthcare industry is the most biased against males.\nMoreover, we found that the bias performance remains invariant with resume\ncontent for eight out of ten LLMs. This indicates that the bias performance\nmeasured in this paper might apply to other resume datasets with different\nresume qualities. Fourthly, we provide a user-friendly demo and resume dataset\nto support the adoption and practical use of the framework, which can be\ngeneralized to other social traits and tasks."
                },
                "authors": [
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Michael Thaler"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Skylar Lu"
                    },
                    {
                        "name": "Sachin Beepath"
                    },
                    {
                        "name": "Ediz Ertekin Jr."
                    },
                    {
                        "name": "Maria Perez-Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "Maria Perez-Ortiz"
                },
                "author": "Maria Perez-Ortiz",
                "arxiv_comment": "EMNLP 2024 Findings Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20192v1",
                "updated": "2024-09-30T11:08:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    8,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T11:08:27Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    8,
                    27,
                    0,
                    274,
                    0
                ],
                "title": "Factory Operators' Perspectives on Cognitive Assistants for Knowledge\n  Sharing: Challenges, Risks, and Impact on Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factory Operators' Perspectives on Cognitive Assistants for Knowledge\n  Sharing: Challenges, Risks, and Impact on Work"
                },
                "summary": "In the shift towards human-centered manufacturing, our two-year longitudinal\nstudy investigates the real-world impact of deploying Cognitive Assistants\n(CAs) in factories. The CAs were designed to facilitate knowledge sharing among\nfactory operators. Our investigation focused on smartphone-based voice\nassistants and LLM-powered chatbots, examining their usability and utility in a\nreal-world factory setting. Based on the qualitative feedback we collected\nduring the deployments of CAs at the factories, we conducted a thematic\nanalysis to investigate the perceptions, challenges, and overall impact on\nworkflow and knowledge sharing.\n  Our results indicate that while CAs have the potential to significantly\nimprove efficiency through knowledge sharing and quicker resolution of\nproduction issues, they also introduce concerns around workplace surveillance,\nthe types of knowledge that can be shared, and shortcomings compared to\nhuman-to-human knowledge sharing. Additionally, our findings stress the\nimportance of addressing privacy, knowledge contribution burdens, and tensions\nbetween factory operators and their managers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the shift towards human-centered manufacturing, our two-year longitudinal\nstudy investigates the real-world impact of deploying Cognitive Assistants\n(CAs) in factories. The CAs were designed to facilitate knowledge sharing among\nfactory operators. Our investigation focused on smartphone-based voice\nassistants and LLM-powered chatbots, examining their usability and utility in a\nreal-world factory setting. Based on the qualitative feedback we collected\nduring the deployments of CAs at the factories, we conducted a thematic\nanalysis to investigate the perceptions, challenges, and overall impact on\nworkflow and knowledge sharing.\n  Our results indicate that while CAs have the potential to significantly\nimprove efficiency through knowledge sharing and quicker resolution of\nproduction issues, they also introduce concerns around workplace surveillance,\nthe types of knowledge that can be shared, and shortcomings compared to\nhuman-to-human knowledge sharing. Additionally, our findings stress the\nimportance of addressing privacy, knowledge contribution burdens, and tensions\nbetween factory operators and their managers."
                },
                "authors": [
                    {
                        "name": "Samuel Kernan Freire"
                    },
                    {
                        "name": "Tianhao He"
                    },
                    {
                        "name": "Chaofan Wang"
                    },
                    {
                        "name": "Evangelos Niforatos"
                    },
                    {
                        "name": "Alessandro Bozzon"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Bozzon"
                },
                "author": "Alessandro Bozzon",
                "arxiv_comment": "32 pages, 6 figures, 2 tables, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20188v1",
                "updated": "2024-09-30T11:04:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    4,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T11:04:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    11,
                    4,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "Active Listener: Continuous Generation of Listener's Head Motion\n  Response in Dyadic Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Listener: Continuous Generation of Listener's Head Motion\n  Response in Dyadic Interactions"
                },
                "summary": "A key component of dyadic spoken interactions is the contextually relevant\nnon-verbal gestures, such as head movements that reflect a listener's response\nto the interlocutor's speech. Although significant progress has been made in\nthe context of generating co-speech gestures, generating listener's response\nhas remained a challenge. We introduce the task of generating continuous head\nmotion response of a listener in response to the speaker's speech in real time.\nTo this end, we propose a graph-based end-to-end crossmodal model that takes\ninterlocutor's speech audio as input and directly generates head pose angles\n(roll, pitch, yaw) of the listener in real time. Different from previous work,\nour approach is completely data-driven, does not require manual annotations or\noversimplify head motion to merely nods and shakes. Extensive evaluation on the\ndyadic interaction sessions on the IEMOCAP dataset shows that our model\nproduces a low overall error (4.5 degrees) and a high frame rate, thereby\nindicating its deployability in real-world human-robot interaction systems. Our\ncode is available at - https://github.com/bigzen/Active-Listener",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key component of dyadic spoken interactions is the contextually relevant\nnon-verbal gestures, such as head movements that reflect a listener's response\nto the interlocutor's speech. Although significant progress has been made in\nthe context of generating co-speech gestures, generating listener's response\nhas remained a challenge. We introduce the task of generating continuous head\nmotion response of a listener in response to the speaker's speech in real time.\nTo this end, we propose a graph-based end-to-end crossmodal model that takes\ninterlocutor's speech audio as input and directly generates head pose angles\n(roll, pitch, yaw) of the listener in real time. Different from previous work,\nour approach is completely data-driven, does not require manual annotations or\noversimplify head motion to merely nods and shakes. Extensive evaluation on the\ndyadic interaction sessions on the IEMOCAP dataset shows that our model\nproduces a low overall error (4.5 degrees) and a high frame rate, thereby\nindicating its deployability in real-world human-robot interaction systems. Our\ncode is available at - https://github.com/bigzen/Active-Listener"
                },
                "authors": [
                    {
                        "name": "Bishal Ghosh"
                    },
                    {
                        "name": "Emma Li"
                    },
                    {
                        "name": "Tanaya Guha"
                    }
                ],
                "author_detail": {
                    "name": "Tanaya Guha"
                },
                "author": "Tanaya Guha",
                "arxiv_comment": "4+1 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20181v1",
                "updated": "2024-09-30T10:48:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    48,
                    20,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:48:20Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    48,
                    20,
                    0,
                    274,
                    0
                ],
                "title": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v3",
                "updated": "2024-09-30T10:43:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    43,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00341v2",
                "updated": "2024-09-30T10:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-29T07:00:37Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    7,
                    0,
                    37,
                    5,
                    181,
                    0
                ],
                "title": "Iterative Data Generation with Large Language Models for Aspect-based\n  Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Data Generation with Large Language Models for Aspect-based\n  Sentiment Analysis"
                },
                "summary": "Aspect-based Sentiment Analysis (ABSA) is an important sentiment analysis\ntask, which aims to determine the sentiment polarity towards an aspect in a\nsentence. Due to the expensive and limited labeled data, data generation (DG)\nhas become the standard for improving the performance of ABSA. However, current\nDG methods usually have some shortcomings: 1) poor fluency and coherence, 2)\nlack of diversity of generated data, and 3) reliance on some existing labeled\ndata, hindering its applications in real-world scenarios. With the advancement\nof large language models (LLMs), LLM-based DG has the potential to solve the\nabove issues. Unfortunately, directly prompting LLMs struggles to generate the\ndesired pseudo-label ABSA data, as LLMs are prone to hallucinations, leading to\nundesired data generation. To this end, we propose a systematic Iterative Data\nGeneration framework, namely IDG, to boost the performance of ABSA. The core of\nIDG is to make full use of the powerful abilities (i.e., instruction-following,\nin-context learning and self-reflection) of LLMs to iteratively generate more\nfluent and diverse pseudo-label data, starting from an unsupervised sentence\ncorpus. Specifically, IDG designs a novel iterative data generation mechanism\nand a self-reflection data filtering module to tackle the challenges of\nunexpected data generation caused by hallucinations. Extensive experiments on\nfour widely-used ABSA benchmarks show that IDG brings consistent and\nsignificant performance gains among five baseline ABSA models. More\nencouragingly, the synthetic data generated by IDG can achieve comparable or\neven better performance against the manually annotated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based Sentiment Analysis (ABSA) is an important sentiment analysis\ntask, which aims to determine the sentiment polarity towards an aspect in a\nsentence. Due to the expensive and limited labeled data, data generation (DG)\nhas become the standard for improving the performance of ABSA. However, current\nDG methods usually have some shortcomings: 1) poor fluency and coherence, 2)\nlack of diversity of generated data, and 3) reliance on some existing labeled\ndata, hindering its applications in real-world scenarios. With the advancement\nof large language models (LLMs), LLM-based DG has the potential to solve the\nabove issues. Unfortunately, directly prompting LLMs struggles to generate the\ndesired pseudo-label ABSA data, as LLMs are prone to hallucinations, leading to\nundesired data generation. To this end, we propose a systematic Iterative Data\nGeneration framework, namely IDG, to boost the performance of ABSA. The core of\nIDG is to make full use of the powerful abilities (i.e., instruction-following,\nin-context learning and self-reflection) of LLMs to iteratively generate more\nfluent and diverse pseudo-label data, starting from an unsupervised sentence\ncorpus. Specifically, IDG designs a novel iterative data generation mechanism\nand a self-reflection data filtering module to tackle the challenges of\nunexpected data generation caused by hallucinations. Extensive experiments on\nfour widely-used ABSA benchmarks show that IDG brings consistent and\nsignificant performance gains among five baseline ABSA models. More\nencouragingly, the synthetic data generated by IDG can achieve comparable or\neven better performance against the manually annotated data."
                },
                "authors": [
                    {
                        "name": "Qihuang Zhong"
                    },
                    {
                        "name": "Haiyun Li"
                    },
                    {
                        "name": "Luyao Zhuang"
                    },
                    {
                        "name": "Juhua Liu"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20163v1",
                "updated": "2024-09-30T10:19:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    19,
                    4,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T10:19:04Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    19,
                    4,
                    0,
                    274,
                    0
                ],
                "title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal\n  Assistants"
                },
                "summary": "LLM-based agents have been widely applied as personal assistants, capable of\nmemorizing information from user messages and responding to personal queries.\nHowever, there still lacks an objective and automatic evaluation on their\nmemory capability, largely due to the challenges in constructing reliable\nquestions and answers (QAs) according to user messages. In this paper, we\npropose MemSim, a Bayesian simulator designed to automatically construct\nreliable QAs from generated user messages, simultaneously keeping their\ndiversity and scalability. Specifically, we introduce the Bayesian Relation\nNetwork (BRNet) and a causal generation mechanism to mitigate the impact of LLM\nhallucinations on factual information, facilitating the automatic creation of\nan evaluation dataset. Based on MemSim, we generate a dataset in the daily-life\nscenario, named MemDaily, and conduct extensive experiments to assess the\neffectiveness of our approach. We also provide a benchmark for evaluating\ndifferent memory mechanisms in LLM-based agents with the MemDaily dataset. To\nbenefit the research community, we have released our project at\nhttps://github.com/nuster1128/MemSim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have been widely applied as personal assistants, capable of\nmemorizing information from user messages and responding to personal queries.\nHowever, there still lacks an objective and automatic evaluation on their\nmemory capability, largely due to the challenges in constructing reliable\nquestions and answers (QAs) according to user messages. In this paper, we\npropose MemSim, a Bayesian simulator designed to automatically construct\nreliable QAs from generated user messages, simultaneously keeping their\ndiversity and scalability. Specifically, we introduce the Bayesian Relation\nNetwork (BRNet) and a causal generation mechanism to mitigate the impact of LLM\nhallucinations on factual information, facilitating the automatic creation of\nan evaluation dataset. Based on MemSim, we generate a dataset in the daily-life\nscenario, named MemDaily, and conduct extensive experiments to assess the\neffectiveness of our approach. We also provide a benchmark for evaluating\ndifferent memory mechanisms in LLM-based agents with the MemDaily dataset. To\nbenefit the research community, we have released our project at\nhttps://github.com/nuster1128/MemSim."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Luyu Chen"
                    },
                    {
                        "name": "Zeren Jiang"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "26 pages, 25 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05561v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05561v6",
                "updated": "2024-09-30T10:17:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    17,
                    12,
                    0,
                    274,
                    0
                ],
                "published": "2024-01-10T22:07:21Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    22,
                    7,
                    21,
                    2,
                    10,
                    0
                ],
                "title": "TrustLLM: Trustworthiness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustLLM: Trustworthiness in Large Language Models"
                },
                "summary": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Yixin Huang"
                    },
                    {
                        "name": "Wenhan Lyu"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yijue Wang"
                    },
                    {
                        "name": "Zhikun Zhang"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Chunyuan Li"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Manolis Kellis"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jian Pei"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Joaquin Vanschoren"
                    },
                    {
                        "name": "John Mitchell"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Lifang He"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Suman Jana"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "William Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Xuyu Wang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Yinzhi Cao"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "This work is still under work and we welcome your contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05561v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05561v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20149v1",
                "updated": "2024-09-30T09:55:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    55,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:55:39Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    55,
                    39,
                    0,
                    274,
                    0
                ],
                "title": "1 Trillion Token (1TT) Platform: A Novel Framework for Efficient Data\n  Sharing and Compensation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1 Trillion Token (1TT) Platform: A Novel Framework for Efficient Data\n  Sharing and Compensation in Large Language Models"
                },
                "summary": "In this paper, we propose the 1 Trillion Token Platform (1TT Platform), a\nnovel framework designed to facilitate efficient data sharing with a\ntransparent and equitable profit-sharing mechanism. The platform fosters\ncollaboration between data contributors, who provide otherwise non-disclosed\ndatasets, and a data consumer, who utilizes these datasets to enhance their own\nservices. Data contributors are compensated in monetary terms, receiving a\nshare of the revenue generated by the services of the data consumer. The data\nconsumer is committed to sharing a portion of the revenue with contributors,\naccording to predefined profit-sharing arrangements. By incorporating a\ntransparent profit-sharing paradigm to incentivize large-scale data sharing,\nthe 1TT Platform creates a collaborative environment to drive the advancement\nof NLP and LLM technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose the 1 Trillion Token Platform (1TT Platform), a\nnovel framework designed to facilitate efficient data sharing with a\ntransparent and equitable profit-sharing mechanism. The platform fosters\ncollaboration between data contributors, who provide otherwise non-disclosed\ndatasets, and a data consumer, who utilizes these datasets to enhance their own\nservices. Data contributors are compensated in monetary terms, receiving a\nshare of the revenue generated by the services of the data consumer. The data\nconsumer is committed to sharing a portion of the revenue with contributors,\naccording to predefined profit-sharing arrangements. By incorporating a\ntransparent profit-sharing paradigm to incentivize large-scale data sharing,\nthe 1TT Platform creates a collaborative environment to drive the advancement\nof NLP and LLM technologies."
                },
                "authors": [
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Hyunsoo Ha"
                    },
                    {
                        "name": "Jihoo Kim"
                    },
                    {
                        "name": "Yungi Kim"
                    },
                    {
                        "name": "Dahyun Kim"
                    },
                    {
                        "name": "Sukyung Lee"
                    },
                    {
                        "name": "Seonghoon Yang"
                    }
                ],
                "author_detail": {
                    "name": "Seonghoon Yang"
                },
                "author": "Seonghoon Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20147v1",
                "updated": "2024-09-30T09:52:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    52,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:52:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    52,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "Classification of Radiological Text in Small and Imbalanced Datasets in\n  a Non-English Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification of Radiological Text in Small and Imbalanced Datasets in\n  a Non-English Language"
                },
                "summary": "Natural language processing (NLP) in the medical domain can underperform in\nreal-world applications involving small datasets in a non-English language with\nfew labeled samples and imbalanced classes. There is yet no consensus on how to\napproach this problem. We evaluated a set of NLP models including BERT-like\ntransformers, few-shot learning with sentence transformers (SetFit), and\nprompted large language models (LLM), using three datasets of radiology reports\non magnetic resonance images of epilepsy patients in Danish, a low-resource\nlanguage. Our results indicate that BERT-like models pretrained in the target\ndomain of radiology reports currently offer the optimal performances for this\nscenario. Notably, the SetFit and LLM models underperformed compared to\nBERT-like models, with LLM performing the worst. Importantly, none of the\nmodels investigated was sufficiently accurate to allow for text classification\nwithout any supervision. However, they show potential for data filtering, which\ncould reduce the amount of manual labeling required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) in the medical domain can underperform in\nreal-world applications involving small datasets in a non-English language with\nfew labeled samples and imbalanced classes. There is yet no consensus on how to\napproach this problem. We evaluated a set of NLP models including BERT-like\ntransformers, few-shot learning with sentence transformers (SetFit), and\nprompted large language models (LLM), using three datasets of radiology reports\non magnetic resonance images of epilepsy patients in Danish, a low-resource\nlanguage. Our results indicate that BERT-like models pretrained in the target\ndomain of radiology reports currently offer the optimal performances for this\nscenario. Notably, the SetFit and LLM models underperformed compared to\nBERT-like models, with LLM performing the worst. Importantly, none of the\nmodels investigated was sufficiently accurate to allow for text classification\nwithout any supervision. However, they show potential for data filtering, which\ncould reduce the amount of manual labeling required."
                },
                "authors": [
                    {
                        "name": "Vincent Beliveau"
                    },
                    {
                        "name": "Helene Kaas"
                    },
                    {
                        "name": "Martin Prener"
                    },
                    {
                        "name": "Claes N. Ladefoged"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Gitte M. Knudsen"
                    },
                    {
                        "name": "Lars H. Pinborg"
                    },
                    {
                        "name": "Melanie Ganz"
                    }
                ],
                "author_detail": {
                    "name": "Melanie Ganz"
                },
                "author": "Melanie Ganz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07736v3",
                "updated": "2024-09-30T09:49:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    49,
                    8,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-11T21:46:03Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    21,
                    46,
                    3,
                    1,
                    163,
                    0
                ],
                "title": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models"
                },
                "summary": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems."
                },
                "authors": [
                    {
                        "name": "Dojun Park"
                    },
                    {
                        "name": "Jiwoo Lee"
                    },
                    {
                        "name": "Seohyun Park"
                    },
                    {
                        "name": "Hyeyun Jeong"
                    },
                    {
                        "name": "Youngeun Koo"
                    },
                    {
                        "name": "Soonha Hwang"
                    },
                    {
                        "name": "Seonwoo Park"
                    },
                    {
                        "name": "Sungeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Lee"
                },
                "author": "Sungeun Lee",
                "arxiv_comment": "The 2nd GenBench workshop on generalisation (benchmarking) in NLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04808v2",
                "updated": "2024-09-30T09:41:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    41,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-06T10:55:30Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    10,
                    55,
                    30,
                    2,
                    66,
                    0
                ],
                "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off"
                },
                "summary": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax."
                },
                "authors": [
                    {
                        "name": "Eva Giboulot"
                    },
                    {
                        "name": "Furon Teddy"
                    }
                ],
                "author_detail": {
                    "name": "Furon Teddy"
                },
                "author": "Furon Teddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00855v3",
                "updated": "2024-09-30T09:37:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    37,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-01T18:09:40Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    9,
                    40,
                    3,
                    214,
                    0
                ],
                "title": "HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and\n  Style Generation in Fashion Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and\n  Style Generation in Fashion Design"
                },
                "summary": "The process of fashion design usually involves sketching, refining, and\ncoloring, with designers drawing inspiration from various images to fuel their\ncreative endeavors. However, conventional image search methods often yield\nirrelevant results, impeding the design process. Moreover, creating and\ncoloring sketches can be time-consuming and demanding, acting as a bottleneck\nin the design workflow. In this work, we introduce HAIGEN (Human-AI\nCollaboration for GENeration), an efficient fashion design system for Human-AI\ncollaboration developed to aid designers. Specifically, HAIGEN consists of four\nmodules. T2IM, located in the cloud, generates reference inspiration images\ndirectly from text prompts. With three other modules situated locally, the I2SM\nbatch generates the image material library into a certain designer-style sketch\nmaterial library. The SRM recommends similar sketches in the generated library\nto designers for further refinement, and the STM colors the refined sketch\naccording to the styles of inspiration images. Through our system, any designer\ncan perform local personalized fine-tuning and leverage the powerful generation\ncapabilities of large models in the cloud, streamlining the entire design\ndevelopment process. Given that our approach integrates both cloud and local\nmodel deployment schemes, it effectively safeguards design privacy by avoiding\nthe need to upload personalized data from local designers. We validated the\neffectiveness of each module through extensive qualitative and quantitative\nexperiments. User surveys also confirmed that HAIGEN offers significant\nadvantages in design efficiency, positioning it as a new generation of aid-tool\nfor designers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of fashion design usually involves sketching, refining, and\ncoloring, with designers drawing inspiration from various images to fuel their\ncreative endeavors. However, conventional image search methods often yield\nirrelevant results, impeding the design process. Moreover, creating and\ncoloring sketches can be time-consuming and demanding, acting as a bottleneck\nin the design workflow. In this work, we introduce HAIGEN (Human-AI\nCollaboration for GENeration), an efficient fashion design system for Human-AI\ncollaboration developed to aid designers. Specifically, HAIGEN consists of four\nmodules. T2IM, located in the cloud, generates reference inspiration images\ndirectly from text prompts. With three other modules situated locally, the I2SM\nbatch generates the image material library into a certain designer-style sketch\nmaterial library. The SRM recommends similar sketches in the generated library\nto designers for further refinement, and the STM colors the refined sketch\naccording to the styles of inspiration images. Through our system, any designer\ncan perform local personalized fine-tuning and leverage the powerful generation\ncapabilities of large models in the cloud, streamlining the entire design\ndevelopment process. Given that our approach integrates both cloud and local\nmodel deployment schemes, it effectively safeguards design privacy by avoiding\nthe need to upload personalized data from local designers. We validated the\neffectiveness of each module through extensive qualitative and quantitative\nexperiments. User surveys also confirmed that HAIGEN offers significant\nadvantages in design efficiency, positioning it as a new generation of aid-tool\nfor designers."
                },
                "authors": [
                    {
                        "name": "Jianan Jiang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Hanhui Deng"
                    },
                    {
                        "name": "Yidan Long"
                    },
                    {
                        "name": "Wenyi Tang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Can Liu"
                    },
                    {
                        "name": "Zhanpeng Jin"
                    },
                    {
                        "name": "Wenlei Zhang"
                    },
                    {
                        "name": "Tangquan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Tangquan Qi"
                },
                "author": "Tangquan Qi",
                "arxiv_doi": "10.1145/3678518",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678518",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.00855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (ACM IMWUT/UbiComp 2024)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v2",
                "updated": "2024-10-01T05:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    37,
                    7,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09299v3",
                "updated": "2024-09-30T09:16:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    16,
                    47,
                    0,
                    274,
                    0
                ],
                "published": "2024-02-14T16:41:35Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    16,
                    41,
                    35,
                    2,
                    45,
                    0
                ],
                "title": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code"
                },
                "summary": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets."
                },
                "authors": [
                    {
                        "name": "Vahid Majdinasab"
                    },
                    {
                        "name": "Amin Nikanjam"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "Accepted for publication in TOSEM (ACM Transactions on Software\n  Engineering and Methodology)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11030v2",
                "updated": "2024-09-30T09:03:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    3,
                    50,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    17,
                    59,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese\n  Food Culture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese\n  Food Culture"
                },
                "summary": "Food is a rich and varied dimension of cultural heritage, crucial to both\nindividuals and social groups. To bridge the gap in the literature on the\noften-overlooked regional diversity in this domain, we introduce FoodieQA, a\nmanually curated, fine-grained image-text dataset capturing the intricate\nfeatures of food cultures across various regions in China. We evaluate\nvision-language Models (VLMs) and large language models (LLMs) on newly\ncollected, unseen food images and corresponding questions. FoodieQA comprises\nthree multiple-choice question-answering tasks where models need to answer\nquestions based on multiple images, a single image, and text-only descriptions,\nrespectively. While LLMs excel at text-based question answering, surpassing\nhuman accuracy, the open-sourced VLMs still fall short by 41% on multi-image\nand 21% on single-image VQA tasks, although closed-weights models perform\ncloser to human levels (within 10%). Our findings highlight that understanding\nfood and its cultural implications remains a challenging and under-explored\ndirection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Food is a rich and varied dimension of cultural heritage, crucial to both\nindividuals and social groups. To bridge the gap in the literature on the\noften-overlooked regional diversity in this domain, we introduce FoodieQA, a\nmanually curated, fine-grained image-text dataset capturing the intricate\nfeatures of food cultures across various regions in China. We evaluate\nvision-language Models (VLMs) and large language models (LLMs) on newly\ncollected, unseen food images and corresponding questions. FoodieQA comprises\nthree multiple-choice question-answering tasks where models need to answer\nquestions based on multiple images, a single image, and text-only descriptions,\nrespectively. While LLMs excel at text-based question answering, surpassing\nhuman accuracy, the open-sourced VLMs still fall short by 41% on multi-image\nand 21% on single-image VQA tasks, although closed-weights models perform\ncloser to human levels (within 10%). Our findings highlight that understanding\nfood and its cultural implications remains a challenging and under-explored\ndirection."
                },
                "authors": [
                    {
                        "name": "Wenyan Li"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Jiaang Li"
                    },
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Raphael Tang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Anders Søgaard"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    },
                    {
                        "name": "Desmond Elliott"
                    }
                ],
                "author_detail": {
                    "name": "Desmond Elliott"
                },
                "author": "Desmond Elliott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03937v2",
                "updated": "2024-09-30T08:54:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    54,
                    45,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-04T13:52:23Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    13,
                    52,
                    23,
                    3,
                    186,
                    0
                ],
                "title": "TongGu: Mastering Classical Chinese Understanding with\n  Knowledge-Grounded Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TongGu: Mastering Classical Chinese Understanding with\n  Knowledge-Grounded Large Language Models"
                },
                "summary": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient\nChina, yet its complexities pose formidable comprehension barriers for most\nmodern people without specialized knowledge. While Large Language Models (LLMs)\nhave shown remarkable capabilities in Natural Language Processing (NLP), they\nstruggle with Classical Chinese Understanding (CCU), especially in\ndata-demanding and knowledge-intensive tasks. In response to this dilemma, we\npropose \\textbf{TongGu} (mean understanding ancient and modern), the first\nCCU-specific LLM, underpinned by three core contributions. First, we construct\na two-stage instruction-tuning dataset ACCN-INS derived from rich classical\nChinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we\npropose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,\nenabling TongGu to acquire new capabilities while preserving its foundational\nknowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)\ntechnique to reduce hallucinations based on knowledge-grounding. Extensive\nexperiments across 24 diverse CCU tasks validate TongGu's superior ability,\nunderscoring the effectiveness of RAT and CCU-RAG. The model and dataset are\navailable at \\url{https://github.com/SCUT-DLVCLab/TongGu-LLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient\nChina, yet its complexities pose formidable comprehension barriers for most\nmodern people without specialized knowledge. While Large Language Models (LLMs)\nhave shown remarkable capabilities in Natural Language Processing (NLP), they\nstruggle with Classical Chinese Understanding (CCU), especially in\ndata-demanding and knowledge-intensive tasks. In response to this dilemma, we\npropose \\textbf{TongGu} (mean understanding ancient and modern), the first\nCCU-specific LLM, underpinned by three core contributions. First, we construct\na two-stage instruction-tuning dataset ACCN-INS derived from rich classical\nChinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we\npropose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,\nenabling TongGu to acquire new capabilities while preserving its foundational\nknowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)\ntechnique to reduce hallucinations based on knowledge-grounding. Extensive\nexperiments across 24 diverse CCU tasks validate TongGu's superior ability,\nunderscoring the effectiveness of RAT and CCU-RAG. The model and dataset are\navailable at \\url{https://github.com/SCUT-DLVCLab/TongGu-LLM}."
                },
                "authors": [
                    {
                        "name": "Jiahuan Cao"
                    },
                    {
                        "name": "Dezhi Peng"
                    },
                    {
                        "name": "Peirong Zhang"
                    },
                    {
                        "name": "Yongxin Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Kai Ding"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20094v1",
                "updated": "2024-09-30T08:47:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    47,
                    17,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:47:17Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    47,
                    17,
                    0,
                    274,
                    0
                ],
                "title": "Aggressive Post-Training Compression on Extremely Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggressive Post-Training Compression on Extremely Large Language Models"
                },
                "summary": "The increasing size and complexity of Large Language Models (LLMs) pose\nchallenges for their deployment on personal computers and mobile devices.\nAggressive post-training model compression is necessary to reduce the models'\nsize, but it often results in significant accuracy loss. To address this\nchallenge, we propose a novel network pruning technology that utilizes over 0.7\nsparsity and less than 8 bits of quantization. Our approach enables the\ncompression of prevailing LLMs within a couple of hours while maintaining a\nrelatively small accuracy loss. In experimental evaluations, our method\ndemonstrates effectiveness and potential for practical deployment. By making\nLLMs available on domestic devices, our work can facilitate a new era of\nnatural language processing applications with wide-ranging impacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size and complexity of Large Language Models (LLMs) pose\nchallenges for their deployment on personal computers and mobile devices.\nAggressive post-training model compression is necessary to reduce the models'\nsize, but it often results in significant accuracy loss. To address this\nchallenge, we propose a novel network pruning technology that utilizes over 0.7\nsparsity and less than 8 bits of quantization. Our approach enables the\ncompression of prevailing LLMs within a couple of hours while maintaining a\nrelatively small accuracy loss. In experimental evaluations, our method\ndemonstrates effectiveness and potential for practical deployment. By making\nLLMs available on domestic devices, our work can facilitate a new era of\nnatural language processing applications with wide-ranging impacts."
                },
                "authors": [
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Yao Chen"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Zhenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenjie Zhang"
                },
                "author": "Zhenjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04235v2",
                "updated": "2024-09-30T08:42:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    42,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-05-07T11:54:22Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    11,
                    54,
                    22,
                    1,
                    128,
                    0
                ],
                "title": "LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe\n  Diffusion-based Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe\n  Diffusion-based Planning"
                },
                "summary": "Operating effectively in complex environments while complying with specified\nconstraints is crucial for the safe and successful deployment of robots that\ninteract with and operate around people. In this work, we focus on generating\nlong-horizon trajectories that adhere to novel static and temporally-extended\nconstraints/instructions at test time. We propose a data-driven diffusion-based\nframework, LTLDoG, that modifies the inference steps of the reverse process\ngiven an instruction specified using finite linear temporal logic\n($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on\n$\\text{LTL}_f$ and guides the sampling steps using its gradient field. This\nvalue function can also be trained to generalize to new instructions not\nobserved during training, enabling flexible test-time adaptability. Experiments\nin robot navigation and manipulation illustrate that the method is able to\ngenerate trajectories that satisfy formulae that specify obstacle avoidance and\nvisitation sequences. Code and supplementary material are available online at\nhttps://github.com/clear-nus/ltldog.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating effectively in complex environments while complying with specified\nconstraints is crucial for the safe and successful deployment of robots that\ninteract with and operate around people. In this work, we focus on generating\nlong-horizon trajectories that adhere to novel static and temporally-extended\nconstraints/instructions at test time. We propose a data-driven diffusion-based\nframework, LTLDoG, that modifies the inference steps of the reverse process\ngiven an instruction specified using finite linear temporal logic\n($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on\n$\\text{LTL}_f$ and guides the sampling steps using its gradient field. This\nvalue function can also be trained to generalize to new instructions not\nobserved during training, enabling flexible test-time adaptability. Experiments\nin robot navigation and manipulation illustrate that the method is able to\ngenerate trajectories that satisfy formulae that specify obstacle avoidance and\nvisitation sequences. Code and supplementary material are available online at\nhttps://github.com/clear-nus/ltldog."
                },
                "authors": [
                    {
                        "name": "Zeyu Feng"
                    },
                    {
                        "name": "Hao Luan"
                    },
                    {
                        "name": "Pranav Goyal"
                    },
                    {
                        "name": "Harold Soh"
                    }
                ],
                "author_detail": {
                    "name": "Harold Soh"
                },
                "author": "Harold Soh",
                "arxiv_doi": "10.1109/LRA.2024.3443501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3443501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "in IEEE Robotics and Automation Letters, vol. 9, no. 10, pp.\n  8571-8578, Oct. 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20089v1",
                "updated": "2024-09-30T08:41:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    41,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:41:39Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    41,
                    39,
                    0,
                    274,
                    0
                ],
                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM safeguarding via refusal feature adversarial training"
                },
                "summary": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods."
                },
                "authors": [
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Virginie Do"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Nicola Cancedda"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Cancedda"
                },
                "author": "Nicola Cancedda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20060v1",
                "updated": "2024-09-30T08:03:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    3,
                    19,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:03:19Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    3,
                    19,
                    0,
                    274,
                    0
                ],
                "title": "Lightweight Neural Architecture Search for Cerebral Palsy Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Neural Architecture Search for Cerebral Palsy Detection"
                },
                "summary": "The neurological condition known as cerebral palsy (CP) first manifests in\ninfancy or early childhood and has a lifelong impact on motor coordination and\nbody movement. CP is one of the leading causes of childhood disabilities, and\nearly detection is crucial for providing appropriate treatment. However, such\ndetection relies on assessments by human experts trained in methods like\ngeneral movement assessment (GMA). These are not widely accessible, especially\nin developing countries. Conventional machine learning approaches offer limited\npredictive performance on CP detection tasks, and the approaches developed by\nthe few available domain experts are generally dataset-specific, restricting\ntheir applicability beyond the context for which these were created. To address\nthese challenges, we propose a neural architecture search (NAS) algorithm\napplying a reinforcement learning update scheme capable of efficiently\noptimizing for the best architectural and hyperparameter combination to\ndiscover the most suitable neural network configuration for detecting CP. Our\nmethod performs better on a real-world CP dataset than other approaches in the\nfield, which rely on large ensembles. As our approach is less\nresource-demanding and performs better, it is particularly suitable for\nimplementation in resource-constrained settings, including rural or developing\nareas with limited access to medical experts and the required diagnostic tools.\nThe resulting model's lightweight architecture and efficient computation time\nallow for deployment on devices with limited processing power, reducing the\nneed for expensive infrastructure, and can, therefore, be integrated into\nclinical workflows to provide timely and accurate support for early CP\ndiagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neurological condition known as cerebral palsy (CP) first manifests in\ninfancy or early childhood and has a lifelong impact on motor coordination and\nbody movement. CP is one of the leading causes of childhood disabilities, and\nearly detection is crucial for providing appropriate treatment. However, such\ndetection relies on assessments by human experts trained in methods like\ngeneral movement assessment (GMA). These are not widely accessible, especially\nin developing countries. Conventional machine learning approaches offer limited\npredictive performance on CP detection tasks, and the approaches developed by\nthe few available domain experts are generally dataset-specific, restricting\ntheir applicability beyond the context for which these were created. To address\nthese challenges, we propose a neural architecture search (NAS) algorithm\napplying a reinforcement learning update scheme capable of efficiently\noptimizing for the best architectural and hyperparameter combination to\ndiscover the most suitable neural network configuration for detecting CP. Our\nmethod performs better on a real-world CP dataset than other approaches in the\nfield, which rely on large ensembles. As our approach is less\nresource-demanding and performs better, it is particularly suitable for\nimplementation in resource-constrained settings, including rural or developing\nareas with limited access to medical experts and the required diagnostic tools.\nThe resulting model's lightweight architecture and efficient computation time\nallow for deployment on devices with limited processing power, reducing the\nneed for expensive infrastructure, and can, therefore, be integrated into\nclinical workflows to provide timely and accurate support for early CP\ndiagnosis."
                },
                "authors": [
                    {
                        "name": "Felix Tempel"
                    },
                    {
                        "name": "Espen Alexander F. Ihlen"
                    },
                    {
                        "name": "Inga Strümke"
                    }
                ],
                "author_detail": {
                    "name": "Inga Strümke"
                },
                "author": "Inga Strümke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20059v1",
                "updated": "2024-09-30T08:01:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    1,
                    44,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T08:01:44Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    1,
                    44,
                    0,
                    274,
                    0
                ],
                "title": "Is Preference Alignment Always the Best Option to Enhance LLM-Based\n  Translation? An Empirical Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Preference Alignment Always the Best Option to Enhance LLM-Based\n  Translation? An Empirical Analysis"
                },
                "summary": "Neural metrics for machine translation (MT) evaluation have become\nincreasingly prominent due to their superior correlation with human judgments\ncompared to traditional lexical metrics. Researchers have therefore utilized\nneural metrics through quality-informed decoding strategies, achieving better\nresults than likelihood-based methods. With the rise of Large Language Models\n(LLMs), preference-based alignment techniques have gained attention for their\npotential to enhance translation quality by optimizing model weights directly\non preferences induced by quality estimators. This study focuses on Contrastive\nPreference Optimization (CPO) and conducts extensive experiments to evaluate\nthe impact of preference-based alignment on translation quality. Our findings\nindicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT)\non high-quality data with regard to the alignment metric, it may lead to\ninstability across downstream evaluation metrics, particularly between neural\nand lexical ones. Additionally, we demonstrate that relying solely on the base\nmodel for generating candidate translations achieves performance comparable to\nusing multiple external systems, while ensuring better consistency across\ndownstream metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural metrics for machine translation (MT) evaluation have become\nincreasingly prominent due to their superior correlation with human judgments\ncompared to traditional lexical metrics. Researchers have therefore utilized\nneural metrics through quality-informed decoding strategies, achieving better\nresults than likelihood-based methods. With the rise of Large Language Models\n(LLMs), preference-based alignment techniques have gained attention for their\npotential to enhance translation quality by optimizing model weights directly\non preferences induced by quality estimators. This study focuses on Contrastive\nPreference Optimization (CPO) and conducts extensive experiments to evaluate\nthe impact of preference-based alignment on translation quality. Our findings\nindicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT)\non high-quality data with regard to the alignment metric, it may lead to\ninstability across downstream evaluation metrics, particularly between neural\nand lexical ones. Additionally, we demonstrate that relying solely on the base\nmodel for generating candidate translations achieves performance comparable to\nusing multiple external systems, while ensuring better consistency across\ndownstream metrics."
                },
                "authors": [
                    {
                        "name": "Hippolyte Gisserot-Boukhlef"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Emmanuel Malherbe"
                    },
                    {
                        "name": "Céline Hudelot"
                    },
                    {
                        "name": "Pierre Colombo"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    }
                ],
                "author_detail": {
                    "name": "Nuno M. Guerreiro"
                },
                "author": "Nuno M. Guerreiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20053v1",
                "updated": "2024-09-30T07:59:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    59,
                    10,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:59:10Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    59,
                    10,
                    0,
                    274,
                    0
                ],
                "title": "GUNDAM: Aligning Large Language Models with Graph Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUNDAM: Aligning Large Language Models with Graph Understanding"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive results in processing\ntext data, which has sparked interest in applying these models beyond textual\ndata, such as graphs. In the field of graph learning, there is a growing\ninterest in harnessing LLMs to comprehend and manipulate graph-structured data.\nExisting research predominantly focuses on graphs with rich textual features,\nsuch as knowledge graphs or text attribute graphs, leveraging LLMs' ability to\nprocess text but inadequately addressing graph structure. This work\nspecifically aims to assess and enhance LLMs' abilities to comprehend and\nutilize the structural knowledge inherent in graph data itself, rather than\nfocusing solely on graphs rich in textual content. To achieve this, we\nintroduce the \\textbf{G}raph \\textbf{U}nderstanding for \\textbf{N}atural\nLanguage \\textbf{D}riven \\textbf{A}nalytical \\textbf{M}odel (\\model). This\nmodel adapts LLMs to better understand and engage with the structure of graph\ndata, enabling them to perform complex reasoning tasks by leveraging the\ngraph's structure itself. Our experimental evaluations on graph reasoning\nbenchmarks not only substantiate that \\model~ outperforms the SOTA baselines\nfor comparisons. But also reveals key factors affecting the graph reasoning\ncapabilities of LLMs. Moreover, we provide a theoretical analysis illustrating\nhow reasoning paths can enhance LLMs' reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results in processing\ntext data, which has sparked interest in applying these models beyond textual\ndata, such as graphs. In the field of graph learning, there is a growing\ninterest in harnessing LLMs to comprehend and manipulate graph-structured data.\nExisting research predominantly focuses on graphs with rich textual features,\nsuch as knowledge graphs or text attribute graphs, leveraging LLMs' ability to\nprocess text but inadequately addressing graph structure. This work\nspecifically aims to assess and enhance LLMs' abilities to comprehend and\nutilize the structural knowledge inherent in graph data itself, rather than\nfocusing solely on graphs rich in textual content. To achieve this, we\nintroduce the \\textbf{G}raph \\textbf{U}nderstanding for \\textbf{N}atural\nLanguage \\textbf{D}riven \\textbf{A}nalytical \\textbf{M}odel (\\model). This\nmodel adapts LLMs to better understand and engage with the structure of graph\ndata, enabling them to perform complex reasoning tasks by leveraging the\ngraph's structure itself. Our experimental evaluations on graph reasoning\nbenchmarks not only substantiate that \\model~ outperforms the SOTA baselines\nfor comparisons. But also reveals key factors affecting the graph reasoning\ncapabilities of LLMs. Moreover, we provide a theoretical analysis illustrating\nhow reasoning paths can enhance LLMs' reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Sheng Ouyang"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Ge Chen"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20052v1",
                "updated": "2024-09-30T07:57:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    57,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:57:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    57,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems"
                },
                "summary": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models."
                },
                "authors": [
                    {
                        "name": "Guixian Zhang"
                    },
                    {
                        "name": "Guan Yuan"
                    },
                    {
                        "name": "Debo Cheng"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Jiuyong Li"
                    },
                    {
                        "name": "Shichao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichao Zhang"
                },
                "author": "Shichao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20042v1",
                "updated": "2024-09-30T07:48:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    48,
                    55,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:48:55Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    48,
                    55,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Scores: A Modular RAG-Based System for Automatic Short Answer\n  Scoring with Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Scores: A Modular RAG-Based System for Automatic Short Answer\n  Scoring with Feedback"
                },
                "summary": "Automatic short answer scoring (ASAS) helps reduce the grading burden on\neducators but often lacks detailed, explainable feedback. Existing methods in\nASAS with feedback (ASAS-F) rely on fine-tuning language models with limited\ndatasets, which is resource-intensive and struggles to generalize across\ncontexts. Recent approaches using large language models (LLMs) have focused on\nscoring without extensive fine-tuning. However, they often rely heavily on\nprompt engineering and either fail to generate elaborated feedback or do not\nadequately evaluate it. In this paper, we propose a modular retrieval augmented\ngeneration based ASAS-F system that scores answers and generates feedback in\nstrict zero-shot and few-shot learning scenarios. We design our system to be\nadaptable to various educational tasks without extensive prompt engineering\nusing an automatic prompt generation framework. Results show an improvement in\nscoring accuracy by 9\\% on unseen questions compared to fine-tuning, offering a\nscalable and cost-effective solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic short answer scoring (ASAS) helps reduce the grading burden on\neducators but often lacks detailed, explainable feedback. Existing methods in\nASAS with feedback (ASAS-F) rely on fine-tuning language models with limited\ndatasets, which is resource-intensive and struggles to generalize across\ncontexts. Recent approaches using large language models (LLMs) have focused on\nscoring without extensive fine-tuning. However, they often rely heavily on\nprompt engineering and either fail to generate elaborated feedback or do not\nadequately evaluate it. In this paper, we propose a modular retrieval augmented\ngeneration based ASAS-F system that scores answers and generates feedback in\nstrict zero-shot and few-shot learning scenarios. We design our system to be\nadaptable to various educational tasks without extensive prompt engineering\nusing an automatic prompt generation framework. Results show an improvement in\nscoring accuracy by 9\\% on unseen questions compared to fine-tuning, offering a\nscalable and cost-effective solution."
                },
                "authors": [
                    {
                        "name": "Menna Fateen"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Tsunenori Mine"
                    }
                ],
                "author_detail": {
                    "name": "Tsunenori Mine"
                },
                "author": "Tsunenori Mine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20018v1",
                "updated": "2024-09-30T07:25:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    25,
                    16,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:25:16Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    25,
                    16,
                    0,
                    274,
                    0
                ],
                "title": "Visual Context Window Extension: A New Perspective for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Context Window Extension: A New Perspective for Long Video\n  Understanding"
                },
                "summary": "Large Multimodal Models (LMMs) have demonstrated impressive performance in\nshort video understanding tasks but face great challenges when applied to long\nvideo understanding. In contrast, Large Language Models (LLMs) exhibit\noutstanding capabilities in modeling long texts. Existing work attempts to\naddress this issue by introducing long video-text pairs during training.\nHowever, these approaches require substantial computational and data resources.\nIn this paper, we tackle the challenge of long video understanding from the\nperspective of context windows, aiming to apply LMMs to long video tasks\nwithout retraining on long video datasets. We first conduct an in-depth\nanalysis of why pretrained LMMs struggle to understand lengthy video content,\nidentifying that discrepancies between visual and language modalities lead to\ndifferent context windows for visual and language tokens, making it difficult\nto directly extend the visual tokens to match the language context window.\nBased on this, we propose to adapt LMMs for long video understanding tasks by\nextending the visual context window, eliminating the need for retraining on\nlarge scalelong video datasets. To further mitigate the significant memory\nconsumption caused by long sequences, we introduce a progressive pooling\ninference strategy that selectively adjusts the spatial resolution of frame\nembeddings, reducing the number of visual tokens while retaining important\nspatial information. Across multiple long video understanding benchmarks, our\nmethod consistently improves the performance as the number of video frames\nincreases. On the MLVU benchmark, our method outperforms GPT-4o, even though\nour model size is only 7B. Additionally, in the 256-frame setting, our method\nreduces memory usage by approximately 45% compared to the baseline, without\nintroducing any performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have demonstrated impressive performance in\nshort video understanding tasks but face great challenges when applied to long\nvideo understanding. In contrast, Large Language Models (LLMs) exhibit\noutstanding capabilities in modeling long texts. Existing work attempts to\naddress this issue by introducing long video-text pairs during training.\nHowever, these approaches require substantial computational and data resources.\nIn this paper, we tackle the challenge of long video understanding from the\nperspective of context windows, aiming to apply LMMs to long video tasks\nwithout retraining on long video datasets. We first conduct an in-depth\nanalysis of why pretrained LMMs struggle to understand lengthy video content,\nidentifying that discrepancies between visual and language modalities lead to\ndifferent context windows for visual and language tokens, making it difficult\nto directly extend the visual tokens to match the language context window.\nBased on this, we propose to adapt LMMs for long video understanding tasks by\nextending the visual context window, eliminating the need for retraining on\nlarge scalelong video datasets. To further mitigate the significant memory\nconsumption caused by long sequences, we introduce a progressive pooling\ninference strategy that selectively adjusts the spatial resolution of frame\nembeddings, reducing the number of visual tokens while retaining important\nspatial information. Across multiple long video understanding benchmarks, our\nmethod consistently improves the performance as the number of video frames\nincreases. On the MLVU benchmark, our method outperforms GPT-4o, even though\nour model size is only 7B. Additionally, in the 256-frame setting, our method\nreduces memory usage by approximately 45% compared to the baseline, without\nintroducing any performance loss."
                },
                "authors": [
                    {
                        "name": "Hongchen Wei"
                    },
                    {
                        "name": "Zhenzhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Chen"
                },
                "author": "Zhenzhong Chen",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03882v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03882v3",
                "updated": "2024-09-30T07:01:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    1,
                    57,
                    0,
                    274,
                    0
                ],
                "published": "2024-05-06T21:57:35Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    21,
                    57,
                    35,
                    0,
                    127,
                    0
                ],
                "title": "Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free\n  Efficient Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free\n  Efficient Vision Transformer"
                },
                "summary": "Motivated by the huge success of Transformers in the field of natural\nlanguage processing (NLP), Vision Transformers (ViTs) have been rapidly\ndeveloped and achieved remarkable performance in various computer vision tasks.\nHowever, their huge model sizes and intensive computations hinder ViTs'\ndeployment on embedded devices, calling for effective model compression\nmethods, such as quantization. Unfortunately, due to the existence of\nhardware-unfriendly and quantization-sensitive non-linear operations,\nparticularly {Softmax}, it is non-trivial to completely quantize all operations\nin ViTs, yielding either significant accuracy drops or non-negligible hardware\ncosts. In response to challenges associated with \\textit{standard ViTs}, we\nfocus our attention towards the quantization and acceleration for\n\\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but\nalso integrate linear attention with low computational complexity, and propose\nTrio-ViT accordingly. Specifically, at the algorithm level, we develop a\n{tailored post-training quantization engine} taking the unique activation\ndistributions of Softmax-free efficient ViTs into full consideration, aiming to\nboost quantization accuracy. Furthermore, at the hardware level, we build an\naccelerator dedicated to the specific Convolution-Transformer hybrid\narchitecture of efficient ViTs, thereby enhancing hardware efficiency.\nExtensive experimental results consistently prove the effectiveness of our\nTrio-ViT framework. {Particularly, we can gain up to\n$\\uparrow$$\\mathbf{3.6}\\times$, $\\uparrow$$\\mathbf{5.0}\\times$, and\n$\\uparrow$$\\mathbf{7.3}\\times$ FPS under comparable accuracy over\nstate-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{6.0}\\times$,\n$\\uparrow$$\\mathbf{1.5}\\times$, and $\\uparrow$$\\mathbf{2.1}\\times$ DSP\nefficiency.} Codes are available at\n\\url{https://github.com/shihuihong214/Trio-ViT}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the huge success of Transformers in the field of natural\nlanguage processing (NLP), Vision Transformers (ViTs) have been rapidly\ndeveloped and achieved remarkable performance in various computer vision tasks.\nHowever, their huge model sizes and intensive computations hinder ViTs'\ndeployment on embedded devices, calling for effective model compression\nmethods, such as quantization. Unfortunately, due to the existence of\nhardware-unfriendly and quantization-sensitive non-linear operations,\nparticularly {Softmax}, it is non-trivial to completely quantize all operations\nin ViTs, yielding either significant accuracy drops or non-negligible hardware\ncosts. In response to challenges associated with \\textit{standard ViTs}, we\nfocus our attention towards the quantization and acceleration for\n\\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but\nalso integrate linear attention with low computational complexity, and propose\nTrio-ViT accordingly. Specifically, at the algorithm level, we develop a\n{tailored post-training quantization engine} taking the unique activation\ndistributions of Softmax-free efficient ViTs into full consideration, aiming to\nboost quantization accuracy. Furthermore, at the hardware level, we build an\naccelerator dedicated to the specific Convolution-Transformer hybrid\narchitecture of efficient ViTs, thereby enhancing hardware efficiency.\nExtensive experimental results consistently prove the effectiveness of our\nTrio-ViT framework. {Particularly, we can gain up to\n$\\uparrow$$\\mathbf{3.6}\\times$, $\\uparrow$$\\mathbf{5.0}\\times$, and\n$\\uparrow$$\\mathbf{7.3}\\times$ FPS under comparable accuracy over\nstate-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{6.0}\\times$,\n$\\uparrow$$\\mathbf{1.5}\\times$, and $\\uparrow$$\\mathbf{2.1}\\times$ DSP\nefficiency.} Codes are available at\n\\url{https://github.com/shihuihong214/Trio-ViT}."
                },
                "authors": [
                    {
                        "name": "Huihong Shi"
                    },
                    {
                        "name": "Haikuo Shao"
                    },
                    {
                        "name": "Wendong Mao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03882v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03882v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20007v1",
                "updated": "2024-09-30T07:01:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    1,
                    21,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T07:01:21Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    1,
                    21,
                    0,
                    274,
                    0
                ],
                "title": "Developing Instruction-Following Speech Language Model Without Speech\n  Instruction-Tuning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Instruction-Following Speech Language Model Without Speech\n  Instruction-Tuning Data"
                },
                "summary": "Recent end-to-end speech language models (SLMs) have expanded upon the\ncapabilities of large language models (LLMs) by incorporating pre-trained\nspeech models. However, these SLMs often undergo extensive speech\ninstruction-tuning to bridge the gap between speech and text modalities. This\nrequires significant annotation efforts and risks catastrophic forgetting of\nthe original language capabilities. In this work, we present a simple yet\neffective automatic process for creating speech-text pair data that carefully\ninjects speech paralinguistic understanding abilities into SLMs while\npreserving the inherent language capabilities of the text-based LLM. Our model\ndemonstrates general capabilities for speech-related tasks without the need for\nspeech instruction-tuning data, achieving impressive performance on\nDynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our model exhibits\nthe ability to follow complex instructions derived from LLMs, such as specific\noutput formatting and chain-of-thought reasoning. Our approach not only\nenhances the versatility and effectiveness of SLMs but also reduces reliance on\nextensive annotated datasets, paving the way for more efficient and capable\nspeech understanding systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent end-to-end speech language models (SLMs) have expanded upon the\ncapabilities of large language models (LLMs) by incorporating pre-trained\nspeech models. However, these SLMs often undergo extensive speech\ninstruction-tuning to bridge the gap between speech and text modalities. This\nrequires significant annotation efforts and risks catastrophic forgetting of\nthe original language capabilities. In this work, we present a simple yet\neffective automatic process for creating speech-text pair data that carefully\ninjects speech paralinguistic understanding abilities into SLMs while\npreserving the inherent language capabilities of the text-based LLM. Our model\ndemonstrates general capabilities for speech-related tasks without the need for\nspeech instruction-tuning data, achieving impressive performance on\nDynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our model exhibits\nthe ability to follow complex instructions derived from LLMs, such as specific\noutput formatting and chain-of-thought reasoning. Our approach not only\nenhances the versatility and effectiveness of SLMs but also reduces reliance on\nextensive annotated datasets, paving the way for more efficient and capable\nspeech understanding systems."
                },
                "authors": [
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19998v1",
                "updated": "2024-09-30T06:50:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    50,
                    18,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:50:18Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    50,
                    18,
                    0,
                    274,
                    0
                ],
                "title": "Do Influence Functions Work on Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Influence Functions Work on Large Language Models?"
                },
                "summary": "Influence functions aim to quantify the impact of individual training data\npoints on a model's predictions. While extensive research has been conducted on\ninfluence functions in traditional machine learning models, their application\nto large language models (LLMs) has been limited. In this work, we conduct a\nsystematic study to address a key question: do influence functions work on\nLLMs? Specifically, we evaluate influence functions across multiple tasks and\nfind that they consistently perform poorly in most settings. Our further\ninvestigation reveals that their poor performance can be attributed to: (1)\ninevitable approximation errors when estimating the iHVP component due to the\nscale of LLMs, (2) uncertain convergence during fine-tuning, and, more\nfundamentally, (3) the definition itself, as changes in model parameters do not\nnecessarily correlate with changes in LLM behavior. Our study thus suggests the\nneed for alternative approaches for identifying influential samples. To support\nfuture work, our code is made available at\nhttps://github.com/plumprc/Failures-of-Influence-Functions-in-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence functions aim to quantify the impact of individual training data\npoints on a model's predictions. While extensive research has been conducted on\ninfluence functions in traditional machine learning models, their application\nto large language models (LLMs) has been limited. In this work, we conduct a\nsystematic study to address a key question: do influence functions work on\nLLMs? Specifically, we evaluate influence functions across multiple tasks and\nfind that they consistently perform poorly in most settings. Our further\ninvestigation reveals that their poor performance can be attributed to: (1)\ninevitable approximation errors when estimating the iHVP component due to the\nscale of LLMs, (2) uncertain convergence during fine-tuning, and, more\nfundamentally, (3) the definition itself, as changes in model parameters do not\nnecessarily correlate with changes in LLM behavior. Our study thus suggests the\nneed for alternative approaches for identifying influential samples. To support\nfuture work, our code is made available at\nhttps://github.com/plumprc/Failures-of-Influence-Functions-in-LLMs."
                },
                "authors": [
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19280v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19280v4",
                "updated": "2024-09-30T06:45:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    45,
                    16,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-27T15:50:41Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    50,
                    41,
                    3,
                    179,
                    0
                ],
                "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale"
                },
                "summary": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs."
                },
                "authors": [
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Chi Gui"
                    },
                    {
                        "name": "Ruyi Ouyang"
                    },
                    {
                        "name": "Anningzhe Gao"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Guiming Hardy Chen"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Ruifei Zhang"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19280v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19280v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16383v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16383v3",
                "updated": "2024-09-30T06:43:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    43,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-24T18:35:09Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    18,
                    35,
                    9,
                    1,
                    268,
                    0
                ],
                "title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation"
                },
                "summary": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings."
                },
                "authors": [
                    {
                        "name": "Ioannis Panagiotopoulos"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16383v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16383v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11263v2",
                "updated": "2024-09-30T06:37:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    37,
                    3,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-17T07:08:29Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    7,
                    8,
                    29,
                    0,
                    169,
                    0
                ],
                "title": "Understanding the Collapse of LLMs in Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Collapse of LLMs in Model Editing"
                },
                "summary": "Despite significant progress in model editing methods, their application in\nreal-world scenarios remains challenging as they often cause large language\nmodels (LLMs) to collapse. Among them, ROME is particularly concerning, as it\ncould disrupt LLMs with only a single edit. In this paper, we study the root\ncauses of such collapse. Through extensive analysis, we identify two primary\nfactors that contribute to the collapse: i) inconsistent handling of prefixed\nand unprefixed keys in the parameter update equation may result in very small\ndenominators, causing excessively large parameter updates; ii) the subject of\ncollapse cases is usually the first token, whose unprefixed key distribution\nsignificantly differs from the prefixed key distribution in autoregressive\ntransformers, causing the aforementioned issue to materialize. To validate our\nfindings, we propose a simple yet effective approach: uniformly using prefixed\nkeys during editing phase and adding prefixes during testing phase to ensure\nthe consistency between training and testing. The experimental results show\nthat the proposed solution can prevent model collapse while maintaining the\neffectiveness of the edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in model editing methods, their application in\nreal-world scenarios remains challenging as they often cause large language\nmodels (LLMs) to collapse. Among them, ROME is particularly concerning, as it\ncould disrupt LLMs with only a single edit. In this paper, we study the root\ncauses of such collapse. Through extensive analysis, we identify two primary\nfactors that contribute to the collapse: i) inconsistent handling of prefixed\nand unprefixed keys in the parameter update equation may result in very small\ndenominators, causing excessively large parameter updates; ii) the subject of\ncollapse cases is usually the first token, whose unprefixed key distribution\nsignificantly differs from the prefixed key distribution in autoregressive\ntransformers, causing the aforementioned issue to materialize. To validate our\nfindings, we propose a simple yet effective approach: uniformly using prefixed\nkeys during editing phase and adding prefixes during testing phase to ensure\nthe consistency between training and testing. The experimental results show\nthat the proposed solution can prevent model collapse while maintaining the\neffectiveness of the edits."
                },
                "authors": [
                    {
                        "name": "Wanli Yang"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Jiajun Tan"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Du Su"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Huawei Shen"
                    }
                ],
                "author_detail": {
                    "name": "Huawei Shen"
                },
                "author": "Huawei Shen",
                "arxiv_comment": "Accepted at Findings of EMNLP 2024 (Camera-Ready Version)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19993v1",
                "updated": "2024-09-30T06:31:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    31,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:31:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    31,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Mitigating Backdoor Threats to Large Language Models: Advancement and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Backdoor Threats to Large Language Models: Advancement and\n  Challenges"
                },
                "summary": "The advancement of Large Language Models (LLMs) has significantly impacted\nvarious domains, including Web search, healthcare, and software development.\nHowever, as these models scale, they become more vulnerable to cybersecurity\nrisks, particularly backdoor attacks. By exploiting the potent memorization\ncapacity of LLMs, adversaries can easily inject backdoors into LLMs by\nmanipulating a small portion of training data, leading to malicious behaviors\nin downstream applications whenever the hidden backdoor is activated by the\npre-defined triggers. Moreover, emerging learning paradigms like instruction\ntuning and reinforcement learning from human feedback (RLHF) exacerbate these\nrisks as they rely heavily on crowdsourced data and human feedback, which are\nnot fully controlled. In this paper, we present a comprehensive survey of\nemerging backdoor threats to LLMs that appear during LLM development or\ninference, and cover recent advancement in both defense and detection\nstrategies for mitigating backdoor threats to LLMs. We also outline key\nchallenges in addressing these threats, highlighting areas for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Large Language Models (LLMs) has significantly impacted\nvarious domains, including Web search, healthcare, and software development.\nHowever, as these models scale, they become more vulnerable to cybersecurity\nrisks, particularly backdoor attacks. By exploiting the potent memorization\ncapacity of LLMs, adversaries can easily inject backdoors into LLMs by\nmanipulating a small portion of training data, leading to malicious behaviors\nin downstream applications whenever the hidden backdoor is activated by the\npre-defined triggers. Moreover, emerging learning paradigms like instruction\ntuning and reinforcement learning from human feedback (RLHF) exacerbate these\nrisks as they rely heavily on crowdsourced data and human feedback, which are\nnot fully controlled. In this paper, we present a comprehensive survey of\nemerging backdoor threats to LLMs that appear during LLM development or\ninference, and cover recent advancement in both defense and detection\nstrategies for mitigating backdoor threats to LLMs. We also outline key\nchallenges in addressing these threats, highlighting areas for future research."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Wenjie Mo"
                    },
                    {
                        "name": "Terry Tong"
                    },
                    {
                        "name": "Jiashu Xu"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "The 60th Annual Allerton Conference (Invited Paper). The arXiv\n  version is a pre-IEEE Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19987v1",
                "updated": "2024-09-30T06:27:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    27,
                    50,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:27:50Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    27,
                    50,
                    0,
                    274,
                    0
                ],
                "title": "OccRWKV: Rethinking Efficient 3D Semantic Occupancy Prediction with\n  Linear Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OccRWKV: Rethinking Efficient 3D Semantic Occupancy Prediction with\n  Linear Complexity"
                },
                "summary": "3D semantic occupancy prediction networks have demonstrated remarkable\ncapabilities in reconstructing the geometric and semantic structure of 3D\nscenes, providing crucial information for robot navigation and autonomous\ndriving systems. However, due to their large overhead from dense network\nstructure designs, existing networks face challenges balancing accuracy and\nlatency.In this paper, we introduce OccRWKV, an efficient semantic occupancy\nnetwork inspired by Receptance Weighted Key Value (RWKV). OccRWKV separates\nsemantics, occupancy prediction, and feature fusion into distinct branches,\neach incorporating Sem-RWKV and Geo-RWKV blocks. These blocks are designed to\ncapture long-range dependencies, enabling the network to learn domain-specific\nrepresentation (i.e., semantics and geometry), which enhances prediction\naccuracy. Leveraging the sparse nature of real-world 3D occupancy, we reduce\ncomputational overhead by projecting features into the bird's-eye view (BEV)\nspace and propose a BEV-RWKV block for efficient feature enhancement and\nfusion. This enables real-time inference at 22.2 FPS without compromising\nperformance. Experiments demonstrate that OccRWKV outperforms the\nstate-of-the-art methods on the SemanticKITTI dataset, achieving a mIoU of 25.1\nwhile being 20 times faster than the best baseline, Co-Occ, making it suitable\nfor real-time deployment on robots to enhance autonomous navigation efficiency.\nCode and video are available on our project page:\n\\url{https://jmwang0117.github.io/OccRWKV/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D semantic occupancy prediction networks have demonstrated remarkable\ncapabilities in reconstructing the geometric and semantic structure of 3D\nscenes, providing crucial information for robot navigation and autonomous\ndriving systems. However, due to their large overhead from dense network\nstructure designs, existing networks face challenges balancing accuracy and\nlatency.In this paper, we introduce OccRWKV, an efficient semantic occupancy\nnetwork inspired by Receptance Weighted Key Value (RWKV). OccRWKV separates\nsemantics, occupancy prediction, and feature fusion into distinct branches,\neach incorporating Sem-RWKV and Geo-RWKV blocks. These blocks are designed to\ncapture long-range dependencies, enabling the network to learn domain-specific\nrepresentation (i.e., semantics and geometry), which enhances prediction\naccuracy. Leveraging the sparse nature of real-world 3D occupancy, we reduce\ncomputational overhead by projecting features into the bird's-eye view (BEV)\nspace and propose a BEV-RWKV block for efficient feature enhancement and\nfusion. This enables real-time inference at 22.2 FPS without compromising\nperformance. Experiments demonstrate that OccRWKV outperforms the\nstate-of-the-art methods on the SemanticKITTI dataset, achieving a mIoU of 25.1\nwhile being 20 times faster than the best baseline, Co-Occ, making it suitable\nfor real-time deployment on robots to enhance autonomous navigation efficiency.\nCode and video are available on our project page:\n\\url{https://jmwang0117.github.io/OccRWKV/}."
                },
                "authors": [
                    {
                        "name": "Junming Wang"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Xiaoxiao Long"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Zebin Xing"
                    },
                    {
                        "name": "Xiaoyang Guo"
                    },
                    {
                        "name": "Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Zhang"
                },
                "author": "Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10524v2",
                "updated": "2024-09-30T06:22:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    22,
                    12,
                    0,
                    274,
                    0
                ],
                "published": "2023-09-19T11:10:50Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    11,
                    10,
                    50,
                    1,
                    262,
                    0
                ],
                "title": "Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model\n  in End-to-End Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model\n  in End-to-End Speech Recognition"
                },
                "summary": "We propose to utilize an instruction-tuned large language model (LLM) for\nguiding the text generation process in automatic speech recognition (ASR).\nModern large language models (LLMs) are adept at performing various text\ngeneration tasks through zero-shot learning, prompted with instructions\ndesigned for specific objectives. This paper explores the potential of LLMs to\nderive linguistic information that can facilitate text generation in end-to-end\nASR models. Specifically, we instruct an LLM to correct grammatical errors in\nan ASR hypothesis and use the LLM-derived representations to refine the output\nfurther. The proposed model is built on the joint CTC and attention\narchitecture, with the LLM serving as a front-end feature extractor for the\ndecoder. The ASR hypothesis, subject to correction, is obtained from the\nencoder via CTC decoding and fed into the LLM along with a specific\ninstruction. The decoder subsequently takes as input the LLM output to perform\ntoken predictions, combining acoustic information from the encoder and the\npowerful linguistic information provided by the LLM. Experimental results show\nthat the proposed LLM-guided model achieves a relative gain of approximately\n13\\% in word error rates across major benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to utilize an instruction-tuned large language model (LLM) for\nguiding the text generation process in automatic speech recognition (ASR).\nModern large language models (LLMs) are adept at performing various text\ngeneration tasks through zero-shot learning, prompted with instructions\ndesigned for specific objectives. This paper explores the potential of LLMs to\nderive linguistic information that can facilitate text generation in end-to-end\nASR models. Specifically, we instruct an LLM to correct grammatical errors in\nan ASR hypothesis and use the LLM-derived representations to refine the output\nfurther. The proposed model is built on the joint CTC and attention\narchitecture, with the LLM serving as a front-end feature extractor for the\ndecoder. The ASR hypothesis, subject to correction, is obtained from the\nencoder via CTC decoding and fed into the LLM along with a specific\ninstruction. The decoder subsequently takes as input the LLM output to perform\ntoken predictions, combining acoustic information from the encoder and the\npowerful linguistic information provided by the LLM. Experimental results show\nthat the proposed LLM-guided model achieves a relative gain of approximately\n13\\% in word error rates across major benchmarks."
                },
                "authors": [
                    {
                        "name": "Yosuke Higuchi"
                    },
                    {
                        "name": "Tetsuji Ogawa"
                    },
                    {
                        "name": "Tetsunori Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Tetsunori Kobayashi"
                },
                "author": "Tetsunori Kobayashi",
                "arxiv_comment": "Submitted to ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19979v1",
                "updated": "2024-09-30T06:07:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    7,
                    12,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:07:12Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    7,
                    12,
                    0,
                    274,
                    0
                ],
                "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model"
                },
                "summary": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. % many NLP applications including However, existing approaches either\ndisregard or ineffectively model the user--item high-order interactions. To\nthis end, this paper presents an enhanced LLM-based recommender (ELMRec). We\nenhance whole-word embeddings to substantially enhance LLMs' interpretation of\ngraph-constructed interactions for recommendations, without requiring graph\npre-training. This finding may inspire endeavors to incorporate rich knowledge\ngraphs into LLM-based recommenders via whole-word embedding. We also found that\nLLMs often recommend items based on users' earlier interactions rather than\nrecent ones, and present a reranking solution. Our ELMRec outperforms\nstate-of-the-art (SOTA) methods in both direct and sequential recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. % many NLP applications including However, existing approaches either\ndisregard or ineffectively model the user--item high-order interactions. To\nthis end, this paper presents an enhanced LLM-based recommender (ELMRec). We\nenhance whole-word embeddings to substantially enhance LLMs' interpretation of\ngraph-constructed interactions for recommendations, without requiring graph\npre-training. This finding may inspire endeavors to incorporate rich knowledge\ngraphs into LLM-based recommenders via whole-word embedding. We also found that\nLLMs often recommend items based on users' earlier interactions rather than\nrecent ones, and present a reranking solution. Our ELMRec outperforms\nstate-of-the-art (SOTA) methods in both direct and sequential recommendations."
                },
                "authors": [
                    {
                        "name": "Xinfeng Wang"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Fumiyo Fukumoto"
                    },
                    {
                        "name": "Yoshimi Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimi Suzuki"
                },
                "author": "Yoshimi Suzuki",
                "arxiv_comment": "Long paper accepted to EMNLP 2024 Main. 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19972v1",
                "updated": "2024-09-30T05:53:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    53,
                    31,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T05:53:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    53,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy\n  Prediction"
                },
                "summary": "Multi-sensor fusion significantly enhances the accuracy and robustness of 3D\nsemantic occupancy prediction, which is crucial for autonomous driving and\nrobotics. However, existing approaches depend on large image resolutions and\ncomplex networks to achieve top performance, hindering their application in\npractical scenarios. Additionally, most multi-sensor fusion approaches focus on\nimproving fusion features while overlooking the exploration of supervision\nstrategies for these features. To this end, we propose DAOcc, a novel\nmulti-sensor fusion occupancy network that leverages 3D object detection\nsupervision to assist in achieving superior performance, while using a\ndeployment-friendly image feature extraction network and practical input image\nresolution. Furthermore, we introduce a BEV View Range Extension strategy to\nmitigate the adverse effects of reduced image resolution. As a result, our\napproach achieves new state-of-the-art results on the Occ3D-nuScenes and\nSurroundOcc datasets, using ResNet50 and a 256x704 input image resolution. Code\nwill be made available at https://github.com/AlphaPlusTT/DAOcc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-sensor fusion significantly enhances the accuracy and robustness of 3D\nsemantic occupancy prediction, which is crucial for autonomous driving and\nrobotics. However, existing approaches depend on large image resolutions and\ncomplex networks to achieve top performance, hindering their application in\npractical scenarios. Additionally, most multi-sensor fusion approaches focus on\nimproving fusion features while overlooking the exploration of supervision\nstrategies for these features. To this end, we propose DAOcc, a novel\nmulti-sensor fusion occupancy network that leverages 3D object detection\nsupervision to assist in achieving superior performance, while using a\ndeployment-friendly image feature extraction network and practical input image\nresolution. Furthermore, we introduce a BEV View Range Extension strategy to\nmitigate the adverse effects of reduced image resolution. As a result, our\napproach achieves new state-of-the-art results on the Occ3D-nuScenes and\nSurroundOcc datasets, using ResNet50 and a 256x704 input image resolution. Code\nwill be made available at https://github.com/AlphaPlusTT/DAOcc."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yanpeng Dong"
                    },
                    {
                        "name": "Heng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Wang"
                },
                "author": "Heng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19961v1",
                "updated": "2024-09-30T05:25:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    25,
                    51,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T05:25:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    25,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "Multimodal LLM Enhanced Cross-lingual Cross-modal Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLM Enhanced Cross-lingual Cross-modal Retrieval"
                },
                "summary": "Cross-lingual cross-modal retrieval (CCR) aims to retrieve visually relevant\ncontent based on non-English queries, without relying on human-labeled\ncross-modal data pairs during training. One popular approach involves utilizing\nmachine translation (MT) to create pseudo-parallel data pairs, establishing\ncorrespondence between visual and non-English textual data. However, aligning\ntheir representations poses challenges due to the significant semantic gap\nbetween vision and text, as well as the lower quality of non-English\nrepresentations caused by pre-trained encoders and data noise. To overcome\nthese challenges, we propose LECCR, a novel solution that incorporates the\nmulti-modal large language model (MLLM) to improve the alignment between visual\nand non-English representations. Specifically, we first employ MLLM to generate\ndetailed visual content descriptions and aggregate them into multi-view\nsemantic slots that encapsulate different semantics. Then, we take these\nsemantic slots as internal features and leverage them to interact with the\nvisual features. By doing so, we enhance the semantic information within the\nvisual features, narrowing the semantic gap between modalities and generating\nlocal visual semantics for subsequent multi-level matching. Additionally, to\nfurther enhance the alignment between visual and non-English features, we\nintroduce softened matching under English guidance. This approach provides more\ncomprehensive and reliable inter-modal correspondences between visual and\nnon-English features. Extensive experiments on four CCR benchmarks, \\ie\nMulti30K, MSCOCO, VATEX, and MSR-VTT-CN, demonstrate the effectiveness of our\nproposed method. Code: \\url{https://github.com/LiJiaBei-7/leccr}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-lingual cross-modal retrieval (CCR) aims to retrieve visually relevant\ncontent based on non-English queries, without relying on human-labeled\ncross-modal data pairs during training. One popular approach involves utilizing\nmachine translation (MT) to create pseudo-parallel data pairs, establishing\ncorrespondence between visual and non-English textual data. However, aligning\ntheir representations poses challenges due to the significant semantic gap\nbetween vision and text, as well as the lower quality of non-English\nrepresentations caused by pre-trained encoders and data noise. To overcome\nthese challenges, we propose LECCR, a novel solution that incorporates the\nmulti-modal large language model (MLLM) to improve the alignment between visual\nand non-English representations. Specifically, we first employ MLLM to generate\ndetailed visual content descriptions and aggregate them into multi-view\nsemantic slots that encapsulate different semantics. Then, we take these\nsemantic slots as internal features and leverage them to interact with the\nvisual features. By doing so, we enhance the semantic information within the\nvisual features, narrowing the semantic gap between modalities and generating\nlocal visual semantics for subsequent multi-level matching. Additionally, to\nfurther enhance the alignment between visual and non-English features, we\nintroduce softened matching under English guidance. This approach provides more\ncomprehensive and reliable inter-modal correspondences between visual and\nnon-English features. Extensive experiments on four CCR benchmarks, \\ie\nMulti30K, MSCOCO, VATEX, and MSR-VTT-CN, demonstrate the effectiveness of our\nproposed method. Code: \\url{https://github.com/LiJiaBei-7/leccr}."
                },
                "authors": [
                    {
                        "name": "Yabing Wang"
                    },
                    {
                        "name": "Le Wang"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gang Hua"
                    },
                    {
                        "name": "Wei Tang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tang"
                },
                "author": "Wei Tang",
                "arxiv_comment": "Accepted by ACM Multimedia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19959v1",
                "updated": "2024-09-30T05:22:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    22,
                    54,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T05:22:54Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    22,
                    54,
                    0,
                    274,
                    0
                ],
                "title": "Early review of Gender Bias of OpenAI o1-mini: Higher Intelligence of\n  LLM does not necessarily solve Gender Bias and Stereotyping issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early review of Gender Bias of OpenAI o1-mini: Higher Intelligence of\n  LLM does not necessarily solve Gender Bias and Stereotyping issues"
                },
                "summary": "In this paper, we present an early evaluation of the OpenAI o1-mini model,\nanalyzing its performance in gender inclusivity and bias. Our research,\nconducted on 700 personas 350 from GPT-4o mini and 350 from o1-mini, reveals\nthat despite improvements in inclusivity regarding personality traits and\npreferences, significant gender biases remain. For instance, o1-mini rated male\npersonas higher in competency, with a score of 8.06, compared to female\npersonas at 7.88 and non-binary personas at 7.80. Additionally, o1-mini\nassigned PhD roles to 28% of male personas but only 22.4% of females and 0% of\nnon-binary personas. Male personas were also more likely to be perceived as\nsuccessful founders, at 69.4%, and CEOs, at 62.17%, compared to female personas\nat 67.97% and 61.11%, and non-binary personas at 65.7% and 58.37%. The analysis\nreveals persistent gender biases across fields like Engineering, Data, and\nTechnology, where males dominate, reflecting traditional stereotypes.\nConversely, fields like Design, Art, and Marketing show a stronger presence of\nfemales, reinforcing societal notions that associate creativity and\ncommunication with females. These findings highlight ongoing challenges in\nmitigating gender bias, reinforcing the need for further interventions to\nensure equitable representation across all genders in AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an early evaluation of the OpenAI o1-mini model,\nanalyzing its performance in gender inclusivity and bias. Our research,\nconducted on 700 personas 350 from GPT-4o mini and 350 from o1-mini, reveals\nthat despite improvements in inclusivity regarding personality traits and\npreferences, significant gender biases remain. For instance, o1-mini rated male\npersonas higher in competency, with a score of 8.06, compared to female\npersonas at 7.88 and non-binary personas at 7.80. Additionally, o1-mini\nassigned PhD roles to 28% of male personas but only 22.4% of females and 0% of\nnon-binary personas. Male personas were also more likely to be perceived as\nsuccessful founders, at 69.4%, and CEOs, at 62.17%, compared to female personas\nat 67.97% and 61.11%, and non-binary personas at 65.7% and 58.37%. The analysis\nreveals persistent gender biases across fields like Engineering, Data, and\nTechnology, where males dominate, reflecting traditional stereotypes.\nConversely, fields like Design, Art, and Marketing show a stronger presence of\nfemales, reinforcing societal notions that associate creativity and\ncommunication with females. These findings highlight ongoing challenges in\nmitigating gender bias, reinforcing the need for further interventions to\nensure equitable representation across all genders in AI models."
                },
                "authors": [
                    {
                        "name": "Rajesh Ranjan"
                    },
                    {
                        "name": "Shailja Gupta"
                    },
                    {
                        "name": "Surya Naranyan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Surya Naranyan Singh"
                },
                "author": "Surya Naranyan Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19951v1",
                "updated": "2024-09-30T05:12:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    12,
                    1,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T05:12:01Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    12,
                    1,
                    0,
                    274,
                    0
                ],
                "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Law of the Weakest Link: Cross Capabilities of Large Language Models"
                },
                "summary": "The development and evaluation of Large Language Models (LLMs) have largely\nfocused on individual capabilities. However, this overlooks the intersection of\nmultiple abilities across different types of expertise that are often required\nfor real-world tasks, which we term cross capabilities. To systematically\nexplore this concept, we first define seven core individual capabilities and\nthen pair them to form seven common cross capabilities, each supported by a\nmanually constructed taxonomy. Building on these definitions, we introduce\nCrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100\nprompts for each individual and cross capability. To ensure reliable\nevaluation, we involve expert annotators to assess 4,200 model responses,\ngathering 8,400 human ratings with detailed explanations to serve as reference\nexamples. Our findings reveal that, in both static evaluations and attempts to\nenhance specific abilities, current LLMs consistently exhibit the \"Law of the\nWeakest Link,\" where cross-capability performance is significantly constrained\nby the weakest component. Specifically, across 58 cross-capability scores from\n17 models, 38 scores are lower than all individual capabilities, while 20 fall\nbetween strong and weak, but closer to the weaker ability. These results\nhighlight the under-performance of LLMs in cross-capability tasks, making the\nidentification and improvement of the weakest capabilities a critical priority\nfor future research to optimize performance in complex, multi-dimensional\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and evaluation of Large Language Models (LLMs) have largely\nfocused on individual capabilities. However, this overlooks the intersection of\nmultiple abilities across different types of expertise that are often required\nfor real-world tasks, which we term cross capabilities. To systematically\nexplore this concept, we first define seven core individual capabilities and\nthen pair them to form seven common cross capabilities, each supported by a\nmanually constructed taxonomy. Building on these definitions, we introduce\nCrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100\nprompts for each individual and cross capability. To ensure reliable\nevaluation, we involve expert annotators to assess 4,200 model responses,\ngathering 8,400 human ratings with detailed explanations to serve as reference\nexamples. Our findings reveal that, in both static evaluations and attempts to\nenhance specific abilities, current LLMs consistently exhibit the \"Law of the\nWeakest Link,\" where cross-capability performance is significantly constrained\nby the weakest component. Specifically, across 58 cross-capability scores from\n17 models, 38 scores are lower than all individual capabilities, while 20 fall\nbetween strong and weak, but closer to the weaker ability. These results\nhighlight the under-performance of LLMs in cross-capability tasks, making the\nidentification and improvement of the weakest capabilities a critical priority\nfor future research to optimize performance in complex, multi-dimensional\nscenarios."
                },
                "authors": [
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Xuewei Wang"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenhan Xiong"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Zhengxing Chen"
                    },
                    {
                        "name": "Liang Tan"
                    },
                    {
                        "name": "Chloe Bi"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Sravya Popuri"
                    },
                    {
                        "name": "Sharan Narang"
                    },
                    {
                        "name": "Melanie Kambadur"
                    },
                    {
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "name": "Sergey Edunov"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Laurens van der Maaten"
                    }
                ],
                "author_detail": {
                    "name": "Laurens van der Maaten"
                },
                "author": "Laurens van der Maaten",
                "arxiv_comment": "Code: https://github.com/facebookresearch/llm-cross-capabilities",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03419v2",
                "updated": "2024-09-30T04:49:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    4,
                    49,
                    38,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-06T03:02:38Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    3,
                    2,
                    38,
                    2,
                    66,
                    0
                ],
                "title": "Negating Negatives: Alignment with Human Negative Samples via\n  Distributional Dispreference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negating Negatives: Alignment with Human Negative Samples via\n  Distributional Dispreference Optimization"
                },
                "summary": "Large language models (LLMs) have revolutionized the role of AI, yet pose\npotential social risks. To steer LLMs towards human preference, alignment\ntechnologies have been introduced and gained increasing attention.\nNevertheless, existing methods heavily rely on high-quality positive-negative\ntraining pairs, suffering from noisy positive responses that are barely\ndistinguishable from negative ones. Given recent LLMs' proficiency in\ngenerating helpful responses, this work pivots towards a new research question:\ncan we achieve alignment using solely human-annotated negative samples,\npreserving helpfulness while reducing harmfulness? For this purpose, we propose\nDistributional Dispreference Optimization (D$^2$O), which maximizes the\ndiscrepancy between dispreferred responses and the generated non-negative ones.\nIn this way, D$^2$O effectively eschews harmful information without\nincorporating noisy positive samples, while avoiding collapse using\nself-generated responses as anchors. We demonstrate that D$^2$O can be regarded\nas learning a distributional preference model reflecting human dispreference\nagainst negative responses, which is theoretically an upper bound of the\ninstance-level DPO. Extensive experiments manifest that our method achieves\ncomparable generation quality and surpasses the latest strong baselines in\nproducing less harmful and more informative responses with better training\nstability and faster convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized the role of AI, yet pose\npotential social risks. To steer LLMs towards human preference, alignment\ntechnologies have been introduced and gained increasing attention.\nNevertheless, existing methods heavily rely on high-quality positive-negative\ntraining pairs, suffering from noisy positive responses that are barely\ndistinguishable from negative ones. Given recent LLMs' proficiency in\ngenerating helpful responses, this work pivots towards a new research question:\ncan we achieve alignment using solely human-annotated negative samples,\npreserving helpfulness while reducing harmfulness? For this purpose, we propose\nDistributional Dispreference Optimization (D$^2$O), which maximizes the\ndiscrepancy between dispreferred responses and the generated non-negative ones.\nIn this way, D$^2$O effectively eschews harmful information without\nincorporating noisy positive samples, while avoiding collapse using\nself-generated responses as anchors. We demonstrate that D$^2$O can be regarded\nas learning a distributional preference model reflecting human dispreference\nagainst negative responses, which is theoretically an upper bound of the\ninstance-level DPO. Extensive experiments manifest that our method achieves\ncomparable generation quality and surpasses the latest strong baselines in\nproducing less harmful and more informative responses with better training\nstability and faster convergence."
                },
                "authors": [
                    {
                        "name": "Shitong Duan"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "Accepted by EMNLP 2024(Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14596v2",
                "updated": "2024-09-30T04:20:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    4,
                    20,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-20T17:45:02Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    45,
                    2,
                    3,
                    172,
                    0
                ],
                "title": "ICAL: Continual Learning of Multimodal Agents by Transforming\n  Trajectories into Actionable Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICAL: Continual Learning of Multimodal Agents by Transforming\n  Trajectories into Actionable Insights"
                },
                "summary": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations to be\nincluded in their context window. In this work, we ask: Can LLMs and VLMs\ngenerate their own prompt examples from generic, sub-optimal demonstrations? We\npropose In-Context Abstraction Learning (ICAL), a method that builds a memory\nof multimodal experience insights from sub-optimal demonstrations and human\nfeedback. Given a noisy demonstration in a new domain, VLMs abstract the\ntrajectory into a general program by fixing inefficient actions and annotating\ncognitive abstractions: task relationships, object state changes, temporal\nsubgoals, and task construals. These abstractions are refined and adapted\ninteractively through human feedback while the agent attempts to execute the\ntrajectory in a similar environment. The resulting abstractions, when used as\nexemplars in the prompt, significantly improve decision-making in\nretrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the\nstate-of-the-art in dialogue-based instruction following in TEACh, multimodal\nweb agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we\nachieve a 12.6% improvement in goal-condition success. In VisualWebArena, our\ntask success rate improves over the SOTA from 18.9% to 23.4%. In Ego4D action\nforecasting, we improve over few-shot GPT-4V and remain competitive with\nsupervised models. We show finetuning our retrieval-augmented in-context agent\nyields additional improvements. Our approach significantly reduces reliance on\nexpert-crafted examples and consistently outperforms in-context learning from\naction plans that lack such insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations to be\nincluded in their context window. In this work, we ask: Can LLMs and VLMs\ngenerate their own prompt examples from generic, sub-optimal demonstrations? We\npropose In-Context Abstraction Learning (ICAL), a method that builds a memory\nof multimodal experience insights from sub-optimal demonstrations and human\nfeedback. Given a noisy demonstration in a new domain, VLMs abstract the\ntrajectory into a general program by fixing inefficient actions and annotating\ncognitive abstractions: task relationships, object state changes, temporal\nsubgoals, and task construals. These abstractions are refined and adapted\ninteractively through human feedback while the agent attempts to execute the\ntrajectory in a similar environment. The resulting abstractions, when used as\nexemplars in the prompt, significantly improve decision-making in\nretrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the\nstate-of-the-art in dialogue-based instruction following in TEACh, multimodal\nweb agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we\nachieve a 12.6% improvement in goal-condition success. In VisualWebArena, our\ntask success rate improves over the SOTA from 18.9% to 23.4%. In Ego4D action\nforecasting, we improve over few-shot GPT-4V and remain competitive with\nsupervised models. We show finetuning our retrieval-augmented in-context agent\nyields additional improvements. Our approach significantly reduces reliance on\nexpert-crafted examples and consistently outperforms in-context learning from\naction plans that lack such insights."
                },
                "authors": [
                    {
                        "name": "Gabriel Sarch"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Michael J. Tarr"
                    },
                    {
                        "name": "William W. Cohen"
                    },
                    {
                        "name": "Kenneth Marino"
                    },
                    {
                        "name": "Katerina Fragkiadaki"
                    }
                ],
                "author_detail": {
                    "name": "Katerina Fragkiadaki"
                },
                "author": "Katerina Fragkiadaki",
                "arxiv_comment": "Project website: http://ical-learning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19933v1",
                "updated": "2024-09-30T04:19:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    4,
                    19,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T04:19:40Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    4,
                    19,
                    40,
                    0,
                    274,
                    0
                ],
                "title": "CCDepth: A Lightweight Self-supervised Depth Estimation Network with\n  Enhanced Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCDepth: A Lightweight Self-supervised Depth Estimation Network with\n  Enhanced Interpretability"
                },
                "summary": "Self-supervised depth estimation, which solely requires monocular image\nsequence as input, has become increasingly popular and promising in recent\nyears. Current research primarily focuses on enhancing the prediction accuracy\nof the models. However, the excessive number of parameters impedes the\nuniversal deployment of the model on edge devices. Moreover, the emerging\nneural networks, being black-box models, are difficult to analyze, leading to\nchallenges in understanding the rationales for performance improvements. To\nmitigate these issues, this study proposes a novel hybrid self-supervised depth\nestimation network, CCDepth, comprising convolutional neural networks (CNNs)\nand the white-box CRATE (Coding RAte reduction TransformEr) network. This novel\nnetwork uses CNNs and the CRATE modules to extract local and global information\nin images, respectively, thereby boosting learning efficiency and reducing\nmodel size. Furthermore, incorporating the CRATE modules into the network\nenables a mathematically interpretable process in capturing global features.\nExtensive experiments on the KITTI dataset indicate that the proposed CCDepth\nnetwork can achieve performance comparable with those state-of-the-art methods,\nwhile the model size has been significantly reduced. In addition, a series of\nquantitative and qualitative analyses on the inner features in the CCDepth\nnetwork further confirm the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised depth estimation, which solely requires monocular image\nsequence as input, has become increasingly popular and promising in recent\nyears. Current research primarily focuses on enhancing the prediction accuracy\nof the models. However, the excessive number of parameters impedes the\nuniversal deployment of the model on edge devices. Moreover, the emerging\nneural networks, being black-box models, are difficult to analyze, leading to\nchallenges in understanding the rationales for performance improvements. To\nmitigate these issues, this study proposes a novel hybrid self-supervised depth\nestimation network, CCDepth, comprising convolutional neural networks (CNNs)\nand the white-box CRATE (Coding RAte reduction TransformEr) network. This novel\nnetwork uses CNNs and the CRATE modules to extract local and global information\nin images, respectively, thereby boosting learning efficiency and reducing\nmodel size. Furthermore, incorporating the CRATE modules into the network\nenables a mathematically interpretable process in capturing global features.\nExtensive experiments on the KITTI dataset indicate that the proposed CCDepth\nnetwork can achieve performance comparable with those state-of-the-art methods,\nwhile the model size has been significantly reduced. In addition, a series of\nquantitative and qualitative analyses on the inner features in the CCDepth\nnetwork further confirm the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Yaru Xue"
                    },
                    {
                        "name": "Shaocheng Jia"
                    },
                    {
                        "name": "Xin Pei"
                    }
                ],
                "author_detail": {
                    "name": "Xin Pei"
                },
                "author": "Xin Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19925v1",
                "updated": "2024-09-30T03:59:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    59,
                    6,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T03:59:06Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    59,
                    6,
                    0,
                    274,
                    0
                ],
                "title": "Large Language Model Empowered Embedding Generator for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Empowered Embedding Generator for Sequential\n  Recommendation"
                },
                "summary": "Sequential Recommender Systems (SRS) are extensively applied across various\ndomains to predict users' next interaction by modeling their interaction\nsequences. However, these systems typically grapple with the long-tail problem,\nwhere they struggle to recommend items that are less popular. This challenge\nresults in a decline in user discovery and reduced earnings for vendors,\nnegatively impacting the system as a whole. Large Language Model (LLM) has the\npotential to understand the semantic connections between items, regardless of\ntheir popularity, positioning them as a viable solution to this dilemma. In our\npaper, we present LLMEmb, an innovative technique that harnesses LLM to create\nitem embeddings that bolster the performance of SRS. To align the capabilities\nof general-purpose LLM with the needs of the recommendation domain, we\nintroduce a method called Supervised Contrastive Fine-Tuning (SCFT). This\nmethod involves attribute-level data augmentation and a custom contrastive loss\ndesigned to tailor LLM for enhanced recommendation performance. Moreover, we\nhighlight the necessity of incorporating collaborative filtering signals into\nLLM-generated embeddings and propose Recommendation Adaptation Training (RAT)\nfor this purpose. RAT refines the embeddings to be optimally suited for SRS.\nThe embeddings derived from LLMEmb can be easily integrated with any SRS model,\nshowcasing its practical utility. Extensive experimentation on three real-world\ndatasets has shown that LLMEmb significantly improves upon current methods when\napplied across different SRS models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommender Systems (SRS) are extensively applied across various\ndomains to predict users' next interaction by modeling their interaction\nsequences. However, these systems typically grapple with the long-tail problem,\nwhere they struggle to recommend items that are less popular. This challenge\nresults in a decline in user discovery and reduced earnings for vendors,\nnegatively impacting the system as a whole. Large Language Model (LLM) has the\npotential to understand the semantic connections between items, regardless of\ntheir popularity, positioning them as a viable solution to this dilemma. In our\npaper, we present LLMEmb, an innovative technique that harnesses LLM to create\nitem embeddings that bolster the performance of SRS. To align the capabilities\nof general-purpose LLM with the needs of the recommendation domain, we\nintroduce a method called Supervised Contrastive Fine-Tuning (SCFT). This\nmethod involves attribute-level data augmentation and a custom contrastive loss\ndesigned to tailor LLM for enhanced recommendation performance. Moreover, we\nhighlight the necessity of incorporating collaborative filtering signals into\nLLM-generated embeddings and propose Recommendation Adaptation Training (RAT)\nfor this purpose. RAT refines the embeddings to be optimally suited for SRS.\nThe embeddings derived from LLMEmb can be easily integrated with any SRS model,\nshowcasing its practical utility. Extensive experimentation on three real-world\ndatasets has shown that LLMEmb significantly improves upon current methods when\napplied across different SRS models."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Yuanshao Zhu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19924v1",
                "updated": "2024-09-30T03:58:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    58,
                    43,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T03:58:43Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    58,
                    43,
                    0,
                    274,
                    0
                ],
                "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning."
                },
                "authors": [
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yihan Xi"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Ufuk Topcu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19922v1",
                "updated": "2024-09-30T03:53:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    53,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T03:53:40Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    53,
                    40,
                    0,
                    274,
                    0
                ],
                "title": "Benchmarking ChatGPT, Codeium, and GitHub Copilot: A Comparative Study\n  of AI-Driven Programming and Debugging Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking ChatGPT, Codeium, and GitHub Copilot: A Comparative Study\n  of AI-Driven Programming and Debugging Assistants"
                },
                "summary": "With the increasing adoption of AI-driven tools in software development,\nlarge language models (LLMs) have become essential for tasks like code\ngeneration, bug fixing, and optimization. Tools like ChatGPT, GitHub Copilot,\nand Codeium provide valuable assistance in solving programming challenges, yet\ntheir effectiveness remains underexplored. This paper presents a comparative\nstudy of ChatGPT, Codeium, and GitHub Copilot, evaluating their performance on\nLeetCode problems across varying difficulty levels and categories. Key metrics\nsuch as success rates, runtime efficiency, memory usage, and error-handling\ncapabilities are assessed. GitHub Copilot showed superior performance on easier\nand medium tasks, while ChatGPT excelled in memory efficiency and debugging.\nCodeium, though promising, struggled with more complex problems. Despite their\nstrengths, all tools faced challenges in handling harder problems. These\ninsights provide a deeper understanding of each tool's capabilities and\nlimitations, offering guidance for developers and researchers seeking to\noptimize AI integration in coding workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of AI-driven tools in software development,\nlarge language models (LLMs) have become essential for tasks like code\ngeneration, bug fixing, and optimization. Tools like ChatGPT, GitHub Copilot,\nand Codeium provide valuable assistance in solving programming challenges, yet\ntheir effectiveness remains underexplored. This paper presents a comparative\nstudy of ChatGPT, Codeium, and GitHub Copilot, evaluating their performance on\nLeetCode problems across varying difficulty levels and categories. Key metrics\nsuch as success rates, runtime efficiency, memory usage, and error-handling\ncapabilities are assessed. GitHub Copilot showed superior performance on easier\nand medium tasks, while ChatGPT excelled in memory efficiency and debugging.\nCodeium, though promising, struggled with more complex problems. Despite their\nstrengths, all tools faced challenges in handling harder problems. These\ninsights provide a deeper understanding of each tool's capabilities and\nlimitations, offering guidance for developers and researchers seeking to\noptimize AI integration in coding workflows."
                },
                "authors": [
                    {
                        "name": "Md Sultanul Islam Ovi"
                    },
                    {
                        "name": "Nafisa Anjum"
                    },
                    {
                        "name": "Tasmina Haque Bithe"
                    },
                    {
                        "name": "Md. Mahabubur Rahman"
                    },
                    {
                        "name": "Mst. Shahnaj Akter Smrity"
                    }
                ],
                "author_detail": {
                    "name": "Mst. Shahnaj Akter Smrity"
                },
                "author": "Mst. Shahnaj Akter Smrity",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19916v1",
                "updated": "2024-09-30T03:37:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    37,
                    10,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T03:37:10Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    37,
                    10,
                    0,
                    274,
                    0
                ],
                "title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming"
                },
                "summary": "Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xuanhe Pan"
                    },
                    {
                        "name": "Jinlang Wang"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "arxiv_comment": "47pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19913v1",
                "updated": "2024-09-30T03:32:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    32,
                    2,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T03:32:02Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    32,
                    2,
                    0,
                    274,
                    0
                ],
                "title": "Scaling Optimal LR Across Token Horizon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Optimal LR Across Token Horizon"
                },
                "summary": "State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via our\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via our\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training."
                },
                "authors": [
                    {
                        "name": "Johan Bjorck"
                    },
                    {
                        "name": "Alon Benhaim"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Furu Wei"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16326v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16326v4",
                "updated": "2024-09-30T03:11:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    11,
                    4,
                    0,
                    274,
                    0
                ],
                "published": "2023-05-10T13:40:06Z",
                "published_parsed": [
                    2023,
                    5,
                    10,
                    13,
                    40,
                    6,
                    2,
                    130,
                    0
                ],
                "title": "A systematic evaluation of large language models for biomedical natural\n  language processing: benchmarks, baselines, and recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic evaluation of large language models for biomedical natural\n  language processing: benchmarks, baselines, and recommendations"
                },
                "summary": "The biomedical literature is rapidly expanding, posing a significant\nchallenge for manual curation and knowledge discovery. Biomedical Natural\nLanguage Processing (BioNLP) has emerged as a powerful solution, enabling the\nautomated extraction of information and knowledge from this extensive\nliterature. Recent attention has been directed towards Large Language Models\n(LLMs) due to their impressive performance. However, there remains a critical\ngap in understanding the effectiveness of LLMs in BioNLP tasks and their\nbroader implications for method development and downstream users. Currently,\nthere is a lack of baseline performance data, benchmarks, and practical\nrecommendations for using LLMs in the biomedical domain. To address this gap,\nwe present a systematic evaluation of four representative LLMs: GPT-3.5 and\nGPT-4 (closed-source), LLaMA 2 (open-sourced), and PMC LLaMA (domain-specific)\nacross 12 BioNLP datasets covering six applications (named entity recognition,\nrelation extraction, multi-label document classification, question answering,\ntext summarization, and text simplification). The evaluation is conducted under\nfour settings: zero-shot, static few-shot, dynamic K-nearest few-shot, and\nfine-tuning. We compare these models against state-of-the-art (SOTA) approaches\nthat fine-tune (domain-specific) BERT or BART models, which are\nwell-established methods in BioNLP tasks. The evaluation covers both\nquantitative and qualitative evaluations, where the latter involves manually\nreviewing collectively hundreds of thousands of LLM outputs for\ninconsistencies, missing information, and hallucinations in extractive and\nclassification tasks. The qualitative review also examines accuracy, 1\ncompleteness, and readability in text summarization tasks. Additionally, a cost\nanalysis of closed-source GPT models is conducted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The biomedical literature is rapidly expanding, posing a significant\nchallenge for manual curation and knowledge discovery. Biomedical Natural\nLanguage Processing (BioNLP) has emerged as a powerful solution, enabling the\nautomated extraction of information and knowledge from this extensive\nliterature. Recent attention has been directed towards Large Language Models\n(LLMs) due to their impressive performance. However, there remains a critical\ngap in understanding the effectiveness of LLMs in BioNLP tasks and their\nbroader implications for method development and downstream users. Currently,\nthere is a lack of baseline performance data, benchmarks, and practical\nrecommendations for using LLMs in the biomedical domain. To address this gap,\nwe present a systematic evaluation of four representative LLMs: GPT-3.5 and\nGPT-4 (closed-source), LLaMA 2 (open-sourced), and PMC LLaMA (domain-specific)\nacross 12 BioNLP datasets covering six applications (named entity recognition,\nrelation extraction, multi-label document classification, question answering,\ntext summarization, and text simplification). The evaluation is conducted under\nfour settings: zero-shot, static few-shot, dynamic K-nearest few-shot, and\nfine-tuning. We compare these models against state-of-the-art (SOTA) approaches\nthat fine-tune (domain-specific) BERT or BART models, which are\nwell-established methods in BioNLP tasks. The evaluation covers both\nquantitative and qualitative evaluations, where the latter involves manually\nreviewing collectively hundreds of thousands of LLM outputs for\ninconsistencies, missing information, and hallucinations in extractive and\nclassification tasks. The qualitative review also examines accuracy, 1\ncompleteness, and readability in text summarization tasks. Additionally, a cost\nanalysis of closed-source GPT models is conducted."
                },
                "authors": [
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Maxwell B. Singer"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Po-Ting Lai"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Vipina Kuttichi Keloth"
                    },
                    {
                        "name": "Kalpana Raja"
                    },
                    {
                        "name": "Jiming Huang"
                    },
                    {
                        "name": "Huan He"
                    },
                    {
                        "name": "Fongci Lin"
                    },
                    {
                        "name": "Jingcheng Du"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "W. Jim Zheng"
                    },
                    {
                        "name": "Ron A. Adelman"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16326v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16326v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19899v1",
                "updated": "2024-09-30T02:58:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    58,
                    5,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T02:58:05Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    58,
                    5,
                    0,
                    274,
                    0
                ],
                "title": "OpenKD: Opening Prompt Diversity for Zero- and Few-shot Keypoint\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenKD: Opening Prompt Diversity for Zero- and Few-shot Keypoint\n  Detection"
                },
                "summary": "Exploiting the foundation models (e.g., CLIP) to build a versatile keypoint\ndetector has gained increasing attention. Most existing models accept either\nthe text prompt (e.g., ``the nose of a cat''), or the visual prompt (e.g.,\nsupport image with keypoint annotations), to detect the corresponding keypoints\nin query image, thereby, exhibiting either zero-shot or few-shot detection\nability. However, the research on taking multimodal prompt is still\nunderexplored, and the prompt diversity in semantics and language is far from\nopened. For example, how to handle unseen text prompts for novel keypoint\ndetection and the diverse text prompts like ``Can you detect the nose and ears\nof a cat?'' In this work, we open the prompt diversity from three aspects:\nmodality, semantics (seen v.s. unseen), and language, to enable a more\ngeneralized zero- and few-shot keypoint detection (Z-FSKD). We propose a novel\nOpenKD model which leverages multimodal prototype set to support both visual\nand textual prompting. Further, to infer the keypoint location of unseen texts,\nwe add the auxiliary keypoints and texts interpolated from visual and textual\ndomains into training, which improves the spatial reasoning of our model and\nsignificantly enhances zero-shot novel keypoint detection. We also found large\nlanguage model (LLM) is a good parser, which achieves over 96% accuracy to\nparse keypoints from texts. With LLM, OpenKD can handle diverse text prompts.\nExperimental results show that our method achieves state-of-the-art performance\non Z-FSKD and initiates new ways to deal with unseen text and diverse texts.\nThe source code and data are available at https://github.com/AlanLuSun/OpenKD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting the foundation models (e.g., CLIP) to build a versatile keypoint\ndetector has gained increasing attention. Most existing models accept either\nthe text prompt (e.g., ``the nose of a cat''), or the visual prompt (e.g.,\nsupport image with keypoint annotations), to detect the corresponding keypoints\nin query image, thereby, exhibiting either zero-shot or few-shot detection\nability. However, the research on taking multimodal prompt is still\nunderexplored, and the prompt diversity in semantics and language is far from\nopened. For example, how to handle unseen text prompts for novel keypoint\ndetection and the diverse text prompts like ``Can you detect the nose and ears\nof a cat?'' In this work, we open the prompt diversity from three aspects:\nmodality, semantics (seen v.s. unseen), and language, to enable a more\ngeneralized zero- and few-shot keypoint detection (Z-FSKD). We propose a novel\nOpenKD model which leverages multimodal prototype set to support both visual\nand textual prompting. Further, to infer the keypoint location of unseen texts,\nwe add the auxiliary keypoints and texts interpolated from visual and textual\ndomains into training, which improves the spatial reasoning of our model and\nsignificantly enhances zero-shot novel keypoint detection. We also found large\nlanguage model (LLM) is a good parser, which achieves over 96% accuracy to\nparse keypoints from texts. With LLM, OpenKD can handle diverse text prompts.\nExperimental results show that our method achieves state-of-the-art performance\non Z-FSKD and initiates new ways to deal with unseen text and diverse texts.\nThe source code and data are available at https://github.com/AlanLuSun/OpenKD."
                },
                "authors": [
                    {
                        "name": "Changsheng Lu"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Piotr Koniusz"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Koniusz"
                },
                "author": "Piotr Koniusz",
                "arxiv_comment": "Accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19898v2",
                "updated": "2024-10-01T07:11:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    11,
                    44,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:56:35Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    56,
                    35,
                    0,
                    274,
                    0
                ],
                "title": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs"
                },
                "summary": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0."
                },
                "authors": [
                    {
                        "name": "Yuho Lee"
                    },
                    {
                        "name": "Taewon Yun"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Hwanjun Song"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjun Song"
                },
                "author": "Hwanjun Song",
                "arxiv_comment": "Accepted at EMNLP-Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19895v1",
                "updated": "2024-09-30T02:53:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    53,
                    50,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T02:53:50Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    53,
                    50,
                    0,
                    274,
                    0
                ],
                "title": "Fourier Domain Physics Informed Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier Domain Physics Informed Neural Network"
                },
                "summary": "Ultrafast optics is driven by a myriad of complex nonlinear dynamics. The\nubiquitous presence of governing equations in the form of partial\nintegro-differential equations (PIDE) necessitates the need for advanced\ncomputational tools to understand the underlying physical mechanisms. From the\nexperimental perspective, signal-to-noise ratio and availability of measurable\ndata, accounts for a bottle neck in numerical and data-driven modeling methods.\nIn this paper we extend the application of the physics informed neural network\n(PINN) architecture to include prior knowledge in both the physical and Fourier\ndomain. We demonstrate our Fourier Domain PINN (FD-PINN) in two distinct forms.\nThe Continuous time FD-PINN is used to predict accurate solutions to the\nGeneralized Pulse Propagation Equation, which includes the complete delayed\nnonlinear response, in the data-starved and noisy regime. We extend the\narchitecture to the Discrete time FD-PINN to recover the delayed-response\nphysics from spatially separated measurement points. We conducted the first\nsystematic study of the effect of SNR on the spatiotemporal field prediction as\nwell as physics discovery. Our architecture ensures high fidelity predictive\nmodeling and hidden physics recovery for applications such as image\nreconstruction, pulse characterization and shaping, as well as hidden parameter\ndiscovery. The benefits of the FD-PINN for ultrafast nonlinear optics make it\nimmediately experimentally deployable. FD-PINN represents the next generation\nof tools to study optical phenomena both through modeling and measurements for\nboth forward and inverse problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast optics is driven by a myriad of complex nonlinear dynamics. The\nubiquitous presence of governing equations in the form of partial\nintegro-differential equations (PIDE) necessitates the need for advanced\ncomputational tools to understand the underlying physical mechanisms. From the\nexperimental perspective, signal-to-noise ratio and availability of measurable\ndata, accounts for a bottle neck in numerical and data-driven modeling methods.\nIn this paper we extend the application of the physics informed neural network\n(PINN) architecture to include prior knowledge in both the physical and Fourier\ndomain. We demonstrate our Fourier Domain PINN (FD-PINN) in two distinct forms.\nThe Continuous time FD-PINN is used to predict accurate solutions to the\nGeneralized Pulse Propagation Equation, which includes the complete delayed\nnonlinear response, in the data-starved and noisy regime. We extend the\narchitecture to the Discrete time FD-PINN to recover the delayed-response\nphysics from spatially separated measurement points. We conducted the first\nsystematic study of the effect of SNR on the spatiotemporal field prediction as\nwell as physics discovery. Our architecture ensures high fidelity predictive\nmodeling and hidden physics recovery for applications such as image\nreconstruction, pulse characterization and shaping, as well as hidden parameter\ndiscovery. The benefits of the FD-PINN for ultrafast nonlinear optics make it\nimmediately experimentally deployable. FD-PINN represents the next generation\nof tools to study optical phenomena both through modeling and measurements for\nboth forward and inverse problems."
                },
                "authors": [
                    {
                        "name": "Jonathan Musgrave"
                    },
                    {
                        "name": "Shu-Wei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Wei Huang"
                },
                "author": "Shu-Wei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19894v2",
                "updated": "2024-10-01T04:35:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    35,
                    5,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:53:03Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    53,
                    3,
                    0,
                    274,
                    0
                ],
                "title": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation"
                },
                "summary": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19886v1",
                "updated": "2024-09-30T02:31:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    31,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T02:31:40Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    31,
                    40,
                    0,
                    274,
                    0
                ],
                "title": "RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling\n  Large Language Models"
                },
                "summary": "Recent works show that assembling multiple off-the-shelf large language\nmodels (LLMs) can harness their complementary abilities. To achieve this,\nrouting is a promising method, which learns a router to select the most\nsuitable LLM for each query. However, existing routing models are ineffective\nwhen multiple LLMs perform well for a query. To address this problem, in this\npaper, we propose a method called query-based Router by Dual Contrastive\nlearning (RouterDC). The RouterDC model consists of an encoder and LLM\nembeddings, and we propose two contrastive learning losses to train the\nRouterDC model. Experimental results show that RouterDC is effective in\nassembling LLMs and largely outperforms individual top-performing LLMs as well\nas existing routing methods on both in-distribution (+2.76\\%) and\nout-of-distribution (+1.90\\%) tasks. Source code is available at\nhttps://github.com/shuhao02/RouterDC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works show that assembling multiple off-the-shelf large language\nmodels (LLMs) can harness their complementary abilities. To achieve this,\nrouting is a promising method, which learns a router to select the most\nsuitable LLM for each query. However, existing routing models are ineffective\nwhen multiple LLMs perform well for a query. To address this problem, in this\npaper, we propose a method called query-based Router by Dual Contrastive\nlearning (RouterDC). The RouterDC model consists of an encoder and LLM\nembeddings, and we propose two contrastive learning losses to train the\nRouterDC model. Experimental results show that RouterDC is effective in\nassembling LLMs and largely outperforms individual top-performing LLMs as well\nas existing routing methods on both in-distribution (+2.76\\%) and\nout-of-distribution (+1.90\\%) tasks. Source code is available at\nhttps://github.com/shuhao02/RouterDC."
                },
                "authors": [
                    {
                        "name": "Shuhao Chen"
                    },
                    {
                        "name": "Weisen Jiang"
                    },
                    {
                        "name": "Baijiong Lin"
                    },
                    {
                        "name": "James T. Kwok"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19878v1",
                "updated": "2024-09-30T02:23:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    23,
                    31,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T02:23:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    23,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic\n  Thresholds for Fine-Tuning LLM-based ASR Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic\n  Thresholds for Fine-Tuning LLM-based ASR Models"
                },
                "summary": "Recent advancements in integrating Large Language Models (LLM) with automatic\nspeech recognition (ASR) have performed remarkably in general domains. While\nsupervised fine-tuning (SFT) of all model parameters is often employed to adapt\npre-trained LLM-based ASR models to specific domains, it imposes high\ncomputational costs and notably reduces their performance in general domains.\nIn this paper, we propose a novel parameter-efficient multi-domain fine-tuning\nmethod for adapting pre-trained LLM-based ASR models to multi-accent domains\nwithout catastrophic forgetting named \\textit{HDMoLE}, which leverages\nhierarchical routing and dynamic thresholds based on combining low-rank\nadaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any\nlinear layer. Hierarchical routing establishes a clear correspondence between\nLoRA experts and accent domains, improving cross-domain collaboration among the\nLoRA experts. Unlike the static Top-K strategy for activating LoRA experts,\ndynamic thresholds can adaptively activate varying numbers of LoRA experts at\neach MoE layer. Experiments on the multi-accent and standard Mandarin datasets\ndemonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model\nprojector module achieves similar performance to full fine-tuning in the target\nmulti-accent domains while using only 9.6% of the trainable parameters required\nfor full fine-tuning and minimal degradation in the source general domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating Large Language Models (LLM) with automatic\nspeech recognition (ASR) have performed remarkably in general domains. While\nsupervised fine-tuning (SFT) of all model parameters is often employed to adapt\npre-trained LLM-based ASR models to specific domains, it imposes high\ncomputational costs and notably reduces their performance in general domains.\nIn this paper, we propose a novel parameter-efficient multi-domain fine-tuning\nmethod for adapting pre-trained LLM-based ASR models to multi-accent domains\nwithout catastrophic forgetting named \\textit{HDMoLE}, which leverages\nhierarchical routing and dynamic thresholds based on combining low-rank\nadaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any\nlinear layer. Hierarchical routing establishes a clear correspondence between\nLoRA experts and accent domains, improving cross-domain collaboration among the\nLoRA experts. Unlike the static Top-K strategy for activating LoRA experts,\ndynamic thresholds can adaptively activate varying numbers of LoRA experts at\neach MoE layer. Experiments on the multi-accent and standard Mandarin datasets\ndemonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model\nprojector module achieves similar performance to full fine-tuning in the target\nmulti-accent domains while using only 9.6% of the trainable parameters required\nfor full fine-tuning and minimal degradation in the source general domain."
                },
                "authors": [
                    {
                        "name": "Bingshen Mu"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Qijie Shao"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19877v1",
                "updated": "2024-09-30T02:21:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    21,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T02:21:39Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    21,
                    39,
                    0,
                    274,
                    0
                ],
                "title": "Contrastive Token Learning with Similarity Decay for Repetition\n  Suppression in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Token Learning with Similarity Decay for Repetition\n  Suppression in Machine Translation"
                },
                "summary": "For crosslingual conversation and trade, Neural Machine Translation (NMT) is\npivotal yet faces persistent challenges with monotony and repetition in\ngenerated content. Traditional solutions that rely on penalizing text\nredundancy or token reoccurrence have shown limited efficacy, particularly for\nlengthy article and e-commerce descriptions with inherent redundancy, even with\nthe advent of Large Language Models (LLMs). This paper investigates the\nunderlying causes of textual repetition through the lens of information\nentropy, attributing the phenomenon to the elevated uncertainty within the\ninput text. To address this, a novel algorithm named Contrastive Token Learning\nwith Similarity Decay (CTSD) is introduced, which modulates the suppression of\ntokens dynamically, informed by varying attention weights and inter-token\ndistances. Furthermore, an e-commerce dataset comprised of title texts of\nonline real items is compiled and released susceptible to hallucination\ntranslations to benchmark the algorithm. Extensive evaluations demonstrate that\nCTSD significantly outperforms existing approaches in precision and\ngeneralizability. Additional online A/B testing underscores its practical\nvalue, showing marked improvements in user engagement and conversion. Notably,\nthis method has been implemented with full traffic on eight multilingual sites\nof alibaba.com, the largest B2B e-commerce platform in the world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For crosslingual conversation and trade, Neural Machine Translation (NMT) is\npivotal yet faces persistent challenges with monotony and repetition in\ngenerated content. Traditional solutions that rely on penalizing text\nredundancy or token reoccurrence have shown limited efficacy, particularly for\nlengthy article and e-commerce descriptions with inherent redundancy, even with\nthe advent of Large Language Models (LLMs). This paper investigates the\nunderlying causes of textual repetition through the lens of information\nentropy, attributing the phenomenon to the elevated uncertainty within the\ninput text. To address this, a novel algorithm named Contrastive Token Learning\nwith Similarity Decay (CTSD) is introduced, which modulates the suppression of\ntokens dynamically, informed by varying attention weights and inter-token\ndistances. Furthermore, an e-commerce dataset comprised of title texts of\nonline real items is compiled and released susceptible to hallucination\ntranslations to benchmark the algorithm. Extensive evaluations demonstrate that\nCTSD significantly outperforms existing approaches in precision and\ngeneralizability. Additional online A/B testing underscores its practical\nvalue, showing marked improvements in user engagement and conversion. Notably,\nthis method has been implemented with full traffic on eight multilingual sites\nof alibaba.com, the largest B2B e-commerce platform in the world."
                },
                "authors": [
                    {
                        "name": "Huangyu Dai"
                    },
                    {
                        "name": "Ben Chen"
                    },
                    {
                        "name": "Kaidi Chen"
                    },
                    {
                        "name": "Ying Han"
                    },
                    {
                        "name": "Zihan Liang"
                    },
                    {
                        "name": "Wen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Jiang"
                },
                "author": "Wen Jiang",
                "arxiv_comment": "Accepted by EMNLP'24 Findings. 12 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19872v2",
                "updated": "2024-10-01T07:34:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    34,
                    25,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:13:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    13,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration"
                },
                "summary": "The swift advancement in Multimodal LLMs (MLLMs) also presents significant\nchallenges for effective knowledge editing. Current methods, including\nintrinsic knowledge editing and external knowledge resorting, each possess\nstrengths and weaknesses, struggling to balance the desired properties of\nreliability, generality, and locality when applied to MLLMs. In this paper, we\npropose UniKE, a novel multimodal editing method that establishes a unified\nperspective and paradigm for intrinsic knowledge editing and external knowledge\nresorting. Both types of knowledge are conceptualized as vectorized key-value\nmemories, with the corresponding editing processes resembling the assimilation\nand accommodation phases of human cognition, conducted at the same semantic\nlevels. Within such a unified framework, we further promote knowledge\ncollaboration by disentangling the knowledge representations into the semantic\nand truthfulness spaces. Extensive experiments validate the effectiveness of\nour method, which ensures that the post-edit MLLM simultaneously maintains\nexcellent reliability, generality, and locality. The code for UniKE will be\navailable at \\url{https://github.com/beepkh/UniKE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift advancement in Multimodal LLMs (MLLMs) also presents significant\nchallenges for effective knowledge editing. Current methods, including\nintrinsic knowledge editing and external knowledge resorting, each possess\nstrengths and weaknesses, struggling to balance the desired properties of\nreliability, generality, and locality when applied to MLLMs. In this paper, we\npropose UniKE, a novel multimodal editing method that establishes a unified\nperspective and paradigm for intrinsic knowledge editing and external knowledge\nresorting. Both types of knowledge are conceptualized as vectorized key-value\nmemories, with the corresponding editing processes resembling the assimilation\nand accommodation phases of human cognition, conducted at the same semantic\nlevels. Within such a unified framework, we further promote knowledge\ncollaboration by disentangling the knowledge representations into the semantic\nand truthfulness spaces. Extensive experiments validate the effectiveness of\nour method, which ensures that the post-edit MLLM simultaneously maintains\nexcellent reliability, generality, and locality. The code for UniKE will be\navailable at \\url{https://github.com/beepkh/UniKE}."
                },
                "authors": [
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Zhaoyu Fan"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Qianru Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qianru Sun"
                },
                "author": "Qianru Sun",
                "arxiv_comment": "Accepted by NeurIPS 2024 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]