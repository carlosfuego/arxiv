[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Sren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Sren Stobbe"
                },
                "author": "Sren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v1",
                "updated": "2025-06-25T07:26:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uro Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uro Seljak"
                },
                "author": "Uro Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1604.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1604.01713v2",
                "updated": "2025-06-19T10:23:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    23,
                    50,
                    3,
                    170,
                    0
                ],
                "published": "2016-04-06T18:07:19Z",
                "published_parsed": [
                    2016,
                    4,
                    6,
                    18,
                    7,
                    19,
                    2,
                    97,
                    0
                ],
                "title": "A block Recycled GMRES method with investigations into aspects of solver\n  performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A block Recycled GMRES method with investigations into aspects of solver\n  performance"
                },
                "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
                },
                "authors": [
                    {
                        "name": "Michael L. Parks"
                    },
                    {
                        "name": "Kirk M. Soodhalter"
                    },
                    {
                        "name": "Daniel B. Szyld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Szyld"
                },
                "author": "Daniel B. Szyld",
                "arxiv_comment": "35 pages, 26 pages of manuscript text, 13 figures, 1 table, Temple\n  University Research Report 16-04-04",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1604.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1604.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thvenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thvenet"
                },
                "author": "M. Thvenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v1",
                "updated": "2025-06-18T17:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21593v1",
                "updated": "2025-06-18T07:54:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T07:54:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications"
                },
                "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems."
                },
                "authors": [
                    {
                        "name": "Abu Hanif Muhammad Syarubany"
                    },
                    {
                        "name": "Chang Dong Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang Dong Yoo"
                },
                "author": "Chang Dong Yoo",
                "arxiv_comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v1",
                "updated": "2025-06-18T05:07:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22463v1",
                "updated": "2025-06-18T03:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T03:31:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization"
                },
                "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff."
                },
                "authors": [
                    {
                        "name": "Weizhi Gao"
                    },
                    {
                        "name": "Zhichao Hou"
                    },
                    {
                        "name": "Junqi Yin"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Linyu Peng"
                    },
                    {
                        "name": "Xiaorui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorui Liu"
                },
                "author": "Xiaorui Liu",
                "arxiv_comment": "26 pages, accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rben Ado"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "Joo Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v1",
                "updated": "2025-06-15T07:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.24124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24124v2",
                "updated": "2025-07-01T03:40:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    40,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    59,
                    14,
                    0,
                    181,
                    0
                ],
                "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives"
                },
                "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP."
                },
                "authors": [
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Teresa Wu"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "arxiv_comment": "Code: https://github.com/Ironieser/TimesCLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24120v1",
                "updated": "2025-06-30T17:58:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    30,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:58:30Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    30,
                    0,
                    181,
                    0
                ],
                "title": "Data Uniformity Improves Training Efficiency and More, with a\n  Convergence Framework Beyond the NTK Regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Uniformity Improves Training Efficiency and More, with a\n  Convergence Framework Beyond the NTK Regime"
                },
                "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Shangding Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shangding Gu"
                },
                "author": "Shangding Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24119v2",
                "updated": "2025-07-01T02:29:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    29,
                    52,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:58:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning"
                },
                "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Leon Guertler"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Daniel Balcells"
                    },
                    {
                        "name": "Mickel Liu"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24118v1",
                "updated": "2025-06-30T17:57:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    57,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:57:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    57,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Scaling Human Judgment in Community Notes with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Human Judgment in Community Notes with LLMs"
                },
                "summary": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Soham De"
                    },
                    {
                        "name": "Manon Revel"
                    },
                    {
                        "name": "Andreas Haupt"
                    },
                    {
                        "name": "Brad Miller"
                    },
                    {
                        "name": "Keith Coleman"
                    },
                    {
                        "name": "Jay Baxter"
                    },
                    {
                        "name": "Martin Saveski"
                    },
                    {
                        "name": "Michiel A. Bakker"
                    }
                ],
                "author_detail": {
                    "name": "Michiel A. Bakker"
                },
                "author": "Michiel A. Bakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02113v2",
                "updated": "2025-06-30T17:50:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    50,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-03T03:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    10,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Trust & Safety of LLMs and LLMs in Trust & Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust & Safety of LLMs and LLMs in Trust & Safety"
                },
                "summary": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm."
                },
                "authors": [
                    {
                        "name": "Doohee You"
                    },
                    {
                        "name": "Dan Chon"
                    }
                ],
                "author_detail": {
                    "name": "Dan Chon"
                },
                "author": "Dan Chon",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02811v2",
                "updated": "2025-06-30T17:46:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    46,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T17:39:35Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    39,
                    35,
                    0,
                    125,
                    0
                ],
                "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing"
                },
                "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance. This paper aims to\naddress these limitations by introducing a new framework, SIM-RAG, to\nexplicitly enhance RAG systems' self-awareness and multi-round retrieval\ncapabilities. To train SIM-RAG, we first let a RAG system self-practice\nmulti-round retrieval, augmenting existing question-answer pairs with\nintermediate inner monologue reasoning steps to generate synthetic training\ndata. For each pair, the system may explore multiple retrieval paths, which are\nlabeled as successful if they reach the correct answer and unsuccessful\notherwise. Using this data, we train a lightweight information sufficiency\nCritic. At inference time, the Critic evaluates whether the RAG system has\nretrieved sufficient information at each round, guiding retrieval decisions and\nimproving system-level self-awareness through in-context reinforcement\nlearning. Experiments across multiple prominent RAG benchmarks show that\nSIM-RAG is an effective multi-round RAG solution. Furthermore, this framework\nis system-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance. This paper aims to\naddress these limitations by introducing a new framework, SIM-RAG, to\nexplicitly enhance RAG systems' self-awareness and multi-round retrieval\ncapabilities. To train SIM-RAG, we first let a RAG system self-practice\nmulti-round retrieval, augmenting existing question-answer pairs with\nintermediate inner monologue reasoning steps to generate synthetic training\ndata. For each pair, the system may explore multiple retrieval paths, which are\nlabeled as successful if they reach the correct answer and unsuccessful\notherwise. Using this data, we train a lightweight information sufficiency\nCritic. At inference time, the Critic evaluates whether the RAG system has\nretrieved sufficient information at each round, guiding retrieval decisions and\nimproving system-level self-awareness through in-context reinforcement\nlearning. Experiments across multiple prominent RAG benchmarks show that\nSIM-RAG is an effective multi-round RAG solution. Furthermore, this framework\nis system-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."
                },
                "authors": [
                    {
                        "name": "Diji Yang"
                    },
                    {
                        "name": "Linda Zeng"
                    },
                    {
                        "name": "Jinmeng Rao"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "arxiv_comment": "Proceedings of the 48th International ACM SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18797v2",
                "updated": "2025-06-30T17:45:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    45,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-27T22:46:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    22,
                    46,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?"
                },
                "summary": "Recent advancements in LLMs unlearning have shown remarkable success in\nremoving unwanted data-model influences while preserving the model's utility\nfor legitimate knowledge. Despite these strides, sparse Mixture-of-Experts\n(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the\ncontext of unlearning. As MoE LLMs are celebrated for their exceptional\nperformance, we ask:How can unlearning be performed effectively and efficiently\non MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs\nintroduces unique challenges, leading to excessive forgetting, uncontrolled\nknowledge erasure and substantial utility drops when existing unlearning\nmethods are applied. To address this, we propose a novel Selected-Expert\nUnlearning Framework (SEUF). Through expert attribution, unlearning is\nconcentrated on the most actively engaged experts for the specified knowledge.\nConcurrently, an anchor loss is applied to the router to stabilize the active\nstate of this targeted expert, ensuring focused and controlled unlearning. SEUF\nis compatible with various standard unlearning algorithms. Extensive\nexperiments demonstrate that SEUF enhances both forget quality up to 5% and\nmodel utility by 35% on MoE LLMs across various benchmarks and LLM\narchitectures (compared to standard unlearning algorithms), while only\nunlearning 0.06% of the model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs unlearning have shown remarkable success in\nremoving unwanted data-model influences while preserving the model's utility\nfor legitimate knowledge. Despite these strides, sparse Mixture-of-Experts\n(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the\ncontext of unlearning. As MoE LLMs are celebrated for their exceptional\nperformance, we ask:How can unlearning be performed effectively and efficiently\non MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs\nintroduces unique challenges, leading to excessive forgetting, uncontrolled\nknowledge erasure and substantial utility drops when existing unlearning\nmethods are applied. To address this, we propose a novel Selected-Expert\nUnlearning Framework (SEUF). Through expert attribution, unlearning is\nconcentrated on the most actively engaged experts for the specified knowledge.\nConcurrently, an anchor loss is applied to the router to stabilize the active\nstate of this targeted expert, ensuring focused and controlled unlearning. SEUF\nis compatible with various standard unlearning algorithms. Extensive\nexperiments demonstrate that SEUF enhances both forget quality up to 5% and\nmodel utility by 35% on MoE LLMs across various benchmarks and LLM\narchitectures (compared to standard unlearning algorithms), while only\nunlearning 0.06% of the model parameters."
                },
                "authors": [
                    {
                        "name": "Haomin Zhuang"
                    },
                    {
                        "name": "Yihua Zhang"
                    },
                    {
                        "name": "Kehan Guo"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Sijia Liu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "arxiv_comment": "Accepted to ACL'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09258v2",
                "updated": "2025-06-30T17:42:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    42,
                    13,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-13T12:15:32Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    15,
                    32,
                    3,
                    44,
                    0
                ],
                "title": "Accounting for motion of supernova host galaxy in statistical inference\n  from SNIa data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accounting for motion of supernova host galaxy in statistical inference\n  from SNIa data"
                },
                "summary": "We introduce a Bayesian method to estimate peculiar velocities of Type Ia\nsupernova (SNIa) host galaxies by employing the magnitude-redshift relationship\nof SNIa. Random peculiar motions act as noise in the estimation of redshift,\nand constitute independent variables in the SNIa data. We develop a method to\ntake into account errors in independent variables for general nonlinear models.\nUsing the MCMC sampling technique, we implement numerical codes to estimate\npeculiar velocities along with other cosmological parameters. In this paper, we\nstudy the impact of these velocities on cosmological parameters by treating\nthem as nuisance parameters. We apply our proposed method on the Pantheon\nsample of SNIa and show a few percent shift on the central values of inferred\ncosmological parameters. Although the current data are not particularly\nsensitive to this error, using simulated data, we also gauge the efficacy of\nour method on the future SNIa observations and show that future data require\nthe inclusion of this error in cosmological parameter estimation. Our method\ncomplements several existing methods that seek to estimate peculiar velocities\nusing galaxy data and N-body simulations and extends to higher redshifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Bayesian method to estimate peculiar velocities of Type Ia\nsupernova (SNIa) host galaxies by employing the magnitude-redshift relationship\nof SNIa. Random peculiar motions act as noise in the estimation of redshift,\nand constitute independent variables in the SNIa data. We develop a method to\ntake into account errors in independent variables for general nonlinear models.\nUsing the MCMC sampling technique, we implement numerical codes to estimate\npeculiar velocities along with other cosmological parameters. In this paper, we\nstudy the impact of these velocities on cosmological parameters by treating\nthem as nuisance parameters. We apply our proposed method on the Pantheon\nsample of SNIa and show a few percent shift on the central values of inferred\ncosmological parameters. Although the current data are not particularly\nsensitive to this error, using simulated data, we also gauge the efficacy of\nour method on the future SNIa observations and show that future data require\nthe inclusion of this error in cosmological parameter estimation. Our method\ncomplements several existing methods that seek to estimate peculiar velocities\nusing galaxy data and N-body simulations and extends to higher redshifts."
                },
                "authors": [
                    {
                        "name": "Ujjwal Upadhyay"
                    },
                    {
                        "name": "Tarun Deep Saini"
                    },
                    {
                        "name": "Shiv K. Sethi"
                    }
                ],
                "author_detail": {
                    "name": "Shiv K. Sethi"
                },
                "author": "Shiv K. Sethi",
                "arxiv_comment": "14 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17677v2",
                "updated": "2025-06-30T17:30:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    30,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-24T15:47:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    47,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models"
                },
                "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. Based on interviews with teaching staff, this paper\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\nteaching staff and students in the process of solving exercises. INSIGHT has a\nmodular design that allows it to be integrated into various higher education\ncourses. We analyze students' questions to an LLM by extracting keywords, which\nwe use to dynamically build an FAQ from students' questions and provide new\ninsights for the teaching staff to use for more personalized face-to-face\nsupport. Future work could build upon INSIGHT by using the collected data to\nprovide adaptive learning and adjust content based on student progress and\nlearning styles to offer a more interactive and inclusive learning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. Based on interviews with teaching staff, this paper\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\nteaching staff and students in the process of solving exercises. INSIGHT has a\nmodular design that allows it to be integrated into various higher education\ncourses. We analyze students' questions to an LLM by extracting keywords, which\nwe use to dynamically build an FAQ from students' questions and provide new\ninsights for the teaching staff to use for more personalized face-to-face\nsupport. Future work could build upon INSIGHT by using the collected data to\nprovide adaptive learning and adjust content based on student progress and\nlearning styles to offer a more interactive and inclusive learning experience."
                },
                "authors": [
                    {
                        "name": "Jarne Thys"
                    },
                    {
                        "name": "Sebe Vanbrabant"
                    },
                    {
                        "name": "Davy Vanacken"
                    },
                    {
                        "name": "Gustavo Rovelo Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Rovelo Ruiz"
                },
                "author": "Gustavo Rovelo Ruiz",
                "arxiv_comment": "Accepted author version for the D-SAIL Workshop - Transformative\n  Curriculum Design: Digitalisation, Sustainability, and AI Literacy for 21st\n  Century Learning, July 22, 2025, Palermo, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04194v2",
                "updated": "2025-06-30T17:21:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    41,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-04T17:40:55Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    40,
                    55,
                    2,
                    155,
                    0
                ],
                "title": "What Makes Treatment Effects Identifiable? Characterizations and\n  Estimators Beyond Unconfoundedness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes Treatment Effects Identifiable? Characterizations and\n  Estimators Beyond Unconfoundedness"
                },
                "summary": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions - popularly known\nas Regression Discontinuity designs - violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and necessary for the identification of ATE. Moreover, this\ncondition also characterizes the identification of the average treatment effect\non the treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions - popularly known\nas Regression Discontinuity designs - violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and necessary for the identification of ATE. Moreover, this\ncondition also characterizes the identification of the average treatment effect\non the treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms."
                },
                "authors": [
                    {
                        "name": "Yang Cai"
                    },
                    {
                        "name": "Alkis Kalavasis"
                    },
                    {
                        "name": "Katerina Mamali"
                    },
                    {
                        "name": "Anay Mehrotra"
                    },
                    {
                        "name": "Manolis Zampetakis"
                    }
                ],
                "author_detail": {
                    "name": "Manolis Zampetakis"
                },
                "author": "Manolis Zampetakis",
                "arxiv_comment": "Accepted for presentation at the 38th Conference on Learning Theory\n  (COLT) 2025. v2 strengthens results to give a tight characterization for ATE\n  identification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24068v1",
                "updated": "2025-06-30T17:21:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    8,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:21:08Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    8,
                    0,
                    181,
                    0
                ],
                "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STACK: Adversarial Attacks on LLM Safeguard Pipelines"
                },
                "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks."
                },
                "authors": [
                    {
                        "name": "Ian R. McKenzie"
                    },
                    {
                        "name": "Oskar J. Hollinsworth"
                    },
                    {
                        "name": "Tom Tseng"
                    },
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Aaron D. Tucker"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Adam Gleave"
                    }
                ],
                "author_detail": {
                    "name": "Adam Gleave"
                },
                "author": "Adam Gleave",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11933v2",
                "updated": "2025-06-30T17:17:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    17,
                    11,
                    0,
                    181,
                    0
                ],
                "published": "2024-08-21T18:36:28Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    18,
                    36,
                    28,
                    2,
                    234,
                    0
                ],
                "title": "Analysis of quasi-planar defects using the Thomas-Fermi-von Weiszacker\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of quasi-planar defects using the Thomas-Fermi-von Weiszacker\n  model"
                },
                "summary": "We analyze the convergence of the electron density and relative energy with\nrespect to a perfect crystal of a class of volume defects that are compactly\ncontained along one direction while being of infinite extent along the other\ntwo using the Thomas-Fermi-von Weiszacker (TFW) model. We take advantage of\nprior work on the thermodynamic limit and stability estimates in the TFW\nsetting, and specialize it to the case of quasi-planar defects. In particular,\nwe prove that the relative energy of the defective crystal with respect to a\nperfect crystal is finite, and in fact conforms to a well-posed minimization\nproblem. In order to show the existence of the minimization problem, we modify\nthe TFW theory for thin films and establish convergence of the electronic\nfields due to the perturbation caused by the quasi-planar defect. We also show\nthat perturbations to both the density and electrostatic potential due to the\npresence of the quasi-planar defect decay exponentially away from the defect,\nin agreement with the known locality property of the TFW model. We use these\nresults to infer bounds on the generalized stacking fault energy, in particular\nthe finiteness of this energy, and discuss its implications for numerical\ncalculations. We conclude with a brief presentation of numerical results on the\n(non-convex) Thomas-Fermi-von Weiszacker-Dirac (TFWD) model that includes the\nDirac exchange in the universal functional, and discuss its implications for\nfuture work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the convergence of the electron density and relative energy with\nrespect to a perfect crystal of a class of volume defects that are compactly\ncontained along one direction while being of infinite extent along the other\ntwo using the Thomas-Fermi-von Weiszacker (TFW) model. We take advantage of\nprior work on the thermodynamic limit and stability estimates in the TFW\nsetting, and specialize it to the case of quasi-planar defects. In particular,\nwe prove that the relative energy of the defective crystal with respect to a\nperfect crystal is finite, and in fact conforms to a well-posed minimization\nproblem. In order to show the existence of the minimization problem, we modify\nthe TFW theory for thin films and establish convergence of the electronic\nfields due to the perturbation caused by the quasi-planar defect. We also show\nthat perturbations to both the density and electrostatic potential due to the\npresence of the quasi-planar defect decay exponentially away from the defect,\nin agreement with the known locality property of the TFW model. We use these\nresults to infer bounds on the generalized stacking fault energy, in particular\nthe finiteness of this energy, and discuss its implications for numerical\ncalculations. We conclude with a brief presentation of numerical results on the\n(non-convex) Thomas-Fermi-von Weiszacker-Dirac (TFWD) model that includes the\nDirac exchange in the universal functional, and discuss its implications for\nfuture work."
                },
                "authors": [
                    {
                        "name": "Dharamveer Kumar"
                    },
                    {
                        "name": "Amuthan A. Ramabathiran"
                    }
                ],
                "author_detail": {
                    "name": "Amuthan A. Ramabathiran"
                },
                "author": "Amuthan A. Ramabathiran",
                "arxiv_comment": "43 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24062v1",
                "updated": "2025-06-30T17:13:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    13,
                    0,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:13:00Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    13,
                    0,
                    0,
                    181,
                    0
                ],
                "title": "Scout-Dose-TCM: Direct and Prospective Scout-Based Estimation of\n  Personalized Organ Doses from Tube Current Modulated CT Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scout-Dose-TCM: Direct and Prospective Scout-Based Estimation of\n  Personalized Organ Doses from Tube Current Modulated CT Exams"
                },
                "summary": "This study proposes Scout-Dose-TCM for direct, prospective estimation of\norgan-level doses under tube current modulation (TCM) and compares its\nperformance to two established methods. We analyzed contrast-enhanced\nchest-abdomen-pelvis CT scans from 130 adults (120 kVp, TCM). Reference doses\nfor six organs (lungs, kidneys, liver, pancreas, bladder, spleen) were\ncalculated using MC-GPU and TotalSegmentator. Based on these, we trained\nScout-Dose-TCM, a deep learning model that predicts organ doses corresponding\nto discrete cosine transform (DCT) basis functions, enabling real-time\nestimates for any TCM profile. The model combines a feature learning module\nthat extracts contextual information from lateral and frontal scouts and scan\nrange with a dose learning module that output DCT-based dose estimates. A\ncustomized loss function incorporated the DCT formulation during training. For\ncomparison, we implemented size-specific dose estimation per AAPM TG 204\n(Global CTDIvol) and its organ-level TCM-adapted version (Organ CTDIvol). A\n5-fold cross-validation assessed generalizability by comparing mean absolute\npercentage dose errors and r-squared correlations with benchmark doses. Average\nabsolute percentage errors were 13% (Global CTDIvol), 9% (Organ CTDIvol), and\n7% (Scout-Dose-TCM), with bladder showing the largest discrepancies (15%, 13%,\nand 9%). Statistical tests confirmed Scout-Dose-TCM significantly reduced\nerrors vs. Global CTDIvol across most organs and improved over Organ CTDIvol\nfor the liver, bladder, and pancreas. It also achieved higher r-squared values,\nindicating stronger agreement with Monte Carlo benchmarks. Scout-Dose-TCM\noutperformed Global CTDIvol and was comparable to or better than Organ CTDIvol,\nwithout requiring organ segmentations at inference, demonstrating its promise\nas a tool for prospective organ-level dose estimation in CT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes Scout-Dose-TCM for direct, prospective estimation of\norgan-level doses under tube current modulation (TCM) and compares its\nperformance to two established methods. We analyzed contrast-enhanced\nchest-abdomen-pelvis CT scans from 130 adults (120 kVp, TCM). Reference doses\nfor six organs (lungs, kidneys, liver, pancreas, bladder, spleen) were\ncalculated using MC-GPU and TotalSegmentator. Based on these, we trained\nScout-Dose-TCM, a deep learning model that predicts organ doses corresponding\nto discrete cosine transform (DCT) basis functions, enabling real-time\nestimates for any TCM profile. The model combines a feature learning module\nthat extracts contextual information from lateral and frontal scouts and scan\nrange with a dose learning module that output DCT-based dose estimates. A\ncustomized loss function incorporated the DCT formulation during training. For\ncomparison, we implemented size-specific dose estimation per AAPM TG 204\n(Global CTDIvol) and its organ-level TCM-adapted version (Organ CTDIvol). A\n5-fold cross-validation assessed generalizability by comparing mean absolute\npercentage dose errors and r-squared correlations with benchmark doses. Average\nabsolute percentage errors were 13% (Global CTDIvol), 9% (Organ CTDIvol), and\n7% (Scout-Dose-TCM), with bladder showing the largest discrepancies (15%, 13%,\nand 9%). Statistical tests confirmed Scout-Dose-TCM significantly reduced\nerrors vs. Global CTDIvol across most organs and improved over Organ CTDIvol\nfor the liver, bladder, and pancreas. It also achieved higher r-squared values,\nindicating stronger agreement with Monte Carlo benchmarks. Scout-Dose-TCM\noutperformed Global CTDIvol and was comparable to or better than Organ CTDIvol,\nwithout requiring organ segmentations at inference, demonstrating its promise\nas a tool for prospective organ-level dose estimation in CT."
                },
                "authors": [
                    {
                        "name": "Maria Jose Medrano"
                    },
                    {
                        "name": "Sen Wang"
                    },
                    {
                        "name": "Liyan Sun"
                    },
                    {
                        "name": "Abdullah-Al-Zubaer Imran"
                    },
                    {
                        "name": "Jennie Cao"
                    },
                    {
                        "name": "Grant Stevens"
                    },
                    {
                        "name": "Justin Ruey Tse"
                    },
                    {
                        "name": "Adam S. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Adam S. Wang"
                },
                "author": "Adam S. Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05651v2",
                "updated": "2025-06-30T16:54:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    54,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-08T17:53:41Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    17,
                    53,
                    41,
                    5,
                    39,
                    0
                ],
                "title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for\n  Psychotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for\n  Psychotherapy"
                },
                "summary": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI."
                },
                "authors": [
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Suyeon Lee"
                    },
                    {
                        "name": "Yeongjae Cho"
                    },
                    {
                        "name": "Eunseo Ryu"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Suran Seong"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "Accepted at NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24045v1",
                "updated": "2025-06-30T16:50:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    50,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:50:48Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    50,
                    48,
                    0,
                    181,
                    0
                ],
                "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC"
                },
                "summary": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines."
                },
                "authors": [
                    {
                        "name": "Xinming Wei"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Rui Qu"
                    },
                    {
                        "name": "Maoliang Li"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Guojie Luo"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Luo"
                },
                "author": "Guojie Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24041v1",
                "updated": "2025-06-30T16:48:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    48,
                    49,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:48:49Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    48,
                    49,
                    0,
                    181,
                    0
                ],
                "title": "Unsupervised Sparse Coding-based Spiking Neural Network for Real-time\n  Spike Sorting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Sparse Coding-based Spiking Neural Network for Real-time\n  Spike Sorting"
                },
                "summary": "Spike sorting is a crucial step in decoding multichannel extracellular neural\nsignals, enabling the identification of individual neuronal activity. A key\nchallenge in brain-machine interfaces (BMIs) is achieving real-time, low-power\nspike sorting at the edge while keeping high neural decoding performance. This\nstudy introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer\nspiking neural network optimized for efficient spike sorting. NSS leverages the\nLocally Competitive Algorithm (LCA) for sparse coding to extract relevant\nfeatures from noisy events with reduced computational demands. NSS learns to\nsort detected spike waveforms in an online fashion and operates entirely\nunsupervised. To exploit multi-bit spike coding capabilities of neuromorphic\nplatforms like Intel's Loihi 2, a custom neuron model was implemented, enabling\nflexible power-performance trade-offs via adjustable spike bit-widths.\nEvaluations on simulated and real-world tetrode signals with biological drift\nshowed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans.\nWith 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with\nleaky integrate-and-fire neuron and achieved an F1-score of 77% (+10%\nimprovement) while consuming 8.6mW (+1.65mW) when tested on a drifting\nrecording, with a computational processing time of 0.25ms (+60 us) per\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spike sorting is a crucial step in decoding multichannel extracellular neural\nsignals, enabling the identification of individual neuronal activity. A key\nchallenge in brain-machine interfaces (BMIs) is achieving real-time, low-power\nspike sorting at the edge while keeping high neural decoding performance. This\nstudy introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer\nspiking neural network optimized for efficient spike sorting. NSS leverages the\nLocally Competitive Algorithm (LCA) for sparse coding to extract relevant\nfeatures from noisy events with reduced computational demands. NSS learns to\nsort detected spike waveforms in an online fashion and operates entirely\nunsupervised. To exploit multi-bit spike coding capabilities of neuromorphic\nplatforms like Intel's Loihi 2, a custom neuron model was implemented, enabling\nflexible power-performance trade-offs via adjustable spike bit-widths.\nEvaluations on simulated and real-world tetrode signals with biological drift\nshowed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans.\nWith 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with\nleaky integrate-and-fire neuron and achieved an F1-score of 77% (+10%\nimprovement) while consuming 8.6mW (+1.65mW) when tested on a drifting\nrecording, with a computational processing time of 0.25ms (+60 us) per\ninference."
                },
                "authors": [
                    {
                        "name": "Alexis Melot"
                    },
                    {
                        "name": "Sean U. N. Wood"
                    },
                    {
                        "name": "Yannick Coffinier"
                    },
                    {
                        "name": "Pierre Yger"
                    },
                    {
                        "name": "Fabien Alibart"
                    }
                ],
                "author_detail": {
                    "name": "Fabien Alibart"
                },
                "author": "Fabien Alibart",
                "arxiv_comment": "Main article : 16 pages, 7 figures and 4 tables. Supplementary\n  Material starts at page 17 with 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00306v2",
                "updated": "2025-06-30T16:37:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    37,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-01T04:01:18Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    1,
                    18,
                    5,
                    32,
                    0
                ],
                "title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference."
                },
                "authors": [
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Yuefeng Peng"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Alina Oprea"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "arxiv_comment": "This is the full version (27 pages) of the paper 'Riddle Me This!\n  Stealthy Membership Inference for Retrieval-Augmented Generation' published\n  at CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08842v2",
                "updated": "2025-06-30T16:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    31,
                    53,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-13T12:58:11Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    58,
                    11,
                    1,
                    133,
                    0
                ],
                "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for\n  Uncovering Hidden Vulnerabilities in Open-Source AI Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for\n  Uncovering Hidden Vulnerabilities in Open-Source AI Libraries"
                },
                "summary": "Open-source AI libraries are foundational to modern AI systems, yet they\npresent significant, underexamined risks spanning security, licensing,\nmaintenance, supply chain integrity, and regulatory compliance. We introduce\nLibVulnWatch, a system that leverages recent advances in large language models\nand agentic workflows to perform deep, evidence-based evaluations of these\nlibraries. Built on a graph-based orchestration of specialized agents, the\nframework extracts, verifies, and quantifies risk using information from\nrepositories, documentation, and vulnerability databases. LibVulnWatch produces\nreproducible, governance-aligned scores across five critical domains,\npublishing results to a public leaderboard for ongoing ecosystem monitoring.\nApplied to 20 widely used libraries, including ML frameworks, LLM inference\nengines, and agent orchestration tools, our approach covers up to 88% of\nOpenSSF Scorecard checks while surfacing up to 19 additional risks per library,\nsuch as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By\nintegrating advanced language technologies with the practical demands of\nsoftware risk assessment, this work demonstrates a scalable, transparent\nmechanism for continuous supply chain evaluation and informed library\nselection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source AI libraries are foundational to modern AI systems, yet they\npresent significant, underexamined risks spanning security, licensing,\nmaintenance, supply chain integrity, and regulatory compliance. We introduce\nLibVulnWatch, a system that leverages recent advances in large language models\nand agentic workflows to perform deep, evidence-based evaluations of these\nlibraries. Built on a graph-based orchestration of specialized agents, the\nframework extracts, verifies, and quantifies risk using information from\nrepositories, documentation, and vulnerability databases. LibVulnWatch produces\nreproducible, governance-aligned scores across five critical domains,\npublishing results to a public leaderboard for ongoing ecosystem monitoring.\nApplied to 20 widely used libraries, including ML frameworks, LLM inference\nengines, and agent orchestration tools, our approach covers up to 88% of\nOpenSSF Scorecard checks while surfacing up to 19 additional risks per library,\nsuch as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By\nintegrating advanced language technologies with the practical demands of\nsoftware risk assessment, this work demonstrates a scalable, transparent\nmechanism for continuous supply chain evaluation and informed library\nselection."
                },
                "authors": [
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Seonglae Cho"
                    },
                    {
                        "name": "Umar Mohammed"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Kleyton Costa"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Koshiyama"
                },
                "author": "Adriano Koshiyama",
                "arxiv_comment": "ACL 2025 Student Research Workshop and ICML 2025 TAIG Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06926v3",
                "updated": "2025-06-30T16:30:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    30,
                    42,
                    0,
                    181,
                    0
                ],
                "published": "2025-01-12T20:35:28Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    35,
                    28,
                    6,
                    12,
                    0
                ],
                "title": "Semiparametric Double Reinforcement Learning with Applications to\n  Long-Term Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric Double Reinforcement Learning with Applications to\n  Long-Term Causal Inference"
                },
                "summary": "Long-term causal effects often must be estimated from short-term data due to\nlimited follow-up in healthcare, economics, and online platforms. Markov\nDecision Processes (MDPs) provide a natural framework for capturing such\nlong-term dynamics through sequences of states, actions, and rewards. Double\nReinforcement Learning (DRL) enables efficient inference on policy values in\nMDPs, but nonparametric implementations require strong intertemporal overlap\nassumptions and often exhibit high variance and instability. We propose a\nsemiparametric extension of DRL for efficient inference on linear functionals\nof the Q-function--such as policy values--in infinite-horizon, time-homogeneous\nMDPs. By imposing structural restrictions on the Q-function, our approach\nrelaxes the strong overlap conditions required by nonparametric methods and\nimproves statistical efficiency. Under model misspecification, our estimators\ntarget the functional of the best-approximating Q-function, with only\nsecond-order bias. We provide conditions for valid inference using sieve\nmethods and data-driven model selection. A central challenge in DRL is the\nestimation of nuisance functions, such as density ratios, which often involve\ndifficult minimax optimization. To address this, we introduce a novel plug-in\nestimator based on isotonic Bellman calibration, which combines fitted\nQ-iteration with an isotonic regression adjustment. The estimator is debiased\nwithout requiring estimation of additional nuisance functions and reduces\nhigh-dimensional overlap assumptions to a one-dimensional condition. Bellman\ncalibration extends isotonic calibration--widely used in prediction and\nclassification--to the MDP setting and may be of independent interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term causal effects often must be estimated from short-term data due to\nlimited follow-up in healthcare, economics, and online platforms. Markov\nDecision Processes (MDPs) provide a natural framework for capturing such\nlong-term dynamics through sequences of states, actions, and rewards. Double\nReinforcement Learning (DRL) enables efficient inference on policy values in\nMDPs, but nonparametric implementations require strong intertemporal overlap\nassumptions and often exhibit high variance and instability. We propose a\nsemiparametric extension of DRL for efficient inference on linear functionals\nof the Q-function--such as policy values--in infinite-horizon, time-homogeneous\nMDPs. By imposing structural restrictions on the Q-function, our approach\nrelaxes the strong overlap conditions required by nonparametric methods and\nimproves statistical efficiency. Under model misspecification, our estimators\ntarget the functional of the best-approximating Q-function, with only\nsecond-order bias. We provide conditions for valid inference using sieve\nmethods and data-driven model selection. A central challenge in DRL is the\nestimation of nuisance functions, such as density ratios, which often involve\ndifficult minimax optimization. To address this, we introduce a novel plug-in\nestimator based on isotonic Bellman calibration, which combines fitted\nQ-iteration with an isotonic regression adjustment. The estimator is debiased\nwithout requiring estimation of additional nuisance functions and reduces\nhigh-dimensional overlap assumptions to a one-dimensional condition. Bellman\ncalibration extends isotonic calibration--widely used in prediction and\nclassification--to the MDP setting and may be of independent interest."
                },
                "authors": [
                    {
                        "name": "Lars van der Laan"
                    },
                    {
                        "name": "David Hubbard"
                    },
                    {
                        "name": "Allen Tran"
                    },
                    {
                        "name": "Nathan Kallus"
                    },
                    {
                        "name": "Aurlien Bibaut"
                    }
                ],
                "author_detail": {
                    "name": "Aurlien Bibaut"
                },
                "author": "Aurlien Bibaut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24015v1",
                "updated": "2025-06-30T16:19:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:19:38Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "title": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via\n  Layered Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via\n  Layered Knowledge Injection"
                },
                "summary": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24013v1",
                "updated": "2025-06-30T16:18:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    18,
                    6,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:18:06Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    18,
                    6,
                    0,
                    181,
                    0
                ],
                "title": "CoMMiT: Co-informed inference of microbiome-metabolome interactions via\n  transfer learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMMiT: Co-informed inference of microbiome-metabolome interactions via\n  transfer learning"
                },
                "summary": "Recent multi-omic microbiome studies enable integrative analysis of microbes\nand metabolites, uncovering their associations with various host conditions.\nSuch analyses require multivariate models capable of accounting for the complex\ncorrelation structures between microbes and metabolites. However, existing\nmultivariate models often suffer from low statistical power for detecting\nmicrobiome-metabolome interactions due to small sample sizes and weak\nbiological signals. To address these challenges, we introduce CoMMiT,\nCo-informed inference of Microbiome-Metabolome Interactions via novel Transfer\nlearning models. Unlike conventional transfer-learning methods that borrow\ninformation from external datasets, CoMMiT leverages similarities across\nmetabolites within a single cohort, reducing the risk of negative transfer\noften caused by differences in sequencing platforms and bioinformatic pipelines\nacross studies. CoMMiT operates under the flexible assumption that auxiliary\nmetabolites are collectively informative for the target metabolite, without\nrequiring individual auxiliary metabolites to be informative. CoMMiT uses a\nnovel data-driven approach to selecting the optimal set of auxiliary\nmetabolites. Using this optimal set, CoMMiT employs a de-biasing framework to\nenable efficient calculation of p-values, facilitating the identification of\nstatistically significant microbiome-metabolome interactions. Applying CoMMiT\nto a feeding study reveals biologically meaningful microbiome-metabolome\ninteractions under a low glycemic load diet, demonstrating the diet-host link\nthrough gut metabolism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multi-omic microbiome studies enable integrative analysis of microbes\nand metabolites, uncovering their associations with various host conditions.\nSuch analyses require multivariate models capable of accounting for the complex\ncorrelation structures between microbes and metabolites. However, existing\nmultivariate models often suffer from low statistical power for detecting\nmicrobiome-metabolome interactions due to small sample sizes and weak\nbiological signals. To address these challenges, we introduce CoMMiT,\nCo-informed inference of Microbiome-Metabolome Interactions via novel Transfer\nlearning models. Unlike conventional transfer-learning methods that borrow\ninformation from external datasets, CoMMiT leverages similarities across\nmetabolites within a single cohort, reducing the risk of negative transfer\noften caused by differences in sequencing platforms and bioinformatic pipelines\nacross studies. CoMMiT operates under the flexible assumption that auxiliary\nmetabolites are collectively informative for the target metabolite, without\nrequiring individual auxiliary metabolites to be informative. CoMMiT uses a\nnovel data-driven approach to selecting the optimal set of auxiliary\nmetabolites. Using this optimal set, CoMMiT employs a de-biasing framework to\nenable efficient calculation of p-values, facilitating the identification of\nstatistically significant microbiome-metabolome interactions. Applying CoMMiT\nto a feeding study reveals biologically meaningful microbiome-metabolome\ninteractions under a low glycemic load diet, demonstrating the diet-host link\nthrough gut metabolism."
                },
                "authors": [
                    {
                        "name": "Leiyue Li"
                    },
                    {
                        "name": "Chenglong Ye"
                    },
                    {
                        "name": "Tim Randolph"
                    },
                    {
                        "name": "Meredith Hullar"
                    },
                    {
                        "name": "Johanna Lampe"
                    },
                    {
                        "name": "Marian Neuhouser"
                    },
                    {
                        "name": "Daniel Raftery"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "arxiv_comment": "38 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20073v2",
                "updated": "2025-06-30T16:15:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    15,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2024-10-26T04:31:17Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    4,
                    31,
                    17,
                    5,
                    300,
                    0
                ],
                "title": "Pixel super-resolved virtual staining of label-free tissue using\n  diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixel super-resolved virtual staining of label-free tissue using\n  diffusion models"
                },
                "summary": "Virtual staining of tissue offers a powerful tool for transforming label-free\nmicroscopy images of unstained tissue into equivalents of histochemically\nstained samples. This study presents a diffusion model-based super-resolution\nvirtual staining approach utilizing a Brownian bridge process to enhance both\nthe spatial resolution and fidelity of label-free virtual tissue staining,\naddressing the limitations of traditional deep learning-based methods. Our\napproach integrates novel sampling techniques into a diffusion model-based\nimage inference process to significantly reduce the variance in the generated\nvirtually stained images, resulting in more stable and accurate outputs.\nBlindly applied to lower-resolution auto-fluorescence images of label-free\nhuman lung tissue samples, the diffusion-based super-resolution virtual\nstaining model consistently outperformed conventional approaches in resolution,\nstructural similarity and perceptual accuracy, successfully achieving a\nsuper-resolution factor of 4-5x, increasing the output space-bandwidth product\nby 16-25-fold compared to the input label-free microscopy images.\nDiffusion-based super-resolved virtual tissue staining not only improves\nresolution and image quality but also enhances the reliability of virtual\nstaining without traditional chemical staining, offering significant potential\nfor clinical diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual staining of tissue offers a powerful tool for transforming label-free\nmicroscopy images of unstained tissue into equivalents of histochemically\nstained samples. This study presents a diffusion model-based super-resolution\nvirtual staining approach utilizing a Brownian bridge process to enhance both\nthe spatial resolution and fidelity of label-free virtual tissue staining,\naddressing the limitations of traditional deep learning-based methods. Our\napproach integrates novel sampling techniques into a diffusion model-based\nimage inference process to significantly reduce the variance in the generated\nvirtually stained images, resulting in more stable and accurate outputs.\nBlindly applied to lower-resolution auto-fluorescence images of label-free\nhuman lung tissue samples, the diffusion-based super-resolution virtual\nstaining model consistently outperformed conventional approaches in resolution,\nstructural similarity and perceptual accuracy, successfully achieving a\nsuper-resolution factor of 4-5x, increasing the output space-bandwidth product\nby 16-25-fold compared to the input label-free microscopy images.\nDiffusion-based super-resolved virtual tissue staining not only improves\nresolution and image quality but also enhances the reliability of virtual\nstaining without traditional chemical staining, offering significant potential\nfor clinical diagnostics."
                },
                "authors": [
                    {
                        "name": "Yijie Zhang"
                    },
                    {
                        "name": "Luzhe Huang"
                    },
                    {
                        "name": "Nir Pillar"
                    },
                    {
                        "name": "Yuzhu Li"
                    },
                    {
                        "name": "Hanlong Chen"
                    },
                    {
                        "name": "Aydogan Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Aydogan Ozcan"
                },
                "author": "Aydogan Ozcan",
                "arxiv_doi": "10.1038/s41467-025-60387-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41467-025-60387-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.20073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "39 Pages, 7 Figures",
                "arxiv_journal_ref": "Nature Communications (2025)",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24010v1",
                "updated": "2025-06-30T16:14:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    14,
                    9,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:14:09Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    14,
                    9,
                    0,
                    181,
                    0
                ],
                "title": "Anchoring Stellar Age Indicators: A Cross-Calibration of [C/N] and\n  Gyrochronology Ages via the Age-Velocity-Dispersion Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchoring Stellar Age Indicators: A Cross-Calibration of [C/N] and\n  Gyrochronology Ages via the Age-Velocity-Dispersion Relation"
                },
                "summary": "Determining stellar ages is challenging, as it depends on other stellar\nparameters in a non-linear way and often relies on stellar evolution models to\ninfer the underlying relation between these parameters and age. This complexity\nincreases when comparing different age-dating methods, as they rely on distinct\nindicators and are often applicable to non-overlapping regions of the\ncolor-magnitude diagram. Moreover, many empirical calibration methods rely on\npre-determined ages, often from open clusters or asteroseismology, which only\ncover a limited parameter space. Fortunately, the age-velocity-dispersion\nrelation (AVR), in which the velocity dispersion increases with age, is a\nuniversal feature among stars of all evolutionary stages. In this paper, we 1)\nexplore the parameter space in which [C/N] and gyrochronology are applicable,\nextending beyond the domains probed by asteroseismology and open clusters, and\n2) assess whether the traditionally assumed [C/N] and gyrochronology relations\nyield ages on a consistent physical scale, after calibrating both using the\nsame AVR. We find gyrochronology can be applied to all partially convective\nstars after they have converged onto the slow rotating sequence and before they\nexperience weakened magnetic braking; [C/N] can be used to infer ages for all\ngiants with metallicity > -0.8 dex and [C/N] < -0.05 dex, and can be used as an\nage-indicator down to [Fe/H] of -1 dex if only selecting the low-$\\alpha$ disk.\nLastly, ages obtained from [C/N] and gyrochronology agree within uncertainty\nafter accounting for systematic offsets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining stellar ages is challenging, as it depends on other stellar\nparameters in a non-linear way and often relies on stellar evolution models to\ninfer the underlying relation between these parameters and age. This complexity\nincreases when comparing different age-dating methods, as they rely on distinct\nindicators and are often applicable to non-overlapping regions of the\ncolor-magnitude diagram. Moreover, many empirical calibration methods rely on\npre-determined ages, often from open clusters or asteroseismology, which only\ncover a limited parameter space. Fortunately, the age-velocity-dispersion\nrelation (AVR), in which the velocity dispersion increases with age, is a\nuniversal feature among stars of all evolutionary stages. In this paper, we 1)\nexplore the parameter space in which [C/N] and gyrochronology are applicable,\nextending beyond the domains probed by asteroseismology and open clusters, and\n2) assess whether the traditionally assumed [C/N] and gyrochronology relations\nyield ages on a consistent physical scale, after calibrating both using the\nsame AVR. We find gyrochronology can be applied to all partially convective\nstars after they have converged onto the slow rotating sequence and before they\nexperience weakened magnetic braking; [C/N] can be used to infer ages for all\ngiants with metallicity > -0.8 dex and [C/N] < -0.05 dex, and can be used as an\nage-indicator down to [Fe/H] of -1 dex if only selecting the low-$\\alpha$ disk.\nLastly, ages obtained from [C/N] and gyrochronology agree within uncertainty\nafter accounting for systematic offsets."
                },
                "authors": [
                    {
                        "name": "Yuxi Lu"
                    },
                    {
                        "name": "Marc H. Pinsonneault"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Phil R. Van-Lane"
                    },
                    {
                        "name": "John D Roberts"
                    },
                    {
                        "name": "Jamie Tayar"
                    },
                    {
                        "name": "Alexander Stone-Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Stone-Martinez"
                },
                "author": "Alexander Stone-Martinez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24006v1",
                "updated": "2025-06-30T16:10:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    10,
                    42,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:10:42Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    10,
                    42,
                    0,
                    181,
                    0
                ],
                "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping\n  Review from a Mathematics Education Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Don't Make Sense of Word Problems. A Scoping\n  Review from a Mathematics Education Perspective"
                },
                "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms."
                },
                "authors": [
                    {
                        "name": "Anselm R. Strohmaier"
                    },
                    {
                        "name": "Wim Van Dooren"
                    },
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Brian Greer"
                    },
                    {
                        "name": "Lieven Verschaffel"
                    }
                ],
                "author_detail": {
                    "name": "Lieven Verschaffel"
                },
                "author": "Lieven Verschaffel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.HO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14607v2",
                "updated": "2025-06-30T16:09:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    9,
                    19,
                    0,
                    181,
                    0
                ],
                "published": "2025-01-24T16:24:15Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    24,
                    15,
                    4,
                    24,
                    0
                ],
                "title": "ReferDINO: Referring Video Object Segmentation with Visual Grounding\n  Foundations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReferDINO: Referring Video Object Segmentation with Visual Grounding\n  Foundations"
                },
                "summary": "Referring video object segmentation (RVOS) aims to segment target objects\nthroughout a video based on a text description. This is challenging as it\ninvolves deep vision-language understanding, pixel-level dense prediction and\nspatiotemporal reasoning. Despite notable progress in recent years, existing\nmethods still exhibit a noticeable gap when considering all these aspects. In\nthis work, we propose \\textbf{ReferDINO}, a strong RVOS model that inherits\nregion-level vision-language alignment from foundational visual grounding\nmodels, and is further endowed with pixel-level dense perception and\ncross-modal spatiotemporal reasoning. In detail, ReferDINO integrates two key\ncomponents: 1) a grounding-guided deformable mask decoder that utilizes\nlocation prediction to progressively guide mask prediction through\ndifferentiable deformation mechanisms; 2) an object-consistent temporal\nenhancer that injects pretrained time-varying text features into inter-frame\ninteraction to capture object-aware dynamic changes. Moreover, a\nconfidence-aware query pruning strategy is designed to accelerate object\ndecoding without compromising model performance. Extensive experimental results\non five benchmarks demonstrate that our ReferDINO significantly outperforms\nprevious methods (e.g., +3.9% (\\mathcal{J}&\\mathcal{F}) on Ref-YouTube-VOS)\nwith real-time inference speed (51 FPS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referring video object segmentation (RVOS) aims to segment target objects\nthroughout a video based on a text description. This is challenging as it\ninvolves deep vision-language understanding, pixel-level dense prediction and\nspatiotemporal reasoning. Despite notable progress in recent years, existing\nmethods still exhibit a noticeable gap when considering all these aspects. In\nthis work, we propose \\textbf{ReferDINO}, a strong RVOS model that inherits\nregion-level vision-language alignment from foundational visual grounding\nmodels, and is further endowed with pixel-level dense perception and\ncross-modal spatiotemporal reasoning. In detail, ReferDINO integrates two key\ncomponents: 1) a grounding-guided deformable mask decoder that utilizes\nlocation prediction to progressively guide mask prediction through\ndifferentiable deformation mechanisms; 2) an object-consistent temporal\nenhancer that injects pretrained time-varying text features into inter-frame\ninteraction to capture object-aware dynamic changes. Moreover, a\nconfidence-aware query pruning strategy is designed to accelerate object\ndecoding without compromising model performance. Extensive experimental results\non five benchmarks demonstrate that our ReferDINO significantly outperforms\nprevious methods (e.g., +3.9% (\\mathcal{J}&\\mathcal{F}) on Ref-YouTube-VOS)\nwith real-time inference speed (51 FPS)."
                },
                "authors": [
                    {
                        "name": "Tianming Liang"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Chaolei Tan"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    },
                    {
                        "name": "Jian-Fang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Fang Hu"
                },
                "author": "Jian-Fang Hu",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  \\url{https://isee-laboratory.github.io/ReferDINO}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24000v1",
                "updated": "2025-06-30T16:05:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    5,
                    55,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:05:55Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    5,
                    55,
                    0,
                    181,
                    0
                ],
                "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for\n  Vision-Language Models"
                },
                "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies."
                },
                "authors": [
                    {
                        "name": "Lijun Sheng"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Zilei Wang"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Github link: https://github.com/TomSheng21/tta-vlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23998v1",
                "updated": "2025-06-30T16:02:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    2,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:02:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    2,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via\n  Multi-Agent Large Language Models with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via\n  Multi-Agent Large Language Models with Reinforcement Learning"
                },
                "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts."
                },
                "authors": [
                    {
                        "name": "Seungjun Yi"
                    },
                    {
                        "name": "Joakim Nguyen"
                    },
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Terence Lim"
                    },
                    {
                        "name": "Andrew Well"
                    },
                    {
                        "name": "Mia Markey"
                    },
                    {
                        "name": "Ying Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ying Ding"
                },
                "author": "Ying Ding",
                "arxiv_comment": "Presented at ACL 2025 SRW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16084v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16084v3",
                "updated": "2025-06-30T15:59:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    59,
                    26,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-22T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "TTRL: Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTRL: Test-Time Reinforcement Learning"
                },
                "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16084v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16084v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23986v1",
                "updated": "2025-06-30T15:50:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    50,
                    8,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:50:08Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    50,
                    8,
                    0,
                    181,
                    0
                ],
                "title": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention\n  Mask for Speech Token Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention\n  Mask for Speech Token Decoding"
                },
                "summary": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}"
                },
                "authors": [
                    {
                        "name": "Dake Guo"
                    },
                    {
                        "name": "Jixun Yao"
                    },
                    {
                        "name": "Linhan Ma"
                    },
                    {
                        "name": "Wang He"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23982v1",
                "updated": "2025-06-30T15:48:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    48,
                    38,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:48:38Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    48,
                    38,
                    0,
                    181,
                    0
                ],
                "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End\n  Autonomous Driving"
                },
                "summary": "While personalization has been explored in traditional autonomous driving\nsystems, it remains largely overlooked in end-to-end autonomous driving\n(E2EAD), despite its growing prominence. This gap is critical, as user-aligned\nbehavior is essential for trust, comfort, and widespread adoption of autonomous\nvehicles. A core challenge is the lack of large-scale real-world datasets\nannotated with diverse and fine-grained driving preferences, hindering the\ndevelopment and evaluation of personalized E2EAD models. In this work, we\npresent the first large-scale real-world dataset enriched with annotations\ncapturing diverse driving preferences, establishing a foundation for\npersonalization in E2EAD. We extract static environmental features from\nreal-world road topology and infer dynamic contextual cues using a fine-tuned\nvisual language model (VLM), enabling consistent and fine-grained scenario\nconstruction. Based on these scenarios, we derive objective preference\nannotations through behavioral distribution analysis and rule-based heuristics.\nTo address the inherent subjectivity of driving style, we further employ the\nVLM to generate subjective annotations by jointly modeling scene semantics and\ndriver behavior. Final high-quality labels are obtained through a\nhuman-in-the-loop verification process that fuses both perspectives. Building\non this dataset, we propose the first benchmark for evaluating personalized\nE2EAD models. We assess several state-of-the-art models with and without\npreference conditioning, demonstrating that incorporating personalized\npreferences results in behavior more aligned with human driving. Our work lays\nthe foundation for personalized E2EAD by providing a standardized platform to\nsystematically integrate human preferences into data-driven E2EAD systems,\ncatalyzing future research in human-centric autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While personalization has been explored in traditional autonomous driving\nsystems, it remains largely overlooked in end-to-end autonomous driving\n(E2EAD), despite its growing prominence. This gap is critical, as user-aligned\nbehavior is essential for trust, comfort, and widespread adoption of autonomous\nvehicles. A core challenge is the lack of large-scale real-world datasets\nannotated with diverse and fine-grained driving preferences, hindering the\ndevelopment and evaluation of personalized E2EAD models. In this work, we\npresent the first large-scale real-world dataset enriched with annotations\ncapturing diverse driving preferences, establishing a foundation for\npersonalization in E2EAD. We extract static environmental features from\nreal-world road topology and infer dynamic contextual cues using a fine-tuned\nvisual language model (VLM), enabling consistent and fine-grained scenario\nconstruction. Based on these scenarios, we derive objective preference\nannotations through behavioral distribution analysis and rule-based heuristics.\nTo address the inherent subjectivity of driving style, we further employ the\nVLM to generate subjective annotations by jointly modeling scene semantics and\ndriver behavior. Final high-quality labels are obtained through a\nhuman-in-the-loop verification process that fuses both perspectives. Building\non this dataset, we propose the first benchmark for evaluating personalized\nE2EAD models. We assess several state-of-the-art models with and without\npreference conditioning, demonstrating that incorporating personalized\npreferences results in behavior more aligned with human driving. Our work lays\nthe foundation for personalized E2EAD by providing a standardized platform to\nsystematically integrate human preferences into data-driven E2EAD systems,\ncatalyzing future research in human-centric autonomy."
                },
                "authors": [
                    {
                        "name": "Ruiyang Hao"
                    },
                    {
                        "name": "Bowen Jing"
                    },
                    {
                        "name": "Haibao Yu"
                    },
                    {
                        "name": "Zaiqing Nie"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqing Nie"
                },
                "author": "Zaiqing Nie",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23979v1",
                "updated": "2025-06-30T15:45:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:45:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference\n  Data Generation"
                },
                "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger."
                },
                "authors": [
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Tianhao Shen"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Dan Shi"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Wuwei Huang"
                    },
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "33 pages, 15 tables, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23978v1",
                "updated": "2025-06-30T15:45:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    17,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:45:17Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    17,
                    0,
                    181,
                    0
                ],
                "title": "LLM Agents Are the Antidote to Walled Gardens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents Are the Antidote to Walled Gardens"
                },
                "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security."
                },
                "authors": [
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Philip Torr"
                    }
                ],
                "author_detail": {
                    "name": "Philip Torr"
                },
                "author": "Philip Torr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68M10, 91B26",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; H.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23967v1",
                "updated": "2025-06-30T15:36:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    53,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:36:53Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    53,
                    0,
                    181,
                    0
                ],
                "title": "Green Metrics Tool: Measuring for fun and profit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green Metrics Tool: Measuring for fun and profit"
                },
                "summary": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software."
                },
                "authors": [
                    {
                        "name": "Geerd-Dietger Hoffmann"
                    },
                    {
                        "name": "Verena Majuntke"
                    }
                ],
                "author_detail": {
                    "name": "Verena Majuntke"
                },
                "author": "Verena Majuntke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23964v1",
                "updated": "2025-06-30T15:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    22,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    22,
                    0,
                    181,
                    0
                ],
                "title": "Learning Constraints Directly from Network Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Constraints Directly from Network Data"
                },
                "summary": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference."
                },
                "authors": [
                    {
                        "name": "Hongyu H"
                    },
                    {
                        "name": "Minhao Jin"
                    },
                    {
                        "name": "Maria Apostolaki"
                    }
                ],
                "author_detail": {
                    "name": "Maria Apostolaki"
                },
                "author": "Maria Apostolaki",
                "arxiv_comment": "13 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.3; I.2.6; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06639v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06639v5",
                "updated": "2025-06-30T15:35:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    35,
                    3,
                    0,
                    181,
                    0
                ],
                "published": "2024-07-09T08:07:39Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    8,
                    7,
                    39,
                    1,
                    191,
                    0
                ],
                "title": "Learning Li-ion battery health and degradation modes from data with\n  aging-aware circuit models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Li-ion battery health and degradation modes from data with\n  aging-aware circuit models"
                },
                "summary": "Non-invasive estimation of Li-ion battery state-of-health from operational\ndata is valuable for battery applications, but remains challenging. Pure\nmodel-based methods may suffer from inaccuracy and long-term instability of\nparameter estimates, whereas pure data-driven methods rely heavily on training\ndata quality and quantity, causing lack of generality when extrapolating to\nunseen cases. We apply an aging-aware equivalent circuit model for health\nestimation, combining the flexibility of data-driven techniques within a\nmodel-based approach. A simplified electrical model with voltage source and\nresistor incorporates Gaussian process regression to learn capacity fade over\ntime and also the dependence of resistance on operating conditions and time.\nThe approach was validated against two datasets and shown to give accurate\nperformance with less than 1% relative root mean square error (RMSE) in\ncapacity and less than 2% mean absolute percentage error (MAPE). Critically, we\nshow that the open circuit voltage versus state-of-charge function must be\naccurately known, and any inaccuracies or changes in this over time strongly\ninfluence the inferred resistance. However, this feature (or bug) may also be\nused to estimate in operando differential voltage curves from operational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-invasive estimation of Li-ion battery state-of-health from operational\ndata is valuable for battery applications, but remains challenging. Pure\nmodel-based methods may suffer from inaccuracy and long-term instability of\nparameter estimates, whereas pure data-driven methods rely heavily on training\ndata quality and quantity, causing lack of generality when extrapolating to\nunseen cases. We apply an aging-aware equivalent circuit model for health\nestimation, combining the flexibility of data-driven techniques within a\nmodel-based approach. A simplified electrical model with voltage source and\nresistor incorporates Gaussian process regression to learn capacity fade over\ntime and also the dependence of resistance on operating conditions and time.\nThe approach was validated against two datasets and shown to give accurate\nperformance with less than 1% relative root mean square error (RMSE) in\ncapacity and less than 2% mean absolute percentage error (MAPE). Critically, we\nshow that the open circuit voltage versus state-of-charge function must be\naccurately known, and any inaccuracies or changes in this over time strongly\ninfluence the inferred resistance. However, this feature (or bug) may also be\nused to estimate in operando differential voltage curves from operational data."
                },
                "authors": [
                    {
                        "name": "Zihao Zhou"
                    },
                    {
                        "name": "Antti Aitio"
                    },
                    {
                        "name": "David Howey"
                    }
                ],
                "author_detail": {
                    "name": "David Howey"
                },
                "author": "David Howey",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06639v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06639v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03668v2",
                "updated": "2025-06-30T15:26:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    26,
                    30,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-19T15:35:56Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    35,
                    56,
                    2,
                    78,
                    0
                ],
                "title": "Intelligent Orchestration of Distributed Large Foundation Model\n  Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Orchestration of Distributed Large Foundation Model\n  Inference at the Edge"
                },
                "summary": "Large Foundation Models (LFMs), including multi-modal and generative models,\npromise to unlock new capabilities for next-generation Edge AI applications.\nHowever, performing inference with LFMs in resource-constrained and\nheterogeneous edge environments, such as Multi-access Edge Computing (MEC),\npresents significant challenges for workload orchestration due to time-varying\nnetwork, compute, and storage conditions. In particular, current split\ninference strategies, which partition LFM layers across nodes, are not designed\nto adapt to fluctuating workloads, dynamic bandwidth conditions, or evolving\nprivacy constraints in high-utilization MEC environments. In this work, we\npropose a novel adaptive split inference orchestration framework that elevates\nboth the placement and partitioning of LFM layers to runtime-tunable variables.\nSpecifically, our framework enables real-time, quality-of-service (QoS)-aware\nmanagement of inference workloads by extending conventional orchestrators with\nthree key services: (1) Capacity-aware workload distribution, which\ncontinuously profiles node resources and selects an optimal subset of MEC\nnodes; (2) Dynamic partition migration, which transparently relocates pre-cut\nLFM segments in response to changes in utilization or network conditions; (3)\nReal-time reconfiguration, which dynamically re-splits LFM layers to balance\nlatency, throughput, and privacy. We formalize the joint placement-partitioning\nproblem, outline a reference architecture and algorithmic workflow, and discuss\napplicability in representative smart city, V2X, and industrial edge scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Foundation Models (LFMs), including multi-modal and generative models,\npromise to unlock new capabilities for next-generation Edge AI applications.\nHowever, performing inference with LFMs in resource-constrained and\nheterogeneous edge environments, such as Multi-access Edge Computing (MEC),\npresents significant challenges for workload orchestration due to time-varying\nnetwork, compute, and storage conditions. In particular, current split\ninference strategies, which partition LFM layers across nodes, are not designed\nto adapt to fluctuating workloads, dynamic bandwidth conditions, or evolving\nprivacy constraints in high-utilization MEC environments. In this work, we\npropose a novel adaptive split inference orchestration framework that elevates\nboth the placement and partitioning of LFM layers to runtime-tunable variables.\nSpecifically, our framework enables real-time, quality-of-service (QoS)-aware\nmanagement of inference workloads by extending conventional orchestrators with\nthree key services: (1) Capacity-aware workload distribution, which\ncontinuously profiles node resources and selects an optimal subset of MEC\nnodes; (2) Dynamic partition migration, which transparently relocates pre-cut\nLFM segments in response to changes in utilization or network conditions; (3)\nReal-time reconfiguration, which dynamically re-splits LFM layers to balance\nlatency, throughput, and privacy. We formalize the joint placement-partitioning\nproblem, outline a reference architecture and algorithmic workflow, and discuss\napplicability in representative smart city, V2X, and industrial edge scenarios."
                },
                "authors": [
                    {
                        "name": "Fernando Koch"
                    },
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Alecio Binotto"
                    }
                ],
                "author_detail": {
                    "name": "Alecio Binotto"
                },
                "author": "Alecio Binotto",
                "arxiv_comment": "25 pages, 3 figures, 4 tables, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23951v1",
                "updated": "2025-06-30T15:18:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    18,
                    50,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:18:50Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    18,
                    50,
                    0,
                    181,
                    0
                ],
                "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction\n  of influential and interpretable concepts with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Decision-Making in LLMs for Text Classification : Extraction\n  of influential and interpretable concepts with Sparse Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features."
                },
                "authors": [
                    {
                        "name": "Mathis Le Bail"
                    },
                    {
                        "name": "Jrmie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23940v2",
                "updated": "2025-07-01T03:26:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    26,
                    52,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T15:07:41Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    7,
                    41,
                    0,
                    181,
                    0
                ],
                "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy\n  for MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy\n  for MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Yang Dai"
                    },
                    {
                        "name": "Jianxiang An"
                    },
                    {
                        "name": "Tianwei Lin"
                    },
                    {
                        "name": "Hongyang He"
                    },
                    {
                        "name": "Hongzhe Huang"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23934v1",
                "updated": "2025-06-30T15:03:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    3,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:03:35Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    3,
                    35,
                    0,
                    181,
                    0
                ],
                "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for\n  Accuracy-aware Edge Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for\n  Accuracy-aware Edge Inference"
                },
                "summary": "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%."
                },
                "authors": [
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Saeid Ghafouri"
                    },
                    {
                        "name": "Bo Ji"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    },
                    {
                        "name": "Deepu John"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23930v1",
                "updated": "2025-06-30T14:59:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    59,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:59:25Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    59,
                    25,
                    0,
                    181,
                    0
                ],
                "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection\n  in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection\n  in Low-Resource Languages"
                },
                "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time."
                },
                "authors": [
                    {
                        "name": "Ruhina Tabasshum Prome"
                    },
                    {
                        "name": "Tarikul Islam Tamiti"
                    },
                    {
                        "name": "Anomadarshi Barua"
                    }
                ],
                "author_detail": {
                    "name": "Anomadarshi Barua"
                },
                "arxiv_affiliation": "George Mason University",
                "author": "Anomadarshi Barua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23929v1",
                "updated": "2025-06-30T14:58:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    58,
                    23,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:58:23Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    58,
                    23,
                    0,
                    181,
                    0
                ],
                "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMPACT: Inflectional Morphology Probes Across Complex Typologies"
                },
                "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework."
                },
                "authors": [
                    {
                        "name": "Mohammed J. Saeed"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Evgeny Fedoseev"
                    },
                    {
                        "name": "Sevil Caliskan"
                    },
                    {
                        "name": "Tatiana Vodolazova"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Vodolazova"
                },
                "author": "Tatiana Vodolazova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23926v1",
                "updated": "2025-06-30T14:54:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    52,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:54:52Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    52,
                    0,
                    181,
                    0
                ],
                "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive\n  decision-making system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial brain: a human-like autonomous neuro-symbolic cognitive\n  decision-making system"
                },
                "summary": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain."
                },
                "authors": [
                    {
                        "name": "Junping Wang"
                    },
                    {
                        "name": "Bicheng Wang"
                    },
                    {
                        "name": "Yibo Xuea"
                    },
                    {
                        "name": "Yuan Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Xie"
                },
                "author": "Yuan Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23924v1",
                "updated": "2025-06-30T14:54:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    15,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:54:15Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    15,
                    0,
                    181,
                    0
                ],
                "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice"
                },
                "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation."
                },
                "authors": [
                    {
                        "name": "Akshit Kumar"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Yuhang Wu"
                    },
                    {
                        "name": "Assaf Zeevi"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Zeevi"
                },
                "author": "Assaf Zeevi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23921v1",
                "updated": "2025-06-30T14:49:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    49,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:49:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    49,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "The Trilemma of Truth in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Trilemma of Truth in Large Language Models"
                },
                "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge."
                },
                "authors": [
                    {
                        "name": "Germans Savcisens"
                    },
                    {
                        "name": "Tina Eliassi-Rad"
                    }
                ],
                "author_detail": {
                    "name": "Tina Eliassi-Rad"
                },
                "author": "Tina Eliassi-Rad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02058v2",
                "updated": "2025-06-30T14:49:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    49,
                    26,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-04T13:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    1,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional\n  Trajectories Through Manifold Learning: A Linear Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional\n  Trajectories Through Manifold Learning: A Linear Approach"
                },
                "summary": "A data-driven approach based on unsupervised machine learning is proposed to\ninfer the intrinsic dimension $m^{\\ast}$ of the high-dimensional trajectories\nof the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis\n(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,\nof the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a\ncritical relationship between $m^{\\ast}$ and the model's nonlinear strength. By\nestimating the intrinsic dimension $m^{\\ast}$ using multiple methods\n(participation ratio, Kaiser rule, and the Kneedle algorithm), it is found that\n$m^{\\ast}$ increases with the model nonlinearity. Interestingly, in the weakly\nnonlinear regime, for trajectories initialized by exciting the first mode, the\nparticipation ratio estimates $m^{\\ast} = 2, 3$, strongly suggesting that\nquasi-periodic motion on a low-dimensional Riemannian manifold underlies the\ncharacteristic energy recurrences observed in the FPUT model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A data-driven approach based on unsupervised machine learning is proposed to\ninfer the intrinsic dimension $m^{\\ast}$ of the high-dimensional trajectories\nof the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis\n(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,\nof the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a\ncritical relationship between $m^{\\ast}$ and the model's nonlinear strength. By\nestimating the intrinsic dimension $m^{\\ast}$ using multiple methods\n(participation ratio, Kaiser rule, and the Kneedle algorithm), it is found that\n$m^{\\ast}$ increases with the model nonlinearity. Interestingly, in the weakly\nnonlinear regime, for trajectories initialized by exciting the first mode, the\nparticipation ratio estimates $m^{\\ast} = 2, 3$, strongly suggesting that\nquasi-periodic motion on a low-dimensional Riemannian manifold underlies the\ncharacteristic energy recurrences observed in the FPUT model."
                },
                "authors": [
                    {
                        "name": "Gionni Marchetti"
                    }
                ],
                "author_detail": {
                    "name": "Gionni Marchetti"
                },
                "author": "Gionni Marchetti",
                "arxiv_comment": "15 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23914v1",
                "updated": "2025-06-30T14:43:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    43,
                    33,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:43:33Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    43,
                    33,
                    0,
                    181,
                    0
                ],
                "title": "Learning robust parameter inference and density reconstruction in flyer\n  plate impact experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning robust parameter inference and density reconstruction in flyer\n  plate impact experiments"
                },
                "summary": "Estimating physical parameters or material properties from experimental\nobservations is a common objective in many areas of physics and material\nscience. In many experiments, especially in shock physics, radiography is the\nprimary means of observing the system of interest. However, radiography does\nnot provide direct access to key state variables, such as density, which\nprevents the application of traditional parameter estimation approaches. Here\nwe focus on flyer plate impact experiments on porous materials, and resolving\nthe underlying parameterized equation of state (EoS) and crush porosity model\nparameters given radiographic observation(s). We use machine learning as a tool\nto demonstrate with high confidence that using only high impact velocity data\ndoes not provide sufficient information to accurately infer both EoS and crush\nmodel parameters, even with fully resolved density fields or a dynamic sequence\nof images. We thus propose an observable data set consisting of low and high\nimpact velocity experiments/simulations that capture different regimes of\ncompaction and shock propagation, and proceed to introduce a generative machine\nlearning approach which produces a posterior distribution of physical\nparameters directly from radiographs. We demonstrate the effectiveness of the\napproach in estimating parameters from simulated flyer plate impact\nexperiments, and show that the obtained estimates of EoS and crush model\nparameters can then be used in hydrodynamic simulations to obtain accurate and\nphysically admissible density reconstructions. Finally, we examine the\nrobustness of the approach to model mismatches, and find that the learned\napproach can provide useful parameter estimates in the presence of\nout-of-distribution radiographic noise and previously unseen physics, thereby\npromoting a potential breakthrough in estimating material properties from\nexperimental radiographic images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating physical parameters or material properties from experimental\nobservations is a common objective in many areas of physics and material\nscience. In many experiments, especially in shock physics, radiography is the\nprimary means of observing the system of interest. However, radiography does\nnot provide direct access to key state variables, such as density, which\nprevents the application of traditional parameter estimation approaches. Here\nwe focus on flyer plate impact experiments on porous materials, and resolving\nthe underlying parameterized equation of state (EoS) and crush porosity model\nparameters given radiographic observation(s). We use machine learning as a tool\nto demonstrate with high confidence that using only high impact velocity data\ndoes not provide sufficient information to accurately infer both EoS and crush\nmodel parameters, even with fully resolved density fields or a dynamic sequence\nof images. We thus propose an observable data set consisting of low and high\nimpact velocity experiments/simulations that capture different regimes of\ncompaction and shock propagation, and proceed to introduce a generative machine\nlearning approach which produces a posterior distribution of physical\nparameters directly from radiographs. We demonstrate the effectiveness of the\napproach in estimating parameters from simulated flyer plate impact\nexperiments, and show that the obtained estimates of EoS and crush model\nparameters can then be used in hydrodynamic simulations to obtain accurate and\nphysically admissible density reconstructions. Finally, we examine the\nrobustness of the approach to model mismatches, and find that the learned\napproach can provide useful parameter estimates in the presence of\nout-of-distribution radiographic noise and previously unseen physics, thereby\npromoting a potential breakthrough in estimating material properties from\nexperimental radiographic images."
                },
                "authors": [
                    {
                        "name": "Evan Bell"
                    },
                    {
                        "name": "Daniel A. Serino"
                    },
                    {
                        "name": "Ben S. Southworth"
                    },
                    {
                        "name": "Trevor Wilcox"
                    },
                    {
                        "name": "Marc L. Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Marc L. Klasky"
                },
                "author": "Marc L. Klasky",
                "arxiv_comment": "24 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01754v2",
                "updated": "2025-06-30T14:43:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    43,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2024-09-03T10:01:51Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    10,
                    1,
                    51,
                    1,
                    247,
                    0
                ],
                "title": "Empirical evidence of Large Language Model's influence on human spoken\n  communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evidence of Large Language Model's influence on human spoken\n  communication"
                },
                "summary": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation."
                },
                "authors": [
                    {
                        "name": "Hiromu Yakura"
                    },
                    {
                        "name": "Ezequiel Lopez-Lopez"
                    },
                    {
                        "name": "Levin Brinkmann"
                    },
                    {
                        "name": "Ignacio Serna"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Ivan Soraperra"
                    },
                    {
                        "name": "Iyad Rahwan"
                    }
                ],
                "author_detail": {
                    "name": "Iyad Rahwan"
                },
                "author": "Iyad Rahwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19592v2",
                "updated": "2025-06-30T14:40:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    40,
                    24,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-24T13:02:06Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    2,
                    6,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning"
                },
                "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment."
                },
                "authors": [
                    {
                        "name": "Harisankar Babu"
                    },
                    {
                        "name": "Philipp Schillinger"
                    },
                    {
                        "name": "Tamim Asfour"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Asfour"
                },
                "author": "Tamim Asfour",
                "arxiv_comment": "Accepted at IEEE CASE 2025, 8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23896v1",
                "updated": "2025-06-30T14:29:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    29,
                    47,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:29:47Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    29,
                    47,
                    0,
                    181,
                    0
                ],
                "title": "Interdependent Bilateral Trade: Information vs Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interdependent Bilateral Trade: Information vs Approximation"
                },
                "summary": "Welfare maximization in bilateral trade has been extensively studied in\nrecent years. Previous literature obtained incentive-compatible approximation\nmechanisms only for the private values case. In this paper, we study welfare\nmaximization in bilateral trade with interdependent values. Designing\nmechanisms for interdependent settings is much more challenging because the\nvalues of the players depend on the private information of the others,\nrequiring complex belief updates and strategic inference. We propose to\nclassify information structures by quantifying the influence that a player's\nprivate signal has on their own valuation. We then paint a picture of where\napproximations are possible and impossible based on these information\nstructures. Finally, we also study the possible approximation ratios for a\nnatural family of information structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Welfare maximization in bilateral trade has been extensively studied in\nrecent years. Previous literature obtained incentive-compatible approximation\nmechanisms only for the private values case. In this paper, we study welfare\nmaximization in bilateral trade with interdependent values. Designing\nmechanisms for interdependent settings is much more challenging because the\nvalues of the players depend on the private information of the others,\nrequiring complex belief updates and strategic inference. We propose to\nclassify information structures by quantifying the influence that a player's\nprivate signal has on their own valuation. We then paint a picture of where\napproximations are possible and impossible based on these information\nstructures. Finally, we also study the possible approximation ratios for a\nnatural family of information structures."
                },
                "authors": [
                    {
                        "name": "Shahar Dobzinski"
                    },
                    {
                        "name": "Alon Eden"
                    },
                    {
                        "name": "Kira Goldner"
                    },
                    {
                        "name": "Ariel Shaulker"
                    },
                    {
                        "name": "Thodoris Tsilivis"
                    }
                ],
                "author_detail": {
                    "name": "Thodoris Tsilivis"
                },
                "author": "Thodoris Tsilivis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23892v1",
                "updated": "2025-06-30T14:20:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    20,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:20:58Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    20,
                    58,
                    0,
                    181,
                    0
                ],
                "title": "Dimension and model reduction approaches for linear Bayesian inverse\n  problems with rank-deficient prior covariances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dimension and model reduction approaches for linear Bayesian inverse\n  problems with rank-deficient prior covariances"
                },
                "summary": "Bayesian inverse problems use observed data to update a prior probability\ndistribution for an unknown state or parameter of a scientific system to a\nposterior distribution conditioned on the data. In many applications, the\nunknown parameter is high-dimensional, making computation of the posterior\nexpensive due to the need to sample in a high-dimensional space and the need to\nevaluate an expensive high-dimensional forward model relating the unknown\nparameter to the data. However, inverse problems often exhibit low-dimensional\nstructure due to the fact that the available data are only informative in a\nlow-dimensional subspace of the parameter space. Dimension reduction approaches\nexploit this structure by restricting inference to the low-dimensional subspace\ninformed by the data, which can be sampled more efficiently. Further\ncomputational cost reductions can be achieved by replacing expensive\nhigh-dimensional forward models with cheaper lower-dimensional reduced models.\nIn this work, we propose new dimension and model reduction approaches for\nlinear Bayesian inverse problems with rank-deficient prior covariances, which\narise in many practical inference settings. The dimension reduction approach is\napplicable to general linear Bayesian inverse problems whereas the model\nreduction approaches are specific to the problem of inferring the initial\ncondition of a linear dynamical system. We provide theoretical approximation\nguarantees as well as numerical experiments demonstrating the accuracy and\nefficiency of the proposed approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inverse problems use observed data to update a prior probability\ndistribution for an unknown state or parameter of a scientific system to a\nposterior distribution conditioned on the data. In many applications, the\nunknown parameter is high-dimensional, making computation of the posterior\nexpensive due to the need to sample in a high-dimensional space and the need to\nevaluate an expensive high-dimensional forward model relating the unknown\nparameter to the data. However, inverse problems often exhibit low-dimensional\nstructure due to the fact that the available data are only informative in a\nlow-dimensional subspace of the parameter space. Dimension reduction approaches\nexploit this structure by restricting inference to the low-dimensional subspace\ninformed by the data, which can be sampled more efficiently. Further\ncomputational cost reductions can be achieved by replacing expensive\nhigh-dimensional forward models with cheaper lower-dimensional reduced models.\nIn this work, we propose new dimension and model reduction approaches for\nlinear Bayesian inverse problems with rank-deficient prior covariances, which\narise in many practical inference settings. The dimension reduction approach is\napplicable to general linear Bayesian inverse problems whereas the model\nreduction approaches are specific to the problem of inferring the initial\ncondition of a linear dynamical system. We provide theoretical approximation\nguarantees as well as numerical experiments demonstrating the accuracy and\nefficiency of the proposed approaches."
                },
                "authors": [
                    {
                        "name": "Josie Knig"
                    },
                    {
                        "name": "Elizabeth Qian"
                    },
                    {
                        "name": "Melina A. Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Melina A. Freitag"
                },
                "author": "Melina A. Freitag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23888v1",
                "updated": "2025-06-30T14:18:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    18,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:18:35Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    18,
                    35,
                    0,
                    181,
                    0
                ],
                "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models\n  through Multi-Layered Self-Reflection with Auto-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multi-Step Mathematical Reasoning in Large Language Models\n  through Multi-Layered Self-Reflection with Auto-Prompting"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance."
                },
                "authors": [
                    {
                        "name": "Andr de Souza Loureiro"
                    },
                    {
                        "name": "Jorge Valverde-Rebaza"
                    },
                    {
                        "name": "Julieta Noguez"
                    },
                    {
                        "name": "David Escarcega"
                    },
                    {
                        "name": "Ricardo Marcacini"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Marcacini"
                },
                "author": "Ricardo Marcacini",
                "arxiv_comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17451v2",
                "updated": "2025-06-30T14:17:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    17,
                    9,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-21T18:57:43Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    18,
                    57,
                    43,
                    1,
                    142,
                    0
                ],
                "title": "Green AI in Action: Strategic Model Selection for Ensembles in\n  Production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green AI in Action: Strategic Model Selection for Ensembles in\n  Production"
                },
                "summary": "Integrating Artificial Intelligence (AI) into software systems has\nsignificantly enhanced their capabilities while escalating energy demands.\nEnsemble learning, combining predictions from multiple models to form a single\nprediction, intensifies this problem due to cumulative energy consumption. This\npaper presents a novel approach to model selection that addresses the challenge\nof balancing the accuracy of AI models with their energy consumption in a live\nAI ensemble system. We explore how reducing the number of models or improving\nthe efficiency of model usage within an ensemble during inference can reduce\nenergy demands without substantially sacrificing accuracy. This study\nintroduces and evaluates two model selection strategies, Static and Dynamic,\nfor optimizing ensemble learning systems performance while minimizing energy\nusage. Our results demonstrate that the Static strategy improves the F1 score\nbeyond the baseline, reducing average energy usage from 100% from the full\nensemble to 62%. The Dynamic strategy further enhances F1 scores, using on\naverage 76% compared to 100% of the full ensemble. Moreover, we propose an\napproach that balances accuracy with resource consumption, significantly\nreducing energy usage without substantially impacting accuracy. This method\ndecreased the average energy usage of the Static strategy from approximately\n62% to 14%, and for the Dynamic strategy, from around 76% to 57%. Our field\nstudy of Green AI using an operational AI system developed by a large\nprofessional services provider shows the practical applicability of adopting\nenergy-conscious model selection strategies in live production environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Artificial Intelligence (AI) into software systems has\nsignificantly enhanced their capabilities while escalating energy demands.\nEnsemble learning, combining predictions from multiple models to form a single\nprediction, intensifies this problem due to cumulative energy consumption. This\npaper presents a novel approach to model selection that addresses the challenge\nof balancing the accuracy of AI models with their energy consumption in a live\nAI ensemble system. We explore how reducing the number of models or improving\nthe efficiency of model usage within an ensemble during inference can reduce\nenergy demands without substantially sacrificing accuracy. This study\nintroduces and evaluates two model selection strategies, Static and Dynamic,\nfor optimizing ensemble learning systems performance while minimizing energy\nusage. Our results demonstrate that the Static strategy improves the F1 score\nbeyond the baseline, reducing average energy usage from 100% from the full\nensemble to 62%. The Dynamic strategy further enhances F1 scores, using on\naverage 76% compared to 100% of the full ensemble. Moreover, we propose an\napproach that balances accuracy with resource consumption, significantly\nreducing energy usage without substantially impacting accuracy. This method\ndecreased the average energy usage of the Static strategy from approximately\n62% to 14%, and for the Dynamic strategy, from around 76% to 57%. Our field\nstudy of Green AI using an operational AI system developed by a large\nprofessional services provider shows the practical applicability of adopting\nenergy-conscious model selection strategies in live production environments."
                },
                "authors": [
                    {
                        "name": "Nienke Nijkamp"
                    },
                    {
                        "name": "June Sallou"
                    },
                    {
                        "name": "Niels van der Heijden"
                    },
                    {
                        "name": "Lus Cruz"
                    }
                ],
                "author_detail": {
                    "name": "Lus Cruz"
                },
                "author": "Lus Cruz",
                "arxiv_comment": "10 pages. Accepted at the 1st ACM International Conference on\n  AI-powered Software (AIware), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23884v1",
                "updated": "2025-06-30T14:13:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    13,
                    34,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:13:34Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    13,
                    34,
                    0,
                    181,
                    0
                ],
                "title": "Spectropolarimetric synthesis of forbidden lines in MHD models of\n  coronal bright points",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectropolarimetric synthesis of forbidden lines in MHD models of\n  coronal bright points"
                },
                "summary": "The inference of the magnetic field vector from spectropolarimetric\nobservations is crucial for understanding the physical processes governing the\nsolar corona. We investigate which information on the magnetic fields of\ncoronal bright points (CBP) can be gained from the intensity and polarization\nof the Fe XIII 10747 A, Fe XIV 5303 A, Si X 14301 A, and Si IX 39343 A\nforbidden lines. We apply the P-CORONA synthesis code to a CBP model in the\nvery low corona, obtained with the Bifrost code, and to a larger global model\nto study the impact of the outer coronal material along the line of sight\n(LoS). The enhanced density within the CBP produces an intensity brightening,\nbut suppresses the linear polarization. The circular polarization from such\nregions often approaches 0.1% of the intensity. The contribution from the\ncoronal material along the LoS depends strongly on its temperature, and is\nweaker for lines with a peak response at higher temperatures (Fe10747 at 1.7\nMK; Fe5303 at 2 MK). The weak field approximation (WFA) provides information on\nthe longitudinal magnetic fields in the strongest-emitting spatial intervals\nalong the LoS, and is more reliable in the regions of the CBP where the field\ndoes not change sign. This tends to coincide with the regions where there is a\nstrong correlation between the circular polarization and the wavelength\nderivative of the intensity. Considering roughly 30 minutes of time evolution,\nthe CBP signals are somewhat attenuated but are still identifiable, while the\narea where the WFA can be suitably applied remains substantial. The circular\npolarization of the Fe5303 and especially Fe10747 lines are valuable\ndiagnostics for the magnetic fields in the higher-temperature regions of the\nCBP, which could be exploited with future coronagraphs with similar\ncapabilities to Cryo-NIRSP/DKIST, but designed to observe below 1.05 solar\nradii.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of the magnetic field vector from spectropolarimetric\nobservations is crucial for understanding the physical processes governing the\nsolar corona. We investigate which information on the magnetic fields of\ncoronal bright points (CBP) can be gained from the intensity and polarization\nof the Fe XIII 10747 A, Fe XIV 5303 A, Si X 14301 A, and Si IX 39343 A\nforbidden lines. We apply the P-CORONA synthesis code to a CBP model in the\nvery low corona, obtained with the Bifrost code, and to a larger global model\nto study the impact of the outer coronal material along the line of sight\n(LoS). The enhanced density within the CBP produces an intensity brightening,\nbut suppresses the linear polarization. The circular polarization from such\nregions often approaches 0.1% of the intensity. The contribution from the\ncoronal material along the LoS depends strongly on its temperature, and is\nweaker for lines with a peak response at higher temperatures (Fe10747 at 1.7\nMK; Fe5303 at 2 MK). The weak field approximation (WFA) provides information on\nthe longitudinal magnetic fields in the strongest-emitting spatial intervals\nalong the LoS, and is more reliable in the regions of the CBP where the field\ndoes not change sign. This tends to coincide with the regions where there is a\nstrong correlation between the circular polarization and the wavelength\nderivative of the intensity. Considering roughly 30 minutes of time evolution,\nthe CBP signals are somewhat attenuated but are still identifiable, while the\narea where the WFA can be suitably applied remains substantial. The circular\npolarization of the Fe5303 and especially Fe10747 lines are valuable\ndiagnostics for the magnetic fields in the higher-temperature regions of the\nCBP, which could be exploited with future coronagraphs with similar\ncapabilities to Cryo-NIRSP/DKIST, but designed to observe below 1.05 solar\nradii."
                },
                "authors": [
                    {
                        "name": "Ernest Alsina Ballester"
                    },
                    {
                        "name": "Daniel Nbrega-Siverio"
                    },
                    {
                        "name": "Fernando Moreno-Insertis"
                    },
                    {
                        "name": "Supriya Hebbur Dayananda"
                    }
                ],
                "author_detail": {
                    "name": "Supriya Hebbur Dayananda"
                },
                "author": "Supriya Hebbur Dayananda",
                "arxiv_comment": "Accepted for publication in Astronomy & Astrophysics. 15 pages, 10\n  figures. Paper additionally contains a one-page appendix with one figure.\n  Additional animation pertaining to the online material is available as an\n  ancillary file",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20471v2",
                "updated": "2025-06-30T14:05:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    5,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-26T19:10:47Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    19,
                    10,
                    47,
                    0,
                    146,
                    0
                ],
                "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field"
                },
                "summary": "In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit"
                },
                "authors": [
                    {
                        "name": "Chenghao Qian"
                    },
                    {
                        "name": "Wenjing Li"
                    },
                    {
                        "name": "Yuhu Guo"
                    },
                    {
                        "name": "Gustav Markkula"
                    }
                ],
                "author_detail": {
                    "name": "Gustav Markkula"
                },
                "author": "Gustav Markkula",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06231v2",
                "updated": "2025-06-30T14:04:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    4,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2024-06-10T13:03:20Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    13,
                    3,
                    20,
                    0,
                    162,
                    0
                ],
                "title": "Statistical Inference for Privatized Data with Unknown Sample Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Privatized Data with Unknown Sample Size"
                },
                "summary": "We develop both theory and algorithms to analyze privatized data in the\nunbounded differential privacy(DP), where even the sample size is considered a\nsensitive quantity that requires privacy protection. We show that the distance\nbetween the sampling distributions under unbounded DP and bounded DP goes to\nzero as the sample size $n$ goes to infinity, provided that the noise used to\nprivatize $n$ is at an appropriate rate; we also establish that Approximate\nBayesian Computation (ABC)-type posterior distributions converge under similar\nassumptions. We further give asymptotic results in the regime where the privacy\nbudget for $n$ goes to zero, establishing similarity of sampling distributions\nas well as showing that the MLE in the unbounded setting converges to the\nbounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference\non privatized data in the unbounded DP setting, we propose a reversible jump\nMCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We\nalso propose a Monte Carlo EM algorithm to compute the MLE from privatized data\nin both bounded and unbounded DP. We apply our methodology to analyze a linear\nregression model as well as a 2019 American Time Use Survey Microdata File\nwhich we model using a Dirichlet distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop both theory and algorithms to analyze privatized data in the\nunbounded differential privacy(DP), where even the sample size is considered a\nsensitive quantity that requires privacy protection. We show that the distance\nbetween the sampling distributions under unbounded DP and bounded DP goes to\nzero as the sample size $n$ goes to infinity, provided that the noise used to\nprivatize $n$ is at an appropriate rate; we also establish that Approximate\nBayesian Computation (ABC)-type posterior distributions converge under similar\nassumptions. We further give asymptotic results in the regime where the privacy\nbudget for $n$ goes to zero, establishing similarity of sampling distributions\nas well as showing that the MLE in the unbounded setting converges to the\nbounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference\non privatized data in the unbounded DP setting, we propose a reversible jump\nMCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We\nalso propose a Monte Carlo EM algorithm to compute the MLE from privatized data\nin both bounded and unbounded DP. We apply our methodology to analyze a linear\nregression model as well as a 2019 American Time Use Survey Microdata File\nwhich we model using a Dirichlet distribution."
                },
                "authors": [
                    {
                        "name": "Jordan Awan"
                    },
                    {
                        "name": "Andres Felipe Barrientos"
                    },
                    {
                        "name": "Nianqiao Ju"
                    }
                ],
                "author_detail": {
                    "name": "Nianqiao Ju"
                },
                "author": "Nianqiao Ju",
                "arxiv_comment": "20 pages before references, 42 pages in total, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23864v1",
                "updated": "2025-06-30T13:57:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:57:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What\n  to Do About It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What\n  to Do About It"
                },
                "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning."
                },
                "authors": [
                    {
                        "name": "Seyed Mahed Mousavi"
                    },
                    {
                        "name": "Edoardo Cecchinato"
                    },
                    {
                        "name": "Lucia Hornikova"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23862v1",
                "updated": "2025-06-30T13:57:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    11,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:57:11Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    11,
                    0,
                    181,
                    0
                ],
                "title": "Large Language Models for Statistical Inference: Context Augmentation\n  with Applications to the Two-Sample Problem and Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Statistical Inference: Context Augmentation\n  with Applications to the Two-Sample Problem and Regression"
                },
                "summary": "We introduce context augmentation, a data-augmentation approach that uses\nlarge language models (LLMs) to generate contexts around observed strings as a\nmeans of facilitating valid frequentist inference. These generated contexts\nserve to reintroduce uncertainty, incorporate auxiliary information, and\nfacilitate interpretability. For example, in the two-sample test, we compare\nthe log-probability of strings under contexts from its own versus the other\ngroup. We show on synthetic data that the method's t-statistics exhibit the\nexpected null behaviour while maintaining power and, through a replication,\nthat the method is powerful and interpretable. We next introduce text-on-text\nregression. Contexts generated around the predictor string are treated as\nmediating variables between the predictor and outcome strings. Using negative\ncontrols, we then distinguish between semantic and syntactic dimensions of\nprediction. Analysis of real-world dialogic data illustrates behaviour\npredicted from a psycholinguistic framework. Theoretically, we provide\nidentification conditions, derive an influence-function decomposition, and show\nthat repeated cross-fitting of a pivotal statistic yields higher-order\nefficiency. We derive bounds linking estimation error, context count, and\nnumber of cross-fits. Taken together, context augmentation offers the ability\nto connect LLMs with longstanding statistical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce context augmentation, a data-augmentation approach that uses\nlarge language models (LLMs) to generate contexts around observed strings as a\nmeans of facilitating valid frequentist inference. These generated contexts\nserve to reintroduce uncertainty, incorporate auxiliary information, and\nfacilitate interpretability. For example, in the two-sample test, we compare\nthe log-probability of strings under contexts from its own versus the other\ngroup. We show on synthetic data that the method's t-statistics exhibit the\nexpected null behaviour while maintaining power and, through a replication,\nthat the method is powerful and interpretable. We next introduce text-on-text\nregression. Contexts generated around the predictor string are treated as\nmediating variables between the predictor and outcome strings. Using negative\ncontrols, we then distinguish between semantic and syntactic dimensions of\nprediction. Analysis of real-world dialogic data illustrates behaviour\npredicted from a psycholinguistic framework. Theoretically, we provide\nidentification conditions, derive an influence-function decomposition, and show\nthat repeated cross-fitting of a pivotal statistic yields higher-order\nefficiency. We derive bounds linking estimation error, context count, and\nnumber of cross-fits. Taken together, context augmentation offers the ability\nto connect LLMs with longstanding statistical practice."
                },
                "authors": [
                    {
                        "name": "Marc Ratkovic"
                    }
                ],
                "author_detail": {
                    "name": "Marc Ratkovic"
                },
                "author": "Marc Ratkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23858v1",
                "updated": "2025-06-30T13:52:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    52,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:52:31Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    52,
                    31,
                    0,
                    181,
                    0
                ],
                "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models"
                },
                "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation."
                },
                "authors": [
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Haotian Yang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Yunhai Tong"
                    }
                ],
                "author_detail": {
                    "name": "Yunhai Tong"
                },
                "author": "Yunhai Tong",
                "arxiv_comment": "Code is at https://github.com/KwaiVGI/VMoBA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23850v1",
                "updated": "2025-06-30T13:39:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    39,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:39:54Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    39,
                    54,
                    0,
                    181,
                    0
                ],
                "title": "Email as the Interface to Generative AI Models: Seamless Administrative\n  Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Email as the Interface to Generative AI Models: Seamless Administrative\n  Automation"
                },
                "summary": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient."
                },
                "authors": [
                    {
                        "name": "Andres Navarro"
                    },
                    {
                        "name": "Carlos de Quinto"
                    },
                    {
                        "name": "Jos Alberto Hernndez"
                    }
                ],
                "author_detail": {
                    "name": "Jos Alberto Hernndez"
                },
                "author": "Jos Alberto Hernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07160v2",
                "updated": "2025-06-30T13:36:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    36,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-08T14:18:15Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    14,
                    18,
                    15,
                    6,
                    159,
                    0
                ],
                "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks."
                },
                "authors": [
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Zimian Peng"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14268v2",
                "updated": "2025-06-30T13:35:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    35,
                    5,
                    0,
                    181,
                    0
                ],
                "published": "2024-06-20T12:46:45Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    12,
                    46,
                    45,
                    3,
                    172,
                    0
                ],
                "title": "A hierarchical Bayesian approach to point source analysis in high-energy\n  neutrino telescopes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A hierarchical Bayesian approach to point source analysis in high-energy\n  neutrino telescopes"
                },
                "summary": "We propose a novel approach to the detection of point-like sources of\nhigh-energy neutrinos. Motivated by evidence for emerging sources in existing\ndata, we focus on the characterization and interpretation of these sources\nrather than the rejection of the background-only hypothesis. The hierarchical\nBayesian model is implemented in the Stan platform, enabling computation of the\nposterior distribution with a Hamiltonian Monte Carlo algorithm. We simulate a\npopulation of weak neutrino sources detected by the IceCube experiment and use\nthe resulting data set to demonstrate and validate our framework. We show that\neven for the challenging case of sources at the threshold of detection and\nusing limited prior information, it is possible to correctly infer the source\nproperties. Additionally, we demonstrate how modeling flexible connections\nbetween similar sources can be used to recover the contribution of sources that\nwould not be detectable individually. While a direct comparison of our method\nto existing approaches is challenged by the fundamental differences in\nfrequentist and Bayesian frameworks, we draw parallels where possible. In\nparticular, we highlight how including more complexity into the source modeling\ncan increase the sensitivity to sources and their populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach to the detection of point-like sources of\nhigh-energy neutrinos. Motivated by evidence for emerging sources in existing\ndata, we focus on the characterization and interpretation of these sources\nrather than the rejection of the background-only hypothesis. The hierarchical\nBayesian model is implemented in the Stan platform, enabling computation of the\nposterior distribution with a Hamiltonian Monte Carlo algorithm. We simulate a\npopulation of weak neutrino sources detected by the IceCube experiment and use\nthe resulting data set to demonstrate and validate our framework. We show that\neven for the challenging case of sources at the threshold of detection and\nusing limited prior information, it is possible to correctly infer the source\nproperties. Additionally, we demonstrate how modeling flexible connections\nbetween similar sources can be used to recover the contribution of sources that\nwould not be detectable individually. While a direct comparison of our method\nto existing approaches is challenged by the fundamental differences in\nfrequentist and Bayesian frameworks, we draw parallels where possible. In\nparticular, we highlight how including more complexity into the source modeling\ncan increase the sensitivity to sources and their populations."
                },
                "authors": [
                    {
                        "name": "F. Capel"
                    },
                    {
                        "name": "J. Kuhlmann"
                    },
                    {
                        "name": "C. Haack"
                    },
                    {
                        "name": "M. Ha Minh"
                    },
                    {
                        "name": "H. Niederhausen"
                    },
                    {
                        "name": "L. Schumacher"
                    }
                ],
                "author_detail": {
                    "name": "L. Schumacher"
                },
                "author": "L. Schumacher",
                "arxiv_doi": "10.3847/1538-4357/ad7fe9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad7fe9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.14268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated to match published version",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23844v1",
                "updated": "2025-06-30T13:34:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    34,
                    34,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:34:34Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    34,
                    34,
                    0,
                    181,
                    0
                ],
                "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop."
                },
                "authors": [
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Luo"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02377v2",
                "updated": "2025-06-30T13:27:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    27,
                    3,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-03T12:14:48Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    12,
                    14,
                    48,
                    4,
                    124,
                    0
                ],
                "title": "Robustness of Decentralised Learning to Nodes and Data Disruption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness of Decentralised Learning to Nodes and Data Disruption"
                },
                "summary": "In the vibrant landscape of AI research, decentralised learning is gaining\nmomentum. Decentralised learning allows individual nodes to keep data locally\nwhere they are generated and to share knowledge extracted from local data among\nthemselves through an interactive process of collaborative refinement. This\nparadigm supports scenarios where data cannot leave local nodes due to privacy\nor sovereignty reasons or real-time constraints imposing proximity of models to\nlocations where inference has to be carried out. The distributed nature of\ndecentralised learning implies significant new research challenges with respect\nto centralised learning. Among them, in this paper, we focus on robustness\nissues. Specifically, we study the effect of nodes' disruption on the\ncollective learning process. Assuming a given percentage of \"central\" nodes\ndisappear from the network, we focus on different cases, characterised by (i)\ndifferent distributions of data across nodes and (ii) different times when\ndisruption occurs with respect to the start of the collaborative learning task.\nThrough these configurations, we are able to show the non-trivial interplay\nbetween the properties of the network connecting nodes, the persistence of\nknowledge acquired collectively before disruption or lack thereof, and the\neffect of data availability pre- and post-disruption. Our results show that\ndecentralised learning processes are remarkably robust to network disruption.\nAs long as even minimum amounts of data remain available somewhere in the\nnetwork, the learning process is able to recover from disruptions and achieve\nsignificant classification accuracy. This clearly varies depending on the\nremaining connectivity after disruption, but we show that even nodes that\nremain completely isolated can retain significant knowledge acquired before the\ndisruption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the vibrant landscape of AI research, decentralised learning is gaining\nmomentum. Decentralised learning allows individual nodes to keep data locally\nwhere they are generated and to share knowledge extracted from local data among\nthemselves through an interactive process of collaborative refinement. This\nparadigm supports scenarios where data cannot leave local nodes due to privacy\nor sovereignty reasons or real-time constraints imposing proximity of models to\nlocations where inference has to be carried out. The distributed nature of\ndecentralised learning implies significant new research challenges with respect\nto centralised learning. Among them, in this paper, we focus on robustness\nissues. Specifically, we study the effect of nodes' disruption on the\ncollective learning process. Assuming a given percentage of \"central\" nodes\ndisappear from the network, we focus on different cases, characterised by (i)\ndifferent distributions of data across nodes and (ii) different times when\ndisruption occurs with respect to the start of the collaborative learning task.\nThrough these configurations, we are able to show the non-trivial interplay\nbetween the properties of the network connecting nodes, the persistence of\nknowledge acquired collectively before disruption or lack thereof, and the\neffect of data availability pre- and post-disruption. Our results show that\ndecentralised learning processes are remarkably robust to network disruption.\nAs long as even minimum amounts of data remain available somewhere in the\nnetwork, the learning process is able to recover from disruptions and achieve\nsignificant classification accuracy. This clearly varies depending on the\nremaining connectivity after disruption, but we show that even nodes that\nremain completely isolated can retain significant knowledge acquired before the\ndisruption."
                },
                "authors": [
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Chiara Boldrini"
                    },
                    {
                        "name": "Lorenzo Valerio"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    },
                    {
                        "name": "Jnos Kertsz"
                    }
                ],
                "author_detail": {
                    "name": "Jnos Kertsz"
                },
                "author": "Jnos Kertsz",
                "arxiv_doi": "10.1016/j.comcom.2025.108250",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.comcom.2025.108250",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.02377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Supported by the H2020 HumaneAI Net (952026), CHIST-ERA-19-XAI010\n  SAI, PNRR - M4C2 - Investimento 1.3, Partenariato Esteso PE00000013 FAIR,\n  PNRR - M4C2 - Investimento 1.3, Partenariato Esteso PE00000001 RESTART",
                "arxiv_journal_ref": "Computer Communications, 108250 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00880v2",
                "updated": "2025-06-30T13:26:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    26,
                    57,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-01T15:08:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    8,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Group Accretion in Milky Way-like Stellar Haloes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Accretion in Milky Way-like Stellar Haloes"
                },
                "summary": "As galaxies form hierarchically, larger satellites may accrete alongside\nsmaller companions in group infall events. This coordinated accretion is likely\nto have left signatures in the Milky Way's stellar halo at the present day. Our\ngoal is to characterise the possible groups of companions that accompanied\nlarger known accretion events of our Galaxy, and infer where their stellar\nmaterial could be in physical and dynamical space at present day. We use the\nAURIGA simulation suite of Milky Way-like haloes to identify analogues to these\nlarge accretion events and their group infall companions, and we follow their\nevolution in time. We find that most of the material from larger accretion\nevents is deposited on much more bound orbits than their companions. This\nimplies a weak dynamical association between companions and debris, but it is\nstrongest with the material lost first. As a result, the companions of the\nMilky Way's earliest building blocks are likely to contribute stars to the\nsolar neighbourhood, whilst the companions of our last major merger are likely\nfound in both the solar neighbourhood and the outer halo. More recently\ninfallen groups of satellites, or those of a smaller mass, are more likely to\nretain dynamical coherence, for example, through clustering in the orientation\nof angular momentum. We conclude that group infall has likely shaped the Milky\nWay's stellar halo. Disentangling this will be challenging for the earliest\naccretion events, although overlap with their less-bound debris may be\nparticularly telling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As galaxies form hierarchically, larger satellites may accrete alongside\nsmaller companions in group infall events. This coordinated accretion is likely\nto have left signatures in the Milky Way's stellar halo at the present day. Our\ngoal is to characterise the possible groups of companions that accompanied\nlarger known accretion events of our Galaxy, and infer where their stellar\nmaterial could be in physical and dynamical space at present day. We use the\nAURIGA simulation suite of Milky Way-like haloes to identify analogues to these\nlarge accretion events and their group infall companions, and we follow their\nevolution in time. We find that most of the material from larger accretion\nevents is deposited on much more bound orbits than their companions. This\nimplies a weak dynamical association between companions and debris, but it is\nstrongest with the material lost first. As a result, the companions of the\nMilky Way's earliest building blocks are likely to contribute stars to the\nsolar neighbourhood, whilst the companions of our last major merger are likely\nfound in both the solar neighbourhood and the outer halo. More recently\ninfallen groups of satellites, or those of a smaller mass, are more likely to\nretain dynamical coherence, for example, through clustering in the orientation\nof angular momentum. We conclude that group infall has likely shaped the Milky\nWay's stellar halo. Disentangling this will be challenging for the earliest\naccretion events, although overlap with their less-bound debris may be\nparticularly telling."
                },
                "authors": [
                    {
                        "name": "Thomas M. Callingham"
                    },
                    {
                        "name": "Amina Helmi"
                    }
                ],
                "author_detail": {
                    "name": "Amina Helmi"
                },
                "author": "Amina Helmi",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23834v1",
                "updated": "2025-06-30T13:25:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    25,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:25:31Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    25,
                    31,
                    0,
                    181,
                    0
                ],
                "title": "Robust Inference with High-Dimensional Instruments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Inference with High-Dimensional Instruments"
                },
                "summary": "We propose a weak-identification-robust test for linear instrumental variable\n(IV) regressions with high-dimensional instruments, whose number is allowed to\nexceed the sample size. In addition, our test is robust to general error\ndependence, such as network dependence and spatial dependence. The test\nstatistic takes a self-normalized form and the asymptotic validity of the test\nis established by using random matrix theory. Simulation studies are conducted\nto assess the numerical performance of the test, confirming good size control\nand satisfactory testing power across a range of various error dependence\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a weak-identification-robust test for linear instrumental variable\n(IV) regressions with high-dimensional instruments, whose number is allowed to\nexceed the sample size. In addition, our test is robust to general error\ndependence, such as network dependence and spatial dependence. The test\nstatistic takes a self-normalized form and the asymptotic validity of the test\nis established by using random matrix theory. Simulation studies are conducted\nto assess the numerical performance of the test, confirming good size control\nand satisfactory testing power across a range of various error dependence\nstructures."
                },
                "authors": [
                    {
                        "name": "Qu Feng"
                    },
                    {
                        "name": "Sombut Jaidee"
                    },
                    {
                        "name": "Wenjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Wang"
                },
                "author": "Wenjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04998v2",
                "updated": "2025-06-30T13:18:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    18,
                    47,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-05T13:09:24Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    9,
                    24,
                    3,
                    156,
                    0
                ],
                "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based\n  Approach for Complex Arithmetic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based\n  Approach for Complex Arithmetic Reasoning"
                },
                "summary": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a 20-question\nproblem set of UAV-centric mathematical problems across four difficulty levels.\nOur experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5\\% with\nRAG), and decreases numerical errors, reducing Mean Squared Error (MSE) by\norders of magnitude for the best-performing models. This pilot study indicates\nthat RAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, questions,\nand answers are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a 20-question\nproblem set of UAV-centric mathematical problems across four difficulty levels.\nOur experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5\\% with\nRAG), and decreases numerical errors, reducing Mean Squared Error (MSE) by\norders of magnitude for the best-performing models. This pilot study indicates\nthat RAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, questions,\nand answers are publicly available."
                },
                "authors": [
                    {
                        "name": "Mehdi Azarafza"
                    },
                    {
                        "name": "Mojtaba Nayyeri"
                    },
                    {
                        "name": "Faezeh Pasandideh"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Achim Rettberg"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettberg"
                },
                "author": "Achim Rettberg",
                "arxiv_comment": "15 pages, 7 figures, 4 appendix subsections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23825v1",
                "updated": "2025-06-30T13:17:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    17,
                    49,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:17:49Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    17,
                    49,
                    0,
                    181,
                    0
                ],
                "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams"
                },
                "summary": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream."
                },
                "authors": [
                    {
                        "name": "Haoji Zhang"
                    },
                    {
                        "name": "Yiqin Wang"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xiaojie Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Jin"
                },
                "author": "Xiaojie Jin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22270v2",
                "updated": "2025-06-30T13:16:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    16,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T14:39:38Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    39,
                    38,
                    4,
                    178,
                    0
                ],
                "title": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values"
                },
                "summary": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content."
                },
                "authors": [
                    {
                        "name": "Ahmad Mel"
                    },
                    {
                        "name": "Sebastien Noir"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Noir"
                },
                "author": "Sebastien Noir",
                "arxiv_journal_ref": "IAMCR 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23816v1",
                "updated": "2025-06-30T13:05:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    5,
                    10,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:05:10Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    5,
                    10,
                    0,
                    181,
                    0
                ],
                "title": "An Improved Inference for IV Regressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Improved Inference for IV Regressions"
                },
                "summary": "Researchers often report empirical results that are based on low-dimensional\nIVs, such as the shift-share IV, together with many IVs. Could we combine these\nresults in an efficient way and take advantage of the information from both\nsides? In this paper, we propose a combination inference procedure to solve the\nproblem. Specifically, we consider a linear combination of three test\nstatistics: a standard cluster-robust Wald statistic based on the\nlow-dimensional IVs, a leave-one-cluster-out Lagrangian Multiplier (LM)\nstatistic, and a leave-one-cluster-out Anderson-Rubin (AR) statistic. We first\nestablish the joint asymptotic normality of the Wald, LM, and AR statistics and\nderive the corresponding limit experiment under local alternatives. Then, under\nthe assumption that at least the low-dimensional IVs can strongly identify the\nparameter of interest, we derive the optimal combination test based on the\nthree statistics and establish that our procedure leads to the uniformly most\npowerful (UMP) unbiased test among the class of tests considered. In\nparticular, the efficiency gain from the combined test is of ``free lunch\" in\nthe sense that it is always at least as powerful as the test that is only based\non the low-dimensional IVs or many IVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Researchers often report empirical results that are based on low-dimensional\nIVs, such as the shift-share IV, together with many IVs. Could we combine these\nresults in an efficient way and take advantage of the information from both\nsides? In this paper, we propose a combination inference procedure to solve the\nproblem. Specifically, we consider a linear combination of three test\nstatistics: a standard cluster-robust Wald statistic based on the\nlow-dimensional IVs, a leave-one-cluster-out Lagrangian Multiplier (LM)\nstatistic, and a leave-one-cluster-out Anderson-Rubin (AR) statistic. We first\nestablish the joint asymptotic normality of the Wald, LM, and AR statistics and\nderive the corresponding limit experiment under local alternatives. Then, under\nthe assumption that at least the low-dimensional IVs can strongly identify the\nparameter of interest, we derive the optimal combination test based on the\nthree statistics and establish that our procedure leads to the uniformly most\npowerful (UMP) unbiased test among the class of tests considered. In\nparticular, the efficiency gain from the combined test is of ``free lunch\" in\nthe sense that it is always at least as powerful as the test that is only based\non the low-dimensional IVs or many IVs."
                },
                "authors": [
                    {
                        "name": "Liyu Dou"
                    },
                    {
                        "name": "Pengjin Min"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yichong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yichong Zhang"
                },
                "author": "Yichong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23815v2",
                "updated": "2025-07-01T07:51:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    51,
                    20,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T13:02:01Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    2,
                    1,
                    0,
                    181,
                    0
                ],
                "title": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment"
                },
                "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods."
                },
                "authors": [
                    {
                        "name": "Patrick Stokkink"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Stokkink"
                },
                "author": "Patrick Stokkink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16078v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16078v3",
                "updated": "2025-06-30T13:01:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    1,
                    16,
                    0,
                    181,
                    0
                ],
                "published": "2024-04-24T12:41:04Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    12,
                    41,
                    4,
                    2,
                    115,
                    0
                ],
                "title": "Learning World Models With Hierarchical Temporal Abstractions: A\n  Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning World Models With Hierarchical Temporal Abstractions: A\n  Probabilistic Perspective"
                },
                "summary": "Machines that can replicate human intelligence with type 2 reasoning\ncapabilities should be able to reason at multiple levels of spatio-temporal\nabstractions and scales using internal world models. Devising formalisms to\ndevelop such internal world models, which accurately reflect the causal\nhierarchies inherent in the dynamics of the real world, is a critical research\nchallenge in the domains of artificial intelligence and machine learning. This\nthesis identifies several limitations with the prevalent use of state space\nmodels (SSMs) as internal world models and propose two new probabilistic\nformalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address\nthese drawbacks. The structure of graphical models in both formalisms\nfacilitates scalable exact probabilistic inference using belief propagation, as\nwell as end-to-end learning via backpropagation through time. This approach\npermits the development of scalable, adaptive hierarchical world models capable\nof representing nonstationary dynamics across multiple temporal abstractions\nand scales. Moreover, these probabilistic formalisms integrate the concept of\nuncertainty in world states, thus improving the system's capacity to emulate\nthe stochastic nature of the real world and quantify the confidence in its\npredictions. The thesis also discuss how these formalisms are in line with\nrelated neuroscience literature on Bayesian brain hypothesis and predicitive\nprocessing. Our experiments on various real and simulated robots demonstrate\nthat our formalisms can match and in many cases exceed the performance of\ncontemporary transformer variants in making long-range future predictions. We\nconclude the thesis by reflecting on the limitations of our current models and\nsuggesting directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machines that can replicate human intelligence with type 2 reasoning\ncapabilities should be able to reason at multiple levels of spatio-temporal\nabstractions and scales using internal world models. Devising formalisms to\ndevelop such internal world models, which accurately reflect the causal\nhierarchies inherent in the dynamics of the real world, is a critical research\nchallenge in the domains of artificial intelligence and machine learning. This\nthesis identifies several limitations with the prevalent use of state space\nmodels (SSMs) as internal world models and propose two new probabilistic\nformalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address\nthese drawbacks. The structure of graphical models in both formalisms\nfacilitates scalable exact probabilistic inference using belief propagation, as\nwell as end-to-end learning via backpropagation through time. This approach\npermits the development of scalable, adaptive hierarchical world models capable\nof representing nonstationary dynamics across multiple temporal abstractions\nand scales. Moreover, these probabilistic formalisms integrate the concept of\nuncertainty in world states, thus improving the system's capacity to emulate\nthe stochastic nature of the real world and quantify the confidence in its\npredictions. The thesis also discuss how these formalisms are in line with\nrelated neuroscience literature on Bayesian brain hypothesis and predicitive\nprocessing. Our experiments on various real and simulated robots demonstrate\nthat our formalisms can match and in many cases exceed the performance of\ncontemporary transformer variants in making long-range future predictions. We\nconclude the thesis by reflecting on the limitations of our current models and\nsuggesting directions for future research."
                },
                "authors": [
                    {
                        "name": "Vaisakh Shaj"
                    }
                ],
                "author_detail": {
                    "name": "Vaisakh Shaj"
                },
                "author": "Vaisakh Shaj",
                "arxiv_comment": "Doctoral Dissertation, Department of Computer Science, Karlsruhe\n  Institute Of Technology, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16078v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16078v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11655v2",
                "updated": "2025-06-30T12:58:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    58,
                    45,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-03T07:17:46Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    7,
                    17,
                    46,
                    0,
                    34,
                    0
                ],
                "title": "Explainable Sentiment Analysis with DeepSeek-R1: Performance,\n  Efficiency, and Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Sentiment Analysis with DeepSeek-R1: Performance,\n  Efficiency, and Few-Shot Learning"
                },
                "summary": "Large language models (LLMs) have transformed sentiment analysis, yet\nbalancing accuracy, efficiency, and explainability remains a critical\nchallenge. This study presents the first comprehensive evaluation of\nDeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and\nGPT-4o-mini. We test the full 671B model and its distilled variants,\nsystematically documenting few-shot learning curves. Our experiments show\nDeepSeek-R1 achieves a 91.39\\% F1 score on 5-class sentiment and 99.31\\%\naccuracy on binary tasks with just 5 shots, an eightfold improvement in\nfew-shot efficiency over GPT-4o. Architecture-specific distillation effects\nemerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant\nby 6.69 percentage points. While its reasoning process reduces throughput,\nDeepSeek-R1 offers superior explainability via transparent, step-by-step\ntraces, establishing it as a powerful, interpretable open-source alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed sentiment analysis, yet\nbalancing accuracy, efficiency, and explainability remains a critical\nchallenge. This study presents the first comprehensive evaluation of\nDeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and\nGPT-4o-mini. We test the full 671B model and its distilled variants,\nsystematically documenting few-shot learning curves. Our experiments show\nDeepSeek-R1 achieves a 91.39\\% F1 score on 5-class sentiment and 99.31\\%\naccuracy on binary tasks with just 5 shots, an eightfold improvement in\nfew-shot efficiency over GPT-4o. Architecture-specific distillation effects\nemerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant\nby 6.69 percentage points. While its reasoning process reduces throughput,\nDeepSeek-R1 offers superior explainability via transparent, step-by-step\ntraces, establishing it as a powerful, interpretable open-source alternative."
                },
                "authors": [
                    {
                        "name": "Donghao Huang"
                    },
                    {
                        "name": "Zhaoxia Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxia Wang"
                },
                "author": "Zhaoxia Wang",
                "arxiv_comment": "10 pages, 2 figures, 6 tables, revised and re-submitted to an IEEE\n  journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09848v3",
                "updated": "2025-06-30T12:50:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    50,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-04-15T15:00:17Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    15,
                    0,
                    17,
                    0,
                    106,
                    0
                ],
                "title": "HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge\n  Representation"
                },
                "summary": "In a hyper-relational knowledge graph (HKG), each fact is composed of a main\ntriple associated with attribute-value qualifiers, which express additional\nfactual knowledge. The hyper-relational knowledge graph completion (HKGC) task\naims at inferring plausible missing links in a HKG. Most existing approaches to\nHKGC focus on enhancing the communication between qualifier pairs and main\ntriples, while overlooking two important properties that emerge from the\nmonotonicity of the hyper-relational graphs representation regime. Stage\nReasoning allows for a two-step reasoning process, facilitating the integration\nof coarse-grained inference results derived solely from main triples and\nfine-grained inference results obtained from hyper-relational facts with\nqualifiers. In the initial stage, coarse-grained results provide an upper bound\nfor correct predictions, which are subsequently refined in the fine-grained\nstep. More generally, Qualifier Monotonicity implies that by attaching more\nqualifier pairs to a main triple, we may only narrow down the answer set, but\nnever enlarge it. This paper proposes the HyperMono model for hyper-relational\nknowledge graph completion, which realizes stage reasoning and qualifier\nmonotonicity. To implement qualifier monotonicity HyperMono resorts to cone\nembeddings. Experiments on three real-world datasets with three different\nscenario conditions demonstrate the strong performance of HyperMono when\ncompared to the SoTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a hyper-relational knowledge graph (HKG), each fact is composed of a main\ntriple associated with attribute-value qualifiers, which express additional\nfactual knowledge. The hyper-relational knowledge graph completion (HKGC) task\naims at inferring plausible missing links in a HKG. Most existing approaches to\nHKGC focus on enhancing the communication between qualifier pairs and main\ntriples, while overlooking two important properties that emerge from the\nmonotonicity of the hyper-relational graphs representation regime. Stage\nReasoning allows for a two-step reasoning process, facilitating the integration\nof coarse-grained inference results derived solely from main triples and\nfine-grained inference results obtained from hyper-relational facts with\nqualifiers. In the initial stage, coarse-grained results provide an upper bound\nfor correct predictions, which are subsequently refined in the fine-grained\nstep. More generally, Qualifier Monotonicity implies that by attaching more\nqualifier pairs to a main triple, we may only narrow down the answer set, but\nnever enlarge it. This paper proposes the HyperMono model for hyper-relational\nknowledge graph completion, which realizes stage reasoning and qualifier\nmonotonicity. To implement qualifier monotonicity HyperMono resorts to cone\nembeddings. Experiments on three real-world datasets with three different\nscenario conditions demonstrate the strong performance of HyperMono when\ncompared to the SoTA."
                },
                "authors": [
                    {
                        "name": "Zhiwei Hu"
                    },
                    {
                        "name": "Vctor Gutirrez-Basulto"
                    },
                    {
                        "name": "Zhiliang Xiang"
                    },
                    {
                        "name": "Ru Li"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23801v1",
                "updated": "2025-06-30T12:45:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:45:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "Controllable Reference-Based Real-World Remote Sensing Image\n  Super-Resolution with Generative Diffusion Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Reference-Based Real-World Remote Sensing Image\n  Super-Resolution with Generative Diffusion Priors"
                },
                "summary": "Super-resolution (SR) techniques can enhance the spatial resolution of remote\nsensing images by utilizing low-resolution (LR) images to reconstruct\nhigh-resolution (HR) images, enabling more efficient large-scale earth\nobservation applications. While single-image super-resolution (SISR) methods\nhave shown progress, reference-based super-resolution (RefSR) offers superior\nperformance by incorporating historical HR images alongside current LR\nobservations. However, existing RefSR methods struggle with real-world\ncomplexities, such as cross-sensor resolution gap and significant land cover\nchanges, often leading to under-generation or over-reliance on reference image.\nTo address these challenges, we propose CRefDiff, a novel controllable\nreference-based diffusion model for real-world remote sensing image SR. To\naddress the under-generation problem, CRefDiff is built upon the pretrained\nStable Diffusion model, leveraging its powerful generative prior to produce\naccurate structures and textures. To mitigate over-reliance on the reference,\nwe introduce a dual-branch fusion mechanism that adaptively integrates both\nlocal and global information from the reference image. Moreover, this novel\ndual-branch design enables reference strength control during inference,\nenhancing interactivity and flexibility of the model. Finally, a strategy named\nBetter Start is proposed to significantly reduce the number of denoising steps,\nthereby accelerating the inference process. To support further research, we\nintroduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing\nimages, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land\ncover changes and significant temporal gaps. Extensive experiments on\nReal-RefRSSRD show that CRefDiff achieves state-of-the-art performance across\nvarious metrics and improves downstream tasks such as scene classification and\nsemantic segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-resolution (SR) techniques can enhance the spatial resolution of remote\nsensing images by utilizing low-resolution (LR) images to reconstruct\nhigh-resolution (HR) images, enabling more efficient large-scale earth\nobservation applications. While single-image super-resolution (SISR) methods\nhave shown progress, reference-based super-resolution (RefSR) offers superior\nperformance by incorporating historical HR images alongside current LR\nobservations. However, existing RefSR methods struggle with real-world\ncomplexities, such as cross-sensor resolution gap and significant land cover\nchanges, often leading to under-generation or over-reliance on reference image.\nTo address these challenges, we propose CRefDiff, a novel controllable\nreference-based diffusion model for real-world remote sensing image SR. To\naddress the under-generation problem, CRefDiff is built upon the pretrained\nStable Diffusion model, leveraging its powerful generative prior to produce\naccurate structures and textures. To mitigate over-reliance on the reference,\nwe introduce a dual-branch fusion mechanism that adaptively integrates both\nlocal and global information from the reference image. Moreover, this novel\ndual-branch design enables reference strength control during inference,\nenhancing interactivity and flexibility of the model. Finally, a strategy named\nBetter Start is proposed to significantly reduce the number of denoising steps,\nthereby accelerating the inference process. To support further research, we\nintroduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing\nimages, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land\ncover changes and significant temporal gaps. Extensive experiments on\nReal-RefRSSRD show that CRefDiff achieves state-of-the-art performance across\nvarious metrics and improves downstream tasks such as scene classification and\nsemantic segmentation."
                },
                "authors": [
                    {
                        "name": "Ce Wang"
                    },
                    {
                        "name": "Wanjie Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wanjie Sun"
                },
                "author": "Wanjie Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23800v2",
                "updated": "2025-07-01T10:16:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    16,
                    28,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T12:44:47Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    44,
                    47,
                    0,
                    181,
                    0
                ],
                "title": "Towards the Training of Deeper Predictive Coding Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Training of Deeper Predictive Coding Neural Networks"
                },
                "summary": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks."
                },
                "authors": [
                    {
                        "name": "Chang Qi"
                    },
                    {
                        "name": "Matteo Forasassi"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Tommaso Salvatori"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Salvatori"
                },
                "author": "Tommaso Salvatori",
                "arxiv_comment": "18 Pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13535v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13535v4",
                "updated": "2025-06-30T12:20:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    20,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-22T11:11:42Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    11,
                    11,
                    42,
                    2,
                    143,
                    0
                ],
                "title": "Addressing the Inconsistency in Bayesian Deep Learning via Generalized\n  Laplace Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the Inconsistency in Bayesian Deep Learning via Generalized\n  Laplace Approximation"
                },
                "summary": "In recent years, inconsistency in Bayesian deep learning has attracted\nsignificant attention. Tempered or generalized posterior distributions are\nfrequently employed as direct and effective solutions. Nonetheless, the\nunderlying mechanisms and the effectiveness of generalized posteriors remain\nactive research topics. In this work, we interpret posterior tempering as a\ncorrection for model misspecification via adjustments to the joint probability,\nand as a recalibration of priors by reducing aleatoric uncertainty. We also\nidentify a unique property of the Laplace approximation: the generalized\nnormalizing constant remains invariant, in contrast to general Bayesian\nlearning, where this constant typically depends on model parameters after\ngeneralization. Leveraging this property, we introduce the generalized Laplace\napproximation, which requires only a simple modification to the Hessian\ncalculation of the regularized loss. This approach provides a flexible and\nscalable framework for high-quality posterior inference. We evaluate the\nproposed method on state-of-the-art neural networks and real-world datasets,\ndemonstrating that the generalized Laplace approximation enhances predictive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, inconsistency in Bayesian deep learning has attracted\nsignificant attention. Tempered or generalized posterior distributions are\nfrequently employed as direct and effective solutions. Nonetheless, the\nunderlying mechanisms and the effectiveness of generalized posteriors remain\nactive research topics. In this work, we interpret posterior tempering as a\ncorrection for model misspecification via adjustments to the joint probability,\nand as a recalibration of priors by reducing aleatoric uncertainty. We also\nidentify a unique property of the Laplace approximation: the generalized\nnormalizing constant remains invariant, in contrast to general Bayesian\nlearning, where this constant typically depends on model parameters after\ngeneralization. Leveraging this property, we introduce the generalized Laplace\napproximation, which requires only a simple modification to the Hessian\ncalculation of the regularized loss. This approach provides a flexible and\nscalable framework for high-quality posterior inference. We evaluate the\nproposed method on state-of-the-art neural networks and real-world datasets,\ndemonstrating that the generalized Laplace approximation enhances predictive\nperformance."
                },
                "authors": [
                    {
                        "name": "Yinsong Chen"
                    },
                    {
                        "name": "Samson S. Yu"
                    },
                    {
                        "name": "Zhong Li"
                    },
                    {
                        "name": "Chee Peng Lim"
                    }
                ],
                "author_detail": {
                    "name": "Chee Peng Lim"
                },
                "author": "Chee Peng Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13535v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13535v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23774v1",
                "updated": "2025-06-30T12:18:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    18,
                    13,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:18:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    18,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate\n  Incidents Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate\n  Incidents Management"
                },
                "summary": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings."
                },
                "authors": [
                    {
                        "name": "Ewelina Gajewska"
                    },
                    {
                        "name": "Michal Wawer"
                    },
                    {
                        "name": "Katarzyna Budzynska"
                    },
                    {
                        "name": "Jarosaw A. Chudziak"
                    }
                ],
                "author_detail": {
                    "name": "Jarosaw A. Chudziak"
                },
                "author": "Jarosaw A. Chudziak",
                "arxiv_comment": "8 pages, 1 figure",
                "arxiv_journal_ref": "The 26th International Conference on Artificial Intelligence in\n  Education (AIED 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15627v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15627v4",
                "updated": "2025-06-30T12:15:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    15,
                    7,
                    0,
                    181,
                    0
                ],
                "published": "2024-06-21T20:06:31Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    20,
                    6,
                    31,
                    4,
                    173,
                    0
                ],
                "title": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph"
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Lyudmila Rvanova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Daniil Vasilev"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Abdelrahman Boda Sadallah"
                    },
                    {
                        "name": "Kirill Grishchenkov"
                    },
                    {
                        "name": "Sergey Petrakov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "arxiv_doi": "10.1162/tacl_a_00737",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1162/tacl_a_00737",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.15627v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15627v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at TACL 2025, presented at ACL 2025. Roman Vashurin,\n  Ekaterina Fadeeva, Artem Vazhentsev contributed equally",
                "arxiv_journal_ref": "Transactions of the Association for Computational Linguistics, 13\n  (2025) 220-248",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23762v1",
                "updated": "2025-06-30T12:09:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    9,
                    29,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:09:29Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    9,
                    29,
                    0,
                    181,
                    0
                ],
                "title": "Software Engineering for Large Language Models: Research Status,\n  Challenges and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering for Large Language Models: Research Status,\n  Challenges and the Road Ahead"
                },
                "summary": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Hongzhou Rao"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23749v1",
                "updated": "2025-06-30T11:46:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    46,
                    1,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:46:01Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    46,
                    1,
                    0,
                    181,
                    0
                ],
                "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design\n  Paradigms, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design\n  Paradigms, and Applications"
                },
                "summary": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Zijian Cai"
                    },
                    {
                        "name": "Fengling Liu"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Haoye Tian"
                },
                "author": "Haoye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20349v3",
                "updated": "2025-06-30T11:43:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    43,
                    16,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-26T09:20:42Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    20,
                    42,
                    2,
                    85,
                    0
                ],
                "title": "Consistency Trajectory Matching for One-Step Generative Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Trajectory Matching for One-Step Generative Super-Resolution"
                },
                "summary": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency."
                },
                "authors": [
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Leheng Zhang"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Kexuan Shi"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23745v1",
                "updated": "2025-06-30T11:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    36,
                    15,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    36,
                    15,
                    0,
                    181,
                    0
                ],
                "title": "EuPdSn2; magnetic structures in view of resonant x-ray Bragg diffraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EuPdSn2; magnetic structures in view of resonant x-ray Bragg diffraction"
                },
                "summary": "The magnetic properties of materials hosting Eu2+(J = 7/2, 4f7) ions have\nattracted much attention in the science of strongly correlated electrons. In\npart because crystal electric field effects are impoverished for an s-state\nion, as with Gd3+ intermetallics, and Eu2+ substitution in biological and\noptically active materials is resourceful. The magnetic structure of EuPdSn2 is\nnot wholly resolved. Ferromagnetic and antiferromagnetic structures coexist in\npowder neutron diffraction patterns, and compete in the ground state. Moreover,\nthe specific heat as a function of temperature is enigmatic and indicative of J\n= 5/2. We present symmetry-informed analytic magnetic structure factors for\nsingle crystal resonant x-ray Bragg diffraction using Eu atomic resonances that\nreveal significant potential for the technique. Europium ions use acentric\nWyckoff positions in magnetic space groups inferred from neutron diffraction.\nIn consequence, axial and polar Eu multipoles are compulsory components of both\nmagnetic neutron and resonant x-ray Bragg diffraction patterns. The proposed\nantiferromagnetic phase of EuPdSn2 supports anapoles (magnetic polar dipoles)\nalready observed in magnetic neutron diffraction patterns presented by Gd doped\nSmAl2, and several resonant x-ray diffraction patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The magnetic properties of materials hosting Eu2+(J = 7/2, 4f7) ions have\nattracted much attention in the science of strongly correlated electrons. In\npart because crystal electric field effects are impoverished for an s-state\nion, as with Gd3+ intermetallics, and Eu2+ substitution in biological and\noptically active materials is resourceful. The magnetic structure of EuPdSn2 is\nnot wholly resolved. Ferromagnetic and antiferromagnetic structures coexist in\npowder neutron diffraction patterns, and compete in the ground state. Moreover,\nthe specific heat as a function of temperature is enigmatic and indicative of J\n= 5/2. We present symmetry-informed analytic magnetic structure factors for\nsingle crystal resonant x-ray Bragg diffraction using Eu atomic resonances that\nreveal significant potential for the technique. Europium ions use acentric\nWyckoff positions in magnetic space groups inferred from neutron diffraction.\nIn consequence, axial and polar Eu multipoles are compulsory components of both\nmagnetic neutron and resonant x-ray Bragg diffraction patterns. The proposed\nantiferromagnetic phase of EuPdSn2 supports anapoles (magnetic polar dipoles)\nalready observed in magnetic neutron diffraction patterns presented by Gd doped\nSmAl2, and several resonant x-ray diffraction patterns."
                },
                "authors": [
                    {
                        "name": "Stephen W. Lovesey"
                    }
                ],
                "author_detail": {
                    "name": "Stephen W. Lovesey"
                },
                "author": "Stephen W. Lovesey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21599v2",
                "updated": "2025-06-30T11:36:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    36,
                    12,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-19T02:51:10Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    51,
                    10,
                    3,
                    170,
                    0
                ],
                "title": "Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next\n  Point-of-Interest Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next\n  Point-of-Interest Recommendation"
                },
                "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance."
                },
                "authors": [
                    {
                        "name": "Peibo Li"
                    },
                    {
                        "name": "Shuang Ao"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Johan Barthlemy"
                    },
                    {
                        "name": "Tomasz Bednarz"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23744v1",
                "updated": "2025-06-30T11:35:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    35,
                    24,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:35:24Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    35,
                    24,
                    0,
                    181,
                    0
                ],
                "title": "On sample-based functional observability of linear systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On sample-based functional observability of linear systems"
                },
                "summary": "Sample-based observability characterizes the ability to reconstruct the\ninternal state of a dynamical system by using limited output information, i.e.,\nwhen measurements are only infrequently and/or irregularly available. In this\nwork, we investigate the concept of functional observability, which refers to\nthe ability to infer a function of the system state from the outputs, within a\nsamplebased framework. Here, we give necessary and sufficient conditions for a\nsystem to be sample-based functionally observable, and formulate conditions on\nthe sampling schemes such that these are satisfied. Furthermore, we provide a\nnumerical example, where we demonstrate the applicability of the obtained\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-based observability characterizes the ability to reconstruct the\ninternal state of a dynamical system by using limited output information, i.e.,\nwhen measurements are only infrequently and/or irregularly available. In this\nwork, we investigate the concept of functional observability, which refers to\nthe ability to infer a function of the system state from the outputs, within a\nsamplebased framework. Here, we give necessary and sufficient conditions for a\nsystem to be sample-based functionally observable, and formulate conditions on\nthe sampling schemes such that these are satisfied. Furthermore, we provide a\nnumerical example, where we demonstrate the applicability of the obtained\nresults."
                },
                "authors": [
                    {
                        "name": "Isabelle Krauss"
                    },
                    {
                        "name": "Victor G. Lopez"
                    },
                    {
                        "name": "Matthias A. Mller"
                    }
                ],
                "author_detail": {
                    "name": "Matthias A. Mller"
                },
                "author": "Matthias A. Mller",
                "arxiv_doi": "10.1109/LCSYS.2025.3582512",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LCSYS.2025.3582512",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.23744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Control Systems Letters, 2025",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23735v1",
                "updated": "2025-06-30T11:18:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    18,
                    56,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:18:56Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    18,
                    56,
                    0,
                    181,
                    0
                ],
                "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM\n  Evaluation Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM\n  Evaluation Data"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval."
                },
                "authors": [
                    {
                        "name": "JiaRu Wu"
                    },
                    {
                        "name": "Mingwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Liu"
                },
                "author": "Mingwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13271v2",
                "updated": "2025-06-30T11:12:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    12,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-19T15:52:19Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    52,
                    19,
                    0,
                    139,
                    0
                ],
                "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD private test set, our 7B model achieves 71.72\\% execution\naccuracy, while the 32B model achieves 73.67\\%. The code has been open sourced\nat https://github.com/CycloneBoy/csc_sql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD private test set, our 7B model achieves 71.72\\% execution\naccuracy, while the 32B model achieves 73.67\\%. The code has been open sourced\nat https://github.com/CycloneBoy/csc_sql."
                },
                "authors": [
                    {
                        "name": "Lei Sheng"
                    },
                    {
                        "name": "Shuai-Shuai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai-Shuai Xu"
                },
                "author": "Shuai-Shuai Xu",
                "arxiv_comment": "25 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23731v1",
                "updated": "2025-06-30T11:08:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    8,
                    10,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:08:10Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    8,
                    10,
                    0,
                    181,
                    0
                ],
                "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative\n  Models"
                },
                "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images."
                },
                "authors": [
                    {
                        "name": "Michel Meintz"
                    },
                    {
                        "name": "Jan Dubiski"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10558v2",
                "updated": "2025-06-30T10:59:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    59,
                    44,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-12T10:31:23Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    10,
                    31,
                    23,
                    3,
                    163,
                    0
                ],
                "title": "StepProof: Step-by-step verification of natural language mathematical\n  proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepProof: Step-by-step verification of natural language mathematical\n  proofs"
                },
                "summary": "Interactive theorem provers (ITPs) are powerful tools for the formal\nverification of mathematical proofs down to the axiom level. However, their\nlack of a natural language interface remains a significant limitation. Recent\nadvancements in large language models (LLMs) have enhanced the understanding of\nnatural language inputs, paving the way for autoformalization - the process of\ntranslating natural language proofs into formal proofs that can be verified.\nDespite these advancements, existing autoformalization approaches are limited\nto verifying complete proofs and lack the capability for finer, sentence-level\nverification. To address this gap, we propose StepProof, a novel\nautoformalization method designed for granular, step-by-step verification.\nStepProof breaks down complete proofs into multiple verifiable subproofs,\nenabling sentence-level verification. Experimental results demonstrate that\nStepProof significantly improves proof success rates and efficiency compared to\ntraditional methods. Additionally, we found that minor manual adjustments to\nthe natural language proofs, tailoring them for step-level verification,\nfurther enhanced StepProof's performance in autoformalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive theorem provers (ITPs) are powerful tools for the formal\nverification of mathematical proofs down to the axiom level. However, their\nlack of a natural language interface remains a significant limitation. Recent\nadvancements in large language models (LLMs) have enhanced the understanding of\nnatural language inputs, paving the way for autoformalization - the process of\ntranslating natural language proofs into formal proofs that can be verified.\nDespite these advancements, existing autoformalization approaches are limited\nto verifying complete proofs and lack the capability for finer, sentence-level\nverification. To address this gap, we propose StepProof, a novel\nautoformalization method designed for granular, step-by-step verification.\nStepProof breaks down complete proofs into multiple verifiable subproofs,\nenabling sentence-level verification. Experimental results demonstrate that\nStepProof significantly improves proof success rates and efficiency compared to\ntraditional methods. Additionally, we found that minor manual adjustments to\nthe natural language proofs, tailoring them for step-level verification,\nfurther enhanced StepProof's performance in autoformalization."
                },
                "authors": [
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Qinghua Zhou"
                    },
                    {
                        "name": "Bogdan Grechuk"
                    },
                    {
                        "name": "Ivan Y. Tyukin"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Y. Tyukin"
                },
                "author": "Ivan Y. Tyukin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23719v1",
                "updated": "2025-06-30T10:49:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    49,
                    21,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:49:21Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    49,
                    21,
                    0,
                    181,
                    0
                ],
                "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DABstep: Data Agent Benchmark for Multi-step Reasoning"
                },
                "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis."
                },
                "authors": [
                    {
                        "name": "Alex Egg"
                    },
                    {
                        "name": "Martin Iglesias Goyanes"
                    },
                    {
                        "name": "Friso Kingma"
                    },
                    {
                        "name": "Andreu Mora"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Thomas Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wolf"
                },
                "author": "Thomas Wolf",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23714v1",
                "updated": "2025-06-30T10:41:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    41,
                    33,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:41:33Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    41,
                    33,
                    0,
                    181,
                    0
                ],
                "title": "Towards an Automated Multimodal Approach for Video Summarization:\n  Building a Bridge Between Text, Audio and Facial Cue-Based Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an Automated Multimodal Approach for Video Summarization:\n  Building a Bridge Between Text, Audio and Facial Cue-Based Summarization"
                },
                "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries."
                },
                "authors": [
                    {
                        "name": "Md Moinul Islam"
                    },
                    {
                        "name": "Sofoklis Kakouros"
                    },
                    {
                        "name": "Janne Heikkil"
                    },
                    {
                        "name": "Mourad Oussalah"
                    }
                ],
                "author_detail": {
                    "name": "Mourad Oussalah"
                },
                "author": "Mourad Oussalah",
                "arxiv_comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23712v1",
                "updated": "2025-06-30T10:38:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    38,
                    12,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:38:12Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    38,
                    12,
                    0,
                    181,
                    0
                ],
                "title": "Predicting any small-scale statistics of high-Reynolds-number turbulence\n  using ensemble simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting any small-scale statistics of high-Reynolds-number turbulence\n  using ensemble simulations"
                },
                "summary": "The complex small-scale statistics of turbulence are a result of the combined\ncascading dynamics through all scales of the flow. Predicting these statistics\nusing fully resolved simulations at the high Reynolds numbers that typically\noccur in engineering, geophysical, and astrophysical flows will exceed the\ncapabilities of even the largest supercomputers for the foreseeable future. A\ncommon observation is that high-Reynolds-number flows are organized in clusters\nof intense turbulent activity separated by large regions of quiescent flow. We\nhere show that small-scale statistics in high-Reynolds-number turbulence can be\npredicted based on an ensemble hypothesis, stating that they can be emulated by\nthe statistical mixture of a heterogeneous ensemble of lower-Reynolds-number\nsimulations. These simulations are forced at smaller scales with energy\ninjection rates varying across the ensemble. We show that our method predicts\ncomplex gradient statistics from the recent literature, including the joint\nQR-PDF and extreme dissipation and enstrophy statistics, to unprecedented\naccuracy. Remarkably, we find that the weight distribution needed for the\nensemble method to make predictions can be inferred from the anomalous scaling\nexponents of turbulence. Thus combining theory with fully resolved simulations,\nour method can be readily applied to predict a wide range of statistics at high\nReynolds number but low cost, while opening up various avenues for further\ntheoretical and numerical exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complex small-scale statistics of turbulence are a result of the combined\ncascading dynamics through all scales of the flow. Predicting these statistics\nusing fully resolved simulations at the high Reynolds numbers that typically\noccur in engineering, geophysical, and astrophysical flows will exceed the\ncapabilities of even the largest supercomputers for the foreseeable future. A\ncommon observation is that high-Reynolds-number flows are organized in clusters\nof intense turbulent activity separated by large regions of quiescent flow. We\nhere show that small-scale statistics in high-Reynolds-number turbulence can be\npredicted based on an ensemble hypothesis, stating that they can be emulated by\nthe statistical mixture of a heterogeneous ensemble of lower-Reynolds-number\nsimulations. These simulations are forced at smaller scales with energy\ninjection rates varying across the ensemble. We show that our method predicts\ncomplex gradient statistics from the recent literature, including the joint\nQR-PDF and extreme dissipation and enstrophy statistics, to unprecedented\naccuracy. Remarkably, we find that the weight distribution needed for the\nensemble method to make predictions can be inferred from the anomalous scaling\nexponents of turbulence. Thus combining theory with fully resolved simulations,\nour method can be readily applied to predict a wide range of statistics at high\nReynolds number but low cost, while opening up various avenues for further\ntheoretical and numerical exploration."
                },
                "authors": [
                    {
                        "name": "Lukas Bentkamp"
                    },
                    {
                        "name": "Michael Wilczek"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wilczek"
                },
                "author": "Michael Wilczek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02231v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02231v3",
                "updated": "2025-06-30T10:36:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    36,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2024-09-03T18:59:20Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    18,
                    59,
                    20,
                    1,
                    247,
                    0
                ],
                "title": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space\n  Exploration"
                },
                "summary": "Here we show that a general-purpose large language model (LLM) chatbot,\nLlama-3.1-8B-Instruct, can be transformed via supervised fine-tuning of\nengineered prompts into a chemical language model (CLM), SmileyLlama, for\nmolecule generation. We benchmark SmileyLlama by comparing it to CLMs trained\nfrom scratch on large amounts of ChEMBL data for their ability to generate\nvalid and novel drug-like molecules. We also use direct preference optimization\nto both improve SmileyLlama's adherence to a prompt and to generate molecules\nwithin the iMiner reinforcement learning framework to predict new drug\nmolecules with optimized 3D conformations and high binding affinity to drug\ntargets, illustrated with the SARS-Cov-2 Main Protease. This overall framework\nallows a LLM to speak directly as a CLM which can generate molecules with\nuser-specified properties, rather than acting only as a chatbot with knowledge\nof chemistry or as a helpful virtual assistant. While our dataset and analyses\nare geared toward drug discovery, this general procedure can be extended to\nother chemical applications such as chemical synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here we show that a general-purpose large language model (LLM) chatbot,\nLlama-3.1-8B-Instruct, can be transformed via supervised fine-tuning of\nengineered prompts into a chemical language model (CLM), SmileyLlama, for\nmolecule generation. We benchmark SmileyLlama by comparing it to CLMs trained\nfrom scratch on large amounts of ChEMBL data for their ability to generate\nvalid and novel drug-like molecules. We also use direct preference optimization\nto both improve SmileyLlama's adherence to a prompt and to generate molecules\nwithin the iMiner reinforcement learning framework to predict new drug\nmolecules with optimized 3D conformations and high binding affinity to drug\ntargets, illustrated with the SARS-Cov-2 Main Protease. This overall framework\nallows a LLM to speak directly as a CLM which can generate molecules with\nuser-specified properties, rather than acting only as a chatbot with knowledge\nof chemistry or as a helpful virtual assistant. While our dataset and analyses\nare geared toward drug discovery, this general procedure can be extended to\nother chemical applications such as chemical synthesis."
                },
                "authors": [
                    {
                        "name": "Joseph M. Cavanagh"
                    },
                    {
                        "name": "Kunyang Sun"
                    },
                    {
                        "name": "Andrew Gritsevskiy"
                    },
                    {
                        "name": "Dorian Bagni"
                    },
                    {
                        "name": "Yingze Wang"
                    },
                    {
                        "name": "Thomas D. Bannister"
                    },
                    {
                        "name": "Teresa Head-Gordon"
                    }
                ],
                "author_detail": {
                    "name": "Teresa Head-Gordon"
                },
                "author": "Teresa Head-Gordon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02231v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02231v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23705v1",
                "updated": "2025-06-30T10:29:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    29,
                    33,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:29:33Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    29,
                    33,
                    0,
                    181,
                    0
                ],
                "title": "Single Image Test-Time Adaptation via Multi-View Co-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single Image Test-Time Adaptation via Multi-View Co-Training"
                },
                "summary": "Test-time adaptation enables a trained model to adjust to a new domain during\ninference, making it particularly valuable in clinical settings where such\non-the-fly adaptation is required. However, existing techniques depend on large\ntarget domain datasets, which are often impractical and unavailable in medical\nscenarios that demand per-patient, real-time inference. Moreover, current\nmethods commonly focus on two-dimensional images, failing to leverage the\nvolumetric richness of medical imaging data. Bridging this gap, we propose a\nPatch-Based Multi-View Co-Training method for Single Image Test-Time\nadaptation. Our method enforces feature and prediction consistency through\nuncertainty-guided self-training, enabling effective volumetric segmentation in\nthe target domain with only a single test-time image. Validated on three\npublicly available breast magnetic resonance imaging datasets for tumor\nsegmentation, our method achieves performance close to the upper bound\nsupervised benchmark while also outperforming all existing state-of-the-art\nmethods, on average by a Dice Similarity Coefficient of 3.75%. We publicly\nshare our accessible codebase, readily integrable with the popular nnUNet\nframework, at https://github.com/smriti-joshi/muvi.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation enables a trained model to adjust to a new domain during\ninference, making it particularly valuable in clinical settings where such\non-the-fly adaptation is required. However, existing techniques depend on large\ntarget domain datasets, which are often impractical and unavailable in medical\nscenarios that demand per-patient, real-time inference. Moreover, current\nmethods commonly focus on two-dimensional images, failing to leverage the\nvolumetric richness of medical imaging data. Bridging this gap, we propose a\nPatch-Based Multi-View Co-Training method for Single Image Test-Time\nadaptation. Our method enforces feature and prediction consistency through\nuncertainty-guided self-training, enabling effective volumetric segmentation in\nthe target domain with only a single test-time image. Validated on three\npublicly available breast magnetic resonance imaging datasets for tumor\nsegmentation, our method achieves performance close to the upper bound\nsupervised benchmark while also outperforming all existing state-of-the-art\nmethods, on average by a Dice Similarity Coefficient of 3.75%. We publicly\nshare our accessible codebase, readily integrable with the popular nnUNet\nframework, at https://github.com/smriti-joshi/muvi.git."
                },
                "authors": [
                    {
                        "name": "Smriti Joshi"
                    },
                    {
                        "name": "Richard Osuala"
                    },
                    {
                        "name": "Lidia Garrucho"
                    },
                    {
                        "name": "Kaisar Kushibar"
                    },
                    {
                        "name": "Dimitri Kessler"
                    },
                    {
                        "name": "Oliver Diaz"
                    },
                    {
                        "name": "Karim Lekadir"
                    }
                ],
                "author_detail": {
                    "name": "Karim Lekadir"
                },
                "author": "Karim Lekadir",
                "arxiv_comment": "MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15010v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15010v3",
                "updated": "2025-06-30T10:25:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    25,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2024-09-23T13:36:34Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    36,
                    34,
                    0,
                    267,
                    0
                ],
                "title": "DepthART: Monocular Depth Estimation as Autoregressive Refinement Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DepthART: Monocular Depth Estimation as Autoregressive Refinement Task"
                },
                "summary": "Monocular depth estimation has seen significant advances through\ndiscriminative approaches, yet their performance remains constrained by the\nlimitations of training datasets. While generative approaches have addressed\nthis challenge by leveraging priors from internet-scale datasets, with recent\nstudies showing state-of-the-art results using fine-tuned text-to-image\ndiffusion models, there is still room for improvement. Notably, autoregressive\ngenerative approaches, particularly Visual AutoRegressive modeling, have\ndemonstrated superior results compared to diffusion models in conditioned image\nsynthesis, while offering faster inference times. In this work, we apply Visual\nAutoregressive Transformer (VAR) to the monocular depth estimation problem.\nHowever, the conventional GPT-2-style training procedure (teacher forcing)\ninherited by VAR yields suboptimal results for depth estimation. To address\nthis limitation, we introduce DepthART - a novel training method formulated as\na Depth Autoregressive Refinement Task. Unlike traditional VAR training with\nstatic inputs and targets, our method implements a dynamic target formulation\nbased on model outputs, enabling self-refinement. By utilizing the model's own\npredictions as inputs instead of ground truth token maps during training, we\nframe the objective as residual minimization, effectively reducing the\ndiscrepancy between training and inference procedures. Our experimental results\ndemonstrate that the proposed training approach significantly enhances the\nperformance of VAR in depth estimation tasks. When trained on Hypersim dataset\nusing our approach, the model achieves superior results across multiple unseen\nbenchmarks compared to existing generative and discriminative baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular depth estimation has seen significant advances through\ndiscriminative approaches, yet their performance remains constrained by the\nlimitations of training datasets. While generative approaches have addressed\nthis challenge by leveraging priors from internet-scale datasets, with recent\nstudies showing state-of-the-art results using fine-tuned text-to-image\ndiffusion models, there is still room for improvement. Notably, autoregressive\ngenerative approaches, particularly Visual AutoRegressive modeling, have\ndemonstrated superior results compared to diffusion models in conditioned image\nsynthesis, while offering faster inference times. In this work, we apply Visual\nAutoregressive Transformer (VAR) to the monocular depth estimation problem.\nHowever, the conventional GPT-2-style training procedure (teacher forcing)\ninherited by VAR yields suboptimal results for depth estimation. To address\nthis limitation, we introduce DepthART - a novel training method formulated as\na Depth Autoregressive Refinement Task. Unlike traditional VAR training with\nstatic inputs and targets, our method implements a dynamic target formulation\nbased on model outputs, enabling self-refinement. By utilizing the model's own\npredictions as inputs instead of ground truth token maps during training, we\nframe the objective as residual minimization, effectively reducing the\ndiscrepancy between training and inference procedures. Our experimental results\ndemonstrate that the proposed training approach significantly enhances the\nperformance of VAR in depth estimation tasks. When trained on Hypersim dataset\nusing our approach, the model achieves superior results across multiple unseen\nbenchmarks compared to existing generative and discriminative baselines."
                },
                "authors": [
                    {
                        "name": "Bulat Gabdullin"
                    },
                    {
                        "name": "Nina Konovalova"
                    },
                    {
                        "name": "Nikolay Patakin"
                    },
                    {
                        "name": "Dmitry Senushkin"
                    },
                    {
                        "name": "Anton Konushin"
                    }
                ],
                "author_detail": {
                    "name": "Anton Konushin"
                },
                "author": "Anton Konushin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15010v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15010v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15397v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15397v3",
                "updated": "2025-06-30T10:22:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    22,
                    45,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-23T00:47:13Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    0,
                    47,
                    13,
                    5,
                    328,
                    0
                ],
                "title": "Efficient Online Inference of Vision Transformers by Training-Free\n  Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Online Inference of Vision Transformers by Training-Free\n  Tokenization"
                },
                "summary": "The cost of deploying vision transformers increasingly represents a barrier\nto wider industrial adoption. Existing compression techniques require\nadditional end-to-end fine-tuning or incur a significant drawback to runtime,\nmaking them ill-suited for online (real-time) inference, where a prediction is\nmade on any new input as it comes in. We introduce the $\\textbf{Visual Word\nTokenizer}$ (VWT), a training-free method for reducing power costs while\nretaining performance and runtime. The VWT groups visual subwords (image\npatches) that are frequently used into visual words while infrequent ones\nremain intact. To do so, $\\textit{intra}$-image or $\\textit{inter}$-image\nstatistics are leveraged to identify similar visual concepts for sequence\ncompression. Experimentally, we demonstrate a reduction in wattage of up to 25%\nwith only a 20% increase in runtime at most. Comparative approaches of 8-bit\nquantization and token merging achieve a lower or similar power efficiency but\nexact a higher toll on runtime (up to 100% or more). Our results indicate that\nVWTs are well-suited for efficient online inference with a marginal compromise\non performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cost of deploying vision transformers increasingly represents a barrier\nto wider industrial adoption. Existing compression techniques require\nadditional end-to-end fine-tuning or incur a significant drawback to runtime,\nmaking them ill-suited for online (real-time) inference, where a prediction is\nmade on any new input as it comes in. We introduce the $\\textbf{Visual Word\nTokenizer}$ (VWT), a training-free method for reducing power costs while\nretaining performance and runtime. The VWT groups visual subwords (image\npatches) that are frequently used into visual words while infrequent ones\nremain intact. To do so, $\\textit{intra}$-image or $\\textit{inter}$-image\nstatistics are leveraged to identify similar visual concepts for sequence\ncompression. Experimentally, we demonstrate a reduction in wattage of up to 25%\nwith only a 20% increase in runtime at most. Comparative approaches of 8-bit\nquantization and token merging achieve a lower or similar power efficiency but\nexact a higher toll on runtime (up to 100% or more). Our results indicate that\nVWTs are well-suited for efficient online inference with a marginal compromise\non performance."
                },
                "authors": [
                    {
                        "name": "Leonidas Gee"
                    },
                    {
                        "name": "Wing Yan Li"
                    },
                    {
                        "name": "Viktoriia Sharmanska"
                    },
                    {
                        "name": "Novi Quadrianto"
                    }
                ],
                "author_detail": {
                    "name": "Novi Quadrianto"
                },
                "author": "Novi Quadrianto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15397v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15397v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23694v1",
                "updated": "2025-06-30T10:15:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    15,
                    44,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:15:44Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    15,
                    44,
                    0,
                    181,
                    0
                ],
                "title": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language\n  Models to Support User Scenario Writing for User Experience Experts and\n  Laypersons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language\n  Models to Support User Scenario Writing for User Experience Experts and\n  Laypersons"
                },
                "summary": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask."
                },
                "authors": [
                    {
                        "name": "Patrick Stadler"
                    },
                    {
                        "name": "Christopher Lazik"
                    },
                    {
                        "name": "Christopher Katins"
                    },
                    {
                        "name": "Thomas Kosch"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Kosch"
                },
                "author": "Thomas Kosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23692v1",
                "updated": "2025-06-30T10:11:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    11,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:11:39Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    11,
                    39,
                    0,
                    181,
                    0
                ],
                "title": "Agent4S: The Transformation of Research Paradigms from the Perspective\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent4S: The Transformation of Research Paradigms from the Perspective\n  of Large Language Models"
                },
                "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery."
                },
                "authors": [
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Zerui Fang"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yiwen Chen"
                    },
                    {
                        "name": "Cunshi Wang"
                    },
                    {
                        "name": "Mengwei Qu"
                    },
                    {
                        "name": "Lei Lei"
                    },
                    {
                        "name": "Zhen Feng"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Mingzhou Tan"
                    },
                    {
                        "name": "Jiaji Wu"
                    },
                    {
                        "name": "Jianwei Shuai"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Fangfu Ye"
                    }
                ],
                "author_detail": {
                    "name": "Fangfu Ye"
                },
                "author": "Fangfu Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23689v1",
                "updated": "2025-06-30T10:09:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    9,
                    13,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:09:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    9,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "PokAI: A Goal-Generating, Battle-Optimizing Multi-agent System for\n  Pokemon Red",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokAI: A Goal-Generating, Battle-Optimizing Multi-agent System for\n  Pokemon Red"
                },
                "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors."
                },
                "authors": [
                    {
                        "name": "Zihao Liu"
                    },
                    {
                        "name": "Xinhang Sui"
                    },
                    {
                        "name": "Yueran Song"
                    },
                    {
                        "name": "Siwen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Siwen Wang"
                },
                "author": "Siwen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22370v2",
                "updated": "2025-06-30T10:08:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    8,
                    5,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T16:34:13Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    13,
                    4,
                    178,
                    0
                ],
                "title": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny"
                },
                "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. Our findings show that\nstudents perform significantly better when using ChatGPT; however, performance\ngains are tied to prompt quality. We conclude with practical recommendations\nfor integrating LLMs into formal methods courses more effectively, including\ndesigning LLM-aware challenges that promote learning rather than substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. Our findings show that\nstudents perform significantly better when using ChatGPT; however, performance\ngains are tied to prompt quality. We conclude with practical recommendations\nfor integrating LLMs into formal methods courses more effectively, including\ndesigning LLM-aware challenges that promote learning rather than substitution."
                },
                "authors": [
                    {
                        "name": "Carolina Carreira"
                    },
                    {
                        "name": "lvaro Silva"
                    },
                    {
                        "name": "Alexandre Abreu"
                    },
                    {
                        "name": "Alexandra Mendes"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Mendes"
                },
                "author": "Alexandra Mendes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10718v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10718v5",
                "updated": "2025-06-30T10:03:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    3,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-14T07:22:03Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    7,
                    22,
                    3,
                    5,
                    349,
                    0
                ],
                "title": "Grid: Omni Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grid: Omni Visual Generation"
                },
                "summary": "Visual generation has witnessed remarkable progress in single-image tasks,\nyet extending these capabilities to temporal sequences remains challenging.\nCurrent approaches either build specialized video models from scratch with\nenormous computational costs or add separate motion modules to image\ngenerators, both requiring learning temporal dynamics anew. We observe that\nmodern image generation models possess underutilized potential in handling\nstructured layouts with implicit temporal understanding. Building on this\ninsight, we introduce GRID, which reformulates temporal sequences as grid\nlayouts, enabling holistic processing of visual sequences while leveraging\nexisting model capabilities. Through a parallel flow-matching training strategy\nwith coarse-to-fine scheduling, our approach achieves up to 67 faster inference\nspeeds while using <1/1000 of the computational resources compared to\nspecialized models. Extensive experiments demonstrate that GRID not only excels\nin temporal tasks from Text-to-Video to 3D Editing but also preserves strong\nperformance in image generation, establishing itself as an efficient and\nversatile omni-solution for visual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation has witnessed remarkable progress in single-image tasks,\nyet extending these capabilities to temporal sequences remains challenging.\nCurrent approaches either build specialized video models from scratch with\nenormous computational costs or add separate motion modules to image\ngenerators, both requiring learning temporal dynamics anew. We observe that\nmodern image generation models possess underutilized potential in handling\nstructured layouts with implicit temporal understanding. Building on this\ninsight, we introduce GRID, which reformulates temporal sequences as grid\nlayouts, enabling holistic processing of visual sequences while leveraging\nexisting model capabilities. Through a parallel flow-matching training strategy\nwith coarse-to-fine scheduling, our approach achieves up to 67 faster inference\nspeeds while using <1/1000 of the computational resources compared to\nspecialized models. Extensive experiments demonstrate that GRID not only excels\nin temporal tasks from Text-to-Video to 3D Editing but also preserves strong\nperformance in image generation, establishing itself as an efficient and\nversatile omni-solution for visual generation."
                },
                "authors": [
                    {
                        "name": "Cong Wan"
                    },
                    {
                        "name": "Xiangyang Luo"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Zijian Cai"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "arxiv_comment": "Codes: https://github.com/Should-AI-Lab/GRID",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10718v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10718v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.24124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24124v2",
                "updated": "2025-07-01T03:40:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    40,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    59,
                    14,
                    0,
                    181,
                    0
                ],
                "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives"
                },
                "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP."
                },
                "authors": [
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Teresa Wu"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "arxiv_comment": "Code: https://github.com/Ironieser/TimesCLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24120v1",
                "updated": "2025-06-30T17:58:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    30,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:58:30Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    30,
                    0,
                    181,
                    0
                ],
                "title": "Data Uniformity Improves Training Efficiency and More, with a\n  Convergence Framework Beyond the NTK Regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Uniformity Improves Training Efficiency and More, with a\n  Convergence Framework Beyond the NTK Regime"
                },
                "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Shangding Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shangding Gu"
                },
                "author": "Shangding Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24119v2",
                "updated": "2025-07-01T02:29:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    29,
                    52,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:58:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning"
                },
                "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Leon Guertler"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Daniel Balcells"
                    },
                    {
                        "name": "Mickel Liu"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24118v1",
                "updated": "2025-06-30T17:57:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    57,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:57:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    57,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Scaling Human Judgment in Community Notes with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Human Judgment in Community Notes with LLMs"
                },
                "summary": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Soham De"
                    },
                    {
                        "name": "Manon Revel"
                    },
                    {
                        "name": "Andreas Haupt"
                    },
                    {
                        "name": "Brad Miller"
                    },
                    {
                        "name": "Keith Coleman"
                    },
                    {
                        "name": "Jay Baxter"
                    },
                    {
                        "name": "Martin Saveski"
                    },
                    {
                        "name": "Michiel A. Bakker"
                    }
                ],
                "author_detail": {
                    "name": "Michiel A. Bakker"
                },
                "author": "Michiel A. Bakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02113v2",
                "updated": "2025-06-30T17:50:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    50,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-03T03:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    10,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Trust & Safety of LLMs and LLMs in Trust & Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust & Safety of LLMs and LLMs in Trust & Safety"
                },
                "summary": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm."
                },
                "authors": [
                    {
                        "name": "Doohee You"
                    },
                    {
                        "name": "Dan Chon"
                    }
                ],
                "author_detail": {
                    "name": "Dan Chon"
                },
                "author": "Dan Chon",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02811v2",
                "updated": "2025-06-30T17:46:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    46,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T17:39:35Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    39,
                    35,
                    0,
                    125,
                    0
                ],
                "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing"
                },
                "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance. This paper aims to\naddress these limitations by introducing a new framework, SIM-RAG, to\nexplicitly enhance RAG systems' self-awareness and multi-round retrieval\ncapabilities. To train SIM-RAG, we first let a RAG system self-practice\nmulti-round retrieval, augmenting existing question-answer pairs with\nintermediate inner monologue reasoning steps to generate synthetic training\ndata. For each pair, the system may explore multiple retrieval paths, which are\nlabeled as successful if they reach the correct answer and unsuccessful\notherwise. Using this data, we train a lightweight information sufficiency\nCritic. At inference time, the Critic evaluates whether the RAG system has\nretrieved sufficient information at each round, guiding retrieval decisions and\nimproving system-level self-awareness through in-context reinforcement\nlearning. Experiments across multiple prominent RAG benchmarks show that\nSIM-RAG is an effective multi-round RAG solution. Furthermore, this framework\nis system-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance. This paper aims to\naddress these limitations by introducing a new framework, SIM-RAG, to\nexplicitly enhance RAG systems' self-awareness and multi-round retrieval\ncapabilities. To train SIM-RAG, we first let a RAG system self-practice\nmulti-round retrieval, augmenting existing question-answer pairs with\nintermediate inner monologue reasoning steps to generate synthetic training\ndata. For each pair, the system may explore multiple retrieval paths, which are\nlabeled as successful if they reach the correct answer and unsuccessful\notherwise. Using this data, we train a lightweight information sufficiency\nCritic. At inference time, the Critic evaluates whether the RAG system has\nretrieved sufficient information at each round, guiding retrieval decisions and\nimproving system-level self-awareness through in-context reinforcement\nlearning. Experiments across multiple prominent RAG benchmarks show that\nSIM-RAG is an effective multi-round RAG solution. Furthermore, this framework\nis system-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."
                },
                "authors": [
                    {
                        "name": "Diji Yang"
                    },
                    {
                        "name": "Linda Zeng"
                    },
                    {
                        "name": "Jinmeng Rao"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "arxiv_comment": "Proceedings of the 48th International ACM SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18797v2",
                "updated": "2025-06-30T17:45:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    45,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-27T22:46:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    22,
                    46,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?"
                },
                "summary": "Recent advancements in LLMs unlearning have shown remarkable success in\nremoving unwanted data-model influences while preserving the model's utility\nfor legitimate knowledge. Despite these strides, sparse Mixture-of-Experts\n(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the\ncontext of unlearning. As MoE LLMs are celebrated for their exceptional\nperformance, we ask:How can unlearning be performed effectively and efficiently\non MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs\nintroduces unique challenges, leading to excessive forgetting, uncontrolled\nknowledge erasure and substantial utility drops when existing unlearning\nmethods are applied. To address this, we propose a novel Selected-Expert\nUnlearning Framework (SEUF). Through expert attribution, unlearning is\nconcentrated on the most actively engaged experts for the specified knowledge.\nConcurrently, an anchor loss is applied to the router to stabilize the active\nstate of this targeted expert, ensuring focused and controlled unlearning. SEUF\nis compatible with various standard unlearning algorithms. Extensive\nexperiments demonstrate that SEUF enhances both forget quality up to 5% and\nmodel utility by 35% on MoE LLMs across various benchmarks and LLM\narchitectures (compared to standard unlearning algorithms), while only\nunlearning 0.06% of the model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs unlearning have shown remarkable success in\nremoving unwanted data-model influences while preserving the model's utility\nfor legitimate knowledge. Despite these strides, sparse Mixture-of-Experts\n(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the\ncontext of unlearning. As MoE LLMs are celebrated for their exceptional\nperformance, we ask:How can unlearning be performed effectively and efficiently\non MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs\nintroduces unique challenges, leading to excessive forgetting, uncontrolled\nknowledge erasure and substantial utility drops when existing unlearning\nmethods are applied. To address this, we propose a novel Selected-Expert\nUnlearning Framework (SEUF). Through expert attribution, unlearning is\nconcentrated on the most actively engaged experts for the specified knowledge.\nConcurrently, an anchor loss is applied to the router to stabilize the active\nstate of this targeted expert, ensuring focused and controlled unlearning. SEUF\nis compatible with various standard unlearning algorithms. Extensive\nexperiments demonstrate that SEUF enhances both forget quality up to 5% and\nmodel utility by 35% on MoE LLMs across various benchmarks and LLM\narchitectures (compared to standard unlearning algorithms), while only\nunlearning 0.06% of the model parameters."
                },
                "authors": [
                    {
                        "name": "Haomin Zhuang"
                    },
                    {
                        "name": "Yihua Zhang"
                    },
                    {
                        "name": "Kehan Guo"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Sijia Liu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "arxiv_comment": "Accepted to ACL'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17677v2",
                "updated": "2025-06-30T17:30:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    30,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-24T15:47:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    47,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models"
                },
                "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. Based on interviews with teaching staff, this paper\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\nteaching staff and students in the process of solving exercises. INSIGHT has a\nmodular design that allows it to be integrated into various higher education\ncourses. We analyze students' questions to an LLM by extracting keywords, which\nwe use to dynamically build an FAQ from students' questions and provide new\ninsights for the teaching staff to use for more personalized face-to-face\nsupport. Future work could build upon INSIGHT by using the collected data to\nprovide adaptive learning and adjust content based on student progress and\nlearning styles to offer a more interactive and inclusive learning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. Based on interviews with teaching staff, this paper\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\nteaching staff and students in the process of solving exercises. INSIGHT has a\nmodular design that allows it to be integrated into various higher education\ncourses. We analyze students' questions to an LLM by extracting keywords, which\nwe use to dynamically build an FAQ from students' questions and provide new\ninsights for the teaching staff to use for more personalized face-to-face\nsupport. Future work could build upon INSIGHT by using the collected data to\nprovide adaptive learning and adjust content based on student progress and\nlearning styles to offer a more interactive and inclusive learning experience."
                },
                "authors": [
                    {
                        "name": "Jarne Thys"
                    },
                    {
                        "name": "Sebe Vanbrabant"
                    },
                    {
                        "name": "Davy Vanacken"
                    },
                    {
                        "name": "Gustavo Rovelo Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Rovelo Ruiz"
                },
                "author": "Gustavo Rovelo Ruiz",
                "arxiv_comment": "Accepted author version for the D-SAIL Workshop - Transformative\n  Curriculum Design: Digitalisation, Sustainability, and AI Literacy for 21st\n  Century Learning, July 22, 2025, Palermo, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24068v1",
                "updated": "2025-06-30T17:21:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    8,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:21:08Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    8,
                    0,
                    181,
                    0
                ],
                "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STACK: Adversarial Attacks on LLM Safeguard Pipelines"
                },
                "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks."
                },
                "authors": [
                    {
                        "name": "Ian R. McKenzie"
                    },
                    {
                        "name": "Oskar J. Hollinsworth"
                    },
                    {
                        "name": "Tom Tseng"
                    },
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Aaron D. Tucker"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Adam Gleave"
                    }
                ],
                "author_detail": {
                    "name": "Adam Gleave"
                },
                "author": "Adam Gleave",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05651v2",
                "updated": "2025-06-30T16:54:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    54,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-08T17:53:41Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    17,
                    53,
                    41,
                    5,
                    39,
                    0
                ],
                "title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for\n  Psychotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for\n  Psychotherapy"
                },
                "summary": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI."
                },
                "authors": [
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Suyeon Lee"
                    },
                    {
                        "name": "Yeongjae Cho"
                    },
                    {
                        "name": "Eunseo Ryu"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Suran Seong"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "Accepted at NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24045v1",
                "updated": "2025-06-30T16:50:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    50,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:50:48Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    50,
                    48,
                    0,
                    181,
                    0
                ],
                "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC"
                },
                "summary": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines."
                },
                "authors": [
                    {
                        "name": "Xinming Wei"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Rui Qu"
                    },
                    {
                        "name": "Maoliang Li"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Guojie Luo"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Luo"
                },
                "author": "Guojie Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00306v2",
                "updated": "2025-06-30T16:37:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    37,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-01T04:01:18Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    1,
                    18,
                    5,
                    32,
                    0
                ],
                "title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference."
                },
                "authors": [
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Yuefeng Peng"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Alina Oprea"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "arxiv_comment": "This is the full version (27 pages) of the paper 'Riddle Me This!\n  Stealthy Membership Inference for Retrieval-Augmented Generation' published\n  at CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08842v2",
                "updated": "2025-06-30T16:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    31,
                    53,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-13T12:58:11Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    58,
                    11,
                    1,
                    133,
                    0
                ],
                "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for\n  Uncovering Hidden Vulnerabilities in Open-Source AI Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for\n  Uncovering Hidden Vulnerabilities in Open-Source AI Libraries"
                },
                "summary": "Open-source AI libraries are foundational to modern AI systems, yet they\npresent significant, underexamined risks spanning security, licensing,\nmaintenance, supply chain integrity, and regulatory compliance. We introduce\nLibVulnWatch, a system that leverages recent advances in large language models\nand agentic workflows to perform deep, evidence-based evaluations of these\nlibraries. Built on a graph-based orchestration of specialized agents, the\nframework extracts, verifies, and quantifies risk using information from\nrepositories, documentation, and vulnerability databases. LibVulnWatch produces\nreproducible, governance-aligned scores across five critical domains,\npublishing results to a public leaderboard for ongoing ecosystem monitoring.\nApplied to 20 widely used libraries, including ML frameworks, LLM inference\nengines, and agent orchestration tools, our approach covers up to 88% of\nOpenSSF Scorecard checks while surfacing up to 19 additional risks per library,\nsuch as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By\nintegrating advanced language technologies with the practical demands of\nsoftware risk assessment, this work demonstrates a scalable, transparent\nmechanism for continuous supply chain evaluation and informed library\nselection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source AI libraries are foundational to modern AI systems, yet they\npresent significant, underexamined risks spanning security, licensing,\nmaintenance, supply chain integrity, and regulatory compliance. We introduce\nLibVulnWatch, a system that leverages recent advances in large language models\nand agentic workflows to perform deep, evidence-based evaluations of these\nlibraries. Built on a graph-based orchestration of specialized agents, the\nframework extracts, verifies, and quantifies risk using information from\nrepositories, documentation, and vulnerability databases. LibVulnWatch produces\nreproducible, governance-aligned scores across five critical domains,\npublishing results to a public leaderboard for ongoing ecosystem monitoring.\nApplied to 20 widely used libraries, including ML frameworks, LLM inference\nengines, and agent orchestration tools, our approach covers up to 88% of\nOpenSSF Scorecard checks while surfacing up to 19 additional risks per library,\nsuch as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By\nintegrating advanced language technologies with the practical demands of\nsoftware risk assessment, this work demonstrates a scalable, transparent\nmechanism for continuous supply chain evaluation and informed library\nselection."
                },
                "authors": [
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Seonglae Cho"
                    },
                    {
                        "name": "Umar Mohammed"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Kleyton Costa"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Koshiyama"
                },
                "author": "Adriano Koshiyama",
                "arxiv_comment": "ACL 2025 Student Research Workshop and ICML 2025 TAIG Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19577v3",
                "updated": "2025-06-30T16:21:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    21,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-26T06:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    6,
                    47,
                    43,
                    0,
                    146,
                    0
                ],
                "title": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous\n  Decoding"
                },
                "summary": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment."
                },
                "authors": [
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Xiaoyu Gu"
                    },
                    {
                        "name": "Yidi Jiang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "Accepted by TASLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24015v1",
                "updated": "2025-06-30T16:19:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:19:38Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "title": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via\n  Layered Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via\n  Layered Knowledge Injection"
                },
                "summary": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24006v1",
                "updated": "2025-06-30T16:10:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    10,
                    42,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:10:42Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    10,
                    42,
                    0,
                    181,
                    0
                ],
                "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping\n  Review from a Mathematics Education Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Don't Make Sense of Word Problems. A Scoping\n  Review from a Mathematics Education Perspective"
                },
                "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms."
                },
                "authors": [
                    {
                        "name": "Anselm R. Strohmaier"
                    },
                    {
                        "name": "Wim Van Dooren"
                    },
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Brian Greer"
                    },
                    {
                        "name": "Lieven Verschaffel"
                    }
                ],
                "author_detail": {
                    "name": "Lieven Verschaffel"
                },
                "author": "Lieven Verschaffel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.HO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23998v1",
                "updated": "2025-06-30T16:02:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    2,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:02:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    2,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via\n  Multi-Agent Large Language Models with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via\n  Multi-Agent Large Language Models with Reinforcement Learning"
                },
                "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts."
                },
                "authors": [
                    {
                        "name": "Seungjun Yi"
                    },
                    {
                        "name": "Joakim Nguyen"
                    },
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Terence Lim"
                    },
                    {
                        "name": "Andrew Well"
                    },
                    {
                        "name": "Mia Markey"
                    },
                    {
                        "name": "Ying Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ying Ding"
                },
                "author": "Ying Ding",
                "arxiv_comment": "Presented at ACL 2025 SRW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16084v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16084v3",
                "updated": "2025-06-30T15:59:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    59,
                    26,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-22T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "TTRL: Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTRL: Test-Time Reinforcement Learning"
                },
                "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16084v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16084v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23995v1",
                "updated": "2025-06-30T15:58:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    58,
                    10,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:58:10Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    58,
                    10,
                    0,
                    181,
                    0
                ],
                "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems"
                },
                "summary": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline."
                },
                "authors": [
                    {
                        "name": "Mingfei Cheng"
                    },
                    {
                        "name": "Renzhi Wang"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23979v1",
                "updated": "2025-06-30T15:45:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:45:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference\n  Data Generation"
                },
                "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger."
                },
                "authors": [
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Tianhao Shen"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Dan Shi"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Wuwei Huang"
                    },
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "33 pages, 15 tables, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23978v1",
                "updated": "2025-06-30T15:45:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    17,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:45:17Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    17,
                    0,
                    181,
                    0
                ],
                "title": "LLM Agents Are the Antidote to Walled Gardens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents Are the Antidote to Walled Gardens"
                },
                "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security."
                },
                "authors": [
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Philip Torr"
                    }
                ],
                "author_detail": {
                    "name": "Philip Torr"
                },
                "author": "Philip Torr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68M10, 91B26",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; H.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23967v1",
                "updated": "2025-06-30T15:36:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    53,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:36:53Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    53,
                    0,
                    181,
                    0
                ],
                "title": "Green Metrics Tool: Measuring for fun and profit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green Metrics Tool: Measuring for fun and profit"
                },
                "summary": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software."
                },
                "authors": [
                    {
                        "name": "Geerd-Dietger Hoffmann"
                    },
                    {
                        "name": "Verena Majuntke"
                    }
                ],
                "author_detail": {
                    "name": "Verena Majuntke"
                },
                "author": "Verena Majuntke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23964v1",
                "updated": "2025-06-30T15:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    22,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    22,
                    0,
                    181,
                    0
                ],
                "title": "Learning Constraints Directly from Network Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Constraints Directly from Network Data"
                },
                "summary": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference."
                },
                "authors": [
                    {
                        "name": "Hongyu H"
                    },
                    {
                        "name": "Minhao Jin"
                    },
                    {
                        "name": "Maria Apostolaki"
                    }
                ],
                "author_detail": {
                    "name": "Maria Apostolaki"
                },
                "author": "Maria Apostolaki",
                "arxiv_comment": "13 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.3; I.2.6; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23951v1",
                "updated": "2025-06-30T15:18:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    18,
                    50,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:18:50Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    18,
                    50,
                    0,
                    181,
                    0
                ],
                "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction\n  of influential and interpretable concepts with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Decision-Making in LLMs for Text Classification : Extraction\n  of influential and interpretable concepts with Sparse Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features."
                },
                "authors": [
                    {
                        "name": "Mathis Le Bail"
                    },
                    {
                        "name": "Jrmie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23944v2",
                "updated": "2025-07-01T01:36:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    36,
                    5,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T15:09:14Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    9,
                    14,
                    0,
                    181,
                    0
                ],
                "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning"
                },
                "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts."
                },
                "authors": [
                    {
                        "name": "Fuhang Kuang"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Yingdong Hu"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Need further modification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23930v1",
                "updated": "2025-06-30T14:59:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    59,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:59:25Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    59,
                    25,
                    0,
                    181,
                    0
                ],
                "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection\n  in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection\n  in Low-Resource Languages"
                },
                "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time."
                },
                "authors": [
                    {
                        "name": "Ruhina Tabasshum Prome"
                    },
                    {
                        "name": "Tarikul Islam Tamiti"
                    },
                    {
                        "name": "Anomadarshi Barua"
                    }
                ],
                "author_detail": {
                    "name": "Anomadarshi Barua"
                },
                "arxiv_affiliation": "George Mason University",
                "author": "Anomadarshi Barua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23929v1",
                "updated": "2025-06-30T14:58:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    58,
                    23,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:58:23Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    58,
                    23,
                    0,
                    181,
                    0
                ],
                "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMPACT: Inflectional Morphology Probes Across Complex Typologies"
                },
                "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework."
                },
                "authors": [
                    {
                        "name": "Mohammed J. Saeed"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Evgeny Fedoseev"
                    },
                    {
                        "name": "Sevil Caliskan"
                    },
                    {
                        "name": "Tatiana Vodolazova"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Vodolazova"
                },
                "author": "Tatiana Vodolazova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23924v1",
                "updated": "2025-06-30T14:54:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    15,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:54:15Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    15,
                    0,
                    181,
                    0
                ],
                "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice"
                },
                "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation."
                },
                "authors": [
                    {
                        "name": "Akshit Kumar"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Yuhang Wu"
                    },
                    {
                        "name": "Assaf Zeevi"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Zeevi"
                },
                "author": "Assaf Zeevi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23921v1",
                "updated": "2025-06-30T14:49:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    49,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:49:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    49,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "The Trilemma of Truth in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Trilemma of Truth in Large Language Models"
                },
                "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge."
                },
                "authors": [
                    {
                        "name": "Germans Savcisens"
                    },
                    {
                        "name": "Tina Eliassi-Rad"
                    }
                ],
                "author_detail": {
                    "name": "Tina Eliassi-Rad"
                },
                "author": "Tina Eliassi-Rad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19592v2",
                "updated": "2025-06-30T14:40:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    40,
                    24,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-24T13:02:06Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    2,
                    6,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning"
                },
                "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment."
                },
                "authors": [
                    {
                        "name": "Harisankar Babu"
                    },
                    {
                        "name": "Philipp Schillinger"
                    },
                    {
                        "name": "Tamim Asfour"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Asfour"
                },
                "author": "Tamim Asfour",
                "arxiv_comment": "Accepted at IEEE CASE 2025, 8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23888v1",
                "updated": "2025-06-30T14:18:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    18,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:18:35Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    18,
                    35,
                    0,
                    181,
                    0
                ],
                "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models\n  through Multi-Layered Self-Reflection with Auto-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multi-Step Mathematical Reasoning in Large Language Models\n  through Multi-Layered Self-Reflection with Auto-Prompting"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance."
                },
                "authors": [
                    {
                        "name": "Andr de Souza Loureiro"
                    },
                    {
                        "name": "Jorge Valverde-Rebaza"
                    },
                    {
                        "name": "Julieta Noguez"
                    },
                    {
                        "name": "David Escarcega"
                    },
                    {
                        "name": "Ricardo Marcacini"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Marcacini"
                },
                "author": "Ricardo Marcacini",
                "arxiv_comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23864v1",
                "updated": "2025-06-30T13:57:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:57:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What\n  to Do About It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What\n  to Do About It"
                },
                "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning."
                },
                "authors": [
                    {
                        "name": "Seyed Mahed Mousavi"
                    },
                    {
                        "name": "Edoardo Cecchinato"
                    },
                    {
                        "name": "Lucia Hornikova"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23862v1",
                "updated": "2025-06-30T13:57:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    11,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:57:11Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    11,
                    0,
                    181,
                    0
                ],
                "title": "Large Language Models for Statistical Inference: Context Augmentation\n  with Applications to the Two-Sample Problem and Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Statistical Inference: Context Augmentation\n  with Applications to the Two-Sample Problem and Regression"
                },
                "summary": "We introduce context augmentation, a data-augmentation approach that uses\nlarge language models (LLMs) to generate contexts around observed strings as a\nmeans of facilitating valid frequentist inference. These generated contexts\nserve to reintroduce uncertainty, incorporate auxiliary information, and\nfacilitate interpretability. For example, in the two-sample test, we compare\nthe log-probability of strings under contexts from its own versus the other\ngroup. We show on synthetic data that the method's t-statistics exhibit the\nexpected null behaviour while maintaining power and, through a replication,\nthat the method is powerful and interpretable. We next introduce text-on-text\nregression. Contexts generated around the predictor string are treated as\nmediating variables between the predictor and outcome strings. Using negative\ncontrols, we then distinguish between semantic and syntactic dimensions of\nprediction. Analysis of real-world dialogic data illustrates behaviour\npredicted from a psycholinguistic framework. Theoretically, we provide\nidentification conditions, derive an influence-function decomposition, and show\nthat repeated cross-fitting of a pivotal statistic yields higher-order\nefficiency. We derive bounds linking estimation error, context count, and\nnumber of cross-fits. Taken together, context augmentation offers the ability\nto connect LLMs with longstanding statistical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce context augmentation, a data-augmentation approach that uses\nlarge language models (LLMs) to generate contexts around observed strings as a\nmeans of facilitating valid frequentist inference. These generated contexts\nserve to reintroduce uncertainty, incorporate auxiliary information, and\nfacilitate interpretability. For example, in the two-sample test, we compare\nthe log-probability of strings under contexts from its own versus the other\ngroup. We show on synthetic data that the method's t-statistics exhibit the\nexpected null behaviour while maintaining power and, through a replication,\nthat the method is powerful and interpretable. We next introduce text-on-text\nregression. Contexts generated around the predictor string are treated as\nmediating variables between the predictor and outcome strings. Using negative\ncontrols, we then distinguish between semantic and syntactic dimensions of\nprediction. Analysis of real-world dialogic data illustrates behaviour\npredicted from a psycholinguistic framework. Theoretically, we provide\nidentification conditions, derive an influence-function decomposition, and show\nthat repeated cross-fitting of a pivotal statistic yields higher-order\nefficiency. We derive bounds linking estimation error, context count, and\nnumber of cross-fits. Taken together, context augmentation offers the ability\nto connect LLMs with longstanding statistical practice."
                },
                "authors": [
                    {
                        "name": "Marc Ratkovic"
                    }
                ],
                "author_detail": {
                    "name": "Marc Ratkovic"
                },
                "author": "Marc Ratkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23850v1",
                "updated": "2025-06-30T13:39:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    39,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:39:54Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    39,
                    54,
                    0,
                    181,
                    0
                ],
                "title": "Email as the Interface to Generative AI Models: Seamless Administrative\n  Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Email as the Interface to Generative AI Models: Seamless Administrative\n  Automation"
                },
                "summary": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient."
                },
                "authors": [
                    {
                        "name": "Andres Navarro"
                    },
                    {
                        "name": "Carlos de Quinto"
                    },
                    {
                        "name": "Jos Alberto Hernndez"
                    }
                ],
                "author_detail": {
                    "name": "Jos Alberto Hernndez"
                },
                "author": "Jos Alberto Hernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07160v2",
                "updated": "2025-06-30T13:36:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    36,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-08T14:18:15Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    14,
                    18,
                    15,
                    6,
                    159,
                    0
                ],
                "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks."
                },
                "authors": [
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Zimian Peng"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23844v1",
                "updated": "2025-06-30T13:34:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    34,
                    34,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:34:34Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    34,
                    34,
                    0,
                    181,
                    0
                ],
                "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop."
                },
                "authors": [
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Luo"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04998v2",
                "updated": "2025-06-30T13:18:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    18,
                    47,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-05T13:09:24Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    9,
                    24,
                    3,
                    156,
                    0
                ],
                "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based\n  Approach for Complex Arithmetic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based\n  Approach for Complex Arithmetic Reasoning"
                },
                "summary": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a 20-question\nproblem set of UAV-centric mathematical problems across four difficulty levels.\nOur experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5\\% with\nRAG), and decreases numerical errors, reducing Mean Squared Error (MSE) by\norders of magnitude for the best-performing models. This pilot study indicates\nthat RAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, questions,\nand answers are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a 20-question\nproblem set of UAV-centric mathematical problems across four difficulty levels.\nOur experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5\\% with\nRAG), and decreases numerical errors, reducing Mean Squared Error (MSE) by\norders of magnitude for the best-performing models. This pilot study indicates\nthat RAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, questions,\nand answers are publicly available."
                },
                "authors": [
                    {
                        "name": "Mehdi Azarafza"
                    },
                    {
                        "name": "Mojtaba Nayyeri"
                    },
                    {
                        "name": "Faezeh Pasandideh"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Achim Rettberg"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettberg"
                },
                "author": "Achim Rettberg",
                "arxiv_comment": "15 pages, 7 figures, 4 appendix subsections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22270v2",
                "updated": "2025-06-30T13:16:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    16,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T14:39:38Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    39,
                    38,
                    4,
                    178,
                    0
                ],
                "title": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values"
                },
                "summary": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content."
                },
                "authors": [
                    {
                        "name": "Ahmad Mel"
                    },
                    {
                        "name": "Sebastien Noir"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Noir"
                },
                "author": "Sebastien Noir",
                "arxiv_journal_ref": "IAMCR 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23815v2",
                "updated": "2025-07-01T07:51:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    51,
                    20,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T13:02:01Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    2,
                    1,
                    0,
                    181,
                    0
                ],
                "title": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment"
                },
                "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods."
                },
                "authors": [
                    {
                        "name": "Patrick Stokkink"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Stokkink"
                },
                "author": "Patrick Stokkink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11655v2",
                "updated": "2025-06-30T12:58:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    58,
                    45,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-03T07:17:46Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    7,
                    17,
                    46,
                    0,
                    34,
                    0
                ],
                "title": "Explainable Sentiment Analysis with DeepSeek-R1: Performance,\n  Efficiency, and Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Sentiment Analysis with DeepSeek-R1: Performance,\n  Efficiency, and Few-Shot Learning"
                },
                "summary": "Large language models (LLMs) have transformed sentiment analysis, yet\nbalancing accuracy, efficiency, and explainability remains a critical\nchallenge. This study presents the first comprehensive evaluation of\nDeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and\nGPT-4o-mini. We test the full 671B model and its distilled variants,\nsystematically documenting few-shot learning curves. Our experiments show\nDeepSeek-R1 achieves a 91.39\\% F1 score on 5-class sentiment and 99.31\\%\naccuracy on binary tasks with just 5 shots, an eightfold improvement in\nfew-shot efficiency over GPT-4o. Architecture-specific distillation effects\nemerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant\nby 6.69 percentage points. While its reasoning process reduces throughput,\nDeepSeek-R1 offers superior explainability via transparent, step-by-step\ntraces, establishing it as a powerful, interpretable open-source alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed sentiment analysis, yet\nbalancing accuracy, efficiency, and explainability remains a critical\nchallenge. This study presents the first comprehensive evaluation of\nDeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and\nGPT-4o-mini. We test the full 671B model and its distilled variants,\nsystematically documenting few-shot learning curves. Our experiments show\nDeepSeek-R1 achieves a 91.39\\% F1 score on 5-class sentiment and 99.31\\%\naccuracy on binary tasks with just 5 shots, an eightfold improvement in\nfew-shot efficiency over GPT-4o. Architecture-specific distillation effects\nemerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant\nby 6.69 percentage points. While its reasoning process reduces throughput,\nDeepSeek-R1 offers superior explainability via transparent, step-by-step\ntraces, establishing it as a powerful, interpretable open-source alternative."
                },
                "authors": [
                    {
                        "name": "Donghao Huang"
                    },
                    {
                        "name": "Zhaoxia Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxia Wang"
                },
                "author": "Zhaoxia Wang",
                "arxiv_comment": "10 pages, 2 figures, 6 tables, revised and re-submitted to an IEEE\n  journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23782v1",
                "updated": "2025-06-30T12:23:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    23,
                    57,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:23:57Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    23,
                    57,
                    0,
                    181,
                    0
                ],
                "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling"
                },
                "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Linwei Tao"
                    },
                    {
                        "name": "Haohui Lu"
                    },
                    {
                        "name": "Minjing Dong"
                    },
                    {
                        "name": "Junbin Gao"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23774v1",
                "updated": "2025-06-30T12:18:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    18,
                    13,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:18:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    18,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate\n  Incidents Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate\n  Incidents Management"
                },
                "summary": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings."
                },
                "authors": [
                    {
                        "name": "Ewelina Gajewska"
                    },
                    {
                        "name": "Michal Wawer"
                    },
                    {
                        "name": "Katarzyna Budzynska"
                    },
                    {
                        "name": "Jarosaw A. Chudziak"
                    }
                ],
                "author_detail": {
                    "name": "Jarosaw A. Chudziak"
                },
                "author": "Jarosaw A. Chudziak",
                "arxiv_comment": "8 pages, 1 figure",
                "arxiv_journal_ref": "The 26th International Conference on Artificial Intelligence in\n  Education (AIED 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15627v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15627v4",
                "updated": "2025-06-30T12:15:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    15,
                    7,
                    0,
                    181,
                    0
                ],
                "published": "2024-06-21T20:06:31Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    20,
                    6,
                    31,
                    4,
                    173,
                    0
                ],
                "title": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph"
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Lyudmila Rvanova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Daniil Vasilev"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Abdelrahman Boda Sadallah"
                    },
                    {
                        "name": "Kirill Grishchenkov"
                    },
                    {
                        "name": "Sergey Petrakov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "arxiv_doi": "10.1162/tacl_a_00737",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1162/tacl_a_00737",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.15627v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15627v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at TACL 2025, presented at ACL 2025. Roman Vashurin,\n  Ekaterina Fadeeva, Artem Vazhentsev contributed equally",
                "arxiv_journal_ref": "Transactions of the Association for Computational Linguistics, 13\n  (2025) 220-248",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23767v1",
                "updated": "2025-06-30T12:13:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    13,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:13:35Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    13,
                    35,
                    0,
                    181,
                    0
                ],
                "title": "Explainable AI for Comprehensive Risk Assessment for Financial Reports:\n  A Lightweight Hierarchical Transformer Network Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI for Comprehensive Risk Assessment for Financial Reports:\n  A Lightweight Hierarchical Transformer Network Approach"
                },
                "summary": "Every publicly traded U.S. company files an annual 10-K report containing\ncritical insights into financial health and risk. We propose Tiny eXplainable\nRisk Assessor (TinyXRA), a lightweight and explainable transformer-based model\nthat automatically assesses company risk from these reports. Unlike prior work\nthat relies solely on the standard deviation of excess returns (adjusted for\nthe Fama-French model), which indiscriminately penalizes both upside and\ndownside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio\nfor more comprehensive risk assessment. We leverage TinyBERT as our encoder to\nefficiently process lengthy financial documents, coupled with a novel dynamic,\nattention-based word cloud mechanism that provides intuitive risk visualization\nwhile filtering irrelevant terms. This lightweight design ensures scalable\ndeployment across diverse computing environments with real-time processing\ncapabilities for thousands of financial documents which is essential for\nproduction systems with constrained computational resources. We employ triplet\nloss for risk quartile classification, improving over pairwise loss approaches\nin existing literature by capturing both the direction and magnitude of risk\ndifferences. Our TinyXRA achieves state-of-the-art predictive accuracy across\nseven test years on a dataset spanning 2013-2024, while providing transparent\nand interpretable risk assessments. We conduct comprehensive ablation studies\nto evaluate our contributions and assess model explanations both quantitatively\nby systematically removing highly attended words and sentences, and\nqualitatively by examining explanation coherence. The paper concludes with\nfindings, practical implications, limitations, and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every publicly traded U.S. company files an annual 10-K report containing\ncritical insights into financial health and risk. We propose Tiny eXplainable\nRisk Assessor (TinyXRA), a lightweight and explainable transformer-based model\nthat automatically assesses company risk from these reports. Unlike prior work\nthat relies solely on the standard deviation of excess returns (adjusted for\nthe Fama-French model), which indiscriminately penalizes both upside and\ndownside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio\nfor more comprehensive risk assessment. We leverage TinyBERT as our encoder to\nefficiently process lengthy financial documents, coupled with a novel dynamic,\nattention-based word cloud mechanism that provides intuitive risk visualization\nwhile filtering irrelevant terms. This lightweight design ensures scalable\ndeployment across diverse computing environments with real-time processing\ncapabilities for thousands of financial documents which is essential for\nproduction systems with constrained computational resources. We employ triplet\nloss for risk quartile classification, improving over pairwise loss approaches\nin existing literature by capturing both the direction and magnitude of risk\ndifferences. Our TinyXRA achieves state-of-the-art predictive accuracy across\nseven test years on a dataset spanning 2013-2024, while providing transparent\nand interpretable risk assessments. We conduct comprehensive ablation studies\nto evaluate our contributions and assess model explanations both quantitatively\nby systematically removing highly attended words and sentences, and\nqualitatively by examining explanation coherence. The paper concludes with\nfindings, practical implications, limitations, and future research directions."
                },
                "authors": [
                    {
                        "name": "Xue Wen Tan"
                    },
                    {
                        "name": "Stanley Kok"
                    }
                ],
                "author_detail": {
                    "name": "Stanley Kok"
                },
                "author": "Stanley Kok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23762v1",
                "updated": "2025-06-30T12:09:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    9,
                    29,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:09:29Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    9,
                    29,
                    0,
                    181,
                    0
                ],
                "title": "Software Engineering for Large Language Models: Research Status,\n  Challenges and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering for Large Language Models: Research Status,\n  Challenges and the Road Ahead"
                },
                "summary": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Hongzhou Rao"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14540v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14540v3",
                "updated": "2025-06-30T11:59:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    59,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-17T14:01:39Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    14,
                    1,
                    39,
                    1,
                    168,
                    0
                ],
                "title": "Aligning Evaluation with Clinical Priorities: Calibration, Label Shift,\n  and Error Costs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Evaluation with Clinical Priorities: Calibration, Label Shift,\n  and Error Costs"
                },
                "summary": "Machine learning-based decision support systems are increasingly deployed in\nclinical settings, where probabilistic scoring functions are used to inform and\nprioritize patient management decisions. However, widely used scoring rules,\nsuch as accuracy and AUC-ROC, fail to adequately reflect key clinical\npriorities, including calibration, robustness to distributional shifts, and\nsensitivity to asymmetric error costs. In this work, we propose a principled\nyet practical evaluation framework for selecting calibrated thresholded\nclassifiers that explicitly accounts for the uncertainty in class prevalences\nand domain-specific cost asymmetries often found in clinical settings. Building\non the theory of proper scoring rules, particularly the Schervish\nrepresentation, we derive an adjusted variant of cross-entropy (log score) that\naverages cost-weighted performance over clinically relevant ranges of class\nbalance. The resulting evaluation is simple to apply, sensitive to clinical\ndeployment conditions, and designed to prioritize models that are both\ncalibrated and robust to real-world variations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning-based decision support systems are increasingly deployed in\nclinical settings, where probabilistic scoring functions are used to inform and\nprioritize patient management decisions. However, widely used scoring rules,\nsuch as accuracy and AUC-ROC, fail to adequately reflect key clinical\npriorities, including calibration, robustness to distributional shifts, and\nsensitivity to asymmetric error costs. In this work, we propose a principled\nyet practical evaluation framework for selecting calibrated thresholded\nclassifiers that explicitly accounts for the uncertainty in class prevalences\nand domain-specific cost asymmetries often found in clinical settings. Building\non the theory of proper scoring rules, particularly the Schervish\nrepresentation, we derive an adjusted variant of cross-entropy (log score) that\naverages cost-weighted performance over clinically relevant ranges of class\nbalance. The resulting evaluation is simple to apply, sensitive to clinical\ndeployment conditions, and designed to prioritize models that are both\ncalibrated and robust to real-world variations."
                },
                "authors": [
                    {
                        "name": "Gerardo A. Flores"
                    },
                    {
                        "name": "Alyssa H. Smith"
                    },
                    {
                        "name": "Julia A. Fukuyama"
                    },
                    {
                        "name": "Ashia C. Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Ashia C. Wilson"
                },
                "author": "Ashia C. Wilson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14540v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14540v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23749v1",
                "updated": "2025-06-30T11:46:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    46,
                    1,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:46:01Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    46,
                    1,
                    0,
                    181,
                    0
                ],
                "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design\n  Paradigms, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design\n  Paradigms, and Applications"
                },
                "summary": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Zijian Cai"
                    },
                    {
                        "name": "Fengling Liu"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Haoye Tian"
                },
                "author": "Haoye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21599v2",
                "updated": "2025-06-30T11:36:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    36,
                    12,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-19T02:51:10Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    51,
                    10,
                    3,
                    170,
                    0
                ],
                "title": "Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next\n  Point-of-Interest Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next\n  Point-of-Interest Recommendation"
                },
                "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance."
                },
                "authors": [
                    {
                        "name": "Peibo Li"
                    },
                    {
                        "name": "Shuang Ao"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Johan Barthlemy"
                    },
                    {
                        "name": "Tomasz Bednarz"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23740v1",
                "updated": "2025-06-30T11:27:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    27,
                    57,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:27:57Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    27,
                    57,
                    0,
                    181,
                    0
                ],
                "title": "Campus5G: A Campus Scale Private 5G Open RAN Testbed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Campus5G: A Campus Scale Private 5G Open RAN Testbed"
                },
                "summary": "Mobile networks are embracing disaggregation, reflected by the industry trend\ntowards Open RAN. Private 5G networks are viewed as particularly suitable\ncontenders as early adopters of Open RAN, owing to their setting, high degree\nof control, and opportunity for innovation they present. Motivated by this, we\nhave recently deployed Campus5G, the first of its kind campus-wide,\nO-RAN-compliant private 5G testbed across the central campus of the University\nof Edinburgh. We present in detail our process developing the testbed, from\nplanning, to architecting, to deployment, and measuring the testbed\nperformance. We then discuss the lessons learned from building the testbed, and\nhighlight some research opportunities that emerged from our deployment\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile networks are embracing disaggregation, reflected by the industry trend\ntowards Open RAN. Private 5G networks are viewed as particularly suitable\ncontenders as early adopters of Open RAN, owing to their setting, high degree\nof control, and opportunity for innovation they present. Motivated by this, we\nhave recently deployed Campus5G, the first of its kind campus-wide,\nO-RAN-compliant private 5G testbed across the central campus of the University\nof Edinburgh. We present in detail our process developing the testbed, from\nplanning, to architecting, to deployment, and measuring the testbed\nperformance. We then discuss the lessons learned from building the testbed, and\nhighlight some research opportunities that emerged from our deployment\nexperience."
                },
                "authors": [
                    {
                        "name": "Andrew E. Ferguson"
                    },
                    {
                        "name": "Ujjwal Pawar"
                    },
                    {
                        "name": "Tianxin Wang"
                    },
                    {
                        "name": "Mahesh K. Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh K. Marina"
                },
                "author": "Mahesh K. Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23735v1",
                "updated": "2025-06-30T11:18:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    18,
                    56,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:18:56Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    18,
                    56,
                    0,
                    181,
                    0
                ],
                "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM\n  Evaluation Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM\n  Evaluation Data"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval."
                },
                "authors": [
                    {
                        "name": "JiaRu Wu"
                    },
                    {
                        "name": "Mingwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Liu"
                },
                "author": "Mingwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13271v2",
                "updated": "2025-06-30T11:12:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    12,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-19T15:52:19Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    52,
                    19,
                    0,
                    139,
                    0
                ],
                "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD private test set, our 7B model achieves 71.72\\% execution\naccuracy, while the 32B model achieves 73.67\\%. The code has been open sourced\nat https://github.com/CycloneBoy/csc_sql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD private test set, our 7B model achieves 71.72\\% execution\naccuracy, while the 32B model achieves 73.67\\%. The code has been open sourced\nat https://github.com/CycloneBoy/csc_sql."
                },
                "authors": [
                    {
                        "name": "Lei Sheng"
                    },
                    {
                        "name": "Shuai-Shuai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai-Shuai Xu"
                },
                "author": "Shuai-Shuai Xu",
                "arxiv_comment": "25 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23731v1",
                "updated": "2025-06-30T11:08:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    8,
                    10,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T11:08:10Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    11,
                    8,
                    10,
                    0,
                    181,
                    0
                ],
                "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative\n  Models"
                },
                "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images."
                },
                "authors": [
                    {
                        "name": "Michel Meintz"
                    },
                    {
                        "name": "Jan Dubiski"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10558v2",
                "updated": "2025-06-30T10:59:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    59,
                    44,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-12T10:31:23Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    10,
                    31,
                    23,
                    3,
                    163,
                    0
                ],
                "title": "StepProof: Step-by-step verification of natural language mathematical\n  proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepProof: Step-by-step verification of natural language mathematical\n  proofs"
                },
                "summary": "Interactive theorem provers (ITPs) are powerful tools for the formal\nverification of mathematical proofs down to the axiom level. However, their\nlack of a natural language interface remains a significant limitation. Recent\nadvancements in large language models (LLMs) have enhanced the understanding of\nnatural language inputs, paving the way for autoformalization - the process of\ntranslating natural language proofs into formal proofs that can be verified.\nDespite these advancements, existing autoformalization approaches are limited\nto verifying complete proofs and lack the capability for finer, sentence-level\nverification. To address this gap, we propose StepProof, a novel\nautoformalization method designed for granular, step-by-step verification.\nStepProof breaks down complete proofs into multiple verifiable subproofs,\nenabling sentence-level verification. Experimental results demonstrate that\nStepProof significantly improves proof success rates and efficiency compared to\ntraditional methods. Additionally, we found that minor manual adjustments to\nthe natural language proofs, tailoring them for step-level verification,\nfurther enhanced StepProof's performance in autoformalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive theorem provers (ITPs) are powerful tools for the formal\nverification of mathematical proofs down to the axiom level. However, their\nlack of a natural language interface remains a significant limitation. Recent\nadvancements in large language models (LLMs) have enhanced the understanding of\nnatural language inputs, paving the way for autoformalization - the process of\ntranslating natural language proofs into formal proofs that can be verified.\nDespite these advancements, existing autoformalization approaches are limited\nto verifying complete proofs and lack the capability for finer, sentence-level\nverification. To address this gap, we propose StepProof, a novel\nautoformalization method designed for granular, step-by-step verification.\nStepProof breaks down complete proofs into multiple verifiable subproofs,\nenabling sentence-level verification. Experimental results demonstrate that\nStepProof significantly improves proof success rates and efficiency compared to\ntraditional methods. Additionally, we found that minor manual adjustments to\nthe natural language proofs, tailoring them for step-level verification,\nfurther enhanced StepProof's performance in autoformalization."
                },
                "authors": [
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Qinghua Zhou"
                    },
                    {
                        "name": "Bogdan Grechuk"
                    },
                    {
                        "name": "Ivan Y. Tyukin"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Y. Tyukin"
                },
                "author": "Ivan Y. Tyukin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23726v1",
                "updated": "2025-06-30T10:58:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    58,
                    49,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:58:49Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    58,
                    49,
                    0,
                    181,
                    0
                ],
                "title": "System-Embedded Diffusion Bridge Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-Embedded Diffusion Bridge Models"
                },
                "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications."
                },
                "authors": [
                    {
                        "name": "Bartlomiej Sobieski"
                    },
                    {
                        "name": "Matthew Tivnan"
                    },
                    {
                        "name": "Yuang Wang"
                    },
                    {
                        "name": "Siyeop Yoon"
                    },
                    {
                        "name": "Pengfei Jin"
                    },
                    {
                        "name": "Dufan Wu"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Przemyslaw Biecek"
                    }
                ],
                "author_detail": {
                    "name": "Przemyslaw Biecek"
                },
                "author": "Przemyslaw Biecek",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23719v1",
                "updated": "2025-06-30T10:49:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    49,
                    21,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:49:21Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    49,
                    21,
                    0,
                    181,
                    0
                ],
                "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DABstep: Data Agent Benchmark for Multi-step Reasoning"
                },
                "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis."
                },
                "authors": [
                    {
                        "name": "Alex Egg"
                    },
                    {
                        "name": "Martin Iglesias Goyanes"
                    },
                    {
                        "name": "Friso Kingma"
                    },
                    {
                        "name": "Andreu Mora"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Thomas Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wolf"
                },
                "author": "Thomas Wolf",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23714v1",
                "updated": "2025-06-30T10:41:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    41,
                    33,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:41:33Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    41,
                    33,
                    0,
                    181,
                    0
                ],
                "title": "Towards an Automated Multimodal Approach for Video Summarization:\n  Building a Bridge Between Text, Audio and Facial Cue-Based Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an Automated Multimodal Approach for Video Summarization:\n  Building a Bridge Between Text, Audio and Facial Cue-Based Summarization"
                },
                "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries."
                },
                "authors": [
                    {
                        "name": "Md Moinul Islam"
                    },
                    {
                        "name": "Sofoklis Kakouros"
                    },
                    {
                        "name": "Janne Heikkil"
                    },
                    {
                        "name": "Mourad Oussalah"
                    }
                ],
                "author_detail": {
                    "name": "Mourad Oussalah"
                },
                "author": "Mourad Oussalah",
                "arxiv_comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02231v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02231v3",
                "updated": "2025-06-30T10:36:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    36,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2024-09-03T18:59:20Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    18,
                    59,
                    20,
                    1,
                    247,
                    0
                ],
                "title": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space\n  Exploration"
                },
                "summary": "Here we show that a general-purpose large language model (LLM) chatbot,\nLlama-3.1-8B-Instruct, can be transformed via supervised fine-tuning of\nengineered prompts into a chemical language model (CLM), SmileyLlama, for\nmolecule generation. We benchmark SmileyLlama by comparing it to CLMs trained\nfrom scratch on large amounts of ChEMBL data for their ability to generate\nvalid and novel drug-like molecules. We also use direct preference optimization\nto both improve SmileyLlama's adherence to a prompt and to generate molecules\nwithin the iMiner reinforcement learning framework to predict new drug\nmolecules with optimized 3D conformations and high binding affinity to drug\ntargets, illustrated with the SARS-Cov-2 Main Protease. This overall framework\nallows a LLM to speak directly as a CLM which can generate molecules with\nuser-specified properties, rather than acting only as a chatbot with knowledge\nof chemistry or as a helpful virtual assistant. While our dataset and analyses\nare geared toward drug discovery, this general procedure can be extended to\nother chemical applications such as chemical synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here we show that a general-purpose large language model (LLM) chatbot,\nLlama-3.1-8B-Instruct, can be transformed via supervised fine-tuning of\nengineered prompts into a chemical language model (CLM), SmileyLlama, for\nmolecule generation. We benchmark SmileyLlama by comparing it to CLMs trained\nfrom scratch on large amounts of ChEMBL data for their ability to generate\nvalid and novel drug-like molecules. We also use direct preference optimization\nto both improve SmileyLlama's adherence to a prompt and to generate molecules\nwithin the iMiner reinforcement learning framework to predict new drug\nmolecules with optimized 3D conformations and high binding affinity to drug\ntargets, illustrated with the SARS-Cov-2 Main Protease. This overall framework\nallows a LLM to speak directly as a CLM which can generate molecules with\nuser-specified properties, rather than acting only as a chatbot with knowledge\nof chemistry or as a helpful virtual assistant. While our dataset and analyses\nare geared toward drug discovery, this general procedure can be extended to\nother chemical applications such as chemical synthesis."
                },
                "authors": [
                    {
                        "name": "Joseph M. Cavanagh"
                    },
                    {
                        "name": "Kunyang Sun"
                    },
                    {
                        "name": "Andrew Gritsevskiy"
                    },
                    {
                        "name": "Dorian Bagni"
                    },
                    {
                        "name": "Yingze Wang"
                    },
                    {
                        "name": "Thomas D. Bannister"
                    },
                    {
                        "name": "Teresa Head-Gordon"
                    }
                ],
                "author_detail": {
                    "name": "Teresa Head-Gordon"
                },
                "author": "Teresa Head-Gordon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02231v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02231v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23694v1",
                "updated": "2025-06-30T10:15:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    15,
                    44,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:15:44Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    15,
                    44,
                    0,
                    181,
                    0
                ],
                "title": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language\n  Models to Support User Scenario Writing for User Experience Experts and\n  Laypersons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language\n  Models to Support User Scenario Writing for User Experience Experts and\n  Laypersons"
                },
                "summary": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask."
                },
                "authors": [
                    {
                        "name": "Patrick Stadler"
                    },
                    {
                        "name": "Christopher Lazik"
                    },
                    {
                        "name": "Christopher Katins"
                    },
                    {
                        "name": "Thomas Kosch"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Kosch"
                },
                "author": "Thomas Kosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23692v1",
                "updated": "2025-06-30T10:11:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    11,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:11:39Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    11,
                    39,
                    0,
                    181,
                    0
                ],
                "title": "Agent4S: The Transformation of Research Paradigms from the Perspective\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent4S: The Transformation of Research Paradigms from the Perspective\n  of Large Language Models"
                },
                "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery."
                },
                "authors": [
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Zerui Fang"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yiwen Chen"
                    },
                    {
                        "name": "Cunshi Wang"
                    },
                    {
                        "name": "Mengwei Qu"
                    },
                    {
                        "name": "Lei Lei"
                    },
                    {
                        "name": "Zhen Feng"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Mingzhou Tan"
                    },
                    {
                        "name": "Jiaji Wu"
                    },
                    {
                        "name": "Jianwei Shuai"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Fangfu Ye"
                    }
                ],
                "author_detail": {
                    "name": "Fangfu Ye"
                },
                "author": "Fangfu Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23689v1",
                "updated": "2025-06-30T10:09:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    9,
                    13,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:09:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    9,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "PokAI: A Goal-Generating, Battle-Optimizing Multi-agent System for\n  Pokemon Red",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokAI: A Goal-Generating, Battle-Optimizing Multi-agent System for\n  Pokemon Red"
                },
                "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors."
                },
                "authors": [
                    {
                        "name": "Zihao Liu"
                    },
                    {
                        "name": "Xinhang Sui"
                    },
                    {
                        "name": "Yueran Song"
                    },
                    {
                        "name": "Siwen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Siwen Wang"
                },
                "author": "Siwen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23688v1",
                "updated": "2025-06-30T10:08:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    8,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:08:25Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    8,
                    25,
                    0,
                    181,
                    0
                ],
                "title": "GUSL: A Novel and Efficient Machine Learning Model for Prostate\n  Segmentation on MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUSL: A Novel and Efficient Machine Learning Model for Prostate\n  Segmentation on MRI"
                },
                "summary": "Prostate and zonal segmentation is a crucial step for clinical diagnosis of\nprostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation\nare based on the deep learning (DL) paradigm. However, deep neural networks are\nperceived as \"black-box\" solutions by physicians, thus making them less\npractical for deployment in the clinical setting. In this paper, we introduce a\nfeed-forward machine learning model, named Green U-shaped Learning (GUSL),\nsuitable for medical image segmentation without backpropagation. GUSL\nintroduces a multi-layer regression scheme for coarse-to-fine segmentation. Its\nfeature extraction is based on a linear model, which enables seamless\ninterpretability during feature extraction. Also, GUSL introduces a mechanism\nfor attention on the prostate boundaries, which is an error-prone region, by\nemploying regression to refine the predictions through residue correction. In\naddition, a two-step pipeline approach is used to mitigate the class imbalance,\nan issue inherent in medical imaging problems. After conducting experiments on\ntwo publicly available datasets and one private dataset, in both prostate gland\nand zonal segmentation tasks, GUSL achieves state-of-the-art performance among\nother DL-based models. Notably, GUSL features a very energy-efficient pipeline,\nsince it has a model size several times smaller and less complexity than the\nrest of the solutions. In all datasets, GUSL achieved a Dice Similarity\nCoefficient (DSC) performance greater than $0.9$ for gland segmentation.\nConsidering also its lightweight model size and transparency in feature\nextraction, it offers a competitive and practical package for medical imaging\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prostate and zonal segmentation is a crucial step for clinical diagnosis of\nprostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation\nare based on the deep learning (DL) paradigm. However, deep neural networks are\nperceived as \"black-box\" solutions by physicians, thus making them less\npractical for deployment in the clinical setting. In this paper, we introduce a\nfeed-forward machine learning model, named Green U-shaped Learning (GUSL),\nsuitable for medical image segmentation without backpropagation. GUSL\nintroduces a multi-layer regression scheme for coarse-to-fine segmentation. Its\nfeature extraction is based on a linear model, which enables seamless\ninterpretability during feature extraction. Also, GUSL introduces a mechanism\nfor attention on the prostate boundaries, which is an error-prone region, by\nemploying regression to refine the predictions through residue correction. In\naddition, a two-step pipeline approach is used to mitigate the class imbalance,\nan issue inherent in medical imaging problems. After conducting experiments on\ntwo publicly available datasets and one private dataset, in both prostate gland\nand zonal segmentation tasks, GUSL achieves state-of-the-art performance among\nother DL-based models. Notably, GUSL features a very energy-efficient pipeline,\nsince it has a model size several times smaller and less complexity than the\nrest of the solutions. In all datasets, GUSL achieved a Dice Similarity\nCoefficient (DSC) performance greater than $0.9$ for gland segmentation.\nConsidering also its lightweight model size and transparency in feature\nextraction, it offers a competitive and practical package for medical imaging\napplications."
                },
                "authors": [
                    {
                        "name": "Jiaxin Yang"
                    },
                    {
                        "name": "Vasileios Magoulianitis"
                    },
                    {
                        "name": "Catherine Aurelia Christie Alexander"
                    },
                    {
                        "name": "Jintang Xue"
                    },
                    {
                        "name": "Masatomo Kaneko"
                    },
                    {
                        "name": "Giovanni Cacciamani"
                    },
                    {
                        "name": "Andre Abreu"
                    },
                    {
                        "name": "Vinay Duddalwar"
                    },
                    {
                        "name": "C. -C. Jay Kuo"
                    },
                    {
                        "name": "Inderbir S. Gill"
                    },
                    {
                        "name": "Chrysostomos Nikias"
                    }
                ],
                "author_detail": {
                    "name": "Chrysostomos Nikias"
                },
                "author": "Chrysostomos Nikias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22370v2",
                "updated": "2025-06-30T10:08:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    8,
                    5,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T16:34:13Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    13,
                    4,
                    178,
                    0
                ],
                "title": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny"
                },
                "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. Our findings show that\nstudents perform significantly better when using ChatGPT; however, performance\ngains are tied to prompt quality. We conclude with practical recommendations\nfor integrating LLMs into formal methods courses more effectively, including\ndesigning LLM-aware challenges that promote learning rather than substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. Our findings show that\nstudents perform significantly better when using ChatGPT; however, performance\ngains are tied to prompt quality. We conclude with practical recommendations\nfor integrating LLMs into formal methods courses more effectively, including\ndesigning LLM-aware challenges that promote learning rather than substitution."
                },
                "authors": [
                    {
                        "name": "Carolina Carreira"
                    },
                    {
                        "name": "lvaro Silva"
                    },
                    {
                        "name": "Alexandre Abreu"
                    },
                    {
                        "name": "Alexandra Mendes"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Mendes"
                },
                "author": "Alexandra Mendes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23678v1",
                "updated": "2025-06-30T10:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    0,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T10:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    0,
                    43,
                    0,
                    181,
                    0
                ],
                "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought\n  Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought\n  Reasoning in Large Language Models"
                },
                "summary": "The output quality of large language models (LLMs) can be improved via\n\"reasoning\": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The output quality of large language models (LLMs) can be improved via\n\"reasoning\": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses."
                },
                "authors": [
                    {
                        "name": "Rock Yuren Pang"
                    },
                    {
                        "name": "K. J. Kevin Feng"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Chu Li"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Jeffrey Heer"
                    },
                    {
                        "name": "Katharina Reinecke"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Reinecke"
                },
                "author": "Katharina Reinecke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02335v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02335v4",
                "updated": "2025-06-30T09:50:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    50,
                    52,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-04T17:59:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity"
                },
                "summary": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable."
                },
                "authors": [
                    {
                        "name": "Yuqi Luo"
                    },
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Liqun Deng"
                    },
                    {
                        "name": "Jiansheng Wei"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "23 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02335v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02335v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23670v1",
                "updated": "2025-06-30T09:47:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    47,
                    37,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T09:47:37Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    47,
                    37,
                    0,
                    181,
                    0
                ],
                "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Interleaved Speech Modeling through Knowledge Distillation"
                },
                "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration."
                },
                "authors": [
                    {
                        "name": "Mohammadmahdi Nouriborji"
                    },
                    {
                        "name": "Morteza Rohanian"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Rohanian"
                },
                "author": "Morteza Rohanian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23667v1",
                "updated": "2025-06-30T09:44:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    44,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T09:44:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    44,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "L0: Reinforcement Learning to Become General Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L0: Reinforcement Learning to Become General Agents"
                },
                "summary": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0)."
                },
                "authors": [
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Jingyi Xi"
                    },
                    {
                        "name": "Zhuoyang Song"
                    },
                    {
                        "name": "Junyu Lu"
                    },
                    {
                        "name": "Yuhua Ke"
                    },
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Yukun Yang"
                    },
                    {
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "name": "Songxin Zhang"
                    },
                    {
                        "name": "Zejian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zejian Xie"
                },
                "author": "Zejian Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23663v1",
                "updated": "2025-06-30T09:39:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    39,
                    33,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T09:39:33Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    39,
                    33,
                    0,
                    181,
                    0
                ],
                "title": "On the Domain Robustness of Contrastive Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Domain Robustness of Contrastive Vision-Language Models"
                },
                "summary": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment."
                },
                "authors": [
                    {
                        "name": "Mario Koddenbrock"
                    },
                    {
                        "name": "Rudolf Hoffmann"
                    },
                    {
                        "name": "David Brodmann"
                    },
                    {
                        "name": "Erik Rodner"
                    }
                ],
                "author_detail": {
                    "name": "Erik Rodner"
                },
                "author": "Erik Rodner",
                "arxiv_comment": "Deepbench is available at https://github.com/ml-lab-htw/deepbench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12484v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12484v3",
                "updated": "2025-06-30T09:37:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    37,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T12:49:51Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    12,
                    49,
                    51,
                    5,
                    165,
                    0
                ],
                "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization"
                },
                "summary": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning."
                },
                "authors": [
                    {
                        "name": "Filip Sondej"
                    },
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Mikoaj Kniejski"
                    },
                    {
                        "name": "Marcel Windys"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Windys"
                },
                "author": "Marcel Windys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12484v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12484v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17821v2",
                "updated": "2025-06-30T09:23:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    23,
                    34,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-26T19:02:21Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    19,
                    2,
                    21,
                    1,
                    331,
                    0
                ],
                "title": "From quantum-enhanced to quantum-inspired Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From quantum-enhanced to quantum-inspired Monte Carlo"
                },
                "summary": "We perform a comprehensive analysis of the quantum-enhanced Monte Carlo\nmethod [Nature, 619, 282-287 (2023)], aimed at identifying the optimal working\npoint of the algorithm. We observe an optimal mixing Hamiltonian strength and\nanalyze the scaling of the total evolution time with the size of the system. We\nalso explore extensions of the circuit, including the use of time-dependent\nHamiltonians and reverse digitized annealing. Additionally, we propose that\nclassical, approximate quantum simulators can be used for the proposal step\ninstead of the original real-hardware implementation. We observe that\ntensor-network simulators, even with unconverged settings, can maintain a\nscaling advantage over standard classical samplers. This may extend the utility\nof quantum-enhanced Monte Carlo as a quantum-inspired algorithm, even before\nthe deployment of large-scale quantum hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a comprehensive analysis of the quantum-enhanced Monte Carlo\nmethod [Nature, 619, 282-287 (2023)], aimed at identifying the optimal working\npoint of the algorithm. We observe an optimal mixing Hamiltonian strength and\nanalyze the scaling of the total evolution time with the size of the system. We\nalso explore extensions of the circuit, including the use of time-dependent\nHamiltonians and reverse digitized annealing. Additionally, we propose that\nclassical, approximate quantum simulators can be used for the proposal step\ninstead of the original real-hardware implementation. We observe that\ntensor-network simulators, even with unconverged settings, can maintain a\nscaling advantage over standard classical samplers. This may extend the utility\nof quantum-enhanced Monte Carlo as a quantum-inspired algorithm, even before\nthe deployment of large-scale quantum hardware."
                },
                "authors": [
                    {
                        "name": "Johannes Christmann"
                    },
                    {
                        "name": "Petr Ivashkov"
                    },
                    {
                        "name": "Mattia Chiurco"
                    },
                    {
                        "name": "Guglielmo Mazzola"
                    }
                ],
                "author_detail": {
                    "name": "Guglielmo Mazzola"
                },
                "author": "Guglielmo Mazzola",
                "arxiv_doi": "10.1103/PhysRevA.111.042615",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevA.111.042615",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.17821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "JC and PI contributed equally",
                "arxiv_journal_ref": "Phys. Rev. A 111, 042615, 2025",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13430v2",
                "updated": "2025-06-30T09:19:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    19,
                    47,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-16T12:47:37Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    47,
                    37,
                    0,
                    167,
                    0
                ],
                "title": "Uncertainty-Aware Remaining Lifespan Prediction from Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Remaining Lifespan Prediction from Images"
                },
                "summary": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research."
                },
                "authors": [
                    {
                        "name": "Tristan Kenneweg"
                    },
                    {
                        "name": "Philip Kenneweg"
                    },
                    {
                        "name": "Barbara Hammer"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Hammer"
                },
                "author": "Barbara Hammer",
                "arxiv_comment": "Submitted to ISVC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23644v1",
                "updated": "2025-06-30T09:14:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    14,
                    49,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T09:14:49Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    14,
                    49,
                    0,
                    181,
                    0
                ],
                "title": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code\n  Analysis Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code\n  Analysis Integration"
                },
                "summary": "We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days."
                },
                "authors": [
                    {
                        "name": "Junze Hu"
                    },
                    {
                        "name": "Xiangyu Jin"
                    },
                    {
                        "name": "Yizhe Zeng"
                    },
                    {
                        "name": "Yuling Liu"
                    },
                    {
                        "name": "Yunpeng Li"
                    },
                    {
                        "name": "Dan Du"
                    },
                    {
                        "name": "Kaiyu Xie"
                    },
                    {
                        "name": "Hongsong Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hongsong Zhu"
                },
                "author": "Hongsong Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23643v1",
                "updated": "2025-06-30T09:13:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    13,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T09:13:54Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    13,
                    54,
                    0,
                    181,
                    0
                ],
                "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative\n  Recommendation"
                },
                "summary": "Generative recommendation (GR) typically encodes behavioral or semantic\naspects of item information into discrete tokens, leveraging the standard\nautoregressive (AR) generation paradigm to make predictions. However, existing\nmethods tend to overlook their intrinsic relationship, that is, the semantic\nusually provides some reasonable explainability \"$\\textbf{why}$\" for the\nbehavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To\nthis end, we present Chunk AutoRegressive Modeling (CAR), a new generation\nparadigm following the decision pattern that users usually think semantic\naspects of items (e.g. brand) and then take actions on target items (e.g.\npurchase). Our CAR, for the $\\textit{first time}$, incorporates semantics\n(SIDs) and behavior (UID) into a single autoregressive transformer from an\n``act-with-think'' dual perspective via chunk-level autoregression.\nSpecifically, CAR packs SIDs and UID into a conceptual chunk for item unified\nrepresentation, allowing each decoding step to make a holistic prediction.\nExperiments show that our CAR significantly outperforms existing methods based\non traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we\nverify the scaling effect between model performance and SIDs bit number,\ndemonstrating that CAR preliminary emulates a kind of slow-thinking style\nmechanism akin to the reasoning processes observed in large language models\n(LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation (GR) typically encodes behavioral or semantic\naspects of item information into discrete tokens, leveraging the standard\nautoregressive (AR) generation paradigm to make predictions. However, existing\nmethods tend to overlook their intrinsic relationship, that is, the semantic\nusually provides some reasonable explainability \"$\\textbf{why}$\" for the\nbehavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To\nthis end, we present Chunk AutoRegressive Modeling (CAR), a new generation\nparadigm following the decision pattern that users usually think semantic\naspects of items (e.g. brand) and then take actions on target items (e.g.\npurchase). Our CAR, for the $\\textit{first time}$, incorporates semantics\n(SIDs) and behavior (UID) into a single autoregressive transformer from an\n``act-with-think'' dual perspective via chunk-level autoregression.\nSpecifically, CAR packs SIDs and UID into a conceptual chunk for item unified\nrepresentation, allowing each decoding step to make a holistic prediction.\nExperiments show that our CAR significantly outperforms existing methods based\non traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we\nverify the scaling effect between model performance and SIDs bit number,\ndemonstrating that CAR preliminary emulates a kind of slow-thinking style\nmechanism akin to the reasoning processes observed in large language models\n(LLMs)."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Longtao Xiao"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Heng Chang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23640v1",
                "updated": "2025-06-30T09:09:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    9,
                    50,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T09:09:50Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    9,
                    50,
                    0,
                    181,
                    0
                ],
                "title": "Geminet: Learning the Duality-based Iterative Process for Lightweight\n  Traffic Engineering in Changing Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geminet: Learning the Duality-based Iterative Process for Lightweight\n  Traffic Engineering in Changing Topologies"
                },
                "summary": "Recently, researchers have explored ML-based Traffic Engineering (TE),\nleveraging neural networks to solve TE problems traditionally addressed by\noptimization. However, existing ML-based TE schemes remain impractical: they\neither fail to handle topology changes or suffer from poor scalability due to\nexcessive computational and memory overhead. To overcome these limitations, we\npropose Geminet, a lightweight and scalable ML-based TE framework that can\nhandle changing topologies. Geminet is built upon two key insights: (i) a\nmethodology that decouples neural networks from topology by learning an\niterative gradient-descent-based adjustment process, as the update rule of\ngradient descent is topology-agnostic, relying only on a few gradient-related\nquantities; (ii) shifting optimization from path-level routing weights to\nedge-level dual variables, reducing memory consumption by leveraging the fact\nthat edges are far fewer than paths. Evaluations on WAN and data center\ndatasets show that Geminet significantly improves scalability. Its neural\nnetwork size is only 0.04% to 7% of existing schemes, while handling topology\nvariations as effectively as HARP, a state-of-the-art ML-based TE approach,\nwithout performance degradation. When trained on large-scale topologies,\nGeminet consumes under 10 GiB of memory, more than eight times less than the\n80-plus GiB required by HARP, while achieving 5.45 times faster convergence\nspeed, demonstrating its potential for large-scale deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, researchers have explored ML-based Traffic Engineering (TE),\nleveraging neural networks to solve TE problems traditionally addressed by\noptimization. However, existing ML-based TE schemes remain impractical: they\neither fail to handle topology changes or suffer from poor scalability due to\nexcessive computational and memory overhead. To overcome these limitations, we\npropose Geminet, a lightweight and scalable ML-based TE framework that can\nhandle changing topologies. Geminet is built upon two key insights: (i) a\nmethodology that decouples neural networks from topology by learning an\niterative gradient-descent-based adjustment process, as the update rule of\ngradient descent is topology-agnostic, relying only on a few gradient-related\nquantities; (ii) shifting optimization from path-level routing weights to\nedge-level dual variables, reducing memory consumption by leveraging the fact\nthat edges are far fewer than paths. Evaluations on WAN and data center\ndatasets show that Geminet significantly improves scalability. Its neural\nnetwork size is only 0.04% to 7% of existing schemes, while handling topology\nvariations as effectively as HARP, a state-of-the-art ML-based TE approach,\nwithout performance degradation. When trained on large-scale topologies,\nGeminet consumes under 10 GiB of memory, more than eight times less than the\n80-plus GiB required by HARP, while achieving 5.45 times faster convergence\nspeed, demonstrating its potential for large-scale deployment."
                },
                "authors": [
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23635v1",
                "updated": "2025-06-30T09:04:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    4,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T09:04:25Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    4,
                    25,
                    0,
                    181,
                    0
                ],
                "title": "Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism\n  on Apple Silicon for Mixture-of-Experts Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism\n  on Apple Silicon for Mixture-of-Experts Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nwith significant advancements such as OpenAI's ChatGPT, Meta's Llama, and\nDatabricks' DBRX. This paper addresses the cost and scalability challenges\nencountered when constructing private LLM systems for personal or small group\nservices, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2\nUltra chips is established as a cost-efficient solution to host and accelerate\nthe pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our\nperformance analysis reveal that parallel execution of the model's experts\nacross two to four machine nodes significantly reduces inference time. We find\nthat computation time for the experts is comparable to the communication time\nfor exchanging their outputs, emphasizing the importance of network latency\nover bandwidth. We also observe significant management overhead due to Apple\nsoftware stack's memory management logic. Based on these findings, we develop\noptimization schemes to eliminate the memory management overhead. As a result,\nthe Mac Studio cluster is 1.15 times more cost-efficient than the\nstate-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we\nconstruct a performance model to estimate system performance under varying\nconfigurations, and the model provides valuable insights for designing private\nLLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nwith significant advancements such as OpenAI's ChatGPT, Meta's Llama, and\nDatabricks' DBRX. This paper addresses the cost and scalability challenges\nencountered when constructing private LLM systems for personal or small group\nservices, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2\nUltra chips is established as a cost-efficient solution to host and accelerate\nthe pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our\nperformance analysis reveal that parallel execution of the model's experts\nacross two to four machine nodes significantly reduces inference time. We find\nthat computation time for the experts is comparable to the communication time\nfor exchanging their outputs, emphasizing the importance of network latency\nover bandwidth. We also observe significant management overhead due to Apple\nsoftware stack's memory management logic. Based on these findings, we develop\noptimization schemes to eliminate the memory management overhead. As a result,\nthe Mac Studio cluster is 1.15 times more cost-efficient than the\nstate-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we\nconstruct a performance model to estimate system performance under varying\nconfigurations, and the model provides valuable insights for designing private\nLLM systems."
                },
                "authors": [
                    {
                        "name": "Mu-Chi Chen"
                    },
                    {
                        "name": "Po-Hsuan Huang"
                    },
                    {
                        "name": "Xiangrui Ke"
                    },
                    {
                        "name": "Chia-Heng Tu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Shih-Hao Hung"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Hao Hung"
                },
                "author": "Shih-Hao Hung",
                "arxiv_doi": "10.1145/3649601.3698722",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649601.3698722",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.23635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "International Conference on Research in Adaptive and Convergent\n  Systems (RACS '24), November 5--8, 2024, Pompei, Italy",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.6.4; I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21853v2",
                "updated": "2025-06-30T08:59:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    59,
                    34,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T02:08:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    2,
                    8,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via\n  Waypoint Interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via\n  Waypoint Interface"
                },
                "summary": "Quadrupedal robots have demonstrated exceptional locomotion capabilities\nthrough Reinforcement Learning (RL), including extreme parkour maneuvers.\nHowever, integrating locomotion skills with navigation in quadrupedal robots\nhas not been fully investigated, which holds promise for enhancing\nlong-distance movement capabilities. In this paper, we propose Skill-Nav, a\nmethod that incorporates quadrupedal locomotion skills into a hierarchical\nnavigation framework using waypoints as an interface. Specifically, we train a\nwaypoint-guided locomotion policy using deep RL, enabling the robot to\nautonomously adjust its locomotion skills to reach targeted positions while\navoiding obstacles. Compared with direct velocity commands, waypoints offer a\nsimpler yet more flexible interface for high-level planning and low-level\ncontrol. Utilizing waypoints as the interface allows for the application of\nvarious general planning tools, such as large language models (LLMs) and path\nplanning algorithms, to guide our locomotion policy in traversing terrains with\ndiverse obstacles. Extensive experiments conducted in both simulated and\nreal-world scenarios demonstrate that Skill-Nav can effectively traverse\ncomplex terrains and complete challenging navigation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadrupedal robots have demonstrated exceptional locomotion capabilities\nthrough Reinforcement Learning (RL), including extreme parkour maneuvers.\nHowever, integrating locomotion skills with navigation in quadrupedal robots\nhas not been fully investigated, which holds promise for enhancing\nlong-distance movement capabilities. In this paper, we propose Skill-Nav, a\nmethod that incorporates quadrupedal locomotion skills into a hierarchical\nnavigation framework using waypoints as an interface. Specifically, we train a\nwaypoint-guided locomotion policy using deep RL, enabling the robot to\nautonomously adjust its locomotion skills to reach targeted positions while\navoiding obstacles. Compared with direct velocity commands, waypoints offer a\nsimpler yet more flexible interface for high-level planning and low-level\ncontrol. Utilizing waypoints as the interface allows for the application of\nvarious general planning tools, such as large language models (LLMs) and path\nplanning algorithms, to guide our locomotion policy in traversing terrains with\ndiverse obstacles. Extensive experiments conducted in both simulated and\nreal-world scenarios demonstrate that Skill-Nav can effectively traverse\ncomplex terrains and complete challenging navigation tasks."
                },
                "authors": [
                    {
                        "name": "Dewei Wang"
                    },
                    {
                        "name": "Chenjia Bai"
                    },
                    {
                        "name": "Chenhui Li"
                    },
                    {
                        "name": "Jiyuan Shi"
                    },
                    {
                        "name": "Yan Ding"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Bin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhao"
                },
                "author": "Bin Zhao",
                "arxiv_comment": "17pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19474v2",
                "updated": "2025-06-30T08:40:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    40,
                    51,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-24T10:00:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    0,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "HMSViT: A Hierarchical Masked Self-Supervised Vision Transformer for\n  Corneal Nerve Segmentation and Diabetic Neuropathy Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMSViT: A Hierarchical Masked Self-Supervised Vision Transformer for\n  Corneal Nerve Segmentation and Diabetic Neuropathy Diagnosis"
                },
                "summary": "Diabetic Peripheral Neuropathy (DPN) affects nearly half of diabetes\npatients, requiring early detection. Corneal Confocal Microscopy (CCM) enables\nnon-invasive diagnosis, but automated methods suffer from inefficient feature\nextraction, reliance on handcrafted priors, and data limitations. We propose\nHMSViT, a novel Hierarchical Masked Self-Supervised Vision Transformer (HMSViT)\ndesigned for corneal nerve segmentation and DPN diagnosis. Unlike existing\nmethods, HMSViT employs pooling-based hierarchical and dual attention\nmechanisms with absolute positional encoding, enabling efficient multi-scale\nfeature extraction by capturing fine-grained local details in early layers and\nintegrating global context in deeper layers, all at a lower computational cost.\nA block-masked self supervised learning framework is designed for the HMSViT\nthat reduces reliance on labelled data, enhancing feature robustness, while a\nmulti-scale decoder is used for segmentation and classification by fusing\nhierarchical features. Experiments on clinical CCM datasets showed HMSViT\nachieves state-of-the-art performance, with 61.34% mIoU for nerve segmentation\nand 70.40% diagnostic accuracy, outperforming leading hierarchical models like\nthe Swin Transformer and HiViT by margins of up to 6.39% in segmentation\naccuracy while using fewer parameters. Detailed ablation studies further reveal\nthat integrating block-masked SSL with hierarchical multi-scale feature\nextraction substantially enhances performance compared to conventional\nsupervised training. Overall, these comprehensive experiments confirm that\nHMSViT delivers excellent, robust, and clinically viable results, demonstrating\nits potential for scalable deployment in real-world diagnostic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetic Peripheral Neuropathy (DPN) affects nearly half of diabetes\npatients, requiring early detection. Corneal Confocal Microscopy (CCM) enables\nnon-invasive diagnosis, but automated methods suffer from inefficient feature\nextraction, reliance on handcrafted priors, and data limitations. We propose\nHMSViT, a novel Hierarchical Masked Self-Supervised Vision Transformer (HMSViT)\ndesigned for corneal nerve segmentation and DPN diagnosis. Unlike existing\nmethods, HMSViT employs pooling-based hierarchical and dual attention\nmechanisms with absolute positional encoding, enabling efficient multi-scale\nfeature extraction by capturing fine-grained local details in early layers and\nintegrating global context in deeper layers, all at a lower computational cost.\nA block-masked self supervised learning framework is designed for the HMSViT\nthat reduces reliance on labelled data, enhancing feature robustness, while a\nmulti-scale decoder is used for segmentation and classification by fusing\nhierarchical features. Experiments on clinical CCM datasets showed HMSViT\nachieves state-of-the-art performance, with 61.34% mIoU for nerve segmentation\nand 70.40% diagnostic accuracy, outperforming leading hierarchical models like\nthe Swin Transformer and HiViT by margins of up to 6.39% in segmentation\naccuracy while using fewer parameters. Detailed ablation studies further reveal\nthat integrating block-masked SSL with hierarchical multi-scale feature\nextraction substantially enhances performance compared to conventional\nsupervised training. Overall, these comprehensive experiments confirm that\nHMSViT delivers excellent, robust, and clinically viable results, demonstrating\nits potential for scalable deployment in real-world diagnostic applications."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Liangxiu Han"
                    },
                    {
                        "name": "Yue Shi"
                    },
                    {
                        "name": "Yanlin Zheng"
                    },
                    {
                        "name": "Uazman Alam"
                    },
                    {
                        "name": "Maryam Ferdousi"
                    },
                    {
                        "name": "Rayaz Malik"
                    }
                ],
                "author_detail": {
                    "name": "Rayaz Malik"
                },
                "author": "Rayaz Malik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07217v2",
                "updated": "2025-06-30T08:31:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    31,
                    7,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-08T16:45:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    45,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "BIMgent: Towards Autonomous Building Modeling via Computer-use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIMgent: Towards Autonomous Building Modeling via Computer-use Agents"
                },
                "summary": "Existing computer-use agents primarily focus on general-purpose desktop\nautomation tasks, with limited exploration of their application in highly\nspecialized domains. In particular, the 3D building modeling process in the\nArchitecture, Engineering, and Construction (AEC) sector involves open-ended\ndesign tasks and complex interaction patterns within Building Information\nModeling (BIM) authoring software, which has yet to be thoroughly addressed by\ncurrent studies. In this paper, we propose BIMgent, an agentic framework\npowered by multimodal large language models (LLMs), designed to enable\nautonomous building model authoring via graphical user interface (GUI)\noperations. BIMgent automates the architectural building modeling process,\nincluding multimodal input for conceptual design, planning of software-specific\nworkflows, and efficient execution of the authoring GUI actions. We evaluate\nBIMgent on real-world building modeling tasks, including both text-based\nconceptual design generation and reconstruction from existing building design.\nThe design quality achieved by BIMgent was found to be reasonable. Its\noperations achieved a 32% success rate, whereas all baseline models failed to\ncomplete the tasks (0% success rate). Results demonstrate that BIMgent\neffectively reduces manual workload while preserving design intent,\nhighlighting its potential for practical deployment in real-world architectural\nmodeling scenarios. Project page: https://tumcms.github.io/BIMgent.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing computer-use agents primarily focus on general-purpose desktop\nautomation tasks, with limited exploration of their application in highly\nspecialized domains. In particular, the 3D building modeling process in the\nArchitecture, Engineering, and Construction (AEC) sector involves open-ended\ndesign tasks and complex interaction patterns within Building Information\nModeling (BIM) authoring software, which has yet to be thoroughly addressed by\ncurrent studies. In this paper, we propose BIMgent, an agentic framework\npowered by multimodal large language models (LLMs), designed to enable\nautonomous building model authoring via graphical user interface (GUI)\noperations. BIMgent automates the architectural building modeling process,\nincluding multimodal input for conceptual design, planning of software-specific\nworkflows, and efficient execution of the authoring GUI actions. We evaluate\nBIMgent on real-world building modeling tasks, including both text-based\nconceptual design generation and reconstruction from existing building design.\nThe design quality achieved by BIMgent was found to be reasonable. Its\noperations achieved a 32% success rate, whereas all baseline models failed to\ncomplete the tasks (0% success rate). Results demonstrate that BIMgent\neffectively reduces manual workload while preserving design intent,\nhighlighting its potential for practical deployment in real-world architectural\nmodeling scenarios. Project page: https://tumcms.github.io/BIMgent.github.io/"
                },
                "authors": [
                    {
                        "name": "Zihan Deng"
                    },
                    {
                        "name": "Changyu Du"
                    },
                    {
                        "name": "Stavros Nousias"
                    },
                    {
                        "name": "Andr Borrmann"
                    }
                ],
                "author_detail": {
                    "name": "Andr Borrmann"
                },
                "author": "Andr Borrmann",
                "arxiv_comment": "ICML 2025 Workshop on Computer Use Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12993v2",
                "updated": "2025-06-30T08:19:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    19,
                    19,
                    0,
                    181,
                    0
                ],
                "published": "2024-02-20T13:21:46Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    13,
                    21,
                    46,
                    1,
                    51,
                    0
                ],
                "title": "ChemMiner: A Large Language Model Agent System for Chemical Literature\n  Data Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemMiner: A Large Language Model Agent System for Chemical Literature\n  Data Mining"
                },
                "summary": "The development of AI-assisted chemical synthesis tools requires\ncomprehensive datasets covering diverse reaction types, yet current\nhigh-throughput experimental (HTE) approaches are expensive and limited in\nscope. Chemical literature represents a vast, underexplored data source\ncontaining thousands of reactions published annually. However, extracting\nreaction information from literature faces significant challenges including\nvaried writing styles, complex coreference relationships, and multimodal\ninformation presentation. This paper proposes ChemMiner, a novel end-to-end\nframework leveraging multiple agents powered by large language models (LLMs) to\nextract high-fidelity chemical data from literature. ChemMiner incorporates\nthree specialized agents: a text analysis agent for coreference mapping, a\nmultimodal agent for non-textual information extraction, and a synthesis\nanalysis agent for data generation. Furthermore, we developed a comprehensive\nbenchmark with expert-annotated chemical literature to evaluate both extraction\nefficiency and precision. Experimental results demonstrate reaction\nidentification rates comparable to human chemists while significantly reducing\nprocessing time, with high accuracy, recall, and F1 scores. Our open-sourced\nbenchmark facilitates future research in chemical literature data mining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of AI-assisted chemical synthesis tools requires\ncomprehensive datasets covering diverse reaction types, yet current\nhigh-throughput experimental (HTE) approaches are expensive and limited in\nscope. Chemical literature represents a vast, underexplored data source\ncontaining thousands of reactions published annually. However, extracting\nreaction information from literature faces significant challenges including\nvaried writing styles, complex coreference relationships, and multimodal\ninformation presentation. This paper proposes ChemMiner, a novel end-to-end\nframework leveraging multiple agents powered by large language models (LLMs) to\nextract high-fidelity chemical data from literature. ChemMiner incorporates\nthree specialized agents: a text analysis agent for coreference mapping, a\nmultimodal agent for non-textual information extraction, and a synthesis\nanalysis agent for data generation. Furthermore, we developed a comprehensive\nbenchmark with expert-annotated chemical literature to evaluate both extraction\nefficiency and precision. Experimental results demonstrate reaction\nidentification rates comparable to human chemists while significantly reducing\nprocessing time, with high accuracy, recall, and F1 scores. Our open-sourced\nbenchmark facilitates future research in chemical literature data mining."
                },
                "authors": [
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Hanqun Cao"
                    },
                    {
                        "name": "Menghao Guo"
                    },
                    {
                        "name": "Xilin Dang"
                    },
                    {
                        "name": "Lanqing Li"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Pheng Ann Heng"
                    },
                    {
                        "name": "Guangyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guangyong Chen"
                },
                "author": "Guangyong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23610v1",
                "updated": "2025-06-30T08:16:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    16,
                    7,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T08:16:07Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    16,
                    7,
                    0,
                    181,
                    0
                ],
                "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to\n  Misinformation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Simulation of Human Personality-Driven Susceptibility to\n  Misinformation with LLMs"
                },
                "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents."
                },
                "authors": [
                    {
                        "name": "Manuel Pratelli"
                    },
                    {
                        "name": "Marinella Petrocchi"
                    }
                ],
                "author_detail": {
                    "name": "Marinella Petrocchi"
                },
                "author": "Marinella Petrocchi",
                "arxiv_comment": "pre-print version - paper actually under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23605v1",
                "updated": "2025-06-30T08:11:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    11,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T08:11:31Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    11,
                    31,
                    0,
                    181,
                    0
                ],
                "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Generated Lecture Slides for Improving Slide Element Detection and\n  Retrieval"
                },
                "summary": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/."
                },
                "authors": [
                    {
                        "name": "Suyash Maniyar"
                    },
                    {
                        "name": "Vishvesh Trivedi"
                    },
                    {
                        "name": "Ajoy Mondal"
                    },
                    {
                        "name": "Anand Mishra"
                    },
                    {
                        "name": "C. V. Jawahar"
                    }
                ],
                "author_detail": {
                    "name": "C. V. Jawahar"
                },
                "author": "C. V. Jawahar",
                "arxiv_comment": "40 pages including supplementary, accepted at ICDAR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17728v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17728v3",
                "updated": "2025-06-30T08:08:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    8,
                    21,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-21T14:58:53Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    14,
                    58,
                    53,
                    5,
                    172,
                    0
                ],
                "title": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via\n  Knowledge-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via\n  Knowledge-Augmented Generation"
                },
                "summary": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition..."
                },
                "authors": [
                    {
                        "name": "Dalong Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Ling Zhong"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Peilong Zhao"
                    },
                    {
                        "name": "QiWei Wang"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Xinkai Du"
                    },
                    {
                        "name": "YangYang Hou"
                    },
                    {
                        "name": "Yu Ao"
                    },
                    {
                        "name": "ZhaoYang Wang"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "ZhiYing Yi"
                    },
                    {
                        "name": "Zhongpu Bo"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17728v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17728v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23603v1",
                "updated": "2025-06-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    8,
                    15,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    8,
                    15,
                    0,
                    181,
                    0
                ],
                "title": "SoK: Semantic Privacy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Semantic Privacy in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs."
                },
                "authors": [
                    {
                        "name": "Baihe Ma"
                    },
                    {
                        "name": "Yanna Jiang"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Guangshen Yu"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Caijun Sun"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Xuelei Qi"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Ren Ping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ren Ping Liu"
                },
                "author": "Ren Ping Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05040v2",
                "updated": "2025-06-30T07:41:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    41,
                    7,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-07T13:05:09Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    5,
                    9,
                    0,
                    97,
                    0
                ],
                "title": "InstructionBench: An Instructional Video Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructionBench: An Instructional Video Understanding Benchmark"
                },
                "summary": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources. All data are available at\nhttps://huggingface.co/datasets/sunwhw/InstructionBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources. All data are available at\nhttps://huggingface.co/datasets/sunwhw/InstructionBench."
                },
                "authors": [
                    {
                        "name": "Haiwan Wei"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Wei Ke"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22200v2",
                "updated": "2025-06-30T07:33:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    33,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T13:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Zedong Dan"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhi Zhang"
                },
                "author": "Yuzhi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23576v1",
                "updated": "2025-06-30T07:29:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    29,
                    7,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T07:29:07Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    29,
                    7,
                    0,
                    181,
                    0
                ],
                "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large\n  Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems."
                },
                "authors": [
                    {
                        "name": "Maria Carolina Cornelia Wit"
                    },
                    {
                        "name": "Jun Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Pang"
                },
                "author": "Jun Pang",
                "arxiv_comment": "26 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23573v1",
                "updated": "2025-06-30T07:25:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    25,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T07:25:31Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    25,
                    31,
                    0,
                    181,
                    0
                ],
                "title": "Online Human Action Detection during Escorting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Human Action Detection during Escorting"
                },
                "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Siddhartha Mondal"
                    },
                    {
                        "name": "Avik Mitra"
                    },
                    {
                        "name": "Chayan Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Chayan Sarkar"
                },
                "author": "Chayan Sarkar",
                "arxiv_comment": "Accepted in IEEE RO-MAN '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23557v1",
                "updated": "2025-06-30T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    4,
                    46,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T07:04:46Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    7,
                    4,
                    46,
                    0,
                    181,
                    0
                ],
                "title": "Data-Driven Modulation Optimization with LMMSE Equalization for\n  Reliability Enhancement in Underwater Acoustic Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Modulation Optimization with LMMSE Equalization for\n  Reliability Enhancement in Underwater Acoustic Communications"
                },
                "summary": "Ultra-reliable underwater acoustic (UWA) communications serve as one of the\nkey enabling technologies for future space-air-ground-underwater integrated\nnetworks. However, the reliability of current UWA transmission is still\ninsufficient since severe performance degradation occurs for conventional\nmulticarrier systems in UWA channels with severe delay-scale spread. To solve\nthis problem, we exploit learning-inspired approaches to optimize the\nmodulation scheme under the assumption of linear minimum mean square error\n(LMMSE) equalization, where the discrete representation of waveforms is adopted\nby utilizing Nyquist filters. The optimization problem is first transferred\ninto maximizing the fairness of estimation mean square error (MSE) for each\ndata symbol since the total MSE is invariant considering the property of\northogonal modulation. The Siamese architecture is then adopted to obtain\nconsistent optimization results across various channel conditions, which avoids\nthe overhead of online feedback, cooperation, and deployment of neural networks\nand guarantees generalization. The overall scheme including the loss function,\nneural network structure, and training process is also investigated in depth in\nthis paper. The excellent performance and robustness of the proposed modulation\nscheme are verified by carrying out the bit error rate test over various UWA\nchannels with severe delay-scale spread.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-reliable underwater acoustic (UWA) communications serve as one of the\nkey enabling technologies for future space-air-ground-underwater integrated\nnetworks. However, the reliability of current UWA transmission is still\ninsufficient since severe performance degradation occurs for conventional\nmulticarrier systems in UWA channels with severe delay-scale spread. To solve\nthis problem, we exploit learning-inspired approaches to optimize the\nmodulation scheme under the assumption of linear minimum mean square error\n(LMMSE) equalization, where the discrete representation of waveforms is adopted\nby utilizing Nyquist filters. The optimization problem is first transferred\ninto maximizing the fairness of estimation mean square error (MSE) for each\ndata symbol since the total MSE is invariant considering the property of\northogonal modulation. The Siamese architecture is then adopted to obtain\nconsistent optimization results across various channel conditions, which avoids\nthe overhead of online feedback, cooperation, and deployment of neural networks\nand guarantees generalization. The overall scheme including the loss function,\nneural network structure, and training process is also investigated in depth in\nthis paper. The excellent performance and robustness of the proposed modulation\nscheme are verified by carrying out the bit error rate test over various UWA\nchannels with severe delay-scale spread."
                },
                "authors": [
                    {
                        "name": "Xuehan Wang"
                    },
                    {
                        "name": "Hengyu Zhang"
                    },
                    {
                        "name": "Jintao Wang"
                    },
                    {
                        "name": "Zhi Sun"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "arxiv_comment": "6 pages, 3 figures. This paper has been accepted for presentation in\n  IEEE/CIC ICCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20166v2",
                "updated": "2025-06-30T06:48:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    6,
                    48,
                    46,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-26T16:08:41Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    8,
                    41,
                    0,
                    146,
                    0
                ],
                "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment\n  with Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment\n  with Synthetic Data"
                },
                "summary": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\ncrucial textual capabilities like instruction-following are lost after training\non audio data. In some cases, models may even hallucinate sounds that are not\npresent in the input audio, raising concerns about reliability. Second,\nachieving cross-modal alignment between audio and language typically relies on\nlarge collections of task-specific question-answer pairs for instruction\ntuning, making it resource-intensive. To address these issues, previous works\nhave leveraged the backbone LLMs to synthesize general-purpose, caption-style\nalignment data. In this paper, we propose a data generation framework that\nproduces contrastive-like training data, designed to enhance ALLMs' ability to\ndifferentiate between present and absent sounds. We further extend our approach\nto multi-audio scenarios, enabling the model to either explain differences\nbetween audio inputs or produce unified captions that describe all inputs,\nthereby enhancing audio-language alignment. We refer to the entire ALLM\ntraining framework as bootstrapping audio-language alignment via synthetic data\ngeneration from backbone LLMs (BALSa). Experimental results indicate that our\nmethod effectively mitigates audio hallucinations while reliably maintaining\nstrong performance on audio understanding and reasoning benchmarks, as well as\ninstruction-following skills. Moreover, incorporating multi-audio training\nfurther enhances the model's comprehension and reasoning capabilities. Overall,\nBALSa offers an efficient and scalable approach to developing ALLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\ncrucial textual capabilities like instruction-following are lost after training\non audio data. In some cases, models may even hallucinate sounds that are not\npresent in the input audio, raising concerns about reliability. Second,\nachieving cross-modal alignment between audio and language typically relies on\nlarge collections of task-specific question-answer pairs for instruction\ntuning, making it resource-intensive. To address these issues, previous works\nhave leveraged the backbone LLMs to synthesize general-purpose, caption-style\nalignment data. In this paper, we propose a data generation framework that\nproduces contrastive-like training data, designed to enhance ALLMs' ability to\ndifferentiate between present and absent sounds. We further extend our approach\nto multi-audio scenarios, enabling the model to either explain differences\nbetween audio inputs or produce unified captions that describe all inputs,\nthereby enhancing audio-language alignment. We refer to the entire ALLM\ntraining framework as bootstrapping audio-language alignment via synthetic data\ngeneration from backbone LLMs (BALSa). Experimental results indicate that our\nmethod effectively mitigates audio hallucinations while reliably maintaining\nstrong performance on audio understanding and reasoning benchmarks, as well as\ninstruction-following skills. Moreover, incorporating multi-audio training\nfurther enhances the model's comprehension and reasoning capabilities. Overall,\nBALSa offers an efficient and scalable approach to developing ALLMs."
                },
                "authors": [
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. Project Website: https://kuan2jiu99.github.io/Balsa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01328v2",
                "updated": "2025-06-30T06:37:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    6,
                    37,
                    5,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-03T09:11:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    11,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory\n  Optimization"
                },
                "summary": "Pipeline parallelism (PP) is widely used for training large language models\n(LLMs), yet its scalability is often constrained by high activation memory\nconsumption as the number of in-flight microbatches grows with the degree of\nPP. In this paper, we focus on addressing this challenge by leveraging the\nunder-explored memory offload strategy in PP. With empirical study, we discover\nthat in the majority of standard configurations, at least half, and potentially\nall, of the activations can be offloaded with negligible overhead. In the cases\nwhere full overload is not possible, we introduce a novel selective offload\nstrategy that decreases peak activation memory in a better-than-linear manner.\nFurthermore, we integrate memory offload with other techniques to jointly\nconsider overall throughput and memory limitation. Our experiments proves that\nthe per-device activation memory effectively reduces with the total number of\nstages, making PP a stronger alternative than TP, offering up to a 19\\%\nacceleration with even lower memory consumption. The implementation is\nopen-sourced at\n\\href{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}{this url}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism (PP) is widely used for training large language models\n(LLMs), yet its scalability is often constrained by high activation memory\nconsumption as the number of in-flight microbatches grows with the degree of\nPP. In this paper, we focus on addressing this challenge by leveraging the\nunder-explored memory offload strategy in PP. With empirical study, we discover\nthat in the majority of standard configurations, at least half, and potentially\nall, of the activations can be offloaded with negligible overhead. In the cases\nwhere full overload is not possible, we introduce a novel selective offload\nstrategy that decreases peak activation memory in a better-than-linear manner.\nFurthermore, we integrate memory offload with other techniques to jointly\nconsider overall throughput and memory limitation. Our experiments proves that\nthe per-device activation memory effectively reduces with the total number of\nstages, making PP a stronger alternative than TP, offering up to a 19\\%\nacceleration with even lower memory consumption. The implementation is\nopen-sourced at\n\\href{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}{this url}."
                },
                "authors": [
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Guangxing Huang"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23535v1",
                "updated": "2025-06-30T05:53:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    53,
                    45,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T05:53:45Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    53,
                    45,
                    0,
                    181,
                    0
                ],
                "title": "Comparative Analysis of the Code Generated by Popular Large Language\n  Models (LLMs) for MISRA C++ Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of the Code Generated by Popular Large Language\n  Models (LLMs) for MISRA C++ Compliance"
                },
                "summary": "Safety-critical systems are engineered systems whose failure or malfunction\ncould result in catastrophic consequences. The software development for\nsafety-critical systems necessitates rigorous engineering practices and\nadherence to certification standards like DO-178C for avionics. DO-178C is a\nguidance document which requires compliance to well-defined software coding\nstandards like MISRA C++ to enforce coding guidelines that prevent the use of\nambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have\ndemonstrated significant capabilities in automatic code generation across a\nwide range of programming languages, including C++. Despite their impressive\nperformance, code generated by LLMs in safety-critical domains must be\ncarefully analyzed for conformance to MISRA C++ coding standards. In this\npaper, I have conducted a comparative analysis of the C++ code generated by\npopular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and\nMicrosoft Copilot for compliance with MISRA C++.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety-critical systems are engineered systems whose failure or malfunction\ncould result in catastrophic consequences. The software development for\nsafety-critical systems necessitates rigorous engineering practices and\nadherence to certification standards like DO-178C for avionics. DO-178C is a\nguidance document which requires compliance to well-defined software coding\nstandards like MISRA C++ to enforce coding guidelines that prevent the use of\nambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have\ndemonstrated significant capabilities in automatic code generation across a\nwide range of programming languages, including C++. Despite their impressive\nperformance, code generated by LLMs in safety-critical domains must be\ncarefully analyzed for conformance to MISRA C++ coding standards. In this\npaper, I have conducted a comparative analysis of the C++ code generated by\npopular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and\nMicrosoft Copilot for compliance with MISRA C++."
                },
                "authors": [
                    {
                        "name": "Malik Muhammad Umer"
                    }
                ],
                "author_detail": {
                    "name": "Malik Muhammad Umer"
                },
                "author": "Malik Muhammad Umer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10798v2",
                "updated": "2025-06-30T05:35:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    35,
                    42,
                    0,
                    181,
                    0
                ],
                "published": "2024-03-16T04:01:50Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    4,
                    1,
                    50,
                    5,
                    76,
                    0
                ],
                "title": "Object Retrieval for Visual Question Answering with Outside Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object Retrieval for Visual Question Answering with Outside Knowledge"
                },
                "summary": "Retrieval-augmented generation (RAG) with large language models (LLMs) plays\na crucial role in question answering, as LLMs possess limited knowledge and are\nnot updated with continuously growing information. Most recent work on RAG has\nfocused primarily on text-based or large-image retrieval, which constrains the\nbroader application of RAG models. We recognize that object-level retrieval is\nessential for addressing questions that extend beyond image content. To tackle\nthis issue, we propose a task of object retrieval for visual question answering\nwith outside knowledge (OR-OK-VQA), aimed to extend image-based content\nunderstanding in conjunction with LLMs. A key challenge in this task is\nretrieving diverse objects-related images that contribute to answering the\nquestions. To enable accurate and robust general object retrieval, it is\nnecessary to learn embeddings for local objects. This paper introduces a novel\nunsupervised deep feature embedding technique called multi-scale group\ncollaborative embedding learning (MS-GCEL), developed to learn embeddings for\nlong-tailed objects at different scales. Additionally, we establish an OK-VQA\nevaluation benchmark using images from the BelgaLogos, Visual Genome, and LVIS\ndatasets. Prior to the OK-VQA evaluation, we construct a benchmark of\nchallenges utilizing objects extracted from the COCO 2017 and VOC 2007 datasets\nto support the training and evaluation of general object retrieval models. Our\nevaluations on both general object retrieval and OK-VQA demonstrate the\neffectiveness of the proposed approach. The code and dataset will be publicly\nreleased for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) with large language models (LLMs) plays\na crucial role in question answering, as LLMs possess limited knowledge and are\nnot updated with continuously growing information. Most recent work on RAG has\nfocused primarily on text-based or large-image retrieval, which constrains the\nbroader application of RAG models. We recognize that object-level retrieval is\nessential for addressing questions that extend beyond image content. To tackle\nthis issue, we propose a task of object retrieval for visual question answering\nwith outside knowledge (OR-OK-VQA), aimed to extend image-based content\nunderstanding in conjunction with LLMs. A key challenge in this task is\nretrieving diverse objects-related images that contribute to answering the\nquestions. To enable accurate and robust general object retrieval, it is\nnecessary to learn embeddings for local objects. This paper introduces a novel\nunsupervised deep feature embedding technique called multi-scale group\ncollaborative embedding learning (MS-GCEL), developed to learn embeddings for\nlong-tailed objects at different scales. Additionally, we establish an OK-VQA\nevaluation benchmark using images from the BelgaLogos, Visual Genome, and LVIS\ndatasets. Prior to the OK-VQA evaluation, we construct a benchmark of\nchallenges utilizing objects extracted from the COCO 2017 and VOC 2007 datasets\nto support the training and evaluation of general object retrieval models. Our\nevaluations on both general object retrieval and OK-VQA demonstrate the\neffectiveness of the proposed approach. The code and dataset will be publicly\nreleased for future research."
                },
                "authors": [
                    {
                        "name": "Shichao Kan"
                    },
                    {
                        "name": "Yuhai Deng"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Lihui Cen"
                    },
                    {
                        "name": "Zhe Qu"
                    },
                    {
                        "name": "Linna Zhang"
                    },
                    {
                        "name": "Yixiong Liang"
                    },
                    {
                        "name": "Yigang Cen"
                    }
                ],
                "author_detail": {
                    "name": "Yigang Cen"
                },
                "author": "Yigang Cen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17960v2",
                "updated": "2025-06-30T05:35:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    35,
                    14,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-27T00:36:27Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    0,
                    36,
                    27,
                    2,
                    332,
                    0
                ],
                "title": "Calibrating DRAMPower Model for HPC: A Runtime Perspective from\n  Real-Time Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating DRAMPower Model for HPC: A Runtime Perspective from\n  Real-Time Measurements"
                },
                "summary": "Main memory's rising energy consumption has emerged as a critical challenge\nin modern computing architectures, particularly in large-scale systems, driven\nby frequent access patterns, growing data volumes, and insufficient power\nmanagement strategies. Accurate modeling of DRAM power consumption is essential\nto address this challenge and optimize energy efficiency. However, existing\nmodeling tools often rely on vendor-provided datasheet values that are obtained\nunder worst-case or idealized conditions. As a result, they fail to capture\nimportant system-level factors, such as temperature variations, chip aging, and\nworkload-induced variability, which leads to significant discrepancies between\nestimated and actual power consumption observed in real deployments. In this\nwork, we propose a runtime calibration methodology for the DRAMPower model\nusing energy measurements collected from real-system experiments. By applying\ncustom memory benchmarks on an HPC cluster and leveraging fine-grained power\nmonitoring infrastructure, we refine key current parameters (IDD values) in the\nmodel. Our calibration reduces the average energy estimation error to less than\n5%, substantially improving modeling accuracy and making DRAMPower a more\nreliable tool for power-aware system design and optimization on the target\nserver platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Main memory's rising energy consumption has emerged as a critical challenge\nin modern computing architectures, particularly in large-scale systems, driven\nby frequent access patterns, growing data volumes, and insufficient power\nmanagement strategies. Accurate modeling of DRAM power consumption is essential\nto address this challenge and optimize energy efficiency. However, existing\nmodeling tools often rely on vendor-provided datasheet values that are obtained\nunder worst-case or idealized conditions. As a result, they fail to capture\nimportant system-level factors, such as temperature variations, chip aging, and\nworkload-induced variability, which leads to significant discrepancies between\nestimated and actual power consumption observed in real deployments. In this\nwork, we propose a runtime calibration methodology for the DRAMPower model\nusing energy measurements collected from real-system experiments. By applying\ncustom memory benchmarks on an HPC cluster and leveraging fine-grained power\nmonitoring infrastructure, we refine key current parameters (IDD values) in the\nmodel. Our calibration reduces the average energy estimation error to less than\n5%, substantially improving modeling accuracy and making DRAMPower a more\nreliable tool for power-aware system design and optimization on the target\nserver platform."
                },
                "authors": [
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Dina Ali Abdelhamid"
                    },
                    {
                        "name": "Thomas Ilsche"
                    },
                    {
                        "name": "Saeideh Alinezhad Chamazcoti"
                    },
                    {
                        "name": "Timon Evenblij"
                    },
                    {
                        "name": "Mohit Gupta"
                    },
                    {
                        "name": "Francky Catthoor"
                    }
                ],
                "author_detail": {
                    "name": "Francky Catthoor"
                },
                "author": "Francky Catthoor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23527v1",
                "updated": "2025-06-30T05:27:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    27,
                    11,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T05:27:11Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    27,
                    11,
                    0,
                    181,
                    0
                ],
                "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your\n  Model a Creative Cook, a Bad Cook, or Merely a Plagiator?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Recipe Memorization and Creativity in Large Language Models: Is Your\n  Model a Creative Cook, a Bad Cook, or Merely a Plagiator?"
                },
                "summary": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities."
                },
                "authors": [
                    {
                        "name": "Jan Kvapil"
                    },
                    {
                        "name": "Martin Fajcik"
                    }
                ],
                "author_detail": {
                    "name": "Martin Fajcik"
                },
                "author": "Martin Fajcik",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00924v2",
                "updated": "2025-06-30T05:21:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-01T09:31:55Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    9,
                    31,
                    55,
                    6,
                    152,
                    0
                ],
                "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using\n  LLM-Based Comment Analysis and Network MOS Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using\n  LLM-Based Comment Analysis and Network MOS Comparison"
                },
                "summary": "This paper introduces a dual-layer framework for network operator-side\nquality of experience (QoE) assessment that integrates both objective network\nmodeling and subjective user perception extracted from live-streaming\nplatforms. On the objective side, we develop a machine learning model trained\non mean opinion scores (MOS) computed via the ITU-T P.1203 reference\nimplementation, allowing accurate prediction of user-perceived video quality\nusing only network parameters such as packet loss, delay, jitter, and\nthroughput without reliance on video content or client-side instrumentation. On\nthe subjective side, we present a semantic filtering and scoring pipeline that\nprocesses user comments from live streams to extract performance-related\nfeedback. A large language model is used to assign scalar MOS scores to\nfiltered comments in a deterministic and reproducible manner. To support\nscalable and interpretable analysis, we construct a labeled dataset of 47,894\nlive-stream comments, of which about 34,000 are identified as QoE-relevant\nthrough multi-layer semantic filtering. Each comment is enriched with simulated\nInternet Service Provider attribution and temporally aligned using synthetic\ntimestamps in 5-min intervals. The resulting dataset enables operator-level\naggregation and time-series analysis of user-perceived quality. A delta MOS\nmetric is proposed to measure each Internet service provider's deviation from\nplatform-wide sentiment, allowing detection of localized degradations even in\nthe absence of direct network telemetry. A controlled outage simulation\nconfirms the framework's effectiveness in identifying service disruptions\nthrough comment-based trends alone. The system provides each operator with its\nown subjective MOS and the global platform average per interval, enabling\nreal-time interpretation of performance deviations and comparison with\nobjective network-based QoE estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a dual-layer framework for network operator-side\nquality of experience (QoE) assessment that integrates both objective network\nmodeling and subjective user perception extracted from live-streaming\nplatforms. On the objective side, we develop a machine learning model trained\non mean opinion scores (MOS) computed via the ITU-T P.1203 reference\nimplementation, allowing accurate prediction of user-perceived video quality\nusing only network parameters such as packet loss, delay, jitter, and\nthroughput without reliance on video content or client-side instrumentation. On\nthe subjective side, we present a semantic filtering and scoring pipeline that\nprocesses user comments from live streams to extract performance-related\nfeedback. A large language model is used to assign scalar MOS scores to\nfiltered comments in a deterministic and reproducible manner. To support\nscalable and interpretable analysis, we construct a labeled dataset of 47,894\nlive-stream comments, of which about 34,000 are identified as QoE-relevant\nthrough multi-layer semantic filtering. Each comment is enriched with simulated\nInternet Service Provider attribution and temporally aligned using synthetic\ntimestamps in 5-min intervals. The resulting dataset enables operator-level\naggregation and time-series analysis of user-perceived quality. A delta MOS\nmetric is proposed to measure each Internet service provider's deviation from\nplatform-wide sentiment, allowing detection of localized degradations even in\nthe absence of direct network telemetry. A controlled outage simulation\nconfirms the framework's effectiveness in identifying service disruptions\nthrough comment-based trends alone. The system provides each operator with its\nown subjective MOS and the global platform average per interval, enabling\nreal-time interpretation of performance deviations and comparison with\nobjective network-based QoE estimates."
                },
                "authors": [
                    {
                        "name": "Parsa Hassani Shariat Panahi"
                    },
                    {
                        "name": "Amir Hossein Jalilvand"
                    },
                    {
                        "name": "M. Hassan Najafi"
                    }
                ],
                "author_detail": {
                    "name": "M. Hassan Najafi"
                },
                "author": "M. Hassan Najafi",
                "arxiv_comment": "19 ppages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01705v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01705v3",
                "updated": "2025-06-30T05:16:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    16,
                    2,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-03T13:30:29Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    30,
                    29,
                    0,
                    34,
                    0
                ],
                "title": "Progressive Binarization with Semi-Structured Pruning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Binarization with Semi-Structured Pruning for LLMs"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress in natural\nlanguage processing, but their high computational and memory costs hinder\ndeployment on resource-constrained devices. Binarization, which reduces model\nweights to 1 bit, is a promising solution for efficient inference. However,\nbinarized LLMs still exhibit redundancy that can be further compressed.\nSemi-structured pruning offers a favorable trade-off between model performance\nand hardware efficiency, but naively combining it with binarization often leads\nto severe performance degradation. To address this, we propose Progressive\nBinarization with Semi-Structured Pruning (PBS$^2$P), a novel post-training\ncompression framework. We propose Stepwise semi-structured Pruning with\nBinarization Optimization (SPBO) to jointly reduce pruning and binarization\nerror. Additionally, we develop a Coarse-to-Fine Search (CFS) strategy to more\neffectively select pruning elements. Extensive experiments across multiple LLM\nfamilies show that PBS$^2$P consistently outperforms state-of-the-art binary\npost-training quantization methods in both perplexity and downstream accuracy.\nThe code and models will be available at:\nhttps://github.com/XIANGLONGYAN/PBS2P.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress in natural\nlanguage processing, but their high computational and memory costs hinder\ndeployment on resource-constrained devices. Binarization, which reduces model\nweights to 1 bit, is a promising solution for efficient inference. However,\nbinarized LLMs still exhibit redundancy that can be further compressed.\nSemi-structured pruning offers a favorable trade-off between model performance\nand hardware efficiency, but naively combining it with binarization often leads\nto severe performance degradation. To address this, we propose Progressive\nBinarization with Semi-Structured Pruning (PBS$^2$P), a novel post-training\ncompression framework. We propose Stepwise semi-structured Pruning with\nBinarization Optimization (SPBO) to jointly reduce pruning and binarization\nerror. Additionally, we develop a Coarse-to-Fine Search (CFS) strategy to more\neffectively select pruning elements. Extensive experiments across multiple LLM\nfamilies show that PBS$^2$P consistently outperforms state-of-the-art binary\npost-training quantization methods in both perplexity and downstream accuracy.\nThe code and models will be available at:\nhttps://github.com/XIANGLONGYAN/PBS2P."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01705v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01705v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23520v2",
                "updated": "2025-07-01T08:11:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    11,
                    18,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T05:11:19Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    11,
                    19,
                    0,
                    181,
                    0
                ],
                "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions\n  with LLM-Generated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions\n  with LLM-Generated Data"
                },
                "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ruijie Yu"
                    },
                    {
                        "name": "Jidong Tian"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Jiapeng Liu"
                    },
                    {
                        "name": "Xiaokang Yang"
                    },
                    {
                        "name": "Yaohui Jin"
                    },
                    {
                        "name": "Yanyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Xu"
                },
                "author": "Yanyan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10135v2",
                "updated": "2025-06-30T04:51:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    4,
                    51,
                    0,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-13T07:55:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    55,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative\n  Decoding"
                },
                "summary": "Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025). Code: https://github.com/AMD-AIG-AIMA/Gumiho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05352v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05352v3",
                "updated": "2025-06-30T04:32:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    4,
                    32,
                    15,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-07T04:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    4,
                    50,
                    4,
                    0,
                    97,
                    0
                ],
                "title": "Achieving binary weight and activation for LLMs using Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving binary weight and activation for LLMs using Post-Training\n  Quantization"
                },
                "summary": "Quantizing large language models (LLMs) to 1-bit precision significantly\nreduces computational costs, but existing quantization techniques suffer from\nnoticeable performance degradation when using weight and activation precisions\nbelow 4 bits (W4A4). In this paper, we propose a post-training quantization\nframework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit\nwith an additional 1 bit for fine-grain grouping and activations are quantized\nto 1 bit with a 4-fold increase in the number of channels. For weight\nquantization, we propose utilizing Hessian-aware fine-grained grouping along\nwith an EM-based quantization scheme. For activation quantization, we decompose\nINT4-quantized activations into a 4 * INT1 format equivalently and\nsimultaneously smooth the scaling factors based on quantization errors, which\nfurther reduces the quantization errors in activations. Our method surpasses\nstate-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple\ntasks, pushing the boundaries of existing LLM quantization methods toward fully\nbinarized models. Code is available at\nhttps://github.com/JimmyCrave/LLM-PTQ-binarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing large language models (LLMs) to 1-bit precision significantly\nreduces computational costs, but existing quantization techniques suffer from\nnoticeable performance degradation when using weight and activation precisions\nbelow 4 bits (W4A4). In this paper, we propose a post-training quantization\nframework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit\nwith an additional 1 bit for fine-grain grouping and activations are quantized\nto 1 bit with a 4-fold increase in the number of channels. For weight\nquantization, we propose utilizing Hessian-aware fine-grained grouping along\nwith an EM-based quantization scheme. For activation quantization, we decompose\nINT4-quantized activations into a 4 * INT1 format equivalently and\nsimultaneously smooth the scaling factors based on quantization errors, which\nfurther reduces the quantization errors in activations. Our method surpasses\nstate-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple\ntasks, pushing the boundaries of existing LLM quantization methods toward fully\nbinarized models. Code is available at\nhttps://github.com/JimmyCrave/LLM-PTQ-binarization."
                },
                "authors": [
                    {
                        "name": "Siqing Song"
                    },
                    {
                        "name": "Chuang Wang"
                    },
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Xu-Yao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xu-Yao Zhang"
                },
                "author": "Xu-Yao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05352v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05352v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]