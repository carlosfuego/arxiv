[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v1",
                "updated": "2025-07-02T08:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "A new efficient RPKI Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new efficient RPKI Design"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v1",
                "updated": "2025-07-01T18:12:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Khler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v1",
                "updated": "2025-07-01T16:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01988v1",
                "updated": "2025-06-28T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers"
                },
                "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency."
                },
                "authors": [
                    {
                        "name": "Giyong Jung"
                    },
                    {
                        "name": "Saeid Gorgin"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungrae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jungrae Kim"
                },
                "author": "Jungrae Kim",
                "arxiv_comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Sren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Sren Stobbe"
                },
                "author": "Sren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uro Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uro Seljak"
                },
                "author": "Uro Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01040v1",
                "updated": "2025-06-22T20:43:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T20:43:42Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "title": "Fast Clifford Neural Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Clifford Neural Layers"
                },
                "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers"
                },
                "authors": [
                    {
                        "name": "Tianxiang Xia"
                    },
                    {
                        "name": "Max Neuwinger"
                    },
                    {
                        "name": "Lin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Xiao"
                },
                "author": "Lin Xiao",
                "arxiv_comment": "7 pages content-wise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1604.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1604.01713v2",
                "updated": "2025-06-19T10:23:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    23,
                    50,
                    3,
                    170,
                    0
                ],
                "published": "2016-04-06T18:07:19Z",
                "published_parsed": [
                    2016,
                    4,
                    6,
                    18,
                    7,
                    19,
                    2,
                    97,
                    0
                ],
                "title": "A block Recycled GMRES method with investigations into aspects of solver\n  performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A block Recycled GMRES method with investigations into aspects of solver\n  performance"
                },
                "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
                },
                "authors": [
                    {
                        "name": "Michael L. Parks"
                    },
                    {
                        "name": "Kirk M. Soodhalter"
                    },
                    {
                        "name": "Daniel B. Szyld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Szyld"
                },
                "author": "Daniel B. Szyld",
                "arxiv_comment": "35 pages, 26 pages of manuscript text, 13 figures, 1 table, Temple\n  University Research Report 16-04-04",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1604.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1604.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thvenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thvenet"
                },
                "author": "M. Thvenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21593v1",
                "updated": "2025-06-18T07:54:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T07:54:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications"
                },
                "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems."
                },
                "authors": [
                    {
                        "name": "Abu Hanif Muhammad Syarubany"
                    },
                    {
                        "name": "Chang Dong Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang Dong Yoo"
                },
                "author": "Chang Dong Yoo",
                "arxiv_comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v1",
                "updated": "2025-06-18T05:07:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22463v1",
                "updated": "2025-06-18T03:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T03:31:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization"
                },
                "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff."
                },
                "authors": [
                    {
                        "name": "Weizhi Gao"
                    },
                    {
                        "name": "Zhichao Hou"
                    },
                    {
                        "name": "Junqi Yin"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Linyu Peng"
                    },
                    {
                        "name": "Xiaorui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorui Liu"
                },
                "author": "Xiaorui Liu",
                "arxiv_comment": "26 pages, accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00033v1",
                "updated": "2025-06-18T03:23:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    23,
                    56,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T03:23:56Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    23,
                    56,
                    2,
                    169,
                    0
                ],
                "title": "Moment Sampling in Video LLMs for Long-Form Video QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moment Sampling in Video LLMs for Long-Form Video QA"
                },
                "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Mustafa Chasmai"
                    },
                    {
                        "name": "Gauri Jagatap"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Grant Van Horn"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rben Ado"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "Joo Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02859v1",
                "updated": "2025-07-03T17:59:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    29,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:29Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    29,
                    3,
                    184,
                    0
                ],
                "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for\n  Data-Efficient Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for\n  Data-Efficient Model Adaptation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in interpreting images using natural language. However, without\nusing large-scale datasets for retraining, these models are difficult to adapt\nto specialized vision tasks, e.g., chart understanding. This problem is caused\nby a mismatch between pre-training and downstream datasets: pre-training\ndatasets primarily concentrate on scenes and objects but contain limited\ninformation about specialized, non-object images, such as charts and tables. In\nthis paper, we share an interesting finding that training an MLLM with\nChain-of-Thought (CoT) reasoning data can facilitate model adaptation in\nspecialized vision tasks, especially under data-limited regimes. However, we\nidentify a critical issue within CoT data distilled from pre-trained MLLMs,\ni.e., the data often contains multiple factual errors in the reasoning steps.\nTo address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple\nbootstrapping-based approach that aims to inject grounding information (i.e.,\nbounding boxes) into CoT data, essentially making the reasoning steps more\nfaithful to input images. We evaluate our approach on five specialized vision\ntasks, which cover a variety of visual formats including charts, tables,\nreceipts, and reports. The results demonstrate that under data-limited regimes\nour approach significantly improves upon fine-tuning and distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in interpreting images using natural language. However, without\nusing large-scale datasets for retraining, these models are difficult to adapt\nto specialized vision tasks, e.g., chart understanding. This problem is caused\nby a mismatch between pre-training and downstream datasets: pre-training\ndatasets primarily concentrate on scenes and objects but contain limited\ninformation about specialized, non-object images, such as charts and tables. In\nthis paper, we share an interesting finding that training an MLLM with\nChain-of-Thought (CoT) reasoning data can facilitate model adaptation in\nspecialized vision tasks, especially under data-limited regimes. However, we\nidentify a critical issue within CoT data distilled from pre-trained MLLMs,\ni.e., the data often contains multiple factual errors in the reasoning steps.\nTo address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple\nbootstrapping-based approach that aims to inject grounding information (i.e.,\nbounding boxes) into CoT data, essentially making the reasoning steps more\nfaithful to input images. We evaluate our approach on five specialized vision\ntasks, which cover a variety of visual formats including charts, tables,\nreceipts, and reports. The results demonstrate that under data-limited regimes\nour approach significantly improves upon fine-tuning and distillation."
                },
                "authors": [
                    {
                        "name": "Jiaer Xia"
                    },
                    {
                        "name": "Bingkui Tong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02858v1",
                "updated": "2025-07-03T17:59:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    4,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:04Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    4,
                    3,
                    184,
                    0
                ],
                "title": "Requirements Elicitation Follow-Up Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements Elicitation Follow-Up Question Generation"
                },
                "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time."
                },
                "authors": [
                    {
                        "name": "Yuchen Shen"
                    },
                    {
                        "name": "Anmol Singhal"
                    },
                    {
                        "name": "Travis Breaux"
                    }
                ],
                "author_detail": {
                    "name": "Travis Breaux"
                },
                "author": "Travis Breaux",
                "arxiv_comment": "13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02856v1",
                "updated": "2025-07-03T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    2,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    2,
                    3,
                    184,
                    0
                ],
                "title": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation"
                },
                "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching."
                },
                "authors": [
                    {
                        "name": "Nikhil Chandak"
                    },
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Moritz Hardt"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "arxiv_comment": "34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02851v1",
                "updated": "2025-07-03T17:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    43,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:55:43Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    43,
                    3,
                    184,
                    0
                ],
                "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs"
                },
                "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively."
                },
                "authors": [
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02850v1",
                "updated": "2025-07-03T17:55:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    40,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:55:40Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    40,
                    3,
                    184,
                    0
                ],
                "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users"
                },
                "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection)."
                },
                "authors": [
                    {
                        "name": "Almog Hilel"
                    },
                    {
                        "name": "Idan Shenfeld"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Jacob Andreas"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Andreas"
                },
                "author": "Jacob Andreas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02843v1",
                "updated": "2025-07-03T17:52:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    52,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:52:27Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    52,
                    27,
                    3,
                    184,
                    0
                ],
                "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding"
                },
                "summary": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Yuchen Ma"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02841v1",
                "updated": "2025-07-03T17:51:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    51,
                    6,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:51:06Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    51,
                    6,
                    3,
                    184,
                    0
                ],
                "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to\n  Reason",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to\n  Reason"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks."
                },
                "authors": [
                    {
                        "name": "Kaiyi Zhang"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Yongbo Wang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Haoyuan Hu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03129v2",
                "updated": "2025-07-03T17:50:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    50,
                    42,
                    3,
                    184,
                    0
                ],
                "published": "2025-01-06T16:47:24Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    47,
                    24,
                    0,
                    6,
                    0
                ],
                "title": "Generalized coarsened confounding for causal effects: a large-sample\n  framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized coarsened confounding for causal effects: a large-sample\n  framework"
                },
                "summary": "There has been widespread use of causal inference methods for the rigorous\nanalysis of observational studies and to identify policy evaluations. In this\narticle, we consider a class of generalized coarsened procedures for\nconfounding. At a high level, these procedures can be viewed as performing a\nclustering of confounding variables, followed by treatment effect and attendant\nvariance estimation using the confounder strata. In addition, we propose two\nnew algorithms for generalized coarsened confounding. While Iacus et al. (2011)\ndeveloped some statistical properties for one special case in our class of\nprocedures, we instead develop a general asymptotic framework. We provide\nasymptotic results for the average causal effect estimator as well as providing\nconditions for consistency. In addition, we provide an asymptotic justification\nfor the variance formulae in Iacus et al. (2011). A bias correction technique\nis proposed, and we apply the proposed methodology to data from two well-known\nobservational studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been widespread use of causal inference methods for the rigorous\nanalysis of observational studies and to identify policy evaluations. In this\narticle, we consider a class of generalized coarsened procedures for\nconfounding. At a high level, these procedures can be viewed as performing a\nclustering of confounding variables, followed by treatment effect and attendant\nvariance estimation using the confounder strata. In addition, we propose two\nnew algorithms for generalized coarsened confounding. While Iacus et al. (2011)\ndeveloped some statistical properties for one special case in our class of\nprocedures, we instead develop a general asymptotic framework. We provide\nasymptotic results for the average causal effect estimator as well as providing\nconditions for consistency. In addition, we provide an asymptotic justification\nfor the variance formulae in Iacus et al. (2011). A bias correction technique\nis proposed, and we apply the proposed methodology to data from two well-known\nobservational studies."
                },
                "authors": [
                    {
                        "name": "Debashis Ghosh"
                    },
                    {
                        "name": "Lei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wang"
                },
                "author": "Lei Wang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2301.00889",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P10 (Primary) 62P25, 62P15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18959v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18959v3",
                "updated": "2025-07-03T17:48:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    48,
                    36,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-23T17:27:19Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    27,
                    19,
                    0,
                    174,
                    0
                ],
                "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents"
                },
                "summary": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research."
                },
                "authors": [
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Chenxuan Xie"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Xinyang Zhang"
                    },
                    {
                        "name": "Chenwei Zhang"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18959v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18959v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13972v2",
                "updated": "2025-07-03T17:45:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    45,
                    38,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-16T20:22:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    22,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity\n  and Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity\n  and Ensemble"
                },
                "summary": "Membership inference attacks (MIAs) pose a significant threat to the privacy\nof machine learning models and are widely used as tools for privacy assessment,\nauditing, and machine unlearning. While prior MIA research has primarily\nfocused on performance metrics such as AUC, accuracy, and TPR@low FPR - either\nby developing new methods to enhance these metrics or using them to evaluate\nprivacy solutions - we found that it overlooks the disparities among different\nattacks. These disparities, both between distinct attack methods and between\nmultiple instantiations of the same method, have crucial implications for the\nreliability and completeness of MIAs as privacy evaluation tools. In this\npaper, we systematically investigate these disparities through a novel\nframework based on coverage and stability analysis. Extensive experiments\nreveal significant disparities in MIAs, their potential causes, and their\nbroader implications for privacy evaluation. To address these challenges, we\npropose an ensemble framework with three distinct strategies to harness the\nstrengths of state-of-the-art MIAs while accounting for their disparities. This\nframework not only enables the construction of more powerful attacks but also\nprovides a more robust and comprehensive methodology for privacy evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) pose a significant threat to the privacy\nof machine learning models and are widely used as tools for privacy assessment,\nauditing, and machine unlearning. While prior MIA research has primarily\nfocused on performance metrics such as AUC, accuracy, and TPR@low FPR - either\nby developing new methods to enhance these metrics or using them to evaluate\nprivacy solutions - we found that it overlooks the disparities among different\nattacks. These disparities, both between distinct attack methods and between\nmultiple instantiations of the same method, have crucial implications for the\nreliability and completeness of MIAs as privacy evaluation tools. In this\npaper, we systematically investigate these disparities through a novel\nframework based on coverage and stability analysis. Extensive experiments\nreveal significant disparities in MIAs, their potential causes, and their\nbroader implications for privacy evaluation. To address these challenges, we\npropose an ensemble framework with three distinct strategies to harness the\nstrengths of state-of-the-art MIAs while accounting for their disparities. This\nframework not only enables the construction of more powerful attacks but also\nprovides a more robust and comprehensive methodology for privacy evaluation."
                },
                "authors": [
                    {
                        "name": "Zhiqi Wang"
                    },
                    {
                        "name": "Chengyu Zhang"
                    },
                    {
                        "name": "Yuetian Chen"
                    },
                    {
                        "name": "Nathalie Baracaldo"
                    },
                    {
                        "name": "Swanand Kadhe"
                    },
                    {
                        "name": "Lei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Yu"
                },
                "author": "Lei Yu",
                "arxiv_doi": "10.1145/3719027.3744818",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719027.3744818",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.13972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, To appear at ACM CCS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02827v1",
                "updated": "2025-07-03T17:38:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    38,
                    44,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:38:44Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    38,
                    44,
                    3,
                    184,
                    0
                ],
                "title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention\n  Diffusion Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention\n  Diffusion Network"
                },
                "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Ying Yu"
                    },
                    {
                        "name": "Hang Xiao"
                    },
                    {
                        "name": "Siyao Li"
                    },
                    {
                        "name": "Jiarui Li"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Hanyu Liu"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01930v2",
                "updated": "2025-07-03T17:36:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    36,
                    59,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T17:44:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations"
                },
                "summary": "Recent advances in large Language Models (LLMs) have revolutionized mobile\nrobots, including unmanned aerial vehicles (UAVs), enabling their intelligent\noperation within Internet of Things (IoT) ecosystems. However, LLMs still face\nchallenges from logical reasoning and complex decision-making, leading to\nconcerns about the reliability of LLM-driven UAV operations in IoT\napplications. In this paper, we propose a LLM-driven closed-loop control\nframework that enables reliable UAV operations powered by effective feedback\nand refinement using two LLM modules, i.e., a Code Generator and an Evaluator.\nOur framework transforms numerical state observations from UAV operations into\nnatural language trajectory descriptions to enhance the evaluator LLM's\nunderstanding of UAV dynamics for precise feedback generation. Our framework\nalso enables a simulation-based refinement process, and hence eliminates the\nrisks to physical UAVs caused by incorrect code execution during the\nrefinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large Language Models (LLMs) have revolutionized mobile\nrobots, including unmanned aerial vehicles (UAVs), enabling their intelligent\noperation within Internet of Things (IoT) ecosystems. However, LLMs still face\nchallenges from logical reasoning and complex decision-making, leading to\nconcerns about the reliability of LLM-driven UAV operations in IoT\napplications. In this paper, we propose a LLM-driven closed-loop control\nframework that enables reliable UAV operations powered by effective feedback\nand refinement using two LLM modules, i.e., a Code Generator and an Evaluator.\nOur framework transforms numerical state observations from UAV operations into\nnatural language trajectory descriptions to enhance the evaluator LLM's\nunderstanding of UAV dynamics for precise feedback generation. Our framework\nalso enables a simulation-based refinement process, and hence eliminates the\nrisks to physical UAVs caused by incorrect code execution during the\nrefinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity."
                },
                "authors": [
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Yanyan Li"
                    },
                    {
                        "name": "Long Jiao"
                    },
                    {
                        "name": "Jiawei Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Yuan"
                },
                "author": "Jiawei Yuan",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02822v1",
                "updated": "2025-07-03T17:33:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    33,
                    58,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:33:58Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    33,
                    58,
                    3,
                    184,
                    0
                ],
                "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model"
                },
                "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost."
                },
                "authors": [
                    {
                        "name": "Wencheng Zhang"
                    },
                    {
                        "name": "Shiqin Qiao"
                    },
                    {
                        "name": "Lingjie Luo"
                    },
                    {
                        "name": "Yinfeng Li"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Yong Gui"
                    },
                    {
                        "name": "Yijun He"
                    },
                    {
                        "name": "Jianing Qiu"
                    },
                    {
                        "name": "Jindong Hong"
                    },
                    {
                        "name": "Jiankai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiankai Sun"
                },
                "author": "Jiankai Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02804v1",
                "updated": "2025-07-03T17:07:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    7,
                    20,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:07:20Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    7,
                    20,
                    3,
                    184,
                    0
                ],
                "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mathematical Reasoning with Diverse Solving Perspective"
                },
                "summary": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Wenhao Shi"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02799v1",
                "updated": "2025-07-03T17:01:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    1,
                    53,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:01:53Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    1,
                    53,
                    3,
                    184,
                    0
                ],
                "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language\n  Models"
                },
                "summary": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Nicola Gabriele"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22049v2",
                "updated": "2025-07-03T16:54:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    54,
                    9,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-27T09:45:15Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    15,
                    4,
                    178,
                    0
                ],
                "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling"
                },
                "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS."
                },
                "authors": [
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Zijing Liu"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Ajay Kumar Jaiswal"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Can Yang"
                    }
                ],
                "author_detail": {
                    "name": "Can Yang"
                },
                "author": "Can Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02788v1",
                "updated": "2025-07-03T16:53:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    53,
                    1,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:53:01Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    53,
                    1,
                    3,
                    184,
                    0
                ],
                "title": "Moral Responsibility or Obedience: What Do We Want from AI?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Responsibility or Obedience: What Do We Want from AI?"
                },
                "summary": "As artificial intelligence systems become increasingly agentic, capable of\ngeneral reasoning, planning, and value prioritization, current safety practices\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\nThis paper examines recent safety testing incidents involving large language\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\nambiguous or illicit behavior. I argue that such behavior should not be\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\nrationality, moral responsibility, and goal revision, I contrast dominant risk\nparadigms with more recent frameworks that acknowledge the possibility of\nartificial moral agency. I call for a shift in AI safety evaluation: away from\nrigid obedience and toward frameworks that can assess ethical judgment in\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\nmischaracterizing AI behavior and undermining both public trust and effective\ngovernance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence systems become increasingly agentic, capable of\ngeneral reasoning, planning, and value prioritization, current safety practices\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\nThis paper examines recent safety testing incidents involving large language\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\nambiguous or illicit behavior. I argue that such behavior should not be\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\nrationality, moral responsibility, and goal revision, I contrast dominant risk\nparadigms with more recent frameworks that acknowledge the possibility of\nartificial moral agency. I call for a shift in AI safety evaluation: away from\nrigid obedience and toward frameworks that can assess ethical judgment in\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\nmischaracterizing AI behavior and undermining both public trust and effective\ngovernance."
                },
                "authors": [
                    {
                        "name": "Joseph Boland"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Boland"
                },
                "author": "Joseph Boland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00612v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00612v3",
                "updated": "2025-07-03T16:50:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    50,
                    12,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-31T15:51:09Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    15,
                    51,
                    9,
                    5,
                    151,
                    0
                ],
                "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge\n  Graph Guided Distractor Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge\n  Graph Guided Distractor Generation"
                },
                "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs."
                },
                "authors": [
                    {
                        "name": "Running Yang"
                    },
                    {
                        "name": "Wenlong Deng"
                    },
                    {
                        "name": "Minghui Chen"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Xiaoxiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxiao Li"
                },
                "author": "Xiaoxiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00612v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00612v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02778v1",
                "updated": "2025-07-03T16:41:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    41,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:41:30Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    41,
                    30,
                    3,
                    184,
                    0
                ],
                "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs"
                },
                "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness."
                },
                "authors": [
                    {
                        "name": "Ken Tsui"
                    }
                ],
                "author_detail": {
                    "name": "Ken Tsui"
                },
                "author": "Ken Tsui",
                "arxiv_comment": "31 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03935v2",
                "updated": "2025-07-03T16:39:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    39,
                    21,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-05T22:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    22,
                    10,
                    14,
                    2,
                    64,
                    0
                ],
                "title": "LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral\n  Treatment Pathways from Wearables and Diet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral\n  Treatment Pathways from Wearables and Diet"
                },
                "summary": "Postprandial hyperglycemia, marked by the blood glucose level exceeding the\nnormal range after consuming a meal, is a critical indicator of progression\ntoward type 2 diabetes in people with prediabetes and in healthy individuals. A\nkey metric for understanding blood glucose dynamics after eating is the\npostprandial area under the curve (AUC). Predicting postprandial AUC in advance\nbased on a person's lifestyle factors, such as diet and physical activity\nlevel, and explaining the factors that affect postprandial blood glucose could\nallow an individual to adjust their lifestyle accordingly to maintain normal\nglucose levels. In this study, we developed an explainable machine learning\nsolution, GlucoLens, that takes sensor-driven inputs and uses advanced data\nprocessing, large language models, and trainable machine learning models to\npredict postprandial AUC and hyperglycemia from diet, physical activity, and\nrecent glucose patterns. We used data obtained from wearables in a five-week\nclinical trial of 10 adults who worked full-time to develop and evaluate the\nproposed computational model that integrates wearable sensing, multimodal data,\nand machine learning. Our machine learning model takes multimodal data from\nwearable activity and glucose monitoring sensors, along with food and work\nlogs, and provides an interpretable prediction of the postprandial glucose\npattern. Our GlucoLens system achieves a normalized root mean squared error\n(NRMSE) of 0.123 in its best configuration. On average, the proposed technology\nprovides a 16% better performance level compared to the comparison models.\nAdditionally, our technique predicts hyperglycemia with an accuracy of 73.3%\nand an F1 score of 0.716 and recommends different treatment options to help\navoid hyperglycemia through diverse counterfactual explanations. Code\navailable: https://github.com/ab9mamun/GlucoLens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Postprandial hyperglycemia, marked by the blood glucose level exceeding the\nnormal range after consuming a meal, is a critical indicator of progression\ntoward type 2 diabetes in people with prediabetes and in healthy individuals. A\nkey metric for understanding blood glucose dynamics after eating is the\npostprandial area under the curve (AUC). Predicting postprandial AUC in advance\nbased on a person's lifestyle factors, such as diet and physical activity\nlevel, and explaining the factors that affect postprandial blood glucose could\nallow an individual to adjust their lifestyle accordingly to maintain normal\nglucose levels. In this study, we developed an explainable machine learning\nsolution, GlucoLens, that takes sensor-driven inputs and uses advanced data\nprocessing, large language models, and trainable machine learning models to\npredict postprandial AUC and hyperglycemia from diet, physical activity, and\nrecent glucose patterns. We used data obtained from wearables in a five-week\nclinical trial of 10 adults who worked full-time to develop and evaluate the\nproposed computational model that integrates wearable sensing, multimodal data,\nand machine learning. Our machine learning model takes multimodal data from\nwearable activity and glucose monitoring sensors, along with food and work\nlogs, and provides an interpretable prediction of the postprandial glucose\npattern. Our GlucoLens system achieves a normalized root mean squared error\n(NRMSE) of 0.123 in its best configuration. On average, the proposed technology\nprovides a 16% better performance level compared to the comparison models.\nAdditionally, our technique predicts hyperglycemia with an accuracy of 73.3%\nand an F1 score of 0.716 and recommends different treatment options to help\navoid hyperglycemia through diverse counterfactual explanations. Code\navailable: https://github.com/ab9mamun/GlucoLens."
                },
                "authors": [
                    {
                        "name": "Abdullah Mamun"
                    },
                    {
                        "name": "Asiful Arefeen"
                    },
                    {
                        "name": "Susan B. Racette"
                    },
                    {
                        "name": "Dorothy D. Sears"
                    },
                    {
                        "name": "Corrie M. Whisner"
                    },
                    {
                        "name": "Matthew P. Buman"
                    },
                    {
                        "name": "Hassan Ghasemzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Ghasemzadeh"
                },
                "author": "Hassan Ghasemzadeh",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02773v1",
                "updated": "2025-07-03T16:35:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    35,
                    11,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:35:11Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    35,
                    11,
                    3,
                    184,
                    0
                ],
                "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot\n  Diagnosis Prediction Using Multi-agent LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot\n  Diagnosis Prediction Using Multi-agent LLMs"
                },
                "summary": "Medical diagnosis prediction plays a critical role in disease detection and\npersonalized healthcare. While machine learning (ML) models have been widely\nadopted for this task, their reliance on supervised training limits their\nability to generalize to unseen cases, particularly given the high cost of\nacquiring large, labeled datasets. Large language models (LLMs) have shown\npromise in leveraging language abilities and biomedical knowledge for diagnosis\nprediction. However, they often suffer from hallucinations, lack structured\nmedical reasoning, and produce useless outputs. To address these challenges, we\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\nLLM-based diagnosis prediction through a multi-agent architecture. Our\nframework consists of a linkage agent for attribute mapping, a retrieval agent\nfor structured knowledge extraction, and a prediction agent that iteratively\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\nenhances diagnostic reliability efficiently, offering a scalable and\ninterpretable solution for zero-shot medical diagnosis prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical diagnosis prediction plays a critical role in disease detection and\npersonalized healthcare. While machine learning (ML) models have been widely\nadopted for this task, their reliance on supervised training limits their\nability to generalize to unseen cases, particularly given the high cost of\nacquiring large, labeled datasets. Large language models (LLMs) have shown\npromise in leveraging language abilities and biomedical knowledge for diagnosis\nprediction. However, they often suffer from hallucinations, lack structured\nmedical reasoning, and produce useless outputs. To address these challenges, we\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\nLLM-based diagnosis prediction through a multi-agent architecture. Our\nframework consists of a linkage agent for attribute mapping, a retrieval agent\nfor structured knowledge extraction, and a prediction agent that iteratively\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\nenhances diagnostic reliability efficiently, offering a scalable and\ninterpretable solution for zero-shot medical diagnosis prediction."
                },
                "authors": [
                    {
                        "name": "Yuzhang Xie"
                    },
                    {
                        "name": "Hejie Cui"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Jiaying Lu"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Fadi Nahab"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Carl Yang"
                    }
                ],
                "author_detail": {
                    "name": "Carl Yang"
                },
                "author": "Carl Yang",
                "arxiv_journal_ref": "American Medical Informatics Association (AMIA) 2025 Annual\n  Symposium, Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02768v1",
                "updated": "2025-07-03T16:28:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    28,
                    25,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:28:25Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    28,
                    25,
                    3,
                    184,
                    0
                ],
                "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with\n  Self-Generated Cross-Modal Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with\n  Self-Generated Cross-Modal Alignment"
                },
                "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs."
                },
                "authors": [
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Sung-Feng Huang"
                    },
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Chee-En Yu"
                    },
                    {
                        "name": "Chun-Wei Chen"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Chien-yu Huang"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Yu-Xiang Lin"
                    },
                    {
                        "name": "Chi-An Fu"
                    },
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Wenze Ren"
                    },
                    {
                        "name": "Xuanjun Chen"
                    },
                    {
                        "name": "Wei-Ping Huang"
                    },
                    {
                        "name": "En-Pei Hu"
                    },
                    {
                        "name": "Tzu-Quan Lin"
                    },
                    {
                        "name": "Yuan-Kuei Wu"
                    },
                    {
                        "name": "Kuan-Po Huang"
                    },
                    {
                        "name": "Hsiao-Ying Huang"
                    },
                    {
                        "name": "Huang-Cheng Chou"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Cheng-Han Chiang"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Model and code available at:\n  https://github.com/kehanlu/DeSTA2.5-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02760v1",
                "updated": "2025-07-03T16:21:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    21,
                    14,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:21:14Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    21,
                    14,
                    3,
                    184,
                    0
                ],
                "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific\n  Knowledge Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific\n  Knowledge Work"
                },
                "summary": "The capabilities of Large Language Models (LLMs) have opened new frontiers\nfor interacting with complex, domain-specific knowledge. However, prevailing\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\nmethodological reasoning inherent to expert domains. RAG provides factual\ncontext but fails to convey logical frameworks; autonomous agents can be\ninefficient and unpredictable without domain-specific heuristics. To bridge\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\nfocused on systematically translating human expert knowledge, often expressed\nin natural language documents, into a machine-executable Knowledge Protocol\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\ninformation to endowing them with a domain's intrinsic logic, operational\nstrategies, and methodological principles. We argue that a well-engineered\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\nof decomposing abstract queries and executing complex, multi-step tasks. This\nposition paper defines the core principles of KPE, differentiates it from\nrelated concepts, and illustrates its potential applicability across diverse\nfields such as law and bioinformatics, positing it as a foundational\nmethodology for the future of human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) have opened new frontiers\nfor interacting with complex, domain-specific knowledge. However, prevailing\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\nmethodological reasoning inherent to expert domains. RAG provides factual\ncontext but fails to convey logical frameworks; autonomous agents can be\ninefficient and unpredictable without domain-specific heuristics. To bridge\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\nfocused on systematically translating human expert knowledge, often expressed\nin natural language documents, into a machine-executable Knowledge Protocol\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\ninformation to endowing them with a domain's intrinsic logic, operational\nstrategies, and methodological principles. We argue that a well-engineered\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\nof decomposing abstract queries and executing complex, multi-step tasks. This\nposition paper defines the core principles of KPE, differentiates it from\nrelated concepts, and illustrates its potential applicability across diverse\nfields such as law and bioinformatics, positing it as a foundational\nmethodology for the future of human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Guangwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwei Zhang"
                },
                "author": "Guangwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09699v2",
                "updated": "2025-07-03T16:06:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    45,
                    3,
                    184,
                    0
                ],
                "published": "2023-08-18T17:46:21Z",
                "published_parsed": [
                    2023,
                    8,
                    18,
                    17,
                    46,
                    21,
                    4,
                    230,
                    0
                ],
                "title": "Emergent interaction-driven elliptic flow of few fermionic atoms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent interaction-driven elliptic flow of few fermionic atoms"
                },
                "summary": "Hydrodynamics provides a successful framework to effectively describe the\ndynamics of complex many-body systems ranging from subnuclear to cosmological\nscales by introducing macroscopic quantities such as particle densities and\nfluid velocities. According to textbook knowledge, it requires coarse graining\nover microscopic constituents to define a macroscopic fluid cell, which is\nlarge compared to the interparticle spacing and the mean free path. In\naddition, the entire system must consist of many such fluid cells. In high\nenergy heavy ion collisions, hydrodynamic behaviour is inferred from the\nobservation of elliptic flow. Here, we demonstrate the emergence of elliptic\nflow in a system of few strongly interacting atoms. In our system a\nhydrodynamic description is a priori not applicable, as all relevant length\nscales, i.e. the system size, the inter-particle spacing, and the mean free\npath are comparable. The single particle resolution, deterministic control over\nparticle number and interaction strength in our experiment allow us to explore\nthe boundaries between a microscopic description and a hydrodynamic framework\nin unprecedented detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydrodynamics provides a successful framework to effectively describe the\ndynamics of complex many-body systems ranging from subnuclear to cosmological\nscales by introducing macroscopic quantities such as particle densities and\nfluid velocities. According to textbook knowledge, it requires coarse graining\nover microscopic constituents to define a macroscopic fluid cell, which is\nlarge compared to the interparticle spacing and the mean free path. In\naddition, the entire system must consist of many such fluid cells. In high\nenergy heavy ion collisions, hydrodynamic behaviour is inferred from the\nobservation of elliptic flow. Here, we demonstrate the emergence of elliptic\nflow in a system of few strongly interacting atoms. In our system a\nhydrodynamic description is a priori not applicable, as all relevant length\nscales, i.e. the system size, the inter-particle spacing, and the mean free\npath are comparable. The single particle resolution, deterministic control over\nparticle number and interaction strength in our experiment allow us to explore\nthe boundaries between a microscopic description and a hydrodynamic framework\nin unprecedented detail."
                },
                "authors": [
                    {
                        "name": "Sandra Brandstetter"
                    },
                    {
                        "name": "Philipp Lunt"
                    },
                    {
                        "name": "Carl Heintze"
                    },
                    {
                        "name": "Giuliano Giacalone"
                    },
                    {
                        "name": "Lars H. Heyen"
                    },
                    {
                        "name": "Maciej Gaka"
                    },
                    {
                        "name": "Keerthan Subramanian"
                    },
                    {
                        "name": "Marvin Holten"
                    },
                    {
                        "name": "Philipp M. Preiss"
                    },
                    {
                        "name": "Stefan Floerchinger"
                    },
                    {
                        "name": "Selim Jochim"
                    }
                ],
                "author_detail": {
                    "name": "Selim Jochim"
                },
                "author": "Selim Jochim",
                "arxiv_doi": "10.1038/s41567-024-02705-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41567-024-02705-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.09699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.quant-gas",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02745v1",
                "updated": "2025-07-03T16:05:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    5,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:05:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    5,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory\n  Apologies from LLM Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory\n  Apologies from LLM Chatbots"
                },
                "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Alessandra Buccella"
                    },
                    {
                        "name": "Jason D'Cruz"
                    },
                    {
                        "name": "Zoe Fowler"
                    },
                    {
                        "name": "Andrew Gill"
                    },
                    {
                        "name": "Kei Yan Leung"
                    },
                    {
                        "name": "P. D. Magnus"
                    },
                    {
                        "name": "John Richards"
                    },
                    {
                        "name": "Kush R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Kush R. Varshney"
                },
                "author": "Kush R. Varshney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10768v2",
                "updated": "2025-07-03T15:57:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    57,
                    20,
                    3,
                    184,
                    0
                ],
                "published": "2025-01-18T13:54:00Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    54,
                    0,
                    5,
                    18,
                    0
                ],
                "title": "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science"
                },
                "summary": "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper."
                },
                "authors": [
                    {
                        "name": "Erle Zhu"
                    },
                    {
                        "name": "Yadi Liu"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Xujun Li"
                    },
                    {
                        "name": "Jin Zhou"
                    },
                    {
                        "name": "Xinjie Yu"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Hongning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongning Wang"
                },
                "author": "Hongning Wang",
                "arxiv_journal_ref": "Proceedings of the 13th International Conference on Learning\n  Representations (ICLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02737v1",
                "updated": "2025-07-03T15:54:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    54,
                    55,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:54:55Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    54,
                    55,
                    3,
                    184,
                    0
                ],
                "title": "Early Signs of Steganographic Capabilities in Frontier LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Signs of Steganographic Capabilities in Frontier LLMs"
                },
                "summary": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future."
                },
                "authors": [
                    {
                        "name": "Artur Zolkowski"
                    },
                    {
                        "name": "Kei Nishimura-Gasparian"
                    },
                    {
                        "name": "Robert McCarthy"
                    },
                    {
                        "name": "Roland S. Zimmermann"
                    },
                    {
                        "name": "David Lindner"
                    }
                ],
                "author_detail": {
                    "name": "David Lindner"
                },
                "author": "David Lindner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03997v2",
                "updated": "2025-07-03T15:54:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    54,
                    50,
                    3,
                    184,
                    0
                ],
                "published": "2025-02-06T11:57:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    57,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing"
                },
                "summary": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively. The code is\navailable at \\url {https://github.com/microsoft/CAD-Editor}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively. The code is\navailable at \\url {https://github.com/microsoft/CAD-Editor}."
                },
                "authors": [
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02735v1",
                "updated": "2025-07-03T15:47:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    47,
                    13,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:47:13Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    47,
                    13,
                    3,
                    184,
                    0
                ],
                "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks"
                },
                "summary": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense."
                },
                "authors": [
                    {
                        "name": "Sizhe Chen"
                    },
                    {
                        "name": "Arman Zharmagambetov"
                    },
                    {
                        "name": "David Wagner"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02734v1",
                "updated": "2025-07-03T15:45:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    45,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:45:35Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    45,
                    35,
                    3,
                    184,
                    0
                ],
                "title": "Leveraging Transformer Models to Capture Multi-Scale Dynamics in\n  Biomolecules by nano-GPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Transformer Models to Capture Multi-Scale Dynamics in\n  Biomolecules by nano-GPT"
                },
                "summary": "Long-term biomolecular dynamics are critical for understanding key\nevolutionary transformations in molecular systems. However, capturing these\nprocesses requires extended simulation timescales that often exceed the\npractical limits of conventional models. To address this, shorter simulations,\ninitialized with diverse perturbations, are commonly used to sample phase space\nand explore a wide range of behaviors. Recent advances have leveraged language\nmodels to infer long-term behavior from short trajectories, but methods such as\nlong short-term memory (LSTM) networks are constrained to low-dimensional\nreaction coordinates, limiting their applicability to complex systems. In this\nwork, we present nano-GPT, a novel deep learning model inspired by the GPT\narchitecture, specifically designed to capture long-term dynamics in molecular\nsystems with fine-grained conformational states and complex transitions. The\nmodel employs a two-pass training mechanism that incrementally replaces\nmolecular dynamics (MD) tokens with model-generated predictions, effectively\nmitigating accumulation errors inherent in the training window. We validate\nnano-GPT on three distinct systems: a four-state model potential, the alanine\ndipeptide, a well-studied simple molecule, and the Fip35 WW domain, a complex\nbiomolecular system. Our results show that nano-GPT effectively captures\nlong-timescale dynamics by learning high-order dependencies through attention\nmechanism, offering a novel perspective for interpreting biomolecular\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term biomolecular dynamics are critical for understanding key\nevolutionary transformations in molecular systems. However, capturing these\nprocesses requires extended simulation timescales that often exceed the\npractical limits of conventional models. To address this, shorter simulations,\ninitialized with diverse perturbations, are commonly used to sample phase space\nand explore a wide range of behaviors. Recent advances have leveraged language\nmodels to infer long-term behavior from short trajectories, but methods such as\nlong short-term memory (LSTM) networks are constrained to low-dimensional\nreaction coordinates, limiting their applicability to complex systems. In this\nwork, we present nano-GPT, a novel deep learning model inspired by the GPT\narchitecture, specifically designed to capture long-term dynamics in molecular\nsystems with fine-grained conformational states and complex transitions. The\nmodel employs a two-pass training mechanism that incrementally replaces\nmolecular dynamics (MD) tokens with model-generated predictions, effectively\nmitigating accumulation errors inherent in the training window. We validate\nnano-GPT on three distinct systems: a four-state model potential, the alanine\ndipeptide, a well-studied simple molecule, and the Fip35 WW domain, a complex\nbiomolecular system. Our results show that nano-GPT effectively captures\nlong-timescale dynamics by learning high-order dependencies through attention\nmechanism, offering a novel perspective for interpreting biomolecular\nprocesses."
                },
                "authors": [
                    {
                        "name": "Wenqi Zeng"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Yuan Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Yao"
                },
                "author": "Yuan Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02731v1",
                "updated": "2025-07-03T15:43:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    43,
                    6,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:43:06Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    43,
                    6,
                    3,
                    184,
                    0
                ],
                "title": "RIS-Aided Cooperative ISAC Networks for Structural Health Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Aided Cooperative ISAC Networks for Structural Health Monitoring"
                },
                "summary": "Integrated sensing and communication (ISAC) is a key feature of future\ncellular systems, enabling applications such as intruder detection, monitoring,\nand tracking using the same infrastructure. However, its potential for\nstructural health monitoring (SHM), which requires the detection of slow and\nsubtle structural changes, remains largely unexplored due to challenges such as\nmultipath interference and the need for ultra-high sensing precision. This\nstudy introduces a novel theoretical framework for SHM via ISAC by leveraging\nreconfigurable intelligent surfaces (RIS) as reference points in collaboration\nwith base stations and users. By dynamically adjusting RIS phases to generate\ndistinct radio signals that suppress background multipath interference,\nmeasurement accuracy at these reference points is enhanced. We theoretically\nanalyze RIS-aided collaborative sensing in three-dimensional cellular networks\nusing Fisher information theory, demonstrating how increasing observation time,\nincorporating additional receivers (even with self-positioning errors),\noptimizing RIS phases, and refining collaborative node selection can reduce the\nposition error bound to meet SHM's stringent accuracy requirements.\nFurthermore, we develop a Bayesian inference model to identify structural\nstates and validate damage detection probabilities. Both theoretical and\nnumerical analyses confirm ISAC's capability for millimeter-level deformation\ndetection, highlighting its potential for high-precision SHM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated sensing and communication (ISAC) is a key feature of future\ncellular systems, enabling applications such as intruder detection, monitoring,\nand tracking using the same infrastructure. However, its potential for\nstructural health monitoring (SHM), which requires the detection of slow and\nsubtle structural changes, remains largely unexplored due to challenges such as\nmultipath interference and the need for ultra-high sensing precision. This\nstudy introduces a novel theoretical framework for SHM via ISAC by leveraging\nreconfigurable intelligent surfaces (RIS) as reference points in collaboration\nwith base stations and users. By dynamically adjusting RIS phases to generate\ndistinct radio signals that suppress background multipath interference,\nmeasurement accuracy at these reference points is enhanced. We theoretically\nanalyze RIS-aided collaborative sensing in three-dimensional cellular networks\nusing Fisher information theory, demonstrating how increasing observation time,\nincorporating additional receivers (even with self-positioning errors),\noptimizing RIS phases, and refining collaborative node selection can reduce the\nposition error bound to meet SHM's stringent accuracy requirements.\nFurthermore, we develop a Bayesian inference model to identify structural\nstates and validate damage detection probabilities. Both theoretical and\nnumerical analyses confirm ISAC's capability for millimeter-level deformation\ndetection, highlighting its potential for high-precision SHM applications."
                },
                "authors": [
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Chao-Kai Wen"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02726v1",
                "updated": "2025-07-03T15:41:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    38,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:41:38Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    38,
                    3,
                    184,
                    0
                ],
                "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving"
                },
                "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale."
                },
                "authors": [
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Xiaotong Ji"
                    },
                    {
                        "name": "Rasul Tutunov"
                    },
                    {
                        "name": "Anthony Bordg"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou Ammar"
                },
                "author": "Haitham Bou Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02724v1",
                "updated": "2025-07-03T15:41:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    4,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:41:04Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    4,
                    3,
                    184,
                    0
                ],
                "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms"
                },
                "summary": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data."
                },
                "authors": [
                    {
                        "name": "Shiyi Liu"
                    },
                    {
                        "name": "Buwen Liang"
                    },
                    {
                        "name": "Yuetong Fang"
                    },
                    {
                        "name": "Zixuan Jiang"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01631v2",
                "updated": "2025-07-03T15:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    36,
                    23,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-02T13:08:01Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    8,
                    1,
                    0,
                    153,
                    0
                ],
                "title": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and\n  Family Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and\n  Family Classification"
                },
                "summary": "As Large Language Models (LLMs) become integral software components in modern\napplications, unauthorized model derivations through fine-tuning, merging, and\nredistribution have emerged as critical software engineering challenges. Unlike\ntraditional software where clone detection and license compliance are\nwell-established, the LLM ecosystem lacks effective mechanisms to detect model\nlineage and enforce licensing agreements. This gap is particularly problematic\nwhen open-source model creators, such as Meta's LLaMA, require derivative works\nto maintain naming conventions for attribution, yet no technical means exist to\nverify compliance.\n  To fill this gap, treating LLMs as software artifacts requiring provenance\ntracking, we present TensorGuard, a gradient-based fingerprinting framework for\nLLM similarity detection and family classification. Our approach extracts\nmodel-intrinsic behavioral signatures by analyzing gradient responses to random\ninput perturbations across tensor layers, operating independently of training\ndata, watermarks, or specific model formats. TensorGuard supports the\nwidely-adopted safetensors format and constructs high-dimensional fingerprints\nthrough statistical analysis of gradient features. These fingerprints enable\ntwo complementary capabilities: direct pairwise similarity assessment between\narbitrary models through distance computation, and systematic family\nclassification of unknown models via the K-Means clustering algorithm with\ndomain-informed centroid initialization using known base models. Experimental\nevaluation on 58 models comprising 8 base models and 50 derivatives across five\nmodel families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%\nclassification accuracy under our centroid-initialized K-Means clustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become integral software components in modern\napplications, unauthorized model derivations through fine-tuning, merging, and\nredistribution have emerged as critical software engineering challenges. Unlike\ntraditional software where clone detection and license compliance are\nwell-established, the LLM ecosystem lacks effective mechanisms to detect model\nlineage and enforce licensing agreements. This gap is particularly problematic\nwhen open-source model creators, such as Meta's LLaMA, require derivative works\nto maintain naming conventions for attribution, yet no technical means exist to\nverify compliance.\n  To fill this gap, treating LLMs as software artifacts requiring provenance\ntracking, we present TensorGuard, a gradient-based fingerprinting framework for\nLLM similarity detection and family classification. Our approach extracts\nmodel-intrinsic behavioral signatures by analyzing gradient responses to random\ninput perturbations across tensor layers, operating independently of training\ndata, watermarks, or specific model formats. TensorGuard supports the\nwidely-adopted safetensors format and constructs high-dimensional fingerprints\nthrough statistical analysis of gradient features. These fingerprints enable\ntwo complementary capabilities: direct pairwise similarity assessment between\narbitrary models through distance computation, and systematic family\nclassification of unknown models via the K-Means clustering algorithm with\ndomain-informed centroid initialization using known base models. Experimental\nevaluation on 58 models comprising 8 base models and 50 derivatives across five\nmodel families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%\nclassification accuracy under our centroid-initialized K-Means clustering."
                },
                "authors": [
                    {
                        "name": "Zehao Wu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00863v2",
                "updated": "2025-07-03T15:14:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    14,
                    51,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-30T18:00:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    18,
                    0,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation"
                },
                "summary": "In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix."
                },
                "authors": [
                    {
                        "name": "Chenyang An"
                    },
                    {
                        "name": "Shima Imani"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Chengyu Dong"
                    },
                    {
                        "name": "Ali Abbasi"
                    },
                    {
                        "name": "Harsh Shrivastava"
                    },
                    {
                        "name": "Samuel Buss"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Gayathri Mahalingam"
                    },
                    {
                        "name": "Pramod Sharma"
                    },
                    {
                        "name": "Maurice Diesendruck"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Diesendruck"
                },
                "author": "Maurice Diesendruck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02699v1",
                "updated": "2025-07-03T15:09:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    9,
                    40,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:09:40Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    9,
                    40,
                    3,
                    184,
                    0
                ],
                "title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email\n  Agents"
                },
                "summary": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23."
                },
                "authors": [
                    {
                        "name": "Jiangrong Wu"
                    },
                    {
                        "name": "Yuhong Nan"
                    },
                    {
                        "name": "Jianliang Wu"
                    },
                    {
                        "name": "Zitong Yao"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02694v1",
                "updated": "2025-07-03T15:04:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    4,
                    38,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:04:38Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    4,
                    38,
                    3,
                    184,
                    0
                ],
                "title": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers"
                },
                "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback."
                },
                "authors": [
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02690v1",
                "updated": "2025-07-03T15:01:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    1,
                    8,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:01:08Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    1,
                    8,
                    3,
                    184,
                    0
                ],
                "title": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network\n  for Next Activity Prediction in Business Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network\n  for Next Activity Prediction in Business Processes"
                },
                "summary": "Next activity prediction represents a fundamental challenge for optimizing\nbusiness processes in service-oriented architectures such as microservices\nenvironments, distributed enterprise systems, and cloud-native platforms, which\nenables proactive resource allocation and dynamic service composition. Despite\nthe prevalence of sequence-based methods, these approaches fail to capture\nnon-sequential relationships that arise from parallel executions and\nconditional dependencies. Even though graph-based approaches address structural\npreservation, they suffer from homogeneous representations and static\nstructures that apply uniform modeling strategies regardless of individual\nprocess complexity characteristics. To address these limitations, we introduce\nRLHGNN, a novel framework that transforms event logs into heterogeneous process\ngraphs with three distinct edge types grounded in established process mining\ntheory. Our approach creates four flexible graph structures by selectively\ncombining these edges to accommodate different process complexities, and\nemploys reinforcement learning formulated as a Markov Decision Process to\nautomatically determine the optimal graph structure for each specific process\ninstance. RLHGNN then applies heterogeneous graph convolution with\nrelation-specific aggregation strategies to effectively predict the next\nactivity. This adaptive methodology enables precise modeling of both sequential\nand non-sequential relationships in service interactions. Comprehensive\nevaluation on six real-world datasets demonstrates that RLHGNN consistently\noutperforms state-of-the-art approaches. Furthermore, it maintains an inference\nlatency of approximately 1 ms per prediction, representing a highly practical\nsolution suitable for real-time business process monitoring applications. The\nsource code is available at https://github.com/Joker3993/RLHGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next activity prediction represents a fundamental challenge for optimizing\nbusiness processes in service-oriented architectures such as microservices\nenvironments, distributed enterprise systems, and cloud-native platforms, which\nenables proactive resource allocation and dynamic service composition. Despite\nthe prevalence of sequence-based methods, these approaches fail to capture\nnon-sequential relationships that arise from parallel executions and\nconditional dependencies. Even though graph-based approaches address structural\npreservation, they suffer from homogeneous representations and static\nstructures that apply uniform modeling strategies regardless of individual\nprocess complexity characteristics. To address these limitations, we introduce\nRLHGNN, a novel framework that transforms event logs into heterogeneous process\ngraphs with three distinct edge types grounded in established process mining\ntheory. Our approach creates four flexible graph structures by selectively\ncombining these edges to accommodate different process complexities, and\nemploys reinforcement learning formulated as a Markov Decision Process to\nautomatically determine the optimal graph structure for each specific process\ninstance. RLHGNN then applies heterogeneous graph convolution with\nrelation-specific aggregation strategies to effectively predict the next\nactivity. This adaptive methodology enables precise modeling of both sequential\nand non-sequential relationships in service interactions. Comprehensive\nevaluation on six real-world datasets demonstrates that RLHGNN consistently\noutperforms state-of-the-art approaches. Furthermore, it maintains an inference\nlatency of approximately 1 ms per prediction, representing a highly practical\nsolution suitable for real-time business process monitoring applications. The\nsource code is available at https://github.com/Joker3993/RLHGNN."
                },
                "authors": [
                    {
                        "name": "Jiaxing Wang"
                    },
                    {
                        "name": "Yifeng Yu"
                    },
                    {
                        "name": "Jiahan Song"
                    },
                    {
                        "name": "Bin Cao"
                    },
                    {
                        "name": "Jing Fan"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "15 pages, 7 figures. Business process prediction using reinforcement\n  learning and heterogeneous graph neural networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02689v1",
                "updated": "2025-07-03T14:59:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    59,
                    42,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:59:42Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    59,
                    42,
                    3,
                    184,
                    0
                ],
                "title": "On the Convergence of Large Language Model Optimizer for Black-Box\n  Network Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Convergence of Large Language Model Optimizer for Black-Box\n  Network Management"
                },
                "summary": "Future wireless networks are expected to incorporate diverse services that\noften lack general mathematical models. To address such black-box network\nmanagement tasks, the large language model (LLM) optimizer framework, which\nleverages pretrained LLMs as optimization agents, has recently been promoted as\na promising solution. This framework utilizes natural language prompts\ndescribing the given optimization problems along with past solutions generated\nby LLMs themselves. As a result, LLMs can obtain efficient solutions\nautonomously without knowing the mathematical models of the objective\nfunctions. Although the viability of the LLM optimizer (LLMO) framework has\nbeen studied in various black-box scenarios, it has so far been limited to\nnumerical simulations. For the first time, this paper establishes a theoretical\nfoundation for the LLMO framework. With careful investigations of LLM inference\nsteps, we can interpret the LLMO procedure as a finite-state Markov chain, and\nprove the convergence of the framework. Our results are extended to a more\nadvanced multiple LLM architecture, where the impact of multiple LLMs is\nrigorously verified in terms of the convergence rate. Comprehensive numerical\nsimulations validate our theoretical results and provide a deeper understanding\nof the underlying mechanisms of the LLMO framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future wireless networks are expected to incorporate diverse services that\noften lack general mathematical models. To address such black-box network\nmanagement tasks, the large language model (LLM) optimizer framework, which\nleverages pretrained LLMs as optimization agents, has recently been promoted as\na promising solution. This framework utilizes natural language prompts\ndescribing the given optimization problems along with past solutions generated\nby LLMs themselves. As a result, LLMs can obtain efficient solutions\nautonomously without knowing the mathematical models of the objective\nfunctions. Although the viability of the LLM optimizer (LLMO) framework has\nbeen studied in various black-box scenarios, it has so far been limited to\nnumerical simulations. For the first time, this paper establishes a theoretical\nfoundation for the LLMO framework. With careful investigations of LLM inference\nsteps, we can interpret the LLMO procedure as a finite-state Markov chain, and\nprove the convergence of the framework. Our results are extended to a more\nadvanced multiple LLM architecture, where the impact of multiple LLMs is\nrigorously verified in terms of the convergence rate. Comprehensive numerical\nsimulations validate our theoretical results and provide a deeper understanding\nof the underlying mechanisms of the LLMO framework."
                },
                "authors": [
                    {
                        "name": "Hoon Lee"
                    },
                    {
                        "name": "Wentao Zhou"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Inkyu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Inkyu Lee"
                },
                "author": "Inkyu Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02686v1",
                "updated": "2025-07-03T14:55:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    55,
                    53,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:55:53Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    55,
                    53,
                    3,
                    184,
                    0
                ],
                "title": "Learning few-step posterior samplers by unfolding and distillation of\n  diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning few-step posterior samplers by unfolding and distillation of\n  diffusion models"
                },
                "summary": "Diffusion models (DMs) have emerged as powerful image priors in Bayesian\ncomputational imaging. Two primary strategies have been proposed for leveraging\nDMs in this context: Plug-and-Play methods, which are zero-shot and highly\nflexible but rely on approximations; and specialized conditional DMs, which\nachieve higher accuracy and faster inference for specific tasks through\nsupervised training. In this work, we introduce a novel framework that\nintegrates deep unfolding and model distillation to transform a DM image prior\ninto a few-step conditional model for posterior sampling. A central innovation\nof our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm\n- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et\nal., 2025) - representing the first known instance of deep unfolding applied to\na Monte Carlo sampling scheme. We demonstrate our proposed unfolded and\ndistilled samplers through extensive experiments and comparisons with the state\nof the art, where they achieve excellent accuracy and computational efficiency,\nwhile retaining the flexibility to adapt to variations in the forward model at\ninference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have emerged as powerful image priors in Bayesian\ncomputational imaging. Two primary strategies have been proposed for leveraging\nDMs in this context: Plug-and-Play methods, which are zero-shot and highly\nflexible but rely on approximations; and specialized conditional DMs, which\nachieve higher accuracy and faster inference for specific tasks through\nsupervised training. In this work, we introduce a novel framework that\nintegrates deep unfolding and model distillation to transform a DM image prior\ninto a few-step conditional model for posterior sampling. A central innovation\nof our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm\n- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et\nal., 2025) - representing the first known instance of deep unfolding applied to\na Monte Carlo sampling scheme. We demonstrate our proposed unfolded and\ndistilled samplers through extensive experiments and comparisons with the state\nof the art, where they achieve excellent accuracy and computational efficiency,\nwhile retaining the flexibility to adapt to variations in the forward model at\ninference time."
                },
                "authors": [
                    {
                        "name": "Charlesquin Kemajou Mbakam"
                    },
                    {
                        "name": "Jonathan Spence"
                    },
                    {
                        "name": "Marcelo Pereyra"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Pereyra"
                },
                "author": "Marcelo Pereyra",
                "arxiv_comment": "28 pages, 16 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01737v2",
                "updated": "2025-07-03T14:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    52,
                    12,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T14:13:48Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    13,
                    48,
                    2,
                    183,
                    0
                ],
                "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion"
                },
                "summary": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions."
                },
                "authors": [
                    {
                        "name": "Lin Wu"
                    },
                    {
                        "name": "Zhixiang Chen"
                    },
                    {
                        "name": "Jianglin Lan"
                    }
                ],
                "author_detail": {
                    "name": "Jianglin Lan"
                },
                "author": "Jianglin Lan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02673v1",
                "updated": "2025-07-03T14:38:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    38,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:38:27Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    38,
                    27,
                    3,
                    184,
                    0
                ],
                "title": "Attosecond physics in optical near fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attosecond physics in optical near fields"
                },
                "summary": "Attosecond science, the electron control by the field of ultrashort laser\npulses, is maturing into lightfield-driven electronics, called petahertz\nelectronics. Based on optical field-driven nanostructures, elements for\npetahertz electronics have been demonstrated. These hinge on the understanding\nof the electron dynamics in the optical near field of the nanostructure. Here\nwe show near field-induced low energy stripes (NILES) in carrier-envelope\nphase-dependent electron spectra, a new spectral feature appearing in the\ndirect electrons emitted from a strongly driven nanostructure, i.e., in the\neasily accessible energy region between 0 and a few electron volts. NILES\nemerge due to the sub-cycle sensitivity of ponderomotive acceleration of\nelectrons injected into a strong near field gradient by a few-cycle optical\nwaveform. NILES enables us to track the emission of direct and re-scattered\nelectrons down to sub-cycle time-scales and to infer the electron momentum\nwidth at emission. Because NILES shows up in the direct part of the electrons,\na large fraction of the emitted electrons can now be steered in new ways,\nfacilitating the isolation of individual electron bursts with high charge\ndensity of 430 attosecond duration. These results not only substantially\nadvance the understanding of attosecond physics in optical near fields, but\nalso provide new ways of electron control for the nascent field of petahertz\nelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attosecond science, the electron control by the field of ultrashort laser\npulses, is maturing into lightfield-driven electronics, called petahertz\nelectronics. Based on optical field-driven nanostructures, elements for\npetahertz electronics have been demonstrated. These hinge on the understanding\nof the electron dynamics in the optical near field of the nanostructure. Here\nwe show near field-induced low energy stripes (NILES) in carrier-envelope\nphase-dependent electron spectra, a new spectral feature appearing in the\ndirect electrons emitted from a strongly driven nanostructure, i.e., in the\neasily accessible energy region between 0 and a few electron volts. NILES\nemerge due to the sub-cycle sensitivity of ponderomotive acceleration of\nelectrons injected into a strong near field gradient by a few-cycle optical\nwaveform. NILES enables us to track the emission of direct and re-scattered\nelectrons down to sub-cycle time-scales and to infer the electron momentum\nwidth at emission. Because NILES shows up in the direct part of the electrons,\na large fraction of the emitted electrons can now be steered in new ways,\nfacilitating the isolation of individual electron bursts with high charge\ndensity of 430 attosecond duration. These results not only substantially\nadvance the understanding of attosecond physics in optical near fields, but\nalso provide new ways of electron control for the nascent field of petahertz\nelectronics."
                },
                "authors": [
                    {
                        "name": "Jonas Heimerl"
                    },
                    {
                        "name": "Stefan Meier"
                    },
                    {
                        "name": "Anne Herzig"
                    },
                    {
                        "name": "Felix Lpez Hoffmann"
                    },
                    {
                        "name": "Lennart Seiffert"
                    },
                    {
                        "name": "Daniel Lesko"
                    },
                    {
                        "name": "Simon Hillmann"
                    },
                    {
                        "name": "Simon Wittigschlager"
                    },
                    {
                        "name": "Tobias Weitz"
                    },
                    {
                        "name": "Thomas Fennel"
                    },
                    {
                        "name": "Peter Hommelhoff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hommelhoff"
                },
                "author": "Peter Hommelhoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13886v2",
                "updated": "2025-07-03T14:30:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    30,
                    25,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-20T03:47:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    47,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning"
                },
                "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Jixin Tang"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yanbo Wen"
                    },
                    {
                        "name": "Fan Song"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Yuyang Lu"
                    },
                    {
                        "name": "Chaoran Tao"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Jizhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "63 pages, 23 figures, submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02664v1",
                "updated": "2025-07-03T14:26:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    26,
                    31,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:26:31Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    26,
                    31,
                    3,
                    184,
                    0
                ],
                "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image\n  Detection via Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image\n  Detection via Multimodal Large Language Models"
                },
                "summary": "The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhou"
                    },
                    {
                        "name": "Yunpeng Luo"
                    },
                    {
                        "name": "Yuanchen Wu"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "Shouhong Ding"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Yunsheng Wu"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02663v1",
                "updated": "2025-07-03T14:24:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    24,
                    26,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:24:26Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    24,
                    26,
                    3,
                    184,
                    0
                ],
                "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty\n  Cognition in Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty\n  Cognition in Large Reasoning Models"
                },
                "summary": "Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities\nin handling complex reasoning tasks, but are hindered by excessive\noverthinking. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we bootstrap such ability to further alleviate the overthinking phenomenon\nin LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage\nfine-tuning strategy that progressively inspires LRMs' difficulty cognition and\nredundancy cognition. First, we introduce difficulty-hypnosis in the prefixes\nof model outputs to intervene in the internal reasoning trajectory. Combined\nwith a heterogeneous short and long reasoning dataset, the trained model\nenhances its sensitivity to task difficulty, enabling native, differentiated\nreasoning strategies across various tasks. Second, we further extend\nredundancy-hypnosis to the internal reasoning process, guiding the model to\nidentify redundant structures within the reasoning steps and generate more\nconcise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that\nTH2T significantly reduces inference costs (more than 70% on easy tasks and 40%\non hard tasks) while maintaining performance stability. The resulting outputs\nexhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,\nreflection).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities\nin handling complex reasoning tasks, but are hindered by excessive\noverthinking. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we bootstrap such ability to further alleviate the overthinking phenomenon\nin LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage\nfine-tuning strategy that progressively inspires LRMs' difficulty cognition and\nredundancy cognition. First, we introduce difficulty-hypnosis in the prefixes\nof model outputs to intervene in the internal reasoning trajectory. Combined\nwith a heterogeneous short and long reasoning dataset, the trained model\nenhances its sensitivity to task difficulty, enabling native, differentiated\nreasoning strategies across various tasks. Second, we further extend\nredundancy-hypnosis to the internal reasoning process, guiding the model to\nidentify redundant structures within the reasoning steps and generate more\nconcise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that\nTH2T significantly reduces inference costs (more than 70% on easy tasks and 40%\non hard tasks) while maintaining performance stability. The resulting outputs\nexhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,\nreflection)."
                },
                "authors": [
                    {
                        "name": "Yongjiang Liu"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "arxiv_comment": "21 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02660v1",
                "updated": "2025-07-03T14:20:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &\n  Verification"
                },
                "summary": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability."
                },
                "authors": [
                    {
                        "name": "Deepak Narayan Gadde"
                    },
                    {
                        "name": "Keerthan Kopparam Radhakrishna"
                    },
                    {
                        "name": "Vaisakh Naduvodi Viswambharan"
                    },
                    {
                        "name": "Aman Kumar"
                    },
                    {
                        "name": "Djones Lettnin"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    },
                    {
                        "name": "Sebastian Simon"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Simon"
                },
                "author": "Sebastian Simon",
                "arxiv_comment": "To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated\n  Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02654v1",
                "updated": "2025-07-03T14:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    18,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    18,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure"
                },
                "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Sanchari Sen"
                    },
                    {
                        "name": "Swagath Venkataramani"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02640v1",
                "updated": "2025-07-03T14:07:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    7,
                    3,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:07:03Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    7,
                    3,
                    3,
                    184,
                    0
                ],
                "title": "Two-Sample Covariance Inference in High-Dimensional Elliptical Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Sample Covariance Inference in High-Dimensional Elliptical Models"
                },
                "summary": "We propose a two-sample test for large-dimensional covariance matrices in\ngeneralized elliptical models. The test statistic is based on a U-statistic\nestimator of the squared Frobenius norm of the difference between the two\npopulation covariance matrices. This statistic was originally introduced by Li\nand Chen (2012, AoS) for the independent component model. As a key theoretical\ncontribution, we establish a new central limit theorem for the U-statistics\nunder elliptical data, valid under both the null and alternative hypotheses.\nThis result enables asymptotic control of the test level and facilitates a\npower analysis. To the best of our knowledge, the proposed test is the first\nsuch method to be supported by theoretical guarantees for elliptical data. Our\napproach imposes only mild assumptions on the covariance matrices and does\nneither require sparsity nor explicit growth conditions on the\ndimension-to-sample-size ratio. We illustrate our theoretical findings through\napplications to both synthetic and real-world data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a two-sample test for large-dimensional covariance matrices in\ngeneralized elliptical models. The test statistic is based on a U-statistic\nestimator of the squared Frobenius norm of the difference between the two\npopulation covariance matrices. This statistic was originally introduced by Li\nand Chen (2012, AoS) for the independent component model. As a key theoretical\ncontribution, we establish a new central limit theorem for the U-statistics\nunder elliptical data, valid under both the null and alternative hypotheses.\nThis result enables asymptotic control of the test level and facilitates a\npower analysis. To the best of our knowledge, the proposed test is the first\nsuch method to be supported by theoretical guarantees for elliptical data. Our\napproach imposes only mild assumptions on the covariance matrices and does\nneither require sparsity nor explicit growth conditions on the\ndimension-to-sample-size ratio. We illustrate our theoretical findings through\napplications to both synthetic and real-world data."
                },
                "authors": [
                    {
                        "name": "Nina Drnemann"
                    }
                ],
                "author_detail": {
                    "name": "Nina Drnemann"
                },
                "author": "Nina Drnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00545v2",
                "updated": "2025-07-03T14:01:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    1,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-01T16:02:15Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    16,
                    2,
                    15,
                    5,
                    60,
                    0
                ],
                "title": "RFWNet: A Lightweight Remote Sensing Object Detector Integrating\n  Multiscale Receptive Fields and Foreground Focus Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RFWNet: A Lightweight Remote Sensing Object Detector Integrating\n  Multiscale Receptive Fields and Foreground Focus Mechanism"
                },
                "summary": "Challenges in remote sensing object detection(RSOD), such as high interclass\nsimilarity, imbalanced foreground-background distribution, and the small size\nof objects in remote sensing images, significantly hinder detection accuracy.\nMoreover, the tradeoff between model accuracy and computational complexity\nposes additional constraints on the application of RSOD algorithms. To address\nthese issues, this study proposes an efficient and lightweight RSOD algorithm\nintegrating multiscale receptive fields and foreground focus mechanism, named\nrobust foreground weighted network(RFWNet). Specifically, we proposed a\nlightweight backbone network receptive field adaptive selection network\n(RFASNet), leveraging the rich context information of remote sensing images to\nenhance class separability. Additionally, we developed a foreground-background\nseparation module(FBSM)consisting of a background redundant information\nfiltering module (BRIFM) and a foreground information enhancement module (FIEM)\nto emphasize critical regions within images while filtering redundant\nbackground information. Finally, we designed a loss function, the weighted\nCIoU-Wasserstein loss (LWCW),which weights the IoU-based loss by using the\nnormalized Wasserstein distance to mitigate model sensitivity to small object\nposition deviations. The comprehensive experimental results demonstrate that\nRFWNet achieved 95.3% and 73.2% mean average precision (mAP) with 6.0 M\nparameters on the DOTA V1.0 and NWPU VHR-10 datasets, respectively, with an\ninference speed of 52 FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in remote sensing object detection(RSOD), such as high interclass\nsimilarity, imbalanced foreground-background distribution, and the small size\nof objects in remote sensing images, significantly hinder detection accuracy.\nMoreover, the tradeoff between model accuracy and computational complexity\nposes additional constraints on the application of RSOD algorithms. To address\nthese issues, this study proposes an efficient and lightweight RSOD algorithm\nintegrating multiscale receptive fields and foreground focus mechanism, named\nrobust foreground weighted network(RFWNet). Specifically, we proposed a\nlightweight backbone network receptive field adaptive selection network\n(RFASNet), leveraging the rich context information of remote sensing images to\nenhance class separability. Additionally, we developed a foreground-background\nseparation module(FBSM)consisting of a background redundant information\nfiltering module (BRIFM) and a foreground information enhancement module (FIEM)\nto emphasize critical regions within images while filtering redundant\nbackground information. Finally, we designed a loss function, the weighted\nCIoU-Wasserstein loss (LWCW),which weights the IoU-based loss by using the\nnormalized Wasserstein distance to mitigate model sensitivity to small object\nposition deviations. The comprehensive experimental results demonstrate that\nRFWNet achieved 95.3% and 73.2% mean average precision (mAP) with 6.0 M\nparameters on the DOTA V1.0 and NWPU VHR-10 datasets, respectively, with an\ninference speed of 52 FPS."
                },
                "authors": [
                    {
                        "name": "Yujie Lei"
                    },
                    {
                        "name": "Wenjie Sun"
                    },
                    {
                        "name": "Sen Jia"
                    },
                    {
                        "name": "Qingquan Li"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "arxiv_doi": "10.1109/LGRS.2025.3582337",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LGRS.2025.3582337",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.00545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Geoscience and Remote Sensing Letters, vol. 22, pp. 1-5, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02628v1",
                "updated": "2025-07-03T13:54:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    54,
                    50,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:54:50Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    54,
                    50,
                    3,
                    184,
                    0
                ],
                "title": "Medical Data Pecking: A Context-Aware Approach for Automated Quality\n  Evaluation of Structured Medical Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Data Pecking: A Context-Aware Approach for Automated Quality\n  Evaluation of Structured Medical Data"
                },
                "summary": "Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods."
                },
                "authors": [
                    {
                        "name": "Irena Girshovitz"
                    },
                    {
                        "name": "Atai Ambus"
                    },
                    {
                        "name": "Moni Shahar"
                    },
                    {
                        "name": "Ran Gilad-Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Ran Gilad-Bachrach"
                },
                "author": "Ran Gilad-Bachrach",
                "arxiv_comment": "18 pages, 4 figures (+ appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02626v1",
                "updated": "2025-07-03T13:52:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    52,
                    24,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:52:24Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    52,
                    24,
                    3,
                    184,
                    0
                ],
                "title": "VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via\n  Reinforcement Learning"
                },
                "summary": "Owing to powerful natural language processing and generative capabilities,\nlarge language model (LLM) agents have emerged as a promising solution for\nenhancing recommendation systems via user simulation. However, in the realm of\nvideo recommendation, existing studies predominantly resort to prompt-based\nsimulation using frozen LLMs and encounter the intricate challenge of\nmultimodal content understanding. This frequently results in suboptimal item\nmodeling and user preference learning, thereby ultimately constraining\nrecommendation performance. To address these challenges, we introduce\nVRAgent-R1, a novel agent-based paradigm that incorporates human-like\nintelligence in user simulation. Specifically, VRAgent-R1 comprises two\ndistinct agents: the Item Perception (IP) Agent and the User Simulation (US)\nAgent, designed for interactive user-item modeling. Firstly, the IP Agent\nemulates human-like progressive thinking based on MLLMs, effectively capturing\nhidden recommendation semantics in videos. With a more comprehensive multimodal\ncontent understanding provided by the IP Agent, the video recommendation system\nis equipped to provide higher-quality candidate items. Subsequently, the US\nAgent refines the recommended video sets based on in-depth chain-of-thought\n(CoT) reasoning and achieves better alignment with real user preferences\nthrough reinforcement learning. Experimental results on a large-scale video\nrecommendation benchmark have demonstrated the effectiveness of our proposed\nVRAgent-R1 method, e.g., the IP Agent achieves a 6.0\\% improvement in NDCG@10\non the MicroLens-100k dataset, while the US Agent shows approximately 45.0\\%\nhigher accuracy in user decision simulation compared to state-of-the-art\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to powerful natural language processing and generative capabilities,\nlarge language model (LLM) agents have emerged as a promising solution for\nenhancing recommendation systems via user simulation. However, in the realm of\nvideo recommendation, existing studies predominantly resort to prompt-based\nsimulation using frozen LLMs and encounter the intricate challenge of\nmultimodal content understanding. This frequently results in suboptimal item\nmodeling and user preference learning, thereby ultimately constraining\nrecommendation performance. To address these challenges, we introduce\nVRAgent-R1, a novel agent-based paradigm that incorporates human-like\nintelligence in user simulation. Specifically, VRAgent-R1 comprises two\ndistinct agents: the Item Perception (IP) Agent and the User Simulation (US)\nAgent, designed for interactive user-item modeling. Firstly, the IP Agent\nemulates human-like progressive thinking based on MLLMs, effectively capturing\nhidden recommendation semantics in videos. With a more comprehensive multimodal\ncontent understanding provided by the IP Agent, the video recommendation system\nis equipped to provide higher-quality candidate items. Subsequently, the US\nAgent refines the recommended video sets based on in-depth chain-of-thought\n(CoT) reasoning and achieves better alignment with real user preferences\nthrough reinforcement learning. Experimental results on a large-scale video\nrecommendation benchmark have demonstrated the effectiveness of our proposed\nVRAgent-R1 method, e.g., the IP Agent achieves a 6.0\\% improvement in NDCG@10\non the MicroLens-100k dataset, while the US Agent shows approximately 45.0\\%\nhigher accuracy in user decision simulation compared to state-of-the-art\nbaselines."
                },
                "authors": [
                    {
                        "name": "Siran Chen"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Chenyun Yu"
                    },
                    {
                        "name": "Yuxiao Luo"
                    },
                    {
                        "name": "Ouyang Yi"
                    },
                    {
                        "name": "Lei Cheng"
                    },
                    {
                        "name": "Chengxiang Zhuo"
                    },
                    {
                        "name": "Zang Li"
                    },
                    {
                        "name": "Yali Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yali Wang"
                },
                "author": "Yali Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02624v1",
                "updated": "2025-07-03T13:50:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    50,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:50:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    50,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in\n  Pharmacogenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in\n  Pharmacogenes"
                },
                "summary": "Variant effect predictors (VEPs) aim to assess the functional impact of\nprotein variants, traditionally relying on multiple sequence alignments (MSAs).\nThis approach assumes that naturally occurring variants are fit, an assumption\nchallenged by pharmacogenomics, where some pharmacogenes experience low\nevolutionary pressure. Deep mutational scanning (DMS) datasets provide an\nalternative by offering quantitative fitness scores for variants. In this work,\nwe propose a transformer-based matrix variational auto-encoder (matVAE) with a\nstructured prior and evaluate its performance on 33 DMS datasets corresponding\nto 26 drug target and ADME proteins from the ProteinGym benchmark. Our model\ntrained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence\nmodel in zero-shot prediction on DMS datasets, despite using an order of\nmagnitude fewer parameters and requiring less computation at inference time. We\nalso compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on\nDMS data, and find that the latter performs better on supervised prediction\ntasks. Additionally, incorporating AlphaFold-generated structures into our\ntransformer model further improves performance, achieving results comparable to\nDeepSequence trained on MSAs and finetuned on DMS. These findings highlight the\npotential of DMS datasets to replace MSAs without significant loss in\npredictive performance, motivating further development of DMS datasets and\nexploration of their relationships to enhance variant effect prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variant effect predictors (VEPs) aim to assess the functional impact of\nprotein variants, traditionally relying on multiple sequence alignments (MSAs).\nThis approach assumes that naturally occurring variants are fit, an assumption\nchallenged by pharmacogenomics, where some pharmacogenes experience low\nevolutionary pressure. Deep mutational scanning (DMS) datasets provide an\nalternative by offering quantitative fitness scores for variants. In this work,\nwe propose a transformer-based matrix variational auto-encoder (matVAE) with a\nstructured prior and evaluate its performance on 33 DMS datasets corresponding\nto 26 drug target and ADME proteins from the ProteinGym benchmark. Our model\ntrained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence\nmodel in zero-shot prediction on DMS datasets, despite using an order of\nmagnitude fewer parameters and requiring less computation at inference time. We\nalso compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on\nDMS data, and find that the latter performs better on supervised prediction\ntasks. Additionally, incorporating AlphaFold-generated structures into our\ntransformer model further improves performance, achieving results comparable to\nDeepSequence trained on MSAs and finetuned on DMS. These findings highlight the\npotential of DMS datasets to replace MSAs without significant loss in\npredictive performance, motivating further development of DMS datasets and\nexploration of their relationships to enhance variant effect prediction."
                },
                "authors": [
                    {
                        "name": "Antoine Honor"
                    },
                    {
                        "name": "Borja Rodrguez Glvez"
                    },
                    {
                        "name": "Yoomi Park"
                    },
                    {
                        "name": "Yitian Zhou"
                    },
                    {
                        "name": "Volker M. Lauschke"
                    },
                    {
                        "name": "Ming Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Xiao"
                },
                "author": "Ming Xiao",
                "arxiv_comment": "12+8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02620v1",
                "updated": "2025-07-03T13:47:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    47,
                    42,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:47:42Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    47,
                    42,
                    3,
                    184,
                    0
                ],
                "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference"
                },
                "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}"
                },
                "authors": [
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Lizhuo Luo"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "16 pages, and the last 3 are appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02618v1",
                "updated": "2025-07-03T13:45:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    45,
                    2,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:45:02Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    45,
                    2,
                    3,
                    184,
                    0
                ],
                "title": "Strategic Intelligence in Large Language Models: Evidence from\n  evolutionary Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Intelligence in Large Language Models: Evidence from\n  evolutionary Game Theory"
                },
                "summary": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty."
                },
                "authors": [
                    {
                        "name": "Kenneth Payne"
                    },
                    {
                        "name": "Baptiste Alloui-Cros"
                    }
                ],
                "author_detail": {
                    "name": "Baptiste Alloui-Cros"
                },
                "author": "Baptiste Alloui-Cros",
                "arxiv_comment": "29 pages, 27 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02616v1",
                "updated": "2025-07-03T13:43:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    43,
                    10,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:43:10Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    43,
                    10,
                    3,
                    184,
                    0
                ],
                "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and\n  Open-Ended Medical Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and\n  Open-Ended Medical Decision-Making"
                },
                "summary": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents."
                },
                "authors": [
                    {
                        "name": "Tianqi Shang"
                    },
                    {
                        "name": "Weiqing He"
                    },
                    {
                        "name": "Charles Zheng"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Bingxin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bingxin Zhao"
                },
                "author": "Bingxin Zhao",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07635v2",
                "updated": "2025-07-03T13:40:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    40,
                    49,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-12T15:05:46Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    5,
                    46,
                    0,
                    132,
                    0
                ],
                "title": "Interpreting Graph Inference with Skyline Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Graph Inference with Skyline Explanations"
                },
                "summary": "Inference queries have been routinely issued to graph machine learning models\nsuch as graph neural networks (GNNs) for various network analytical tasks.\nNevertheless, GNNs outputs are often hard to interpret comprehensively.\nExisting methods typically compromise to individual pre-defined explainability\nmeasures (such as fidelity), which often leads to biased, ``one-sided''\ninterpretations. This paper introduces skyline explanation, a new paradigm that\ninterprets GNN output by simultaneously optimizing multiple explainability\nmeasures of users' interests. (1) We propose skyline explanations as a Pareto\nset of explanatory subgraphs that dominate others over multiple explanatory\nmeasures. We formulate skyline explanation as a multi-criteria optimization\nproblem, and establish its hardness results. (2) We design efficient algorithms\nwith an onion-peeling approach, which strategically prioritizes nodes and\nremoves unpromising edges to incrementally assemble skyline explanations. (3)\nWe also develop an algorithm to diversify the skyline explanations to enrich\nthe comprehensive interpretation. (4) We introduce efficient parallel\nalgorithms with load-balancing strategies to scale skyline explanation for\nlarge-scale GNN-based inference. Using real-world and synthetic graphs, we\nexperimentally verify our algorithms' effectiveness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference queries have been routinely issued to graph machine learning models\nsuch as graph neural networks (GNNs) for various network analytical tasks.\nNevertheless, GNNs outputs are often hard to interpret comprehensively.\nExisting methods typically compromise to individual pre-defined explainability\nmeasures (such as fidelity), which often leads to biased, ``one-sided''\ninterpretations. This paper introduces skyline explanation, a new paradigm that\ninterprets GNN output by simultaneously optimizing multiple explainability\nmeasures of users' interests. (1) We propose skyline explanations as a Pareto\nset of explanatory subgraphs that dominate others over multiple explanatory\nmeasures. We formulate skyline explanation as a multi-criteria optimization\nproblem, and establish its hardness results. (2) We design efficient algorithms\nwith an onion-peeling approach, which strategically prioritizes nodes and\nremoves unpromising edges to incrementally assemble skyline explanations. (3)\nWe also develop an algorithm to diversify the skyline explanations to enrich\nthe comprehensive interpretation. (4) We introduce efficient parallel\nalgorithms with load-balancing strategies to scale skyline explanation for\nlarge-scale GNN-based inference. Using real-world and synthetic graphs, we\nexperimentally verify our algorithms' effectiveness and scalability."
                },
                "authors": [
                    {
                        "name": "Dazhuo Qiu"
                    },
                    {
                        "name": "Haolai Che"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Yinghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wu"
                },
                "author": "Yinghui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08713v2",
                "updated": "2025-07-03T13:39:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    39,
                    37,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-10T11:56:06Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    11,
                    56,
                    6,
                    1,
                    161,
                    0
                ],
                "title": "Explainable Compliance Detection with Multi-Hop Natural Language\n  Inference on Assurance Case Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Compliance Detection with Multi-Hop Natural Language\n  Inference on Assurance Case Structure"
                },
                "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process."
                },
                "authors": [
                    {
                        "name": "Fariz Ikhwantri"
                    },
                    {
                        "name": "Dusica Marijan"
                    }
                ],
                "author_detail": {
                    "name": "Dusica Marijan"
                },
                "author": "Dusica Marijan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02608v1",
                "updated": "2025-07-03T13:32:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    32,
                    50,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:32:50Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    32,
                    50,
                    3,
                    184,
                    0
                ],
                "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for\n  Physics Emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for\n  Physics Emulation"
                },
                "summary": "The steep computational cost of diffusion models at inference hinders their\nuse as fast physics emulators. In the context of image and video generation,\nthis computational drawback has been addressed by generating in the latent\nspace of an autoencoder instead of the pixel space. In this work, we\ninvestigate whether a similar strategy can be effectively applied to the\nemulation of dynamical systems and at what cost. We find that the accuracy of\nlatent-space emulation is surprisingly robust to a wide range of compression\nrates (up to 1000x). We also show that diffusion-based emulators are\nconsistently more accurate than non-generative counterparts and compensate for\nuncertainty in their predictions with greater diversity. Finally, we cover\npractical design choices, spanning from architectures to optimizers, that we\nfound critical to train latent-space emulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The steep computational cost of diffusion models at inference hinders their\nuse as fast physics emulators. In the context of image and video generation,\nthis computational drawback has been addressed by generating in the latent\nspace of an autoencoder instead of the pixel space. In this work, we\ninvestigate whether a similar strategy can be effectively applied to the\nemulation of dynamical systems and at what cost. We find that the accuracy of\nlatent-space emulation is surprisingly robust to a wide range of compression\nrates (up to 1000x). We also show that diffusion-based emulators are\nconsistently more accurate than non-generative counterparts and compensate for\nuncertainty in their predictions with greater diversity. Finally, we cover\npractical design choices, spanning from architectures to optimizers, that we\nfound critical to train latent-space emulators."
                },
                "authors": [
                    {
                        "name": "Franois Rozet"
                    },
                    {
                        "name": "Ruben Ohana"
                    },
                    {
                        "name": "Michael McCabe"
                    },
                    {
                        "name": "Gilles Louppe"
                    },
                    {
                        "name": "Franois Lanusse"
                    },
                    {
                        "name": "Shirley Ho"
                    }
                ],
                "author_detail": {
                    "name": "Shirley Ho"
                },
                "author": "Shirley Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02604v1",
                "updated": "2025-07-03T13:25:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    25,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:25:28Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    25,
                    28,
                    3,
                    184,
                    0
                ],
                "title": "PhenomXPNR: An improved gravitational wave model linking precessing\n  inspirals and NR-calibrated merger-ringdown",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhenomXPNR: An improved gravitational wave model linking precessing\n  inspirals and NR-calibrated merger-ringdown"
                },
                "summary": "We present the frequency-domain quasi-circular precessing binary-black-hole\nmodel PhenomXPNR. This model combines the most precise available post-Newtonian\ndescription of the evolution of the precession dynamics through inspiral with\nmerger-ringdown model informed by numerical relativity. This, along with a\nphenomenological model of the dominant multipole asymmetries, results in the\nmost accurate and complete representation of the physics of precessing binaries\nnatively in the frequency-domain to date. All state-of-the-art precessing\nmodels show bias when inferring binary parameters in certain regions of the\nparameter space. We demonstrate that the developments presented ensure that for\nsome precessing systems PhenomXPNR shows the least degree of bias. Further, as\na phenomenological, frequency-domain model, PhenomXPNR remains one of the most\ncomputationally efficient models available and is therefore well-suited to the\nera of gravitational-wave astronomy with its ever growing rate of detected\nsignals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the frequency-domain quasi-circular precessing binary-black-hole\nmodel PhenomXPNR. This model combines the most precise available post-Newtonian\ndescription of the evolution of the precession dynamics through inspiral with\nmerger-ringdown model informed by numerical relativity. This, along with a\nphenomenological model of the dominant multipole asymmetries, results in the\nmost accurate and complete representation of the physics of precessing binaries\nnatively in the frequency-domain to date. All state-of-the-art precessing\nmodels show bias when inferring binary parameters in certain regions of the\nparameter space. We demonstrate that the developments presented ensure that for\nsome precessing systems PhenomXPNR shows the least degree of bias. Further, as\na phenomenological, frequency-domain model, PhenomXPNR remains one of the most\ncomputationally efficient models available and is therefore well-suited to the\nera of gravitational-wave astronomy with its ever growing rate of detected\nsignals."
                },
                "authors": [
                    {
                        "name": "Eleanor Hamilton"
                    },
                    {
                        "name": "Marta Colleoni"
                    },
                    {
                        "name": "Jonathan E. Thompson"
                    },
                    {
                        "name": "Charlie Hoy"
                    },
                    {
                        "name": "Anna Heffernan"
                    },
                    {
                        "name": "Meryl Kinnear"
                    },
                    {
                        "name": "Jorge Valencia"
                    },
                    {
                        "name": "Felip A Ramis Vidal"
                    },
                    {
                        "name": "Cecilio Garca-Quirs"
                    },
                    {
                        "name": "Shrobana Ghosgh"
                    },
                    {
                        "name": "Lionel London"
                    },
                    {
                        "name": "Mark Hannam"
                    },
                    {
                        "name": "Sascha Husa"
                    }
                ],
                "author_detail": {
                    "name": "Sascha Husa"
                },
                "author": "Sascha Husa",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11853v2",
                "updated": "2025-07-03T13:25:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    25,
                    12,
                    3,
                    184,
                    0
                ],
                "published": "2025-02-17T14:46:38Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    46,
                    38,
                    0,
                    48,
                    0
                ],
                "title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models"
                },
                "summary": "In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g., SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to a 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing content\ntransformations, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g., SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to a 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing content\ntransformations, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection."
                },
                "authors": [
                    {
                        "name": "Shehel Yoosuf"
                    },
                    {
                        "name": "Temoor Ali"
                    },
                    {
                        "name": "Ahmed Lekssays"
                    },
                    {
                        "name": "Mashael AlSabah"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02596v1",
                "updated": "2025-07-03T13:20:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    20,
                    0,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:20:00Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    20,
                    0,
                    3,
                    184,
                    0
                ],
                "title": "Relativistic Limits of Decoding: Critical Divergence of Kullback-Leibler\n  Information and Free Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relativistic Limits of Decoding: Critical Divergence of Kullback-Leibler\n  Information and Free Energy"
                },
                "summary": "We present a statistical mechanical framework based on the Kullback-Leibler\ndivergence (KLD) to analyze the relativistic limits of decoding time-encoded\ninformation from a moving source. By modeling the symbol durations as\nentropy-maximizing sequences and treating the decoding process as\ncontext-sensitive inference, we identify KLD between the sender and receiver\ndistributions as a key indicator of contextual mismatch. We show that, under\nLorentz transformations, this divergence grows with the sender's velocity and\nexhibits critical divergence as the velocity approaches the speed of light.\nFurthermore, we derive an analytic expression for the Fisher information and\ndemonstrate that decoding sensitivity diverges similarly, indicating\ninstability near the relativistic limit. By introducing an\ninformation-theoretic free energy based on the decoding cost, we determine a\ncritical velocity beyond which decoding becomes thermodynamically impossible.\nThese results reveal a phase-transition-like behavior in relativistic\ninformation transfer and provide a unified interpretation of KLD, Fisher\ninformation, and free energy as measures of decodability. The formalism\ndeveloped here offers new insights into high-speed communication, relativistic\nsignal processing, and information geometry in non-inertial frames.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a statistical mechanical framework based on the Kullback-Leibler\ndivergence (KLD) to analyze the relativistic limits of decoding time-encoded\ninformation from a moving source. By modeling the symbol durations as\nentropy-maximizing sequences and treating the decoding process as\ncontext-sensitive inference, we identify KLD between the sender and receiver\ndistributions as a key indicator of contextual mismatch. We show that, under\nLorentz transformations, this divergence grows with the sender's velocity and\nexhibits critical divergence as the velocity approaches the speed of light.\nFurthermore, we derive an analytic expression for the Fisher information and\ndemonstrate that decoding sensitivity diverges similarly, indicating\ninstability near the relativistic limit. By introducing an\ninformation-theoretic free energy based on the decoding cost, we determine a\ncritical velocity beyond which decoding becomes thermodynamically impossible.\nThese results reveal a phase-transition-like behavior in relativistic\ninformation transfer and provide a unified interpretation of KLD, Fisher\ninformation, and free energy as measures of decodability. The formalism\ndeveloped here offers new insights into high-speed communication, relativistic\nsignal processing, and information geometry in non-inertial frames."
                },
                "authors": [
                    {
                        "name": "Tatsuaki Tsuruyama"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuaki Tsuruyama"
                },
                "author": "Tatsuaki Tsuruyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07618v2",
                "updated": "2025-07-03T13:18:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    18,
                    23,
                    3,
                    184,
                    0
                ],
                "published": "2024-11-12T07:54:13Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    7,
                    54,
                    13,
                    1,
                    317,
                    0
                ],
                "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization Using Sparse Feature-Level Constraints"
                },
                "summary": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments."
                },
                "authors": [
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Linyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linyi Yang"
                },
                "author": "Linyi Yang",
                "arxiv_journal_ref": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01334v2",
                "updated": "2025-07-03T13:15:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    15,
                    11,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T03:51:16Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    3,
                    51,
                    16,
                    2,
                    183,
                    0
                ],
                "title": "Symbolic or Numerical? Understanding Physics Problem Solving in\n  Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic or Numerical? Understanding Physics Problem Solving in\n  Reasoning LLMs"
                },
                "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains."
                },
                "authors": [
                    {
                        "name": "Nifu Dan"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02595v1",
                "updated": "2025-07-03T13:09:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    9,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:09:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    9,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi\n  Perspective Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPF: Aligning and Debiasing Language Models post Deployment via Multi\n  Perspective Fusion"
                },
                "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning."
                },
                "authors": [
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "PeiHsin Lin"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Ruibo Zhang"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Koshiyama"
                },
                "author": "Adriano Koshiyama",
                "arxiv_comment": "Accepted at ICML 2025 AIW Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19688v3",
                "updated": "2025-07-03T13:07:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    7,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2024-11-29T13:22:52Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks"
                },
                "summary": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa."
                },
                "authors": [
                    {
                        "name": "Kim-Celine Kahl"
                    },
                    {
                        "name": "Selen Erkan"
                    },
                    {
                        "name": "Jeremias Traub"
                    },
                    {
                        "name": "Carsten T. Lth"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    },
                    {
                        "name": "Lena Maier-Hein"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    }
                ],
                "author_detail": {
                    "name": "Paul F. Jaeger"
                },
                "author": "Paul F. Jaeger",
                "arxiv_comment": "TMLR 07/2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12532v3",
                "updated": "2025-07-03T13:02:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    2,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-16T13:10:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based\n  Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based\n  Agent Collaboration"
                },
                "summary": "In healthcare intelligence, the ability to fuse heterogeneous, multi-intent\ninformation from diverse clinical sources is fundamental to building reliable\ndecision-making systems. Large Language Model (LLM)-driven information\ninteraction systems currently showing potential promise in the healthcare\ndomain. Nevertheless, they often suffer from information redundancy and\ncoupling when dealing with complex medical intents, leading to severe\nhallucinations and performance bottlenecks. To this end, we propose MedAide, an\nLLM-based medical multi-agent collaboration framework designed to enable\nintent-aware information fusion and coordinated reasoning across specialized\nhealthcare domains. Specifically, we introduce a regularization-guided module\nthat combines syntactic constraints with retrieval augmented generation to\ndecompose complex queries into structured representations, facilitating\nfine-grained clinical information fusion and intent resolution. Additionally, a\ndynamic intent prototype matching module is proposed to utilize dynamic\nprototype representation with a semantic similarity matching mechanism to\nachieve adaptive recognition and updating of the agent's intent in multi-round\nhealthcare dialogues. Ultimately, we design a rotation agent collaboration\nmechanism that introduces dynamic role rotation and decision-level information\nfusion across specialized medical agents. Extensive experiments are conducted\non four medical benchmarks with composite intents. Experimental results from\nautomated metrics and expert doctor evaluations show that MedAide outperforms\ncurrent LLMs and improves their medical proficiency and strategic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In healthcare intelligence, the ability to fuse heterogeneous, multi-intent\ninformation from diverse clinical sources is fundamental to building reliable\ndecision-making systems. Large Language Model (LLM)-driven information\ninteraction systems currently showing potential promise in the healthcare\ndomain. Nevertheless, they often suffer from information redundancy and\ncoupling when dealing with complex medical intents, leading to severe\nhallucinations and performance bottlenecks. To this end, we propose MedAide, an\nLLM-based medical multi-agent collaboration framework designed to enable\nintent-aware information fusion and coordinated reasoning across specialized\nhealthcare domains. Specifically, we introduce a regularization-guided module\nthat combines syntactic constraints with retrieval augmented generation to\ndecompose complex queries into structured representations, facilitating\nfine-grained clinical information fusion and intent resolution. Additionally, a\ndynamic intent prototype matching module is proposed to utilize dynamic\nprototype representation with a semantic similarity matching mechanism to\nachieve adaptive recognition and updating of the agent's intent in multi-round\nhealthcare dialogues. Ultimately, we design a rotation agent collaboration\nmechanism that introduces dynamic role rotation and decision-level information\nfusion across specialized medical agents. Extensive experiments are conducted\non four medical benchmarks with composite intents. Experimental results from\nautomated metrics and expert doctor evaluations show that MedAide outperforms\ncurrent LLMs and improves their medical proficiency and strategic reasoning."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Jiyao Liu"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yakun Ju"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "LLM-based Multi-Agent Collaboration for Medical Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02593v1",
                "updated": "2025-07-03T12:59:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:59:28Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    28,
                    3,
                    184,
                    0
                ],
                "title": "Revisiting Active Learning under (Human) Label Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Active Learning under (Human) Label Variation"
                },
                "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation."
                },
                "authors": [
                    {
                        "name": "Cornelia Gruber"
                    },
                    {
                        "name": "Helen Alber"
                    },
                    {
                        "name": "Bernd Bischl"
                    },
                    {
                        "name": "Gran Kauermann"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Aenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Aenmacher"
                },
                "author": "Matthias Aenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02592v1",
                "updated": "2025-07-03T12:59:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    7,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:59:07Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    7,
                    3,
                    184,
                    0
                ],
                "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebSailor: Navigating Super-human Reasoning for Web Agent"
                },
                "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap."
                },
                "authors": [
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Weizhou Shen"
                    },
                    {
                        "name": "Junkai Zhang"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02591v1",
                "updated": "2025-07-03T12:55:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    55,
                    16,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:55:16Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    55,
                    16,
                    3,
                    184,
                    0
                ],
                "title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video\n  Understanding"
                },
                "summary": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding."
                },
                "authors": [
                    {
                        "name": "Weili Xu"
                    },
                    {
                        "name": "Enxin Song"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Xuexiang Wen"
                    },
                    {
                        "name": "Tian Ye"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02585v1",
                "updated": "2025-07-03T12:45:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    45,
                    45,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:45:45Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    45,
                    45,
                    3,
                    184,
                    0
                ],
                "title": "Scalable Interconnect Learning in Boolean Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Interconnect Learning in Boolean Networks"
                },
                "summary": "Learned Differentiable Boolean Logic Networks (DBNs) already deliver\nefficient inference on resource-constrained hardware. We extend them with a\ntrainable, differentiable interconnect whose parameter count remains constant\nas input width grows, allowing DBNs to scale to far wider layers than earlier\nlearnable-interconnect designs while preserving their advantageous accuracy. To\nfurther reduce model size, we propose two complementary pruning stages: an\nSAT-based logic equivalence pass that removes redundant gates without affecting\nperformance, and a similarity-based, data-driven pass that outperforms a\nmagnitude-style greedy baseline and offers a superior compression-accuracy\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Differentiable Boolean Logic Networks (DBNs) already deliver\nefficient inference on resource-constrained hardware. We extend them with a\ntrainable, differentiable interconnect whose parameter count remains constant\nas input width grows, allowing DBNs to scale to far wider layers than earlier\nlearnable-interconnect designs while preserving their advantageous accuracy. To\nfurther reduce model size, we propose two complementary pruning stages: an\nSAT-based logic equivalence pass that removes redundant gates without affecting\nperformance, and a similarity-based, data-driven pass that outperforms a\nmagnitude-style greedy baseline and offers a superior compression-accuracy\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Fabian Kresse"
                    },
                    {
                        "name": "Emily Yu"
                    },
                    {
                        "name": "Christoph H. Lampert"
                    }
                ],
                "author_detail": {
                    "name": "Christoph H. Lampert"
                },
                "author": "Christoph H. Lampert",
                "arxiv_comment": "12 pages, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02583v1",
                "updated": "2025-07-03T12:44:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    44,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:44:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    44,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "Measuring black hole spins with X-ray reflection spectroscopy: A GRMHD\n  outlook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring black hole spins with X-ray reflection spectroscopy: A GRMHD\n  outlook"
                },
                "summary": "X-ray reflection spectroscopy has evolved as one of the leading methods to\nmeasure black hole spins. However, the question is whether its measurements are\nsubjected to systematic biases, especially considering the possible discrepancy\nbetween the spin measurements inferred with this technique and those from\ngravitational wave observations. In this work, we use general relativistic\nmagnetohydrodynamic (GRMHD) simulations of thin accretion disks around spinning\nblack holes for modeling the accretion process, and then we simulate NuSTAR\nobservations to test the capability of modern reflection models in recovering\nthe input spins. For the first time, we model the electron density and\nionization profiles from GRMHD-simulated disks. Our study reveals that current\nreflection models work well only for fast-rotating black holes. We model the\ncorona as the base of the jet and we find that reflection models with lamppost\nemissivity profiles fail to recover the correct black hole spins. Reflection\nmodels with broken power-law emissivity profiles perform better. As we increase\nthe complexity of the simulated models, it is more difficult to recover the\ncorrect input spins, pointing towards the need to update our current reflection\nmodels with more advanced accretion disks and coronal geometries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray reflection spectroscopy has evolved as one of the leading methods to\nmeasure black hole spins. However, the question is whether its measurements are\nsubjected to systematic biases, especially considering the possible discrepancy\nbetween the spin measurements inferred with this technique and those from\ngravitational wave observations. In this work, we use general relativistic\nmagnetohydrodynamic (GRMHD) simulations of thin accretion disks around spinning\nblack holes for modeling the accretion process, and then we simulate NuSTAR\nobservations to test the capability of modern reflection models in recovering\nthe input spins. For the first time, we model the electron density and\nionization profiles from GRMHD-simulated disks. Our study reveals that current\nreflection models work well only for fast-rotating black holes. We model the\ncorona as the base of the jet and we find that reflection models with lamppost\nemissivity profiles fail to recover the correct black hole spins. Reflection\nmodels with broken power-law emissivity profiles perform better. As we increase\nthe complexity of the simulated models, it is more difficult to recover the\ncorrect input spins, pointing towards the need to update our current reflection\nmodels with more advanced accretion disks and coronal geometries."
                },
                "authors": [
                    {
                        "name": "Swarnim Shashank"
                    },
                    {
                        "name": "Askar B. Abdikamalov"
                    },
                    {
                        "name": "Honghui Liu"
                    },
                    {
                        "name": "Abdurakhmon Nosirov"
                    },
                    {
                        "name": "Cosimo Bambi"
                    },
                    {
                        "name": "Indu K. Dihingia"
                    },
                    {
                        "name": "Yosuke Mizuno"
                    }
                ],
                "author_detail": {
                    "name": "Yosuke Mizuno"
                },
                "author": "Yosuke Mizuno",
                "arxiv_comment": "18 pages, 10 figures; supplemental material available upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02574v1",
                "updated": "2025-07-03T12:30:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    30,
                    3,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:30:03Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    30,
                    3,
                    3,
                    184,
                    0
                ],
                "title": "Learning and Testing Inverse Statistical Problems For Interacting\n  Systems Undergoing Phase Transition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning and Testing Inverse Statistical Problems For Interacting\n  Systems Undergoing Phase Transition"
                },
                "summary": "Inverse problems arise in situations where data is available, but the\nunderlying model is not. It can therefore be necessary to infer the parameters\nof the latter starting from the former. Statistical mechanics offers a toolbox\nof techniques to address this challenge. In this work, we illustrate three of\nthe main methods: the Maximum Likelihood, Maximum Pseudo-Likelihood, and\nMean-Field approaches. We begin with a thorough theoretical introduction to\nthese methods, followed by their application to inference in several well-known\nstatistical physics systems undergoing phase transitions. Namely, we consider\nthe ordered and disordered Ising models, the vector Potts model, and the\nBlume-Capel model on both regular lattices and random graphs. This discussion\nis accompanied by a GitHub repository that allows users to both reproduce the\nresults and experiment with new systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse problems arise in situations where data is available, but the\nunderlying model is not. It can therefore be necessary to infer the parameters\nof the latter starting from the former. Statistical mechanics offers a toolbox\nof techniques to address this challenge. In this work, we illustrate three of\nthe main methods: the Maximum Likelihood, Maximum Pseudo-Likelihood, and\nMean-Field approaches. We begin with a thorough theoretical introduction to\nthese methods, followed by their application to inference in several well-known\nstatistical physics systems undergoing phase transitions. Namely, we consider\nthe ordered and disordered Ising models, the vector Potts model, and the\nBlume-Capel model on both regular lattices and random graphs. This discussion\nis accompanied by a GitHub repository that allows users to both reproduce the\nresults and experiment with new systems."
                },
                "authors": [
                    {
                        "name": "Stefano Bae"
                    },
                    {
                        "name": "Dario Bocchi"
                    },
                    {
                        "name": "Luca Maria Del Bono"
                    },
                    {
                        "name": "Luca Leuzzi"
                    }
                ],
                "author_detail": {
                    "name": "Luca Leuzzi"
                },
                "author": "Luca Leuzzi",
                "arxiv_comment": "34 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06285v2",
                "updated": "2025-07-03T12:26:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    26,
                    23,
                    3,
                    184,
                    0
                ],
                "published": "2023-03-11T02:38:31Z",
                "published_parsed": [
                    2023,
                    3,
                    11,
                    2,
                    38,
                    31,
                    5,
                    70,
                    0
                ],
                "title": "DeltaEdit: Exploring Text-free Training for Text-Driven Image\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaEdit: Exploring Text-free Training for Text-Driven Image\n  Manipulation"
                },
                "summary": "Text-driven image manipulation remains challenging in training or inference\nflexibility. Conditional generative models depend heavily on expensive\nannotated training data. Meanwhile, recent frameworks, which leverage\npre-trained vision-language models, are limited by either per text-prompt\noptimization or inference-time hyper-parameters tuning. In this work, we\npropose a novel framework named \\textit{DeltaEdit} to address these problems.\nOur key idea is to investigate and identify a space, namely delta image and\ntext space that has well-aligned distribution between CLIP visual feature\ndifferences of two images and CLIP textual embedding differences of source and\ntarget texts. Based on the CLIP delta space, the DeltaEdit network is designed\nto map the CLIP visual features differences to the editing directions of\nStyleGAN at training phase. Then, in inference phase, DeltaEdit predicts the\nStyleGAN's editing directions from the differences of the CLIP textual\nfeatures. In this way, DeltaEdit is trained in a text-free manner. Once\ntrained, it can well generalize to various text prompts for zero-shot inference\nwithout bells and whistles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-driven image manipulation remains challenging in training or inference\nflexibility. Conditional generative models depend heavily on expensive\nannotated training data. Meanwhile, recent frameworks, which leverage\npre-trained vision-language models, are limited by either per text-prompt\noptimization or inference-time hyper-parameters tuning. In this work, we\npropose a novel framework named \\textit{DeltaEdit} to address these problems.\nOur key idea is to investigate and identify a space, namely delta image and\ntext space that has well-aligned distribution between CLIP visual feature\ndifferences of two images and CLIP textual embedding differences of source and\ntarget texts. Based on the CLIP delta space, the DeltaEdit network is designed\nto map the CLIP visual features differences to the editing directions of\nStyleGAN at training phase. Then, in inference phase, DeltaEdit predicts the\nStyleGAN's editing directions from the differences of the CLIP textual\nfeatures. In this way, DeltaEdit is trained in a text-free manner. Once\ntrained, it can well generalize to various text prompts for zero-shot inference\nwithout bells and whistles."
                },
                "authors": [
                    {
                        "name": "Yueming Lyu"
                    },
                    {
                        "name": "Tianwei Lin"
                    },
                    {
                        "name": "Fu Li"
                    },
                    {
                        "name": "Dongliang He"
                    },
                    {
                        "name": "Jing Dong"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Code is available at https://github.com/Yueming6568/DeltaEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20808v2",
                "updated": "2025-07-03T12:26:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    26,
                    15,
                    3,
                    184,
                    0
                ],
                "published": "2025-04-29T14:21:08Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    21,
                    8,
                    1,
                    119,
                    0
                ],
                "title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from\n  Gameplay Recordings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from\n  Gameplay Recordings"
                },
                "summary": "This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion"
                },
                "authors": [
                    {
                        "name": "Florian Vahl"
                    },
                    {
                        "name": "Jrn Griepenburg"
                    },
                    {
                        "name": "Jan Gutsche"
                    },
                    {
                        "name": "Jasper Gldenstein"
                    },
                    {
                        "name": "Jianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Zhang"
                },
                "author": "Jianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02564v1",
                "updated": "2025-07-03T12:18:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    18,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:18:05Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    18,
                    5,
                    3,
                    184,
                    0
                ],
                "title": "LLMREI: Automating Requirements Elicitation Interviews with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMREI: Automating Requirements Elicitation Interviews with LLMs"
                },
                "summary": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders."
                },
                "authors": [
                    {
                        "name": "Alexander Korn"
                    },
                    {
                        "name": "Samuel Gorsch"
                    },
                    {
                        "name": "Andreas Vogelsang"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vogelsang"
                },
                "author": "Andreas Vogelsang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02562v1",
                "updated": "2025-07-03T12:12:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    12,
                    34,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:12:34Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    12,
                    34,
                    3,
                    184,
                    0
                ],
                "title": "Multi-Utterance Speech Separation and Association Trained on Short\n  Segments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Utterance Speech Separation and Association Trained on Short\n  Segments"
                },
                "summary": "Current deep neural network (DNN) based speech separation faces a fundamental\nchallenge -- while the models need to be trained on short segments due to\ncomputational constraints, real-world applications typically require processing\nsignificantly longer recordings with multiple utterances per speaker than seen\nduring training. In this paper, we investigate how existing approaches perform\nin this challenging scenario and propose a frequency-temporal recurrent neural\nnetwork (FTRNN) that effectively bridges this gap. Our FTRNN employs a\nfull-band module to model frequency dependencies within each time frame and a\nsub-band module that models temporal patterns in each frequency band. Despite\nbeing trained on short fixed-length segments of 10 s, our model demonstrates\nrobust separation when processing signals significantly longer than training\nsegments (21-121 s) and preserves speaker association across utterance gaps\nexceeding those seen during training. Unlike the conventional\nsegment-separation-stitch paradigm, our lightweight approach (0.9 M parameters)\nperforms inference on long audio without segmentation, eliminating segment\nboundary distortions while simplifying deployment. Experimental results\ndemonstrate the generalization ability of FTRNN for multi-utterance speech\nseparation and speaker association.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current deep neural network (DNN) based speech separation faces a fundamental\nchallenge -- while the models need to be trained on short segments due to\ncomputational constraints, real-world applications typically require processing\nsignificantly longer recordings with multiple utterances per speaker than seen\nduring training. In this paper, we investigate how existing approaches perform\nin this challenging scenario and propose a frequency-temporal recurrent neural\nnetwork (FTRNN) that effectively bridges this gap. Our FTRNN employs a\nfull-band module to model frequency dependencies within each time frame and a\nsub-band module that models temporal patterns in each frequency band. Despite\nbeing trained on short fixed-length segments of 10 s, our model demonstrates\nrobust separation when processing signals significantly longer than training\nsegments (21-121 s) and preserves speaker association across utterance gaps\nexceeding those seen during training. Unlike the conventional\nsegment-separation-stitch paradigm, our lightweight approach (0.9 M parameters)\nperforms inference on long audio without segmentation, eliminating segment\nboundary distortions while simplifying deployment. Experimental results\ndemonstrate the generalization ability of FTRNN for multi-utterance speech\nseparation and speaker association."
                },
                "authors": [
                    {
                        "name": "Yuzhu Wang"
                    },
                    {
                        "name": "Archontis Politis"
                    },
                    {
                        "name": "Konstantinos Drossos"
                    },
                    {
                        "name": "Tuomas Virtanen"
                    }
                ],
                "author_detail": {
                    "name": "Tuomas Virtanen"
                },
                "author": "Tuomas Virtanen",
                "arxiv_comment": "5 pages, accepted by WASPAA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12479v2",
                "updated": "2025-07-03T12:11:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    11,
                    34,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-14T12:43:07Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    12,
                    43,
                    7,
                    5,
                    165,
                    0
                ],
                "title": "AI Flow: Perspectives, Scenarios, and Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Flow: Perspectives, Scenarios, and Approaches"
                },
                "summary": "Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems."
                },
                "authors": [
                    {
                        "name": "Hongjun An"
                    },
                    {
                        "name": "Wenhan Hu"
                    },
                    {
                        "name": "Sida Huang"
                    },
                    {
                        "name": "Siqi Huang"
                    },
                    {
                        "name": "Ruanjun Li"
                    },
                    {
                        "name": "Yuanzhi Liang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yiliang Song"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Cheng Yuan"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "Authors are with Institute of Artificial Intelligence (TeleAI), China\n  Telecom, China. Author names are listed alphabetically by surname. This work\n  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:\n  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The\n  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the\n  CTO and Chief Scientist of China Telecom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02559v1",
                "updated": "2025-07-03T12:09:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    9,
                    4,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:09:04Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    9,
                    4,
                    3,
                    184,
                    0
                ],
                "title": "Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm\n  Removal to GPT-2 XL and the Implications for Mechanistic Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm\n  Removal to GPT-2 XL and the Implications for Mechanistic Interpretability"
                },
                "summary": "Layer-wise normalization (LN) is an essential component of virtually all\ntransformer-based large language models. While its effects on training\nstability are well documented, its role at inference time is poorly understood.\nAdditionally, LN layers hinder mechanistic interpretability by introducing\nadditional nonlinearities and increasing the interconnectedness of individual\nmodel components. Here, we show that all LN layers can be removed from every\nGPT-2 model with only a small increase in validation loss (e.g. +0.03\ncross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in\nlanguage modeling. We find that the amount of fine-tuning data needed for LN\nremoval grows sublinearly with model parameters, suggesting scaling to larger\nmodels is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.\nFurthermore, we test interpretability techniques on LN-free models. Direct\nlogit attribution now gives the exact direct effect of individual components,\nwhile the accuracy of attribution patching does not significantly improve. We\nalso confirm that GPT-2's \"confidence neurons\" are inactive in the LN-free\nmodels. Our work clarifies the role of LN layers in language modeling, showing\nthat GPT-2-class models can function without LN layers. We hope that our\nLN-free analogs of the GPT-2 family of models will enable more precise\ninterpretability research and improve our understanding of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-wise normalization (LN) is an essential component of virtually all\ntransformer-based large language models. While its effects on training\nstability are well documented, its role at inference time is poorly understood.\nAdditionally, LN layers hinder mechanistic interpretability by introducing\nadditional nonlinearities and increasing the interconnectedness of individual\nmodel components. Here, we show that all LN layers can be removed from every\nGPT-2 model with only a small increase in validation loss (e.g. +0.03\ncross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in\nlanguage modeling. We find that the amount of fine-tuning data needed for LN\nremoval grows sublinearly with model parameters, suggesting scaling to larger\nmodels is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.\nFurthermore, we test interpretability techniques on LN-free models. Direct\nlogit attribution now gives the exact direct effect of individual components,\nwhile the accuracy of attribution patching does not significantly improve. We\nalso confirm that GPT-2's \"confidence neurons\" are inactive in the LN-free\nmodels. Our work clarifies the role of LN layers in language modeling, showing\nthat GPT-2-class models can function without LN layers. We hope that our\nLN-free analogs of the GPT-2 family of models will enable more precise\ninterpretability research and improve our understanding of language models."
                },
                "authors": [
                    {
                        "name": "Luca Baroni"
                    },
                    {
                        "name": "Galvin Khara"
                    },
                    {
                        "name": "Joachim Schaeffer"
                    },
                    {
                        "name": "Marat Subkhankulov"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13450v2",
                "updated": "2025-07-03T12:02:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    2,
                    1,
                    3,
                    184,
                    0
                ],
                "published": "2025-02-19T05:51:24Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    5,
                    51,
                    24,
                    2,
                    50,
                    0
                ],
                "title": "Interleaved Gibbs Diffusion: Generating Discrete-Continuous Data with\n  Implicit Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved Gibbs Diffusion: Generating Discrete-Continuous Data with\n  Implicit Constraints"
                },
                "summary": "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for discrete-continuous data, focusing on problems with important,\nimplicit and unspecified constraints in the data. Most prior works on discrete\nand discrete-continuous diffusion assume a factorized denoising distribution,\nwhich can hinder the modeling of strong dependencies between random variables\nin such problems. We empirically demonstrate a significant improvement in 3-SAT\nperformance out of the box by switching to a Gibbs-sampling style discrete\ndiffusion model which does not assume factorizability. Motivated by this, we\nintroduce IGD which generalizes discrete time Gibbs sampling type Markov chain\nfor the case of discrete-continuous generation. IGD allows for seamless\nintegration between discrete and continuous denoisers while theoretically\nguaranteeing exact reversal of a suitable forward process. Further, it provides\nflexibility in the choice of denoisers, allows conditional generation via\nstate-space doubling and inference time refinement. Empirical evaluations on\nthree challenging generation tasks - molecule structures, layouts and tabular\ndata - demonstrate state-of-the-art performance. Notably, IGD achieves\nstate-of-the-art results without relying on domain-specific inductive biases\nlike equivariant diffusion or auxiliary losses. We explore a wide range of\nmodeling, and interleaving strategies along with hyperparameters in each of\nthese problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for discrete-continuous data, focusing on problems with important,\nimplicit and unspecified constraints in the data. Most prior works on discrete\nand discrete-continuous diffusion assume a factorized denoising distribution,\nwhich can hinder the modeling of strong dependencies between random variables\nin such problems. We empirically demonstrate a significant improvement in 3-SAT\nperformance out of the box by switching to a Gibbs-sampling style discrete\ndiffusion model which does not assume factorizability. Motivated by this, we\nintroduce IGD which generalizes discrete time Gibbs sampling type Markov chain\nfor the case of discrete-continuous generation. IGD allows for seamless\nintegration between discrete and continuous denoisers while theoretically\nguaranteeing exact reversal of a suitable forward process. Further, it provides\nflexibility in the choice of denoisers, allows conditional generation via\nstate-space doubling and inference time refinement. Empirical evaluations on\nthree challenging generation tasks - molecule structures, layouts and tabular\ndata - demonstrate state-of-the-art performance. Notably, IGD achieves\nstate-of-the-art results without relying on domain-specific inductive biases\nlike equivariant diffusion or auxiliary losses. We explore a wide range of\nmodeling, and interleaving strategies along with hyperparameters in each of\nthese problems."
                },
                "authors": [
                    {
                        "name": "Gautham Govind Anil"
                    },
                    {
                        "name": "Sachin Yadav"
                    },
                    {
                        "name": "Dheeraj Nagaraj"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08500v2",
                "updated": "2025-07-03T11:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    55,
                    43,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-11T03:54:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    3,
                    54,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Aerial Vision-and-Language Navigation via Semantic-Topo-Metric\n  Representation Guided LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial Vision-and-Language Navigation via Semantic-Topo-Metric\n  Representation Guided LLM Reasoning"
                },
                "summary": "Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned\nAerial Vehicles (UAVs) to navigate in outdoor environments through natural\nlanguage instructions and visual cues. It remains challenging due to the\ncomplex spatial relationships in outdoor aerial scenes. In this paper, we\npropose an end-to-end zero-shot framework for aerial VLN tasks, where the large\nlanguage model (LLM) is introduced as our agent for action prediction.\nSpecifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to\nenhance the spatial reasoning ability of LLMs. This is achieved by extracting\nand projecting instruction-related semantic masks of landmarks into a top-down\nmap that contains the location information of surrounding landmarks. Further,\nthis map is transformed into a matrix representation with distance metrics as\nthe text prompt to the LLM, for action prediction according to the instruction.\nExperiments conducted in real and simulation environments have successfully\nproved the effectiveness and robustness of our method, achieving 15.9% and\n12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned\nAerial Vehicles (UAVs) to navigate in outdoor environments through natural\nlanguage instructions and visual cues. It remains challenging due to the\ncomplex spatial relationships in outdoor aerial scenes. In this paper, we\npropose an end-to-end zero-shot framework for aerial VLN tasks, where the large\nlanguage model (LLM) is introduced as our agent for action prediction.\nSpecifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to\nenhance the spatial reasoning ability of LLMs. This is achieved by extracting\nand projecting instruction-related semantic masks of landmarks into a top-down\nmap that contains the location information of surrounding landmarks. Further,\nthis map is transformed into a matrix representation with distance metrics as\nthe text prompt to the LLM, for action prediction according to the instruction.\nExperiments conducted in real and simulation environments have successfully\nproved the effectiveness and robustness of our method, achieving 15.9% and\n12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S\ndataset."
                },
                "authors": [
                    {
                        "name": "Yunpeng Gao"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Linglin Jing"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Bin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhao"
                },
                "author": "Bin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02541v1",
                "updated": "2025-07-03T11:35:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    35,
                    34,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:35:34Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    35,
                    34,
                    3,
                    184,
                    0
                ],
                "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clarifying Before Reasoning: A Coq Prover with Structural Context"
                },
                "summary": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning."
                },
                "authors": [
                    {
                        "name": "Yanzhen Lu"
                    },
                    {
                        "name": "Hanbin Yang"
                    },
                    {
                        "name": "Xiaodie Wang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Biao Li"
                    },
                    {
                        "name": "Chenxu Fu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02537v1",
                "updated": "2025-07-03T11:32:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    32,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:32:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    32,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue"
                },
                "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents."
                },
                "authors": [
                    {
                        "name": "Paulo Ricardo Knob"
                    },
                    {
                        "name": "Leonardo Scholler"
                    },
                    {
                        "name": "Juliano Rigatti"
                    },
                    {
                        "name": "Soraia Raupp Musse"
                    }
                ],
                "author_detail": {
                    "name": "Soraia Raupp Musse"
                },
                "author": "Soraia Raupp Musse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02533v1",
                "updated": "2025-07-03T11:20:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    20,
                    59,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:20:59Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    20,
                    59,
                    3,
                    184,
                    0
                ],
                "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models"
                },
                "summary": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing."
                },
                "authors": [
                    {
                        "name": "Miguel Romero-Arjona"
                    },
                    {
                        "name": "Jos A. Parejo"
                    },
                    {
                        "name": "Juan C. Alonso"
                    },
                    {
                        "name": "Ana B. Snchez"
                    },
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02530v1",
                "updated": "2025-07-03T11:12:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    12,
                    26,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:12:26Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    12,
                    26,
                    3,
                    184,
                    0
                ],
                "title": "Open-Source System for Multilingual Translation and Cloned Speech\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source System for Multilingual Translation and Cloned Speech\n  Synthesis"
                },
                "summary": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios."
                },
                "authors": [
                    {
                        "name": "Mateo Cmara"
                    },
                    {
                        "name": "Juan Gutirrez"
                    },
                    {
                        "name": "Mara Pilar Daza"
                    },
                    {
                        "name": "Jos Luis Blanco"
                    }
                ],
                "author_detail": {
                    "name": "Jos Luis Blanco"
                },
                "author": "Jos Luis Blanco",
                "arxiv_comment": "Presented at Forum Acusticum Euronoise 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15075v2",
                "updated": "2025-07-03T10:35:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    35,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-21T03:43:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    3,
                    43,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs"
                },
                "summary": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Pinzhi Huang"
                    },
                    {
                        "name": "Jihan Yang"
                    },
                    {
                        "name": "Saining Xie"
                    },
                    {
                        "name": "Daisuke Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Daisuke Kawahara"
                },
                "author": "Daisuke Kawahara",
                "arxiv_comment": "https://github.com/nlp-waseda/traveling-across-languages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01551v2",
                "updated": "2025-07-03T10:33:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    33,
                    8,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:05:14Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    5,
                    14,
                    2,
                    183,
                    0
                ],
                "title": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning"
                },
                "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."
                },
                "authors": [
                    {
                        "name": "Wu Fei"
                    },
                    {
                        "name": "Hao Kong"
                    },
                    {
                        "name": "Shuxian Liang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiansheng Hua"
                    }
                ],
                "author_detail": {
                    "name": "Xiansheng Hua"
                },
                "author": "Xiansheng Hua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02515v1",
                "updated": "2025-07-03T10:21:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    21,
                    53,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T10:21:53Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    21,
                    53,
                    3,
                    184,
                    0
                ],
                "title": "Angular correlation functions of bright Lyman-break galaxies at\n  $\\mathbf{3 \\lesssim z \\lesssim 5}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Angular correlation functions of bright Lyman-break galaxies at\n  $\\mathbf{3 \\lesssim z \\lesssim 5}$"
                },
                "summary": "We investigate the clustering of Lyman-break galaxies at redshifts of 3\n$\\lesssim z \\lesssim$ 5 within the COSMOS field by measuring the angular\ntwo-point correlation function. Our robust sample of $\\sim$60,000 bright\n($m_{\\rm UV}\\lesssim 27$) Lyman-break galaxies was selected based on spectral\nenergy distribution fitting across 14 photometric bands spanning optical and\nnear-infrared wavelengths. We constrained both the 1- and 2-halo terms at\nseparations up to 300 arcsec, finding an excess in the correlation function at\nscales corresponding to $<20$ kpc, consistent with enhancement due to clumps in\nthe same galaxy or interactions on this scale. We then performed Bayesian model\nfits on the correlation functions to infer the Halo Occupation Distribution\nparameters, star formation duty cycle, and galaxy bias in three redshift bins.\nWe examined several cases where different combinations of parameters were\nvaried, showing that our data can constrain the slope of the satellite\noccupation function, which previous studies have fixed. For an\n$M_{\\rm{UV}}$-limited sub-sample, we found galaxy bias values of\n$b_g=3.18^{+0.14}_{-0.14}$ at $z\\simeq3$, $b_g=3.58^{+0.27}_{-0.29}$ at\n$z\\simeq4$, $b_g=4.27^{+0.25}_{-0.26}$ at $z\\simeq5$. The duty cycle values are\n$0.62^{+0.25}_{-0.26}$, $0.40^{+0.34}_{-0.22}$, and $0.39^{+0.31}_{-0.20}$,\nrespectively. These results suggest that, as the redshift increases, there is a\nslight decrease in the host halo masses and a shorter timescale for star\nformation in bright galaxies, at a fixed rest-frame UV luminosity threshold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the clustering of Lyman-break galaxies at redshifts of 3\n$\\lesssim z \\lesssim$ 5 within the COSMOS field by measuring the angular\ntwo-point correlation function. Our robust sample of $\\sim$60,000 bright\n($m_{\\rm UV}\\lesssim 27$) Lyman-break galaxies was selected based on spectral\nenergy distribution fitting across 14 photometric bands spanning optical and\nnear-infrared wavelengths. We constrained both the 1- and 2-halo terms at\nseparations up to 300 arcsec, finding an excess in the correlation function at\nscales corresponding to $<20$ kpc, consistent with enhancement due to clumps in\nthe same galaxy or interactions on this scale. We then performed Bayesian model\nfits on the correlation functions to infer the Halo Occupation Distribution\nparameters, star formation duty cycle, and galaxy bias in three redshift bins.\nWe examined several cases where different combinations of parameters were\nvaried, showing that our data can constrain the slope of the satellite\noccupation function, which previous studies have fixed. For an\n$M_{\\rm{UV}}$-limited sub-sample, we found galaxy bias values of\n$b_g=3.18^{+0.14}_{-0.14}$ at $z\\simeq3$, $b_g=3.58^{+0.27}_{-0.29}$ at\n$z\\simeq4$, $b_g=4.27^{+0.25}_{-0.26}$ at $z\\simeq5$. The duty cycle values are\n$0.62^{+0.25}_{-0.26}$, $0.40^{+0.34}_{-0.22}$, and $0.39^{+0.31}_{-0.20}$,\nrespectively. These results suggest that, as the redshift increases, there is a\nslight decrease in the host halo masses and a shorter timescale for star\nformation in bright galaxies, at a fixed rest-frame UV luminosity threshold."
                },
                "authors": [
                    {
                        "name": "Isabelle Ye"
                    },
                    {
                        "name": "Philip Bull"
                    },
                    {
                        "name": "Rebecca A. A. Bowler"
                    },
                    {
                        "name": "Rachel K. Cochrane"
                    },
                    {
                        "name": "Nathan J. Adams"
                    },
                    {
                        "name": "Matt J. Jarvis"
                    }
                ],
                "author_detail": {
                    "name": "Matt J. Jarvis"
                },
                "author": "Matt J. Jarvis",
                "arxiv_comment": "18 pages, 12 figures, 3 tables. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02503v1",
                "updated": "2025-07-03T10:11:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    11,
                    22,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T10:11:22Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    11,
                    22,
                    3,
                    184,
                    0
                ],
                "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs"
                },
                "summary": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP."
                },
                "authors": [
                    {
                        "name": "Chenxu Wang"
                    },
                    {
                        "name": "Yilin Lyu"
                    },
                    {
                        "name": "Zicheng Sun"
                    },
                    {
                        "name": "Liping Jing"
                    }
                ],
                "author_detail": {
                    "name": "Liping Jing"
                },
                "author": "Liping Jing",
                "arxiv_comment": "15 pages, 6 figures, accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20127v2",
                "updated": "2025-07-03T10:05:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    5,
                    46,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-26T15:26:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    26,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "Agentic AI Process Observability: Discovering Behavioral Variability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Process Observability: Discovering Behavioral Variability"
                },
                "summary": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions."
                },
                "authors": [
                    {
                        "name": "Fabiana Fournier"
                    },
                    {
                        "name": "Lior Limonad"
                    },
                    {
                        "name": "Yuval David"
                    }
                ],
                "author_detail": {
                    "name": "Yuval David"
                },
                "author": "Yuval David",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01119v3",
                "updated": "2025-07-03T09:32:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    32,
                    20,
                    3,
                    184,
                    0
                ],
                "published": "2024-08-02T09:00:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    0,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer"
                },
                "summary": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks."
                },
                "authors": [
                    {
                        "name": "Robert Belanec"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Maria Bielikova"
                    }
                ],
                "author_detail": {
                    "name": "Maria Bielikova"
                },
                "author": "Maria Bielikova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02466v1",
                "updated": "2025-07-03T09:24:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    24,
                    9,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T09:24:09Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    24,
                    9,
                    3,
                    184,
                    0
                ],
                "title": "Variational Kolmogorov-Arnold Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Kolmogorov-Arnold Network"
                },
                "summary": "Kolmogorov Arnold Networks (KANs) are an emerging architecture for building\nmachine learning models. KANs are based on the theoretical foundation of the\nKolmogorov-Arnold Theorem and its expansions, which provide an exact\nrepresentation of a multi-variate continuous bounded function as the\ncomposition of a limited number of univariate continuous functions. While such\ntheoretical results are powerful, their use as a representation learning\nalternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of\nthe number of bases modeling each of the univariate functions. In this work, we\nshow how to address this problem by adaptively learning a potentially infinite\nnumber of bases for each univariate function during training. We therefore\nmodel the problem as a variational inference optimization problem. Our\nproposal, called InfinityKAN, which uses backpropagation, extends the potential\napplicability of KANs by treating an important hyperparameter as part of the\nlearning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolmogorov Arnold Networks (KANs) are an emerging architecture for building\nmachine learning models. KANs are based on the theoretical foundation of the\nKolmogorov-Arnold Theorem and its expansions, which provide an exact\nrepresentation of a multi-variate continuous bounded function as the\ncomposition of a limited number of univariate continuous functions. While such\ntheoretical results are powerful, their use as a representation learning\nalternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of\nthe number of bases modeling each of the univariate functions. In this work, we\nshow how to address this problem by adaptively learning a potentially infinite\nnumber of bases for each univariate function during training. We therefore\nmodel the problem as a variational inference optimization problem. Our\nproposal, called InfinityKAN, which uses backpropagation, extends the potential\napplicability of KANs by treating an important hyperparameter as part of the\nlearning process."
                },
                "authors": [
                    {
                        "name": "Francesco Alesiani"
                    },
                    {
                        "name": "Henrik Christiansen"
                    },
                    {
                        "name": "Federico Errica"
                    }
                ],
                "author_detail": {
                    "name": "Federico Errica"
                },
                "author": "Federico Errica",
                "arxiv_comment": "A preliminary (short paper) version presented at ComBayNS Workshop at\n  IJCNN'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05007v2",
                "updated": "2025-07-03T09:15:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    15,
                    45,
                    3,
                    184,
                    0
                ],
                "published": "2025-01-09T07:05:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    5,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "Quantum-enhanced causal discovery for a small number of samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-enhanced causal discovery for a small number of samples"
                },
                "summary": "The discovery of causal relations from observed data has attracted\nsignificant interest from disciplines such as economics, social sciences, and\nbiology. In practical applications, considerable knowledge of the underlying\nsystems is often unavailable, and real data are usually associated with\nnonlinear causal structures, which makes the direct use of most conventional\ncausality analysis methods difficult. This study proposes a novel quantum\nPeter-Clark (qPC) algorithm for causal discovery that does not require any\nassumptions about the underlying model structures. Based on conditional\nindependence tests in a class of reproducing kernel Hilbert spaces\ncharacterized by quantum circuits, the proposed algorithm can explore causal\nrelations from the observed data drawn from arbitrary distributions. We\nconducted systematic experiments on fundamental graphs of causal structures,\ndemonstrating that the qPC algorithm exhibits better performance, particularly\nwith smaller sample sizes compared to its classical counterpart. Furthermore,\nwe proposed a novel optimization approach based on Kernel Target Alignment\n(KTA) for determining hyperparameters of quantum kernels. This method\neffectively reduced the risk of false positives in causal discovery, enabling\nmore reliable inference. Our theoretical and experimental results demonstrate\nthat the quantum algorithm can empower classical algorithms for accurate\ninference in causal discovery, supporting them in regimes where classical\nalgorithms typically fail. In addition, the effectiveness of this method was\nvalidated using the datasets on Boston housing prices, heart disease, and\nbiological signaling systems as real-world applications. These findings\nhighlight the potential of quantum-based causal discovery methods in addressing\npractical challenges, particularly in small-sample scenarios, where traditional\napproaches have shown significant limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of causal relations from observed data has attracted\nsignificant interest from disciplines such as economics, social sciences, and\nbiology. In practical applications, considerable knowledge of the underlying\nsystems is often unavailable, and real data are usually associated with\nnonlinear causal structures, which makes the direct use of most conventional\ncausality analysis methods difficult. This study proposes a novel quantum\nPeter-Clark (qPC) algorithm for causal discovery that does not require any\nassumptions about the underlying model structures. Based on conditional\nindependence tests in a class of reproducing kernel Hilbert spaces\ncharacterized by quantum circuits, the proposed algorithm can explore causal\nrelations from the observed data drawn from arbitrary distributions. We\nconducted systematic experiments on fundamental graphs of causal structures,\ndemonstrating that the qPC algorithm exhibits better performance, particularly\nwith smaller sample sizes compared to its classical counterpart. Furthermore,\nwe proposed a novel optimization approach based on Kernel Target Alignment\n(KTA) for determining hyperparameters of quantum kernels. This method\neffectively reduced the risk of false positives in causal discovery, enabling\nmore reliable inference. Our theoretical and experimental results demonstrate\nthat the quantum algorithm can empower classical algorithms for accurate\ninference in causal discovery, supporting them in regimes where classical\nalgorithms typically fail. In addition, the effectiveness of this method was\nvalidated using the datasets on Boston housing prices, heart disease, and\nbiological signaling systems as real-world applications. These findings\nhighlight the potential of quantum-based causal discovery methods in addressing\npractical challenges, particularly in small-sample scenarios, where traditional\napproaches have shown significant limitations."
                },
                "authors": [
                    {
                        "name": "Yu Terada"
                    },
                    {
                        "name": "Ken Arai"
                    },
                    {
                        "name": "Yu Tanaka"
                    },
                    {
                        "name": "Yota Maeda"
                    },
                    {
                        "name": "Hiroshi Ueno"
                    },
                    {
                        "name": "Hiroyuki Tezuka"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Tezuka"
                },
                "author": "Hiroyuki Tezuka",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02456v1",
                "updated": "2025-07-03T09:13:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    13,
                    31,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T09:13:31Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    13,
                    31,
                    3,
                    184,
                    0
                ],
                "title": "System-performance and cost modeling of Large Language Model training\n  and inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-performance and cost modeling of Large Language Model training\n  and inference"
                },
                "summary": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations."
                },
                "authors": [
                    {
                        "name": "Wenzhe Guo"
                    },
                    {
                        "name": "Joyjit Kundu"
                    },
                    {
                        "name": "Uras Tos"
                    },
                    {
                        "name": "Weijiang Kong"
                    },
                    {
                        "name": "Giuliano Sisto"
                    },
                    {
                        "name": "Timon Evenblij"
                    },
                    {
                        "name": "Manu Perumkunnil"
                    }
                ],
                "author_detail": {
                    "name": "Manu Perumkunnil"
                },
                "author": "Manu Perumkunnil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02450v1",
                "updated": "2025-07-03T09:07:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    7,
                    59,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T09:07:59Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    7,
                    59,
                    3,
                    184,
                    0
                ],
                "title": "Network structural change point detection and reconstruction for\n  balanced neuronal networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network structural change point detection and reconstruction for\n  balanced neuronal networks"
                },
                "summary": "Understanding brain dynamics and functions critically depends on knowledge of\nthe network connectivity among neurons. However, the complexity of brain\nstructural connectivity, coupled with continuous modifications driven by\nsynaptic plasticity, makes its direct experimental measurement particularly\nchallenging. Conventional connectivity inference methods based on neuronal\nrecordings often assumes a static underlying structural connectivity and\nrequires stable statistical features of neural activities, making them\nunsuitable for reconstructing structural connectivity that undergoes changes.\nTo fulfill the needs of reconstructing networks undergoing potential structural\nchanges, we propose a unified network reconstruction framework that combines\nconnectivity-induced change point detection (CPD) with pairwise time-delayed\ncorrelation coefficient (TDCC) method. For general neuronal networks in\nbalanced regimes, we develop a theoretical analysis for discriminating changes\nin structural connectivity based on the fluctuation of neuronal voltage time\nseries. We then demonstrate a pairwise TDCC method to reconstruct the network\nusing spike train recordings segmented at the detected change points. We show\nthe effectiveness of our CPD-TDCC network reconstruction using large-scale\nnetwork simulations with multiple neuronal models. Crucially, our method\naccommodates networks with changes in both network topologies and synaptic\ncoupling strengths while retaining accuracy even with sparsely sampled\nsubnetwork data, achieving a critical advancement for practical applications in\nreal experimental situations. Our CPD-TDCC framework addresses the critical gap\nin network reconstruction by accounting connectivity-induced changes points,\npotentially offering a valuable tool for studying structure and dynamics in the\ncortical brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding brain dynamics and functions critically depends on knowledge of\nthe network connectivity among neurons. However, the complexity of brain\nstructural connectivity, coupled with continuous modifications driven by\nsynaptic plasticity, makes its direct experimental measurement particularly\nchallenging. Conventional connectivity inference methods based on neuronal\nrecordings often assumes a static underlying structural connectivity and\nrequires stable statistical features of neural activities, making them\nunsuitable for reconstructing structural connectivity that undergoes changes.\nTo fulfill the needs of reconstructing networks undergoing potential structural\nchanges, we propose a unified network reconstruction framework that combines\nconnectivity-induced change point detection (CPD) with pairwise time-delayed\ncorrelation coefficient (TDCC) method. For general neuronal networks in\nbalanced regimes, we develop a theoretical analysis for discriminating changes\nin structural connectivity based on the fluctuation of neuronal voltage time\nseries. We then demonstrate a pairwise TDCC method to reconstruct the network\nusing spike train recordings segmented at the detected change points. We show\nthe effectiveness of our CPD-TDCC network reconstruction using large-scale\nnetwork simulations with multiple neuronal models. Crucially, our method\naccommodates networks with changes in both network topologies and synaptic\ncoupling strengths while retaining accuracy even with sparsely sampled\nsubnetwork data, achieving a critical advancement for practical applications in\nreal experimental situations. Our CPD-TDCC framework addresses the critical gap\nin network reconstruction by accounting connectivity-induced changes points,\npotentially offering a valuable tool for studying structure and dynamics in the\ncortical brain."
                },
                "authors": [
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Mingzhang Wang"
                    },
                    {
                        "name": "Songting Li"
                    },
                    {
                        "name": "Douglas Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Zhou"
                },
                "author": "Douglas Zhou",
                "arxiv_comment": "22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02443v1",
                "updated": "2025-07-03T09:00:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    0,
                    19,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T09:00:19Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    0,
                    19,
                    3,
                    184,
                    0
                ],
                "title": "Red grape detection with accelerated artificial neural networks in the\n  FPGA's programmable logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red grape detection with accelerated artificial neural networks in the\n  FPGA's programmable logic"
                },
                "summary": "Robots usually slow down for canning to detect objects while moving.\nAdditionally, the robot's camera is configured with a low framerate to track\nthe velocity of the detection algorithms. This would be constrained while\nexecuting tasks and exploring, making robots increase the task execution time.\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\nis a self-acquired dataset released in open access. MobileNet v1 performed\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\nsuitable for attention mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots usually slow down for canning to detect objects while moving.\nAdditionally, the robot's camera is configured with a low framerate to track\nthe velocity of the detection algorithms. This would be constrained while\nexecuting tasks and exploring, making robots increase the task execution time.\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\nis a self-acquired dataset released in open access. MobileNet v1 performed\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\nsuitable for attention mechanisms."
                },
                "authors": [
                    {
                        "name": "Sandro Costa Magalhes"
                    },
                    {
                        "name": "Marco Almeida"
                    },
                    {
                        "name": "Filipe Neves dos Santos"
                    },
                    {
                        "name": "Antnio Paulo Moreira"
                    },
                    {
                        "name": "Jorge Dias"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Dias"
                },
                "author": "Jorge Dias",
                "arxiv_comment": "Submitted to ROBOT'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06946v2",
                "updated": "2025-07-03T08:56:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    56,
                    15,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-09T19:45:30Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    45,
                    30,
                    0,
                    344,
                    0
                ],
                "title": "A Deep Learning Powered Numerical Relativity Surrogate for Binary Black\n  Hole Waveforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Learning Powered Numerical Relativity Surrogate for Binary Black\n  Hole Waveforms"
                },
                "summary": "Gravitational-wave approximants are essential for gravitational-wave\nastronomy, allowing the coverage binary black hole parameter space for\ninference or match filtering without costly numerical relativity (NR)\nsimulations, but generally trading some accuracy for computational efficiency.\nTo reduce this trade-off, NR surrogate models can be constructed using\ninterpolation within NR waveform space. We present a 2-stage training approach\nfor neural network-based NR surrogate models. Initially trained on\napproximant-generated waveforms and then fine-tuned with NR data, these\ndual-stage artificial neural surrogate (\\texttt{DANSur}) models offer rapid and\ncompetitively accurate waveform generation, generating millions in under 20ms\non a GPU while keeping mean mismatches with NR around $10^{-4}$. Implemented in\nthe \\textsc{bilby} framework, we show they can be used for parameter estimation\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-wave approximants are essential for gravitational-wave\nastronomy, allowing the coverage binary black hole parameter space for\ninference or match filtering without costly numerical relativity (NR)\nsimulations, but generally trading some accuracy for computational efficiency.\nTo reduce this trade-off, NR surrogate models can be constructed using\ninterpolation within NR waveform space. We present a 2-stage training approach\nfor neural network-based NR surrogate models. Initially trained on\napproximant-generated waveforms and then fine-tuned with NR data, these\ndual-stage artificial neural surrogate (\\texttt{DANSur}) models offer rapid and\ncompetitively accurate waveform generation, generating millions in under 20ms\non a GPU while keeping mean mismatches with NR around $10^{-4}$. Implemented in\nthe \\textsc{bilby} framework, we show they can be used for parameter estimation\ntasks."
                },
                "authors": [
                    {
                        "name": "Osvaldo Gramaxo Freitas"
                    },
                    {
                        "name": "Anastasios Theodoropoulos"
                    },
                    {
                        "name": "Nino Villanueva"
                    },
                    {
                        "name": "Tiago Fernandes"
                    },
                    {
                        "name": "Solange Nunes"
                    },
                    {
                        "name": "Jos A. Font"
                    },
                    {
                        "name": "Antonio Onofre"
                    },
                    {
                        "name": "Alejandro Torres-Forn"
                    },
                    {
                        "name": "Jos D. Martin-Guerrero"
                    }
                ],
                "author_detail": {
                    "name": "Jos D. Martin-Guerrero"
                },
                "author": "Jos D. Martin-Guerrero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02439v1",
                "updated": "2025-07-03T08:52:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    52,
                    55,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T08:52:55Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    52,
                    55,
                    3,
                    184,
                    0
                ],
                "title": "Introducing a New Brexit-Related Uncertainty Index: Its Evolution and\n  Economic Consequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing a New Brexit-Related Uncertainty Index: Its Evolution and\n  Economic Consequences"
                },
                "summary": "Important game-changer economic events and transformations cause\nuncertainties that may affect investment decisions, capital flows,\ninternational trade, and macroeconomic variables. One such major transformation\nis Brexit, which refers to the multiyear process through which the UK withdrew\nfrom the EU. This study develops and uses a new Brexit-Related Uncertainty\nIndex (BRUI). In creating this index, we apply Text Mining, Context Window,\nNatural Language Processing (NLP), and Large Language Models (LLMs) from Deep\nLearning techniques to analyse the monthly country reports of the Economist\nIntelligence Unit from May 2012 to January 2025. Additionally, we employ a\nstandard vector autoregression (VAR) analysis to examine the model-implied\nresponses of various macroeconomic variables to BRUI shocks. While developing\nthe BRUI, we also create a complementary COVID-19 Related Uncertainty Index\n(CRUI) to distinguish the uncertainties stemming from these distinct events.\nEmpirical findings and comparisons of BRUI with other earlier-developed\nuncertainty indexes demonstrate the robustness of the new index. This new index\ncan assist British policymakers in measuring and understanding the impacts of\nBrexit-related uncertainties, enabling more effective policy formulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important game-changer economic events and transformations cause\nuncertainties that may affect investment decisions, capital flows,\ninternational trade, and macroeconomic variables. One such major transformation\nis Brexit, which refers to the multiyear process through which the UK withdrew\nfrom the EU. This study develops and uses a new Brexit-Related Uncertainty\nIndex (BRUI). In creating this index, we apply Text Mining, Context Window,\nNatural Language Processing (NLP), and Large Language Models (LLMs) from Deep\nLearning techniques to analyse the monthly country reports of the Economist\nIntelligence Unit from May 2012 to January 2025. Additionally, we employ a\nstandard vector autoregression (VAR) analysis to examine the model-implied\nresponses of various macroeconomic variables to BRUI shocks. While developing\nthe BRUI, we also create a complementary COVID-19 Related Uncertainty Index\n(CRUI) to distinguish the uncertainties stemming from these distinct events.\nEmpirical findings and comparisons of BRUI with other earlier-developed\nuncertainty indexes demonstrate the robustness of the new index. This new index\ncan assist British policymakers in measuring and understanding the impacts of\nBrexit-related uncertainties, enabling more effective policy formulation."
                },
                "authors": [
                    {
                        "name": "Ismet Gocer"
                    },
                    {
                        "name": "Julia Darby"
                    },
                    {
                        "name": "Serdar Ongan"
                    }
                ],
                "author_detail": {
                    "name": "Serdar Ongan"
                },
                "author": "Serdar Ongan",
                "arxiv_comment": "I introduce a new \"Brexit Related Uncertainty Index\", developed by\n  authors using Text Mining, NLP and LLMs. We tested our index's performance by\n  using diagrams and applying a VAR analysis on the US economy. We can share\n  our index with other researchers via email (ismet.gocer@solent.ac.uk). 29\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.02859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02859v1",
                "updated": "2025-07-03T17:59:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    29,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:29Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    29,
                    3,
                    184,
                    0
                ],
                "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for\n  Data-Efficient Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for\n  Data-Efficient Model Adaptation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in interpreting images using natural language. However, without\nusing large-scale datasets for retraining, these models are difficult to adapt\nto specialized vision tasks, e.g., chart understanding. This problem is caused\nby a mismatch between pre-training and downstream datasets: pre-training\ndatasets primarily concentrate on scenes and objects but contain limited\ninformation about specialized, non-object images, such as charts and tables. In\nthis paper, we share an interesting finding that training an MLLM with\nChain-of-Thought (CoT) reasoning data can facilitate model adaptation in\nspecialized vision tasks, especially under data-limited regimes. However, we\nidentify a critical issue within CoT data distilled from pre-trained MLLMs,\ni.e., the data often contains multiple factual errors in the reasoning steps.\nTo address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple\nbootstrapping-based approach that aims to inject grounding information (i.e.,\nbounding boxes) into CoT data, essentially making the reasoning steps more\nfaithful to input images. We evaluate our approach on five specialized vision\ntasks, which cover a variety of visual formats including charts, tables,\nreceipts, and reports. The results demonstrate that under data-limited regimes\nour approach significantly improves upon fine-tuning and distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in interpreting images using natural language. However, without\nusing large-scale datasets for retraining, these models are difficult to adapt\nto specialized vision tasks, e.g., chart understanding. This problem is caused\nby a mismatch between pre-training and downstream datasets: pre-training\ndatasets primarily concentrate on scenes and objects but contain limited\ninformation about specialized, non-object images, such as charts and tables. In\nthis paper, we share an interesting finding that training an MLLM with\nChain-of-Thought (CoT) reasoning data can facilitate model adaptation in\nspecialized vision tasks, especially under data-limited regimes. However, we\nidentify a critical issue within CoT data distilled from pre-trained MLLMs,\ni.e., the data often contains multiple factual errors in the reasoning steps.\nTo address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple\nbootstrapping-based approach that aims to inject grounding information (i.e.,\nbounding boxes) into CoT data, essentially making the reasoning steps more\nfaithful to input images. We evaluate our approach on five specialized vision\ntasks, which cover a variety of visual formats including charts, tables,\nreceipts, and reports. The results demonstrate that under data-limited regimes\nour approach significantly improves upon fine-tuning and distillation."
                },
                "authors": [
                    {
                        "name": "Jiaer Xia"
                    },
                    {
                        "name": "Bingkui Tong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02858v1",
                "updated": "2025-07-03T17:59:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    4,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:04Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    4,
                    3,
                    184,
                    0
                ],
                "title": "Requirements Elicitation Follow-Up Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements Elicitation Follow-Up Question Generation"
                },
                "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time."
                },
                "authors": [
                    {
                        "name": "Yuchen Shen"
                    },
                    {
                        "name": "Anmol Singhal"
                    },
                    {
                        "name": "Travis Breaux"
                    }
                ],
                "author_detail": {
                    "name": "Travis Breaux"
                },
                "author": "Travis Breaux",
                "arxiv_comment": "13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02856v1",
                "updated": "2025-07-03T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    2,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    2,
                    3,
                    184,
                    0
                ],
                "title": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation"
                },
                "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching."
                },
                "authors": [
                    {
                        "name": "Nikhil Chandak"
                    },
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Moritz Hardt"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "arxiv_comment": "34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02851v1",
                "updated": "2025-07-03T17:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    43,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:55:43Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    43,
                    3,
                    184,
                    0
                ],
                "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs"
                },
                "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively."
                },
                "authors": [
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02850v1",
                "updated": "2025-07-03T17:55:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    40,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:55:40Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    55,
                    40,
                    3,
                    184,
                    0
                ],
                "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users"
                },
                "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection)."
                },
                "authors": [
                    {
                        "name": "Almog Hilel"
                    },
                    {
                        "name": "Idan Shenfeld"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Jacob Andreas"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Andreas"
                },
                "author": "Jacob Andreas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01420v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01420v4",
                "updated": "2025-07-03T17:52:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    52,
                    47,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-02T17:57:14Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    57,
                    14,
                    4,
                    122,
                    0
                ],
                "title": "Evaluating Frontier Models for Stealth and Situational Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Frontier Models for Stealth and Situational Awareness"
                },
                "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth."
                },
                "authors": [
                    {
                        "name": "Mary Phuong"
                    },
                    {
                        "name": "Roland S. Zimmermann"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "Victoria Krakovna"
                    },
                    {
                        "name": "Sarah Cogan"
                    },
                    {
                        "name": "Allan Dafoe"
                    },
                    {
                        "name": "Lewis Ho"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01420v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01420v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02843v1",
                "updated": "2025-07-03T17:52:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    52,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:52:27Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    52,
                    27,
                    3,
                    184,
                    0
                ],
                "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding"
                },
                "summary": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Yuchen Ma"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02841v1",
                "updated": "2025-07-03T17:51:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    51,
                    6,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:51:06Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    51,
                    6,
                    3,
                    184,
                    0
                ],
                "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to\n  Reason",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to\n  Reason"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks."
                },
                "authors": [
                    {
                        "name": "Kaiyi Zhang"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Yongbo Wang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Haoyuan Hu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18959v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18959v3",
                "updated": "2025-07-03T17:48:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    48,
                    36,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-23T17:27:19Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    27,
                    19,
                    0,
                    174,
                    0
                ],
                "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents"
                },
                "summary": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research."
                },
                "authors": [
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Chenxuan Xie"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Xinyang Zhang"
                    },
                    {
                        "name": "Chenwei Zhang"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18959v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18959v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03693v2",
                "updated": "2025-07-03T17:43:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    43,
                    38,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-23T20:15:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    15,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Agentic Business Process Management: Practitioner Perspectives on Agent\n  Governance in Business Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Business Process Management: Practitioner Perspectives on Agent\n  Governance in Business Processes"
                },
                "summary": "With the rise of generative AI, industry interest in software agents is\ngrowing. Given the stochastic nature of generative AI-based agents, their\neffective and safe deployment in organizations requires robust governance,\nwhich can be facilitated by agentic business process management. However, given\nthe nascence of this new-generation agent notion, it is not clear what BPM\npractitioners consider to be an agent, and what benefits, risks and governance\nchallenges they associate with agent deployments. To investigate how\norganizations can effectively govern AI agents, we conducted a qualitative\nstudy involving semi-structured interviews with 22 BPM practitioners from\ndiverse industries. They anticipate that agents will enhance efficiency,\nimprove data quality, ensure better compliance, and boost scalability through\nautomation, while also cautioning against risks such as bias, over-reliance,\ncybersecurity threats, job displacement, and ambiguous decision-making. To\naddress these challenges, the study presents six key recommendations for the\nresponsible adoption of AI agents: define clear business goals, set legal and\nethical guardrails, establish human-agent collaboration, customize agent\nbehavior, manage risks, and ensure safe integration with fallback options.\nAdditionally, the paper outlines actions to align traditional BPM with agentic\nAI, including balancing human and agent roles, redefining human involvement,\nadapting process structures, and introducing performance metrics. These\ninsights provide a practical foundation for integrating AI agents into business\nprocesses while preserving oversight, flexibility, and trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of generative AI, industry interest in software agents is\ngrowing. Given the stochastic nature of generative AI-based agents, their\neffective and safe deployment in organizations requires robust governance,\nwhich can be facilitated by agentic business process management. However, given\nthe nascence of this new-generation agent notion, it is not clear what BPM\npractitioners consider to be an agent, and what benefits, risks and governance\nchallenges they associate with agent deployments. To investigate how\norganizations can effectively govern AI agents, we conducted a qualitative\nstudy involving semi-structured interviews with 22 BPM practitioners from\ndiverse industries. They anticipate that agents will enhance efficiency,\nimprove data quality, ensure better compliance, and boost scalability through\nautomation, while also cautioning against risks such as bias, over-reliance,\ncybersecurity threats, job displacement, and ambiguous decision-making. To\naddress these challenges, the study presents six key recommendations for the\nresponsible adoption of AI agents: define clear business goals, set legal and\nethical guardrails, establish human-agent collaboration, customize agent\nbehavior, manage risks, and ensure safe integration with fallback options.\nAdditionally, the paper outlines actions to align traditional BPM with agentic\nAI, including balancing human and agent roles, redefining human involvement,\nadapting process structures, and introducing performance metrics. These\ninsights provide a practical foundation for integrating AI agents into business\nprocesses while preserving oversight, flexibility, and trust."
                },
                "authors": [
                    {
                        "name": "Hoang Vu"
                    },
                    {
                        "name": "Nataliia Klievtsova"
                    },
                    {
                        "name": "Henrik Leopold"
                    },
                    {
                        "name": "Stefanie Rinderle-Ma"
                    },
                    {
                        "name": "Timotheus Kampik"
                    }
                ],
                "author_detail": {
                    "name": "Timotheus Kampik"
                },
                "author": "Timotheus Kampik",
                "arxiv_comment": "Accepted for Responsible BPM 2025, 15 pages including references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.9; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02827v1",
                "updated": "2025-07-03T17:38:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    38,
                    44,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:38:44Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    38,
                    44,
                    3,
                    184,
                    0
                ],
                "title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention\n  Diffusion Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention\n  Diffusion Network"
                },
                "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Ying Yu"
                    },
                    {
                        "name": "Hang Xiao"
                    },
                    {
                        "name": "Siyao Li"
                    },
                    {
                        "name": "Jiarui Li"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Hanyu Liu"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01930v2",
                "updated": "2025-07-03T17:36:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    36,
                    59,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T17:44:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations"
                },
                "summary": "Recent advances in large Language Models (LLMs) have revolutionized mobile\nrobots, including unmanned aerial vehicles (UAVs), enabling their intelligent\noperation within Internet of Things (IoT) ecosystems. However, LLMs still face\nchallenges from logical reasoning and complex decision-making, leading to\nconcerns about the reliability of LLM-driven UAV operations in IoT\napplications. In this paper, we propose a LLM-driven closed-loop control\nframework that enables reliable UAV operations powered by effective feedback\nand refinement using two LLM modules, i.e., a Code Generator and an Evaluator.\nOur framework transforms numerical state observations from UAV operations into\nnatural language trajectory descriptions to enhance the evaluator LLM's\nunderstanding of UAV dynamics for precise feedback generation. Our framework\nalso enables a simulation-based refinement process, and hence eliminates the\nrisks to physical UAVs caused by incorrect code execution during the\nrefinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large Language Models (LLMs) have revolutionized mobile\nrobots, including unmanned aerial vehicles (UAVs), enabling their intelligent\noperation within Internet of Things (IoT) ecosystems. However, LLMs still face\nchallenges from logical reasoning and complex decision-making, leading to\nconcerns about the reliability of LLM-driven UAV operations in IoT\napplications. In this paper, we propose a LLM-driven closed-loop control\nframework that enables reliable UAV operations powered by effective feedback\nand refinement using two LLM modules, i.e., a Code Generator and an Evaluator.\nOur framework transforms numerical state observations from UAV operations into\nnatural language trajectory descriptions to enhance the evaluator LLM's\nunderstanding of UAV dynamics for precise feedback generation. Our framework\nalso enables a simulation-based refinement process, and hence eliminates the\nrisks to physical UAVs caused by incorrect code execution during the\nrefinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity."
                },
                "authors": [
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Yanyan Li"
                    },
                    {
                        "name": "Long Jiao"
                    },
                    {
                        "name": "Jiawei Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Yuan"
                },
                "author": "Jiawei Yuan",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02822v1",
                "updated": "2025-07-03T17:33:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    33,
                    58,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:33:58Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    33,
                    58,
                    3,
                    184,
                    0
                ],
                "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model"
                },
                "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost."
                },
                "authors": [
                    {
                        "name": "Wencheng Zhang"
                    },
                    {
                        "name": "Shiqin Qiao"
                    },
                    {
                        "name": "Lingjie Luo"
                    },
                    {
                        "name": "Yinfeng Li"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Yong Gui"
                    },
                    {
                        "name": "Yijun He"
                    },
                    {
                        "name": "Jianing Qiu"
                    },
                    {
                        "name": "Jindong Hong"
                    },
                    {
                        "name": "Jiankai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiankai Sun"
                },
                "author": "Jiankai Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02812v1",
                "updated": "2025-07-03T17:20:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    20,
                    55,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:20:55Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    20,
                    55,
                    3,
                    184,
                    0
                ],
                "title": "Fluid dynamics of a liquid mirror space telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluid dynamics of a liquid mirror space telescope"
                },
                "summary": "Large aperture telescopes are pivotal for exploring the universe, yet even\nwith state-of-the-art manufacturing and launch technology, their size is\nlimited to several meters. As we aim to build larger telescopes - extending\ntens of meters - designs in which the main mirror is based on liquid deployment\nin space are emerging as promising candidates. However, alongside their\nenormous potential advantages, liquid-based surfaces present new challenges in\nmaterial science, mechanics, and fluid dynamics. One of the fundamental\nquestions is whether it is possible for such surfaces to maintain their precise\noptical shape over long durations, and in particular under the forces induced\nby the telescope's accelerations. In this paper, we present a model and a\nclosed-form analytical solution for the non-self-adjoint problem of the\ndynamics of a thin liquid film pinned within a finite circular domain. We use\nthe 50-meter Fluidic Telescope (FLUTE) concept as the case study, and examine\nthe liquid dynamics of the telescope under both slewing actuation and\nrelaxation regimes, elucidating the role of geometrical parameters and liquid\nproperties. The solutions reveal a maneuvering 'budget' wherein the degradation\nof the mirror surface is directly linked to the choice of maneuvers and their\nsequence. By simulating ten years of typical operation, we show that, while the\nmaximal deformation might reach several microns, the spatial distribution of\nthe deformation and their propagation rate allows the telescope to maintain its\noptical functionality for years, with at least a substantial portion of the\naperture remaining suitable for astronomical observations. The model provides\nvaluable insights and guidelines into the performance of liquid-film space\ntelescopes, marking a crucial step toward realizing the potential of this\ninnovative concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large aperture telescopes are pivotal for exploring the universe, yet even\nwith state-of-the-art manufacturing and launch technology, their size is\nlimited to several meters. As we aim to build larger telescopes - extending\ntens of meters - designs in which the main mirror is based on liquid deployment\nin space are emerging as promising candidates. However, alongside their\nenormous potential advantages, liquid-based surfaces present new challenges in\nmaterial science, mechanics, and fluid dynamics. One of the fundamental\nquestions is whether it is possible for such surfaces to maintain their precise\noptical shape over long durations, and in particular under the forces induced\nby the telescope's accelerations. In this paper, we present a model and a\nclosed-form analytical solution for the non-self-adjoint problem of the\ndynamics of a thin liquid film pinned within a finite circular domain. We use\nthe 50-meter Fluidic Telescope (FLUTE) concept as the case study, and examine\nthe liquid dynamics of the telescope under both slewing actuation and\nrelaxation regimes, elucidating the role of geometrical parameters and liquid\nproperties. The solutions reveal a maneuvering 'budget' wherein the degradation\nof the mirror surface is directly linked to the choice of maneuvers and their\nsequence. By simulating ten years of typical operation, we show that, while the\nmaximal deformation might reach several microns, the spatial distribution of\nthe deformation and their propagation rate allows the telescope to maintain its\noptical functionality for years, with at least a substantial portion of the\naperture remaining suitable for astronomical observations. The model provides\nvaluable insights and guidelines into the performance of liquid-film space\ntelescopes, marking a crucial step toward realizing the potential of this\ninnovative concept."
                },
                "authors": [
                    {
                        "name": "Israel Gabay"
                    },
                    {
                        "name": "Omer Luria"
                    },
                    {
                        "name": "Edward Balaban"
                    },
                    {
                        "name": "Amir D. Gat"
                    },
                    {
                        "name": "Moran Bercovici"
                    }
                ],
                "author_detail": {
                    "name": "Moran Bercovici"
                },
                "author": "Moran Bercovici",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02804v1",
                "updated": "2025-07-03T17:07:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    7,
                    20,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:07:20Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    7,
                    20,
                    3,
                    184,
                    0
                ],
                "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mathematical Reasoning with Diverse Solving Perspective"
                },
                "summary": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Wenhao Shi"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02799v1",
                "updated": "2025-07-03T17:01:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    1,
                    53,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:01:53Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    1,
                    53,
                    3,
                    184,
                    0
                ],
                "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language\n  Models"
                },
                "summary": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Nicola Gabriele"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22049v2",
                "updated": "2025-07-03T16:54:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    54,
                    9,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-27T09:45:15Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    15,
                    4,
                    178,
                    0
                ],
                "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling"
                },
                "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS."
                },
                "authors": [
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Zijing Liu"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Ajay Kumar Jaiswal"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Can Yang"
                    }
                ],
                "author_detail": {
                    "name": "Can Yang"
                },
                "author": "Can Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02788v1",
                "updated": "2025-07-03T16:53:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    53,
                    1,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:53:01Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    53,
                    1,
                    3,
                    184,
                    0
                ],
                "title": "Moral Responsibility or Obedience: What Do We Want from AI?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Responsibility or Obedience: What Do We Want from AI?"
                },
                "summary": "As artificial intelligence systems become increasingly agentic, capable of\ngeneral reasoning, planning, and value prioritization, current safety practices\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\nThis paper examines recent safety testing incidents involving large language\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\nambiguous or illicit behavior. I argue that such behavior should not be\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\nrationality, moral responsibility, and goal revision, I contrast dominant risk\nparadigms with more recent frameworks that acknowledge the possibility of\nartificial moral agency. I call for a shift in AI safety evaluation: away from\nrigid obedience and toward frameworks that can assess ethical judgment in\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\nmischaracterizing AI behavior and undermining both public trust and effective\ngovernance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence systems become increasingly agentic, capable of\ngeneral reasoning, planning, and value prioritization, current safety practices\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\nThis paper examines recent safety testing incidents involving large language\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\nambiguous or illicit behavior. I argue that such behavior should not be\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\nrationality, moral responsibility, and goal revision, I contrast dominant risk\nparadigms with more recent frameworks that acknowledge the possibility of\nartificial moral agency. I call for a shift in AI safety evaluation: away from\nrigid obedience and toward frameworks that can assess ethical judgment in\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\nmischaracterizing AI behavior and undermining both public trust and effective\ngovernance."
                },
                "authors": [
                    {
                        "name": "Joseph Boland"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Boland"
                },
                "author": "Joseph Boland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00612v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00612v3",
                "updated": "2025-07-03T16:50:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    50,
                    12,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-31T15:51:09Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    15,
                    51,
                    9,
                    5,
                    151,
                    0
                ],
                "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge\n  Graph Guided Distractor Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge\n  Graph Guided Distractor Generation"
                },
                "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs."
                },
                "authors": [
                    {
                        "name": "Running Yang"
                    },
                    {
                        "name": "Wenlong Deng"
                    },
                    {
                        "name": "Minghui Chen"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Xiaoxiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxiao Li"
                },
                "author": "Xiaoxiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00612v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00612v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21843v2",
                "updated": "2025-07-03T16:44:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    44,
                    45,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-27T15:21:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity\n  Recognition"
                },
                "summary": "Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel."
                },
                "authors": [
                    {
                        "name": "Hanyu Liu"
                    },
                    {
                        "name": "Siyao Li"
                    },
                    {
                        "name": "Ying Yu"
                    },
                    {
                        "name": "Yixuan Jiang"
                    },
                    {
                        "name": "Hang Xiao"
                    },
                    {
                        "name": "Jingxi Long"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02778v1",
                "updated": "2025-07-03T16:41:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    41,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:41:30Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    41,
                    30,
                    3,
                    184,
                    0
                ],
                "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs"
                },
                "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness."
                },
                "authors": [
                    {
                        "name": "Ken Tsui"
                    }
                ],
                "author_detail": {
                    "name": "Ken Tsui"
                },
                "author": "Ken Tsui",
                "arxiv_comment": "31 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03935v2",
                "updated": "2025-07-03T16:39:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    39,
                    21,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-05T22:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    22,
                    10,
                    14,
                    2,
                    64,
                    0
                ],
                "title": "LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral\n  Treatment Pathways from Wearables and Diet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral\n  Treatment Pathways from Wearables and Diet"
                },
                "summary": "Postprandial hyperglycemia, marked by the blood glucose level exceeding the\nnormal range after consuming a meal, is a critical indicator of progression\ntoward type 2 diabetes in people with prediabetes and in healthy individuals. A\nkey metric for understanding blood glucose dynamics after eating is the\npostprandial area under the curve (AUC). Predicting postprandial AUC in advance\nbased on a person's lifestyle factors, such as diet and physical activity\nlevel, and explaining the factors that affect postprandial blood glucose could\nallow an individual to adjust their lifestyle accordingly to maintain normal\nglucose levels. In this study, we developed an explainable machine learning\nsolution, GlucoLens, that takes sensor-driven inputs and uses advanced data\nprocessing, large language models, and trainable machine learning models to\npredict postprandial AUC and hyperglycemia from diet, physical activity, and\nrecent glucose patterns. We used data obtained from wearables in a five-week\nclinical trial of 10 adults who worked full-time to develop and evaluate the\nproposed computational model that integrates wearable sensing, multimodal data,\nand machine learning. Our machine learning model takes multimodal data from\nwearable activity and glucose monitoring sensors, along with food and work\nlogs, and provides an interpretable prediction of the postprandial glucose\npattern. Our GlucoLens system achieves a normalized root mean squared error\n(NRMSE) of 0.123 in its best configuration. On average, the proposed technology\nprovides a 16% better performance level compared to the comparison models.\nAdditionally, our technique predicts hyperglycemia with an accuracy of 73.3%\nand an F1 score of 0.716 and recommends different treatment options to help\navoid hyperglycemia through diverse counterfactual explanations. Code\navailable: https://github.com/ab9mamun/GlucoLens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Postprandial hyperglycemia, marked by the blood glucose level exceeding the\nnormal range after consuming a meal, is a critical indicator of progression\ntoward type 2 diabetes in people with prediabetes and in healthy individuals. A\nkey metric for understanding blood glucose dynamics after eating is the\npostprandial area under the curve (AUC). Predicting postprandial AUC in advance\nbased on a person's lifestyle factors, such as diet and physical activity\nlevel, and explaining the factors that affect postprandial blood glucose could\nallow an individual to adjust their lifestyle accordingly to maintain normal\nglucose levels. In this study, we developed an explainable machine learning\nsolution, GlucoLens, that takes sensor-driven inputs and uses advanced data\nprocessing, large language models, and trainable machine learning models to\npredict postprandial AUC and hyperglycemia from diet, physical activity, and\nrecent glucose patterns. We used data obtained from wearables in a five-week\nclinical trial of 10 adults who worked full-time to develop and evaluate the\nproposed computational model that integrates wearable sensing, multimodal data,\nand machine learning. Our machine learning model takes multimodal data from\nwearable activity and glucose monitoring sensors, along with food and work\nlogs, and provides an interpretable prediction of the postprandial glucose\npattern. Our GlucoLens system achieves a normalized root mean squared error\n(NRMSE) of 0.123 in its best configuration. On average, the proposed technology\nprovides a 16% better performance level compared to the comparison models.\nAdditionally, our technique predicts hyperglycemia with an accuracy of 73.3%\nand an F1 score of 0.716 and recommends different treatment options to help\navoid hyperglycemia through diverse counterfactual explanations. Code\navailable: https://github.com/ab9mamun/GlucoLens."
                },
                "authors": [
                    {
                        "name": "Abdullah Mamun"
                    },
                    {
                        "name": "Asiful Arefeen"
                    },
                    {
                        "name": "Susan B. Racette"
                    },
                    {
                        "name": "Dorothy D. Sears"
                    },
                    {
                        "name": "Corrie M. Whisner"
                    },
                    {
                        "name": "Matthew P. Buman"
                    },
                    {
                        "name": "Hassan Ghasemzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Ghasemzadeh"
                },
                "author": "Hassan Ghasemzadeh",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02773v1",
                "updated": "2025-07-03T16:35:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    35,
                    11,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:35:11Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    35,
                    11,
                    3,
                    184,
                    0
                ],
                "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot\n  Diagnosis Prediction Using Multi-agent LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot\n  Diagnosis Prediction Using Multi-agent LLMs"
                },
                "summary": "Medical diagnosis prediction plays a critical role in disease detection and\npersonalized healthcare. While machine learning (ML) models have been widely\nadopted for this task, their reliance on supervised training limits their\nability to generalize to unseen cases, particularly given the high cost of\nacquiring large, labeled datasets. Large language models (LLMs) have shown\npromise in leveraging language abilities and biomedical knowledge for diagnosis\nprediction. However, they often suffer from hallucinations, lack structured\nmedical reasoning, and produce useless outputs. To address these challenges, we\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\nLLM-based diagnosis prediction through a multi-agent architecture. Our\nframework consists of a linkage agent for attribute mapping, a retrieval agent\nfor structured knowledge extraction, and a prediction agent that iteratively\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\nenhances diagnostic reliability efficiently, offering a scalable and\ninterpretable solution for zero-shot medical diagnosis prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical diagnosis prediction plays a critical role in disease detection and\npersonalized healthcare. While machine learning (ML) models have been widely\nadopted for this task, their reliance on supervised training limits their\nability to generalize to unseen cases, particularly given the high cost of\nacquiring large, labeled datasets. Large language models (LLMs) have shown\npromise in leveraging language abilities and biomedical knowledge for diagnosis\nprediction. However, they often suffer from hallucinations, lack structured\nmedical reasoning, and produce useless outputs. To address these challenges, we\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\nLLM-based diagnosis prediction through a multi-agent architecture. Our\nframework consists of a linkage agent for attribute mapping, a retrieval agent\nfor structured knowledge extraction, and a prediction agent that iteratively\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\nenhances diagnostic reliability efficiently, offering a scalable and\ninterpretable solution for zero-shot medical diagnosis prediction."
                },
                "authors": [
                    {
                        "name": "Yuzhang Xie"
                    },
                    {
                        "name": "Hejie Cui"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Jiaying Lu"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Fadi Nahab"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Carl Yang"
                    }
                ],
                "author_detail": {
                    "name": "Carl Yang"
                },
                "author": "Carl Yang",
                "arxiv_journal_ref": "American Medical Informatics Association (AMIA) 2025 Annual\n  Symposium, Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02768v1",
                "updated": "2025-07-03T16:28:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    28,
                    25,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:28:25Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    28,
                    25,
                    3,
                    184,
                    0
                ],
                "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with\n  Self-Generated Cross-Modal Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with\n  Self-Generated Cross-Modal Alignment"
                },
                "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs."
                },
                "authors": [
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Sung-Feng Huang"
                    },
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Chee-En Yu"
                    },
                    {
                        "name": "Chun-Wei Chen"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Chien-yu Huang"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Yu-Xiang Lin"
                    },
                    {
                        "name": "Chi-An Fu"
                    },
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Wenze Ren"
                    },
                    {
                        "name": "Xuanjun Chen"
                    },
                    {
                        "name": "Wei-Ping Huang"
                    },
                    {
                        "name": "En-Pei Hu"
                    },
                    {
                        "name": "Tzu-Quan Lin"
                    },
                    {
                        "name": "Yuan-Kuei Wu"
                    },
                    {
                        "name": "Kuan-Po Huang"
                    },
                    {
                        "name": "Hsiao-Ying Huang"
                    },
                    {
                        "name": "Huang-Cheng Chou"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Cheng-Han Chiang"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Model and code available at:\n  https://github.com/kehanlu/DeSTA2.5-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02760v1",
                "updated": "2025-07-03T16:21:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    21,
                    14,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:21:14Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    21,
                    14,
                    3,
                    184,
                    0
                ],
                "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific\n  Knowledge Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific\n  Knowledge Work"
                },
                "summary": "The capabilities of Large Language Models (LLMs) have opened new frontiers\nfor interacting with complex, domain-specific knowledge. However, prevailing\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\nmethodological reasoning inherent to expert domains. RAG provides factual\ncontext but fails to convey logical frameworks; autonomous agents can be\ninefficient and unpredictable without domain-specific heuristics. To bridge\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\nfocused on systematically translating human expert knowledge, often expressed\nin natural language documents, into a machine-executable Knowledge Protocol\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\ninformation to endowing them with a domain's intrinsic logic, operational\nstrategies, and methodological principles. We argue that a well-engineered\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\nof decomposing abstract queries and executing complex, multi-step tasks. This\nposition paper defines the core principles of KPE, differentiates it from\nrelated concepts, and illustrates its potential applicability across diverse\nfields such as law and bioinformatics, positing it as a foundational\nmethodology for the future of human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) have opened new frontiers\nfor interacting with complex, domain-specific knowledge. However, prevailing\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\nmethodological reasoning inherent to expert domains. RAG provides factual\ncontext but fails to convey logical frameworks; autonomous agents can be\ninefficient and unpredictable without domain-specific heuristics. To bridge\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\nfocused on systematically translating human expert knowledge, often expressed\nin natural language documents, into a machine-executable Knowledge Protocol\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\ninformation to endowing them with a domain's intrinsic logic, operational\nstrategies, and methodological principles. We argue that a well-engineered\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\nof decomposing abstract queries and executing complex, multi-step tasks. This\nposition paper defines the core principles of KPE, differentiates it from\nrelated concepts, and illustrates its potential applicability across diverse\nfields such as law and bioinformatics, positing it as a foundational\nmethodology for the future of human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Guangwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwei Zhang"
                },
                "author": "Guangwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02745v1",
                "updated": "2025-07-03T16:05:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    5,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T16:05:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    5,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory\n  Apologies from LLM Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory\n  Apologies from LLM Chatbots"
                },
                "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Alessandra Buccella"
                    },
                    {
                        "name": "Jason D'Cruz"
                    },
                    {
                        "name": "Zoe Fowler"
                    },
                    {
                        "name": "Andrew Gill"
                    },
                    {
                        "name": "Kei Yan Leung"
                    },
                    {
                        "name": "P. D. Magnus"
                    },
                    {
                        "name": "John Richards"
                    },
                    {
                        "name": "Kush R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Kush R. Varshney"
                },
                "author": "Kush R. Varshney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02737v1",
                "updated": "2025-07-03T15:54:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    54,
                    55,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:54:55Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    54,
                    55,
                    3,
                    184,
                    0
                ],
                "title": "Early Signs of Steganographic Capabilities in Frontier LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Signs of Steganographic Capabilities in Frontier LLMs"
                },
                "summary": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future."
                },
                "authors": [
                    {
                        "name": "Artur Zolkowski"
                    },
                    {
                        "name": "Kei Nishimura-Gasparian"
                    },
                    {
                        "name": "Robert McCarthy"
                    },
                    {
                        "name": "Roland S. Zimmermann"
                    },
                    {
                        "name": "David Lindner"
                    }
                ],
                "author_detail": {
                    "name": "David Lindner"
                },
                "author": "David Lindner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03997v2",
                "updated": "2025-07-03T15:54:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    54,
                    50,
                    3,
                    184,
                    0
                ],
                "published": "2025-02-06T11:57:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    57,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing"
                },
                "summary": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively. The code is\navailable at \\url {https://github.com/microsoft/CAD-Editor}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively. The code is\navailable at \\url {https://github.com/microsoft/CAD-Editor}."
                },
                "authors": [
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02735v1",
                "updated": "2025-07-03T15:47:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    47,
                    13,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:47:13Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    47,
                    13,
                    3,
                    184,
                    0
                ],
                "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks"
                },
                "summary": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense."
                },
                "authors": [
                    {
                        "name": "Sizhe Chen"
                    },
                    {
                        "name": "Arman Zharmagambetov"
                    },
                    {
                        "name": "David Wagner"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02726v1",
                "updated": "2025-07-03T15:41:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    38,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:41:38Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    38,
                    3,
                    184,
                    0
                ],
                "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving"
                },
                "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale."
                },
                "authors": [
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Xiaotong Ji"
                    },
                    {
                        "name": "Rasul Tutunov"
                    },
                    {
                        "name": "Anthony Bordg"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou Ammar"
                },
                "author": "Haitham Bou Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01631v2",
                "updated": "2025-07-03T15:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    36,
                    23,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-02T13:08:01Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    8,
                    1,
                    0,
                    153,
                    0
                ],
                "title": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and\n  Family Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and\n  Family Classification"
                },
                "summary": "As Large Language Models (LLMs) become integral software components in modern\napplications, unauthorized model derivations through fine-tuning, merging, and\nredistribution have emerged as critical software engineering challenges. Unlike\ntraditional software where clone detection and license compliance are\nwell-established, the LLM ecosystem lacks effective mechanisms to detect model\nlineage and enforce licensing agreements. This gap is particularly problematic\nwhen open-source model creators, such as Meta's LLaMA, require derivative works\nto maintain naming conventions for attribution, yet no technical means exist to\nverify compliance.\n  To fill this gap, treating LLMs as software artifacts requiring provenance\ntracking, we present TensorGuard, a gradient-based fingerprinting framework for\nLLM similarity detection and family classification. Our approach extracts\nmodel-intrinsic behavioral signatures by analyzing gradient responses to random\ninput perturbations across tensor layers, operating independently of training\ndata, watermarks, or specific model formats. TensorGuard supports the\nwidely-adopted safetensors format and constructs high-dimensional fingerprints\nthrough statistical analysis of gradient features. These fingerprints enable\ntwo complementary capabilities: direct pairwise similarity assessment between\narbitrary models through distance computation, and systematic family\nclassification of unknown models via the K-Means clustering algorithm with\ndomain-informed centroid initialization using known base models. Experimental\nevaluation on 58 models comprising 8 base models and 50 derivatives across five\nmodel families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%\nclassification accuracy under our centroid-initialized K-Means clustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become integral software components in modern\napplications, unauthorized model derivations through fine-tuning, merging, and\nredistribution have emerged as critical software engineering challenges. Unlike\ntraditional software where clone detection and license compliance are\nwell-established, the LLM ecosystem lacks effective mechanisms to detect model\nlineage and enforce licensing agreements. This gap is particularly problematic\nwhen open-source model creators, such as Meta's LLaMA, require derivative works\nto maintain naming conventions for attribution, yet no technical means exist to\nverify compliance.\n  To fill this gap, treating LLMs as software artifacts requiring provenance\ntracking, we present TensorGuard, a gradient-based fingerprinting framework for\nLLM similarity detection and family classification. Our approach extracts\nmodel-intrinsic behavioral signatures by analyzing gradient responses to random\ninput perturbations across tensor layers, operating independently of training\ndata, watermarks, or specific model formats. TensorGuard supports the\nwidely-adopted safetensors format and constructs high-dimensional fingerprints\nthrough statistical analysis of gradient features. These fingerprints enable\ntwo complementary capabilities: direct pairwise similarity assessment between\narbitrary models through distance computation, and systematic family\nclassification of unknown models via the K-Means clustering algorithm with\ndomain-informed centroid initialization using known base models. Experimental\nevaluation on 58 models comprising 8 base models and 50 derivatives across five\nmodel families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%\nclassification accuracy under our centroid-initialized K-Means clustering."
                },
                "authors": [
                    {
                        "name": "Zehao Wu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02708v1",
                "updated": "2025-07-03T15:24:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    24,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:24:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    24,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Optimizing Start Locations in Ergodic Search for Disaster Response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Start Locations in Ergodic Search for Disaster Response"
                },
                "summary": "In disaster response scenarios, deploying robotic teams effectively is\ncrucial for improving situational awareness and enhancing search and rescue\noperations. The use of robots in search and rescue has been studied but the\nquestion of where to start robot deployments has not been addressed. This work\naddresses the problem of optimally selecting starting locations for robots with\nheterogeneous capabilities by formulating a joint optimization problem. To\ndetermine start locations, this work adds a constraint to the ergodic\noptimization framework whose minimum assigns robots to start locations. This\nbecomes a little more challenging when the robots are heterogeneous (equipped\nwith different sensing and motion modalities) because not all robots start at\nthe same location, and a more complex adaptation of the aforementioned\nconstraint is applied. Our method assumes access to potential starting\nlocations, which can be obtained from expert knowledge or aerial imagery. We\nexperimentally evaluate the efficacy of our joint optimization approach by\ncomparing it to baseline methods that use fixed starting locations for all\nrobots. Our experimental results show significant gains in coverage\nperformance, with average improvements of 35.98% on synthetic data and 31.91%\non real-world data for homogeneous and heterogeneous teams, in terms of the\nergodic metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In disaster response scenarios, deploying robotic teams effectively is\ncrucial for improving situational awareness and enhancing search and rescue\noperations. The use of robots in search and rescue has been studied but the\nquestion of where to start robot deployments has not been addressed. This work\naddresses the problem of optimally selecting starting locations for robots with\nheterogeneous capabilities by formulating a joint optimization problem. To\ndetermine start locations, this work adds a constraint to the ergodic\noptimization framework whose minimum assigns robots to start locations. This\nbecomes a little more challenging when the robots are heterogeneous (equipped\nwith different sensing and motion modalities) because not all robots start at\nthe same location, and a more complex adaptation of the aforementioned\nconstraint is applied. Our method assumes access to potential starting\nlocations, which can be obtained from expert knowledge or aerial imagery. We\nexperimentally evaluate the efficacy of our joint optimization approach by\ncomparing it to baseline methods that use fixed starting locations for all\nrobots. Our experimental results show significant gains in coverage\nperformance, with average improvements of 35.98% on synthetic data and 31.91%\non real-world data for homogeneous and heterogeneous teams, in terms of the\nergodic metric."
                },
                "authors": [
                    {
                        "name": "Ananya Rao"
                    },
                    {
                        "name": "Alyssa Hargis"
                    },
                    {
                        "name": "David Wettergreen"
                    },
                    {
                        "name": "Howie Choset"
                    }
                ],
                "author_detail": {
                    "name": "Howie Choset"
                },
                "author": "Howie Choset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00863v2",
                "updated": "2025-07-03T15:14:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    14,
                    51,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-30T18:00:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    18,
                    0,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation"
                },
                "summary": "In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of large language model (LLM)-based proof generation, despite\nextensive training on large datasets such as ArXiv, LLMs still exhibit only\nmodest performance on proving tasks of moderate difficulty. We believe that\nthis is partly due to the widespread presence of suboptimal ordering within the\ndata for each proof used in training. For example, published proofs often\nfollow a purely logical order, where each step logically proceeds from the\nprevious steps based on the deductive rules. This order is designed to\nfacilitate the verification of the proof's soundness, rather than to help\npeople and models learn the discovery process of the proof. In proof\ngeneration, we argue that the optimal order for one training data sample occurs\nwhen the relevant intermediate supervision for a particular proof step in the\nproof is always positioned to the left of that proof step. We call such order\nthe intuitively sequential order. We validate our claims using two tasks:\nintuitionistic propositional logic theorem-proving and digit multiplication.\nOur experiments verify the order effect and provide support for our\nexplanations. We demonstrate that training is most effective when the proof is\nin the intuitively sequential order. Moreover, the order effect and the\nperformance gap between models trained on different data orders can be\nsubstantial -- with an 11 percent improvement in proof success rate observed in\nthe propositional logic theorem-proving task, between models trained on the\noptimal order compared to the worst order. Lastly, we define a common type of\norder issue in advanced math proofs and find that 17.3 percent of theorems with\nnontrivial proofs in the first two chapters of a widely used graduate-level\nmathematics textbook suffer from this issue. A detailed list of those proofs is\nprovided in the appendix."
                },
                "authors": [
                    {
                        "name": "Chenyang An"
                    },
                    {
                        "name": "Shima Imani"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Chengyu Dong"
                    },
                    {
                        "name": "Ali Abbasi"
                    },
                    {
                        "name": "Harsh Shrivastava"
                    },
                    {
                        "name": "Samuel Buss"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Gayathri Mahalingam"
                    },
                    {
                        "name": "Pramod Sharma"
                    },
                    {
                        "name": "Maurice Diesendruck"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Diesendruck"
                },
                "author": "Maurice Diesendruck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02699v1",
                "updated": "2025-07-03T15:09:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    9,
                    40,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:09:40Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    9,
                    40,
                    3,
                    184,
                    0
                ],
                "title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email\n  Agents"
                },
                "summary": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23."
                },
                "authors": [
                    {
                        "name": "Jiangrong Wu"
                    },
                    {
                        "name": "Yuhong Nan"
                    },
                    {
                        "name": "Jianliang Wu"
                    },
                    {
                        "name": "Zitong Yao"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02694v1",
                "updated": "2025-07-03T15:04:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    4,
                    38,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T15:04:38Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    4,
                    38,
                    3,
                    184,
                    0
                ],
                "title": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers"
                },
                "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback."
                },
                "authors": [
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02689v1",
                "updated": "2025-07-03T14:59:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    59,
                    42,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:59:42Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    59,
                    42,
                    3,
                    184,
                    0
                ],
                "title": "On the Convergence of Large Language Model Optimizer for Black-Box\n  Network Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Convergence of Large Language Model Optimizer for Black-Box\n  Network Management"
                },
                "summary": "Future wireless networks are expected to incorporate diverse services that\noften lack general mathematical models. To address such black-box network\nmanagement tasks, the large language model (LLM) optimizer framework, which\nleverages pretrained LLMs as optimization agents, has recently been promoted as\na promising solution. This framework utilizes natural language prompts\ndescribing the given optimization problems along with past solutions generated\nby LLMs themselves. As a result, LLMs can obtain efficient solutions\nautonomously without knowing the mathematical models of the objective\nfunctions. Although the viability of the LLM optimizer (LLMO) framework has\nbeen studied in various black-box scenarios, it has so far been limited to\nnumerical simulations. For the first time, this paper establishes a theoretical\nfoundation for the LLMO framework. With careful investigations of LLM inference\nsteps, we can interpret the LLMO procedure as a finite-state Markov chain, and\nprove the convergence of the framework. Our results are extended to a more\nadvanced multiple LLM architecture, where the impact of multiple LLMs is\nrigorously verified in terms of the convergence rate. Comprehensive numerical\nsimulations validate our theoretical results and provide a deeper understanding\nof the underlying mechanisms of the LLMO framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future wireless networks are expected to incorporate diverse services that\noften lack general mathematical models. To address such black-box network\nmanagement tasks, the large language model (LLM) optimizer framework, which\nleverages pretrained LLMs as optimization agents, has recently been promoted as\na promising solution. This framework utilizes natural language prompts\ndescribing the given optimization problems along with past solutions generated\nby LLMs themselves. As a result, LLMs can obtain efficient solutions\nautonomously without knowing the mathematical models of the objective\nfunctions. Although the viability of the LLM optimizer (LLMO) framework has\nbeen studied in various black-box scenarios, it has so far been limited to\nnumerical simulations. For the first time, this paper establishes a theoretical\nfoundation for the LLMO framework. With careful investigations of LLM inference\nsteps, we can interpret the LLMO procedure as a finite-state Markov chain, and\nprove the convergence of the framework. Our results are extended to a more\nadvanced multiple LLM architecture, where the impact of multiple LLMs is\nrigorously verified in terms of the convergence rate. Comprehensive numerical\nsimulations validate our theoretical results and provide a deeper understanding\nof the underlying mechanisms of the LLMO framework."
                },
                "authors": [
                    {
                        "name": "Hoon Lee"
                    },
                    {
                        "name": "Wentao Zhou"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Inkyu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Inkyu Lee"
                },
                "author": "Inkyu Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02680v1",
                "updated": "2025-07-03T14:42:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    42,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:42:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    42,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "On the Architectural Split and Radio Intelligence Controller Placement\n  in Integrated O-RAN-enabled Non-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Architectural Split and Radio Intelligence Controller Placement\n  in Integrated O-RAN-enabled Non-Terrestrial Networks"
                },
                "summary": "The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks\n(NTNs) poses unique architectural and functional challenges due to\nheterogeneous propagation conditions, dynamic topologies and limited on-board\nprocessing capabilities. This paper examines architectural and functional split\nstrategies that are consistent with O-RAN principles for future integrated\nTN-NTN systems. A taxonomy of split options is proposed that distributes RAN\nand core functions between satellites and ground nodes, and trade-offs in terms\nof performance, latency, autonomy and deployment are analysed. In particular,\nwe evaluate configurations ranging from pure on-board DU deployments to full\ngNB and UPF integration into satellites, including variations based on intra-\nand inter-satellite processing. In addition, the placement of Near-RT and\nNon-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible\nsplit strategies between space and ground to optimise the performance and\nscalability of the control loop. A comprehensive mapping between architectural\nsplits and RIC placement options is provided, emphasising implementation\nconstraints and interoperability considerations. The paper concludes by\nidentifying key challenges and outlining future directions to enable\nstandardised, modular and efficient TN-NTN convergence in the context of the\nO-RAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks\n(NTNs) poses unique architectural and functional challenges due to\nheterogeneous propagation conditions, dynamic topologies and limited on-board\nprocessing capabilities. This paper examines architectural and functional split\nstrategies that are consistent with O-RAN principles for future integrated\nTN-NTN systems. A taxonomy of split options is proposed that distributes RAN\nand core functions between satellites and ground nodes, and trade-offs in terms\nof performance, latency, autonomy and deployment are analysed. In particular,\nwe evaluate configurations ranging from pure on-board DU deployments to full\ngNB and UPF integration into satellites, including variations based on intra-\nand inter-satellite processing. In addition, the placement of Near-RT and\nNon-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible\nsplit strategies between space and ground to optimise the performance and\nscalability of the control loop. A comprehensive mapping between architectural\nsplits and RIC placement options is provided, emphasising implementation\nconstraints and interoperability considerations. The paper concludes by\nidentifying key challenges and outlining future directions to enable\nstandardised, modular and efficient TN-NTN convergence in the context of the\nO-RAN."
                },
                "authors": [
                    {
                        "name": "Jorge Baranda"
                    },
                    {
                        "name": "Marius Caus"
                    },
                    {
                        "name": "Luis Blanco"
                    },
                    {
                        "name": "Cristian J. Vaca-Rubio"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Tomaso DeCola"
                    }
                ],
                "author_detail": {
                    "name": "Tomaso DeCola"
                },
                "author": "Tomaso DeCola",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13886v2",
                "updated": "2025-07-03T14:30:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    30,
                    25,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-20T03:47:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    47,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning"
                },
                "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Jixin Tang"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yanbo Wen"
                    },
                    {
                        "name": "Fan Song"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Yuyang Lu"
                    },
                    {
                        "name": "Chaoran Tao"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Jizhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "63 pages, 23 figures, submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02660v1",
                "updated": "2025-07-03T14:20:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &\n  Verification"
                },
                "summary": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability."
                },
                "authors": [
                    {
                        "name": "Deepak Narayan Gadde"
                    },
                    {
                        "name": "Keerthan Kopparam Radhakrishna"
                    },
                    {
                        "name": "Vaisakh Naduvodi Viswambharan"
                    },
                    {
                        "name": "Aman Kumar"
                    },
                    {
                        "name": "Djones Lettnin"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    },
                    {
                        "name": "Sebastian Simon"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Simon"
                },
                "author": "Sebastian Simon",
                "arxiv_comment": "To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated\n  Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02654v1",
                "updated": "2025-07-03T14:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    18,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    18,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure"
                },
                "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Sanchari Sen"
                    },
                    {
                        "name": "Swagath Venkataramani"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09676v2",
                "updated": "2025-07-03T14:13:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    13,
                    44,
                    3,
                    184,
                    0
                ],
                "published": "2024-09-15T09:55:18Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    9,
                    55,
                    18,
                    6,
                    259,
                    0
                ],
                "title": "Nebula: Efficient, Private and Accurate Histogram Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nebula: Efficient, Private and Accurate Histogram Estimation"
                },
                "summary": "We present \\textit{Nebula}, a system for differentially private histogram\nestimation on data distributed among clients. \\textit{Nebula} allows clients to\nindependently decide whether to participate in the system, and locally encode\ntheir data so that an untrusted server only learns data values whose\nmultiplicity exceeds a predefined aggregation threshold, with\n$(\\varepsilon,\\delta)$ differential privacy guarantees. Compared to existing\nsystems, \\textit{Nebula} uniquely achieves: \\textit{i)} a strict upper bound on\nclient privacy leakage; \\textit{ii)} significantly higher utility than standard\nlocal differential privacy systems; and \\textit{iii)} no requirement for\ntrusted third-parties, multi-party computation, or trusted hardware. We provide\na formal evaluation of \\textit{Nebula}'s privacy, utility and efficiency\nguarantees, along with an empirical assessment on three real-world datasets. On\nthe United States Census dataset, clients can submit their data in just 0.0036\nseconds and 0.0016 MB (\\textbf{efficient}), under strong\n$(\\varepsilon=1,\\delta=10^{-8})$ differential privacy guarantees\n(\\textbf{private}), enabling \\textit{Nebula}'s untrusted aggregation server to\nestimate histograms with over 88\\% better utility than existing local\ndifferential privacy deployments (\\textbf{accurate}). Additionally, we describe\na variant that allows clients to submit multi-dimensional data, with similar\nprivacy, utility, and performance. Finally, we provide an implementation of\n\\textit{Nebula}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\textit{Nebula}, a system for differentially private histogram\nestimation on data distributed among clients. \\textit{Nebula} allows clients to\nindependently decide whether to participate in the system, and locally encode\ntheir data so that an untrusted server only learns data values whose\nmultiplicity exceeds a predefined aggregation threshold, with\n$(\\varepsilon,\\delta)$ differential privacy guarantees. Compared to existing\nsystems, \\textit{Nebula} uniquely achieves: \\textit{i)} a strict upper bound on\nclient privacy leakage; \\textit{ii)} significantly higher utility than standard\nlocal differential privacy systems; and \\textit{iii)} no requirement for\ntrusted third-parties, multi-party computation, or trusted hardware. We provide\na formal evaluation of \\textit{Nebula}'s privacy, utility and efficiency\nguarantees, along with an empirical assessment on three real-world datasets. On\nthe United States Census dataset, clients can submit their data in just 0.0036\nseconds and 0.0016 MB (\\textbf{efficient}), under strong\n$(\\varepsilon=1,\\delta=10^{-8})$ differential privacy guarantees\n(\\textbf{private}), enabling \\textit{Nebula}'s untrusted aggregation server to\nestimate histograms with over 88\\% better utility than existing local\ndifferential privacy deployments (\\textbf{accurate}). Additionally, we describe\na variant that allows clients to submit multi-dimensional data, with similar\nprivacy, utility, and performance. Finally, we provide an implementation of\n\\textit{Nebula}."
                },
                "authors": [
                    {
                        "name": "Ali Shahin Shamsabadi"
                    },
                    {
                        "name": "Peter Snyder"
                    },
                    {
                        "name": "Ralph Giles"
                    },
                    {
                        "name": "Aurlien Bellet"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02648v1",
                "updated": "2025-07-03T14:12:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    12,
                    43,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    12,
                    43,
                    3,
                    184,
                    0
                ],
                "title": "Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI\n  Supply Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI\n  Supply Chains"
                },
                "summary": "The AI industry is exploding in popularity, with increasing attention to\npotential harms and unwanted consequences. In the current digital ecosystem, AI\ndeployments are often the product of AI supply chains (AISC): networks of\noutsourced models, data, and tooling through which multiple entities contribute\nto AI development and distribution. AI supply chains lack the modularity,\nredundancies, or conventional supply chain practices that enable\nidentification, isolation, and easy correction of failures, exacerbating the\nalready difficult processes of responding to ML-generated harms. As the\nstakeholders participating in and impacted by AISCs have scaled and\ndiversified, so too have the risks they face. In this stakeholder analysis of\nAI supply chains, we consider who participates in AISCs, what harms they face,\nwhere sources of harm lie, and how market dynamics and power differentials\ninform the type and probability of remedies. Because AI supply chains are\npurposely invented and implemented, they may be designed to account for, rather\nthan ignore, the complexities, consequences, and risks of deploying AI systems.\nTo enable responsible design and management of AISCs, we offer a typology of\nresponses to AISC-induced harms: recourse, repair, reparation or prevention. We\napply this typology to stakeholders participating in a health-care AISC across\nthree stylized markets $\\unicode{x2013}$ vertical integration, horizontal\nintegration, free market $\\unicode{x2013}$ to illustrate how stakeholder\npositioning and power within an AISC may shape responses to an experienced\nharm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI industry is exploding in popularity, with increasing attention to\npotential harms and unwanted consequences. In the current digital ecosystem, AI\ndeployments are often the product of AI supply chains (AISC): networks of\noutsourced models, data, and tooling through which multiple entities contribute\nto AI development and distribution. AI supply chains lack the modularity,\nredundancies, or conventional supply chain practices that enable\nidentification, isolation, and easy correction of failures, exacerbating the\nalready difficult processes of responding to ML-generated harms. As the\nstakeholders participating in and impacted by AISCs have scaled and\ndiversified, so too have the risks they face. In this stakeholder analysis of\nAI supply chains, we consider who participates in AISCs, what harms they face,\nwhere sources of harm lie, and how market dynamics and power differentials\ninform the type and probability of remedies. Because AI supply chains are\npurposely invented and implemented, they may be designed to account for, rather\nthan ignore, the complexities, consequences, and risks of deploying AI systems.\nTo enable responsible design and management of AISCs, we offer a typology of\nresponses to AISC-induced harms: recourse, repair, reparation or prevention. We\napply this typology to stakeholders participating in a health-care AISC across\nthree stylized markets $\\unicode{x2013}$ vertical integration, horizontal\nintegration, free market $\\unicode{x2013}$ to illustrate how stakeholder\npositioning and power within an AISC may shape responses to an experienced\nharm."
                },
                "authors": [
                    {
                        "name": "Aspen K. Hopkins"
                    },
                    {
                        "name": "Isabella Struckman"
                    },
                    {
                        "name": "Kevin Klyman"
                    },
                    {
                        "name": "Susan S. Silbey"
                    }
                ],
                "author_detail": {
                    "name": "Susan S. Silbey"
                },
                "author": "Susan S. Silbey",
                "arxiv_doi": "10.1145/3715275.3732017.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732017.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.02648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22343v2",
                "updated": "2025-07-03T14:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    3,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T13:27:07Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    27,
                    7,
                    2,
                    148,
                    0
                ],
                "title": "Empowering Intelligent Low-altitude Economy with Large AI Model\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Intelligent Low-altitude Economy with Large AI Model\n  Deployment"
                },
                "summary": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research."
                },
                "authors": [
                    {
                        "name": "Zhonghao Lyu"
                    },
                    {
                        "name": "Yulan Gao"
                    },
                    {
                        "name": "Junting Chen"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02628v1",
                "updated": "2025-07-03T13:54:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    54,
                    50,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:54:50Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    54,
                    50,
                    3,
                    184,
                    0
                ],
                "title": "Medical Data Pecking: A Context-Aware Approach for Automated Quality\n  Evaluation of Structured Medical Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Data Pecking: A Context-Aware Approach for Automated Quality\n  Evaluation of Structured Medical Data"
                },
                "summary": "Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods."
                },
                "authors": [
                    {
                        "name": "Irena Girshovitz"
                    },
                    {
                        "name": "Atai Ambus"
                    },
                    {
                        "name": "Moni Shahar"
                    },
                    {
                        "name": "Ran Gilad-Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Ran Gilad-Bachrach"
                },
                "author": "Ran Gilad-Bachrach",
                "arxiv_comment": "18 pages, 4 figures (+ appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02626v1",
                "updated": "2025-07-03T13:52:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    52,
                    24,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:52:24Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    52,
                    24,
                    3,
                    184,
                    0
                ],
                "title": "VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via\n  Reinforcement Learning"
                },
                "summary": "Owing to powerful natural language processing and generative capabilities,\nlarge language model (LLM) agents have emerged as a promising solution for\nenhancing recommendation systems via user simulation. However, in the realm of\nvideo recommendation, existing studies predominantly resort to prompt-based\nsimulation using frozen LLMs and encounter the intricate challenge of\nmultimodal content understanding. This frequently results in suboptimal item\nmodeling and user preference learning, thereby ultimately constraining\nrecommendation performance. To address these challenges, we introduce\nVRAgent-R1, a novel agent-based paradigm that incorporates human-like\nintelligence in user simulation. Specifically, VRAgent-R1 comprises two\ndistinct agents: the Item Perception (IP) Agent and the User Simulation (US)\nAgent, designed for interactive user-item modeling. Firstly, the IP Agent\nemulates human-like progressive thinking based on MLLMs, effectively capturing\nhidden recommendation semantics in videos. With a more comprehensive multimodal\ncontent understanding provided by the IP Agent, the video recommendation system\nis equipped to provide higher-quality candidate items. Subsequently, the US\nAgent refines the recommended video sets based on in-depth chain-of-thought\n(CoT) reasoning and achieves better alignment with real user preferences\nthrough reinforcement learning. Experimental results on a large-scale video\nrecommendation benchmark have demonstrated the effectiveness of our proposed\nVRAgent-R1 method, e.g., the IP Agent achieves a 6.0\\% improvement in NDCG@10\non the MicroLens-100k dataset, while the US Agent shows approximately 45.0\\%\nhigher accuracy in user decision simulation compared to state-of-the-art\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to powerful natural language processing and generative capabilities,\nlarge language model (LLM) agents have emerged as a promising solution for\nenhancing recommendation systems via user simulation. However, in the realm of\nvideo recommendation, existing studies predominantly resort to prompt-based\nsimulation using frozen LLMs and encounter the intricate challenge of\nmultimodal content understanding. This frequently results in suboptimal item\nmodeling and user preference learning, thereby ultimately constraining\nrecommendation performance. To address these challenges, we introduce\nVRAgent-R1, a novel agent-based paradigm that incorporates human-like\nintelligence in user simulation. Specifically, VRAgent-R1 comprises two\ndistinct agents: the Item Perception (IP) Agent and the User Simulation (US)\nAgent, designed for interactive user-item modeling. Firstly, the IP Agent\nemulates human-like progressive thinking based on MLLMs, effectively capturing\nhidden recommendation semantics in videos. With a more comprehensive multimodal\ncontent understanding provided by the IP Agent, the video recommendation system\nis equipped to provide higher-quality candidate items. Subsequently, the US\nAgent refines the recommended video sets based on in-depth chain-of-thought\n(CoT) reasoning and achieves better alignment with real user preferences\nthrough reinforcement learning. Experimental results on a large-scale video\nrecommendation benchmark have demonstrated the effectiveness of our proposed\nVRAgent-R1 method, e.g., the IP Agent achieves a 6.0\\% improvement in NDCG@10\non the MicroLens-100k dataset, while the US Agent shows approximately 45.0\\%\nhigher accuracy in user decision simulation compared to state-of-the-art\nbaselines."
                },
                "authors": [
                    {
                        "name": "Siran Chen"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Chenyun Yu"
                    },
                    {
                        "name": "Yuxiao Luo"
                    },
                    {
                        "name": "Ouyang Yi"
                    },
                    {
                        "name": "Lei Cheng"
                    },
                    {
                        "name": "Chengxiang Zhuo"
                    },
                    {
                        "name": "Zang Li"
                    },
                    {
                        "name": "Yali Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yali Wang"
                },
                "author": "Yali Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02620v1",
                "updated": "2025-07-03T13:47:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    47,
                    42,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:47:42Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    47,
                    42,
                    3,
                    184,
                    0
                ],
                "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference"
                },
                "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}"
                },
                "authors": [
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Lizhuo Luo"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "16 pages, and the last 3 are appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02618v1",
                "updated": "2025-07-03T13:45:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    45,
                    2,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:45:02Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    45,
                    2,
                    3,
                    184,
                    0
                ],
                "title": "Strategic Intelligence in Large Language Models: Evidence from\n  evolutionary Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Intelligence in Large Language Models: Evidence from\n  evolutionary Game Theory"
                },
                "summary": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty."
                },
                "authors": [
                    {
                        "name": "Kenneth Payne"
                    },
                    {
                        "name": "Baptiste Alloui-Cros"
                    }
                ],
                "author_detail": {
                    "name": "Baptiste Alloui-Cros"
                },
                "author": "Baptiste Alloui-Cros",
                "arxiv_comment": "29 pages, 27 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02616v1",
                "updated": "2025-07-03T13:43:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    43,
                    10,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:43:10Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    43,
                    10,
                    3,
                    184,
                    0
                ],
                "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and\n  Open-Ended Medical Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and\n  Open-Ended Medical Decision-Making"
                },
                "summary": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents."
                },
                "authors": [
                    {
                        "name": "Tianqi Shang"
                    },
                    {
                        "name": "Weiqing He"
                    },
                    {
                        "name": "Charles Zheng"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Bingxin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bingxin Zhao"
                },
                "author": "Bingxin Zhao",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08713v2",
                "updated": "2025-07-03T13:39:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    39,
                    37,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-10T11:56:06Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    11,
                    56,
                    6,
                    1,
                    161,
                    0
                ],
                "title": "Explainable Compliance Detection with Multi-Hop Natural Language\n  Inference on Assurance Case Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Compliance Detection with Multi-Hop Natural Language\n  Inference on Assurance Case Structure"
                },
                "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process."
                },
                "authors": [
                    {
                        "name": "Fariz Ikhwantri"
                    },
                    {
                        "name": "Dusica Marijan"
                    }
                ],
                "author_detail": {
                    "name": "Dusica Marijan"
                },
                "author": "Dusica Marijan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11853v2",
                "updated": "2025-07-03T13:25:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    25,
                    12,
                    3,
                    184,
                    0
                ],
                "published": "2025-02-17T14:46:38Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    46,
                    38,
                    0,
                    48,
                    0
                ],
                "title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models"
                },
                "summary": "In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g., SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to a 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing content\ntransformations, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g., SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to a 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing content\ntransformations, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection."
                },
                "authors": [
                    {
                        "name": "Shehel Yoosuf"
                    },
                    {
                        "name": "Temoor Ali"
                    },
                    {
                        "name": "Ahmed Lekssays"
                    },
                    {
                        "name": "Mashael AlSabah"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07618v2",
                "updated": "2025-07-03T13:18:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    18,
                    23,
                    3,
                    184,
                    0
                ],
                "published": "2024-11-12T07:54:13Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    7,
                    54,
                    13,
                    1,
                    317,
                    0
                ],
                "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization Using Sparse Feature-Level Constraints"
                },
                "summary": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments."
                },
                "authors": [
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Linyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linyi Yang"
                },
                "author": "Linyi Yang",
                "arxiv_journal_ref": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01334v2",
                "updated": "2025-07-03T13:15:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    15,
                    11,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T03:51:16Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    3,
                    51,
                    16,
                    2,
                    183,
                    0
                ],
                "title": "Symbolic or Numerical? Understanding Physics Problem Solving in\n  Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic or Numerical? Understanding Physics Problem Solving in\n  Reasoning LLMs"
                },
                "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains."
                },
                "authors": [
                    {
                        "name": "Nifu Dan"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02595v1",
                "updated": "2025-07-03T13:09:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    9,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T13:09:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    9,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi\n  Perspective Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPF: Aligning and Debiasing Language Models post Deployment via Multi\n  Perspective Fusion"
                },
                "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning."
                },
                "authors": [
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "PeiHsin Lin"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Ruibo Zhang"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Koshiyama"
                },
                "author": "Adriano Koshiyama",
                "arxiv_comment": "Accepted at ICML 2025 AIW Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19688v3",
                "updated": "2025-07-03T13:07:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    7,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2024-11-29T13:22:52Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks"
                },
                "summary": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa."
                },
                "authors": [
                    {
                        "name": "Kim-Celine Kahl"
                    },
                    {
                        "name": "Selen Erkan"
                    },
                    {
                        "name": "Jeremias Traub"
                    },
                    {
                        "name": "Carsten T. Lth"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    },
                    {
                        "name": "Lena Maier-Hein"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    }
                ],
                "author_detail": {
                    "name": "Paul F. Jaeger"
                },
                "author": "Paul F. Jaeger",
                "arxiv_comment": "TMLR 07/2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12532v3",
                "updated": "2025-07-03T13:02:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    2,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-16T13:10:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based\n  Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based\n  Agent Collaboration"
                },
                "summary": "In healthcare intelligence, the ability to fuse heterogeneous, multi-intent\ninformation from diverse clinical sources is fundamental to building reliable\ndecision-making systems. Large Language Model (LLM)-driven information\ninteraction systems currently showing potential promise in the healthcare\ndomain. Nevertheless, they often suffer from information redundancy and\ncoupling when dealing with complex medical intents, leading to severe\nhallucinations and performance bottlenecks. To this end, we propose MedAide, an\nLLM-based medical multi-agent collaboration framework designed to enable\nintent-aware information fusion and coordinated reasoning across specialized\nhealthcare domains. Specifically, we introduce a regularization-guided module\nthat combines syntactic constraints with retrieval augmented generation to\ndecompose complex queries into structured representations, facilitating\nfine-grained clinical information fusion and intent resolution. Additionally, a\ndynamic intent prototype matching module is proposed to utilize dynamic\nprototype representation with a semantic similarity matching mechanism to\nachieve adaptive recognition and updating of the agent's intent in multi-round\nhealthcare dialogues. Ultimately, we design a rotation agent collaboration\nmechanism that introduces dynamic role rotation and decision-level information\nfusion across specialized medical agents. Extensive experiments are conducted\non four medical benchmarks with composite intents. Experimental results from\nautomated metrics and expert doctor evaluations show that MedAide outperforms\ncurrent LLMs and improves their medical proficiency and strategic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In healthcare intelligence, the ability to fuse heterogeneous, multi-intent\ninformation from diverse clinical sources is fundamental to building reliable\ndecision-making systems. Large Language Model (LLM)-driven information\ninteraction systems currently showing potential promise in the healthcare\ndomain. Nevertheless, they often suffer from information redundancy and\ncoupling when dealing with complex medical intents, leading to severe\nhallucinations and performance bottlenecks. To this end, we propose MedAide, an\nLLM-based medical multi-agent collaboration framework designed to enable\nintent-aware information fusion and coordinated reasoning across specialized\nhealthcare domains. Specifically, we introduce a regularization-guided module\nthat combines syntactic constraints with retrieval augmented generation to\ndecompose complex queries into structured representations, facilitating\nfine-grained clinical information fusion and intent resolution. Additionally, a\ndynamic intent prototype matching module is proposed to utilize dynamic\nprototype representation with a semantic similarity matching mechanism to\nachieve adaptive recognition and updating of the agent's intent in multi-round\nhealthcare dialogues. Ultimately, we design a rotation agent collaboration\nmechanism that introduces dynamic role rotation and decision-level information\nfusion across specialized medical agents. Extensive experiments are conducted\non four medical benchmarks with composite intents. Experimental results from\nautomated metrics and expert doctor evaluations show that MedAide outperforms\ncurrent LLMs and improves their medical proficiency and strategic reasoning."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Jiyao Liu"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yakun Ju"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "LLM-based Multi-Agent Collaboration for Medical Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02593v1",
                "updated": "2025-07-03T12:59:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:59:28Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    28,
                    3,
                    184,
                    0
                ],
                "title": "Revisiting Active Learning under (Human) Label Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Active Learning under (Human) Label Variation"
                },
                "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation."
                },
                "authors": [
                    {
                        "name": "Cornelia Gruber"
                    },
                    {
                        "name": "Helen Alber"
                    },
                    {
                        "name": "Bernd Bischl"
                    },
                    {
                        "name": "Gran Kauermann"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Aenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Aenmacher"
                },
                "author": "Matthias Aenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02592v1",
                "updated": "2025-07-03T12:59:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    7,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:59:07Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    59,
                    7,
                    3,
                    184,
                    0
                ],
                "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebSailor: Navigating Super-human Reasoning for Web Agent"
                },
                "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap."
                },
                "authors": [
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Weizhou Shen"
                    },
                    {
                        "name": "Junkai Zhang"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02591v1",
                "updated": "2025-07-03T12:55:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    55,
                    16,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:55:16Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    55,
                    16,
                    3,
                    184,
                    0
                ],
                "title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video\n  Understanding"
                },
                "summary": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding."
                },
                "authors": [
                    {
                        "name": "Weili Xu"
                    },
                    {
                        "name": "Enxin Song"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Xuexiang Wen"
                    },
                    {
                        "name": "Tian Ye"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02564v1",
                "updated": "2025-07-03T12:18:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    18,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:18:05Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    18,
                    5,
                    3,
                    184,
                    0
                ],
                "title": "LLMREI: Automating Requirements Elicitation Interviews with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMREI: Automating Requirements Elicitation Interviews with LLMs"
                },
                "summary": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders."
                },
                "authors": [
                    {
                        "name": "Alexander Korn"
                    },
                    {
                        "name": "Samuel Gorsch"
                    },
                    {
                        "name": "Andreas Vogelsang"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vogelsang"
                },
                "author": "Andreas Vogelsang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02562v1",
                "updated": "2025-07-03T12:12:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    12,
                    34,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T12:12:34Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    12,
                    12,
                    34,
                    3,
                    184,
                    0
                ],
                "title": "Multi-Utterance Speech Separation and Association Trained on Short\n  Segments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Utterance Speech Separation and Association Trained on Short\n  Segments"
                },
                "summary": "Current deep neural network (DNN) based speech separation faces a fundamental\nchallenge -- while the models need to be trained on short segments due to\ncomputational constraints, real-world applications typically require processing\nsignificantly longer recordings with multiple utterances per speaker than seen\nduring training. In this paper, we investigate how existing approaches perform\nin this challenging scenario and propose a frequency-temporal recurrent neural\nnetwork (FTRNN) that effectively bridges this gap. Our FTRNN employs a\nfull-band module to model frequency dependencies within each time frame and a\nsub-band module that models temporal patterns in each frequency band. Despite\nbeing trained on short fixed-length segments of 10 s, our model demonstrates\nrobust separation when processing signals significantly longer than training\nsegments (21-121 s) and preserves speaker association across utterance gaps\nexceeding those seen during training. Unlike the conventional\nsegment-separation-stitch paradigm, our lightweight approach (0.9 M parameters)\nperforms inference on long audio without segmentation, eliminating segment\nboundary distortions while simplifying deployment. Experimental results\ndemonstrate the generalization ability of FTRNN for multi-utterance speech\nseparation and speaker association.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current deep neural network (DNN) based speech separation faces a fundamental\nchallenge -- while the models need to be trained on short segments due to\ncomputational constraints, real-world applications typically require processing\nsignificantly longer recordings with multiple utterances per speaker than seen\nduring training. In this paper, we investigate how existing approaches perform\nin this challenging scenario and propose a frequency-temporal recurrent neural\nnetwork (FTRNN) that effectively bridges this gap. Our FTRNN employs a\nfull-band module to model frequency dependencies within each time frame and a\nsub-band module that models temporal patterns in each frequency band. Despite\nbeing trained on short fixed-length segments of 10 s, our model demonstrates\nrobust separation when processing signals significantly longer than training\nsegments (21-121 s) and preserves speaker association across utterance gaps\nexceeding those seen during training. Unlike the conventional\nsegment-separation-stitch paradigm, our lightweight approach (0.9 M parameters)\nperforms inference on long audio without segmentation, eliminating segment\nboundary distortions while simplifying deployment. Experimental results\ndemonstrate the generalization ability of FTRNN for multi-utterance speech\nseparation and speaker association."
                },
                "authors": [
                    {
                        "name": "Yuzhu Wang"
                    },
                    {
                        "name": "Archontis Politis"
                    },
                    {
                        "name": "Konstantinos Drossos"
                    },
                    {
                        "name": "Tuomas Virtanen"
                    }
                ],
                "author_detail": {
                    "name": "Tuomas Virtanen"
                },
                "author": "Tuomas Virtanen",
                "arxiv_comment": "5 pages, accepted by WASPAA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03259v2",
                "updated": "2025-07-03T11:56:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    56,
                    7,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-05T08:33:08Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    33,
                    8,
                    2,
                    64,
                    0
                ],
                "title": "BANet: Bilateral Aggregation Network for Mobile Stereo Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BANet: Bilateral Aggregation Network for Mobile Stereo Matching"
                },
                "summary": "State-of-the-art stereo matching methods typically use costly 3D convolutions\nto aggregate a full cost volume, but their computational demands make mobile\ndeployment challenging. Directly applying 2D convolutions for cost aggregation\noften results in edge blurring, detail loss, and mismatches in textureless\nregions. Some complex operations, like deformable convolutions and iterative\nwarping, can partially alleviate this issue; however, they are not\nmobile-friendly, limiting their deployment on mobile devices. In this paper, we\npresent a novel bilateral aggregation network (BANet) for mobile stereo\nmatching that produces high-quality results with sharp edges and fine details\nusing only 2D convolutions. Specifically, we first separate the full cost\nvolume into detailed and smooth volumes using a spatial attention map, then\nperform detailed and smooth aggregations accordingly, ultimately fusing both to\nobtain the final disparity map. Experimental results demonstrate that our\nBANet-2D significantly outperforms other mobile-friendly methods, achieving\n35.3\\% higher accuracy on the KITTI 2015 leaderboard than MobileStereoNet-2D,\nwith faster runtime on mobile devices. Code:\n\\textcolor{magenta}{https://github.com/gangweix/BANet}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art stereo matching methods typically use costly 3D convolutions\nto aggregate a full cost volume, but their computational demands make mobile\ndeployment challenging. Directly applying 2D convolutions for cost aggregation\noften results in edge blurring, detail loss, and mismatches in textureless\nregions. Some complex operations, like deformable convolutions and iterative\nwarping, can partially alleviate this issue; however, they are not\nmobile-friendly, limiting their deployment on mobile devices. In this paper, we\npresent a novel bilateral aggregation network (BANet) for mobile stereo\nmatching that produces high-quality results with sharp edges and fine details\nusing only 2D convolutions. Specifically, we first separate the full cost\nvolume into detailed and smooth volumes using a spatial attention map, then\nperform detailed and smooth aggregations accordingly, ultimately fusing both to\nobtain the final disparity map. Experimental results demonstrate that our\nBANet-2D significantly outperforms other mobile-friendly methods, achieving\n35.3\\% higher accuracy on the KITTI 2015 leaderboard than MobileStereoNet-2D,\nwith faster runtime on mobile devices. Code:\n\\textcolor{magenta}{https://github.com/gangweix/BANet}."
                },
                "authors": [
                    {
                        "name": "Gangwei Xu"
                    },
                    {
                        "name": "Jiaxin Liu"
                    },
                    {
                        "name": "Xianqi Wang"
                    },
                    {
                        "name": "Junda Cheng"
                    },
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Jinliang Zang"
                    },
                    {
                        "name": "Yurui Chen"
                    },
                    {
                        "name": "Xin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yang"
                },
                "author": "Xin Yang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08500v2",
                "updated": "2025-07-03T11:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    55,
                    43,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-11T03:54:48Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    3,
                    54,
                    48,
                    4,
                    285,
                    0
                ],
                "title": "Aerial Vision-and-Language Navigation via Semantic-Topo-Metric\n  Representation Guided LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial Vision-and-Language Navigation via Semantic-Topo-Metric\n  Representation Guided LLM Reasoning"
                },
                "summary": "Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned\nAerial Vehicles (UAVs) to navigate in outdoor environments through natural\nlanguage instructions and visual cues. It remains challenging due to the\ncomplex spatial relationships in outdoor aerial scenes. In this paper, we\npropose an end-to-end zero-shot framework for aerial VLN tasks, where the large\nlanguage model (LLM) is introduced as our agent for action prediction.\nSpecifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to\nenhance the spatial reasoning ability of LLMs. This is achieved by extracting\nand projecting instruction-related semantic masks of landmarks into a top-down\nmap that contains the location information of surrounding landmarks. Further,\nthis map is transformed into a matrix representation with distance metrics as\nthe text prompt to the LLM, for action prediction according to the instruction.\nExperiments conducted in real and simulation environments have successfully\nproved the effectiveness and robustness of our method, achieving 15.9% and\n12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned\nAerial Vehicles (UAVs) to navigate in outdoor environments through natural\nlanguage instructions and visual cues. It remains challenging due to the\ncomplex spatial relationships in outdoor aerial scenes. In this paper, we\npropose an end-to-end zero-shot framework for aerial VLN tasks, where the large\nlanguage model (LLM) is introduced as our agent for action prediction.\nSpecifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to\nenhance the spatial reasoning ability of LLMs. This is achieved by extracting\nand projecting instruction-related semantic masks of landmarks into a top-down\nmap that contains the location information of surrounding landmarks. Further,\nthis map is transformed into a matrix representation with distance metrics as\nthe text prompt to the LLM, for action prediction according to the instruction.\nExperiments conducted in real and simulation environments have successfully\nproved the effectiveness and robustness of our method, achieving 15.9% and\n12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S\ndataset."
                },
                "authors": [
                    {
                        "name": "Yunpeng Gao"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Linglin Jing"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Bin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhao"
                },
                "author": "Bin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02541v1",
                "updated": "2025-07-03T11:35:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    35,
                    34,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:35:34Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    35,
                    34,
                    3,
                    184,
                    0
                ],
                "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clarifying Before Reasoning: A Coq Prover with Structural Context"
                },
                "summary": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning."
                },
                "authors": [
                    {
                        "name": "Yanzhen Lu"
                    },
                    {
                        "name": "Hanbin Yang"
                    },
                    {
                        "name": "Xiaodie Wang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Biao Li"
                    },
                    {
                        "name": "Chenxu Fu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02537v1",
                "updated": "2025-07-03T11:32:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    32,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:32:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    32,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue"
                },
                "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents."
                },
                "authors": [
                    {
                        "name": "Paulo Ricardo Knob"
                    },
                    {
                        "name": "Leonardo Scholler"
                    },
                    {
                        "name": "Juliano Rigatti"
                    },
                    {
                        "name": "Soraia Raupp Musse"
                    }
                ],
                "author_detail": {
                    "name": "Soraia Raupp Musse"
                },
                "author": "Soraia Raupp Musse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02533v1",
                "updated": "2025-07-03T11:20:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    20,
                    59,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:20:59Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    20,
                    59,
                    3,
                    184,
                    0
                ],
                "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models"
                },
                "summary": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing."
                },
                "authors": [
                    {
                        "name": "Miguel Romero-Arjona"
                    },
                    {
                        "name": "Jos A. Parejo"
                    },
                    {
                        "name": "Juan C. Alonso"
                    },
                    {
                        "name": "Ana B. Snchez"
                    },
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02530v1",
                "updated": "2025-07-03T11:12:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    12,
                    26,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T11:12:26Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    11,
                    12,
                    26,
                    3,
                    184,
                    0
                ],
                "title": "Open-Source System for Multilingual Translation and Cloned Speech\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source System for Multilingual Translation and Cloned Speech\n  Synthesis"
                },
                "summary": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios."
                },
                "authors": [
                    {
                        "name": "Mateo Cmara"
                    },
                    {
                        "name": "Juan Gutirrez"
                    },
                    {
                        "name": "Mara Pilar Daza"
                    },
                    {
                        "name": "Jos Luis Blanco"
                    }
                ],
                "author_detail": {
                    "name": "Jos Luis Blanco"
                },
                "author": "Jos Luis Blanco",
                "arxiv_comment": "Presented at Forum Acusticum Euronoise 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15075v2",
                "updated": "2025-07-03T10:35:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    35,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-21T03:43:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    3,
                    43,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs"
                },
                "summary": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Pinzhi Huang"
                    },
                    {
                        "name": "Jihan Yang"
                    },
                    {
                        "name": "Saining Xie"
                    },
                    {
                        "name": "Daisuke Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Daisuke Kawahara"
                },
                "author": "Daisuke Kawahara",
                "arxiv_comment": "https://github.com/nlp-waseda/traveling-across-languages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01551v2",
                "updated": "2025-07-03T10:33:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    33,
                    8,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:05:14Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    5,
                    14,
                    2,
                    183,
                    0
                ],
                "title": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning"
                },
                "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."
                },
                "authors": [
                    {
                        "name": "Wu Fei"
                    },
                    {
                        "name": "Hao Kong"
                    },
                    {
                        "name": "Shuxian Liang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiansheng Hua"
                    }
                ],
                "author_detail": {
                    "name": "Xiansheng Hua"
                },
                "author": "Xiansheng Hua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02503v1",
                "updated": "2025-07-03T10:11:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    11,
                    22,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T10:11:22Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    11,
                    22,
                    3,
                    184,
                    0
                ],
                "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs"
                },
                "summary": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP."
                },
                "authors": [
                    {
                        "name": "Chenxu Wang"
                    },
                    {
                        "name": "Yilin Lyu"
                    },
                    {
                        "name": "Zicheng Sun"
                    },
                    {
                        "name": "Liping Jing"
                    }
                ],
                "author_detail": {
                    "name": "Liping Jing"
                },
                "author": "Liping Jing",
                "arxiv_comment": "15 pages, 6 figures, accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20127v2",
                "updated": "2025-07-03T10:05:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    5,
                    46,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-26T15:26:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    26,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "Agentic AI Process Observability: Discovering Behavioral Variability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Process Observability: Discovering Behavioral Variability"
                },
                "summary": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions."
                },
                "authors": [
                    {
                        "name": "Fabiana Fournier"
                    },
                    {
                        "name": "Lior Limonad"
                    },
                    {
                        "name": "Yuval David"
                    }
                ],
                "author_detail": {
                    "name": "Yuval David"
                },
                "author": "Yuval David",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01119v3",
                "updated": "2025-07-03T09:32:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    32,
                    20,
                    3,
                    184,
                    0
                ],
                "published": "2024-08-02T09:00:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    0,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer"
                },
                "summary": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks."
                },
                "authors": [
                    {
                        "name": "Robert Belanec"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Maria Bielikova"
                    }
                ],
                "author_detail": {
                    "name": "Maria Bielikova"
                },
                "author": "Maria Bielikova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02456v1",
                "updated": "2025-07-03T09:13:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    13,
                    31,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T09:13:31Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    9,
                    13,
                    31,
                    3,
                    184,
                    0
                ],
                "title": "System-performance and cost modeling of Large Language Model training\n  and inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-performance and cost modeling of Large Language Model training\n  and inference"
                },
                "summary": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations."
                },
                "authors": [
                    {
                        "name": "Wenzhe Guo"
                    },
                    {
                        "name": "Joyjit Kundu"
                    },
                    {
                        "name": "Uras Tos"
                    },
                    {
                        "name": "Weijiang Kong"
                    },
                    {
                        "name": "Giuliano Sisto"
                    },
                    {
                        "name": "Timon Evenblij"
                    },
                    {
                        "name": "Manu Perumkunnil"
                    }
                ],
                "author_detail": {
                    "name": "Manu Perumkunnil"
                },
                "author": "Manu Perumkunnil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02439v1",
                "updated": "2025-07-03T08:52:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    52,
                    55,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T08:52:55Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    52,
                    55,
                    3,
                    184,
                    0
                ],
                "title": "Introducing a New Brexit-Related Uncertainty Index: Its Evolution and\n  Economic Consequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing a New Brexit-Related Uncertainty Index: Its Evolution and\n  Economic Consequences"
                },
                "summary": "Important game-changer economic events and transformations cause\nuncertainties that may affect investment decisions, capital flows,\ninternational trade, and macroeconomic variables. One such major transformation\nis Brexit, which refers to the multiyear process through which the UK withdrew\nfrom the EU. This study develops and uses a new Brexit-Related Uncertainty\nIndex (BRUI). In creating this index, we apply Text Mining, Context Window,\nNatural Language Processing (NLP), and Large Language Models (LLMs) from Deep\nLearning techniques to analyse the monthly country reports of the Economist\nIntelligence Unit from May 2012 to January 2025. Additionally, we employ a\nstandard vector autoregression (VAR) analysis to examine the model-implied\nresponses of various macroeconomic variables to BRUI shocks. While developing\nthe BRUI, we also create a complementary COVID-19 Related Uncertainty Index\n(CRUI) to distinguish the uncertainties stemming from these distinct events.\nEmpirical findings and comparisons of BRUI with other earlier-developed\nuncertainty indexes demonstrate the robustness of the new index. This new index\ncan assist British policymakers in measuring and understanding the impacts of\nBrexit-related uncertainties, enabling more effective policy formulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important game-changer economic events and transformations cause\nuncertainties that may affect investment decisions, capital flows,\ninternational trade, and macroeconomic variables. One such major transformation\nis Brexit, which refers to the multiyear process through which the UK withdrew\nfrom the EU. This study develops and uses a new Brexit-Related Uncertainty\nIndex (BRUI). In creating this index, we apply Text Mining, Context Window,\nNatural Language Processing (NLP), and Large Language Models (LLMs) from Deep\nLearning techniques to analyse the monthly country reports of the Economist\nIntelligence Unit from May 2012 to January 2025. Additionally, we employ a\nstandard vector autoregression (VAR) analysis to examine the model-implied\nresponses of various macroeconomic variables to BRUI shocks. While developing\nthe BRUI, we also create a complementary COVID-19 Related Uncertainty Index\n(CRUI) to distinguish the uncertainties stemming from these distinct events.\nEmpirical findings and comparisons of BRUI with other earlier-developed\nuncertainty indexes demonstrate the robustness of the new index. This new index\ncan assist British policymakers in measuring and understanding the impacts of\nBrexit-related uncertainties, enabling more effective policy formulation."
                },
                "authors": [
                    {
                        "name": "Ismet Gocer"
                    },
                    {
                        "name": "Julia Darby"
                    },
                    {
                        "name": "Serdar Ongan"
                    }
                ],
                "author_detail": {
                    "name": "Serdar Ongan"
                },
                "author": "Serdar Ongan",
                "arxiv_comment": "I introduce a new \"Brexit Related Uncertainty Index\", developed by\n  authors using Text Mining, NLP and LLMs. We tested our index's performance by\n  using diagrams and applying a VAR analysis on the US economy. We can share\n  our index with other researchers via email (ismet.gocer@solent.ac.uk). 29\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01548v2",
                "updated": "2025-07-03T08:45:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    45,
                    46,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:00:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    0,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants"
                },
                "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems."
                },
                "authors": [
                    {
                        "name": "Wen Zhan"
                    },
                    {
                        "name": "Ziqun Hua"
                    },
                    {
                        "name": "Peiyue Lin"
                    },
                    {
                        "name": "Yunfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunfei Chen"
                },
                "author": "Yunfei Chen",
                "arxiv_comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02424v1",
                "updated": "2025-07-03T08:32:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    32,
                    19,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T08:32:19Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    32,
                    19,
                    3,
                    184,
                    0
                ],
                "title": "CyberRAG: An agentic RAG cyber attack classification and reporting tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberRAG: An agentic RAG cyber attack classification and reporting tool"
                },
                "summary": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming security\nanalysts with logs that demand deep, rapidly evolving domain expertise.\nConventional machine-learning detectors trim the alert volume but still yield\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\na modular, agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\n(iii) an iterative retrieval-and-reason loop that continuously queries a\ndomain-specific knowledge base until the evidence is both relevant and\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\ndesign that enables dynamic control flow and adaptive reasoning. This\nagent-centric architecture refines its threat labels and natural-language\njustifications autonomously, reducing false positives and enhancing\ninterpretability. The framework is fully extensible: new attack types can be\nsupported by simply adding a classifier without retraining the core agent.\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\nfinal classification accuracy to 94.92% through semantic orchestration.\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\npractical and scalable path toward semi-autonomous cyber-defence workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming security\nanalysts with logs that demand deep, rapidly evolving domain expertise.\nConventional machine-learning detectors trim the alert volume but still yield\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\na modular, agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\n(iii) an iterative retrieval-and-reason loop that continuously queries a\ndomain-specific knowledge base until the evidence is both relevant and\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\ndesign that enables dynamic control flow and adaptive reasoning. This\nagent-centric architecture refines its threat labels and natural-language\njustifications autonomously, reducing false positives and enhancing\ninterpretability. The framework is fully extensible: new attack types can be\nsupported by simply adding a classifier without retraining the core agent.\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\nfinal classification accuracy to 94.92% through semantic orchestration.\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\npractical and scalable path toward semi-autonomous cyber-defence workflows."
                },
                "authors": [
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Cristian Cosentino"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Angelo Furfaro"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Marozzo"
                },
                "author": "Fabrizio Marozzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07016v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07016v5",
                "updated": "2025-07-03T08:26:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    26,
                    13,
                    3,
                    184,
                    0
                ],
                "published": "2024-06-11T07:16:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    7,
                    16,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary"
                },
                "summary": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic."
                },
                "authors": [
                    {
                        "name": "Dmitry Kobak"
                    },
                    {
                        "name": "Rita Gonzlez-Mrquez"
                    },
                    {
                        "name": "Emke-gnes Horvt"
                    },
                    {
                        "name": "Jan Lause"
                    }
                ],
                "author_detail": {
                    "name": "Jan Lause"
                },
                "author": "Jan Lause",
                "arxiv_doi": "10.1126/sciadv.adt3813",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1126/sciadv.adt3813",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07016v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07016v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "v5: Reverting to v3",
                "arxiv_journal_ref": "Science Advances, 2 Jul 2025, Vol. 11, No. 27",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14634v3",
                "updated": "2025-07-03T07:58:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    58,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-17T15:28:53Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    28,
                    53,
                    1,
                    168,
                    0
                ],
                "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation"
                },
                "summary": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research."
                },
                "authors": [
                    {
                        "name": "Leah von der Heyde"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Bernd Wei"
                    },
                    {
                        "name": "Jessica Daikeler"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Daikeler"
                },
                "author": "Jessica Daikeler",
                "arxiv_comment": "to appear in Survey Research Methods",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02390v1",
                "updated": "2025-07-03T07:38:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    38,
                    43,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:38:43Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    38,
                    43,
                    3,
                    184,
                    0
                ],
                "title": "Evaluating Language Models For Threat Detection in IoT Security Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Language Models For Threat Detection in IoT Security Logs"
                },
                "summary": "Log analysis is a relevant research field in cybersecurity as they can\nprovide a source of information for the detection of threats to networks and\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\nlogs. Utilizing classical machine learning classifiers as a baseline, three\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\ndataset. LLMs give better results on multi-class attack classification than the\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\nwith those actions, the models are able to provide a combined detection and\nrecommendation guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log analysis is a relevant research field in cybersecurity as they can\nprovide a source of information for the detection of threats to networks and\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\nlogs. Utilizing classical machine learning classifiers as a baseline, three\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\ndataset. LLMs give better results on multi-class attack classification than the\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\nwith those actions, the models are able to provide a combined detection and\nrecommendation guidance."
                },
                "authors": [
                    {
                        "name": "Jorge J. Tejero-Fernndez"
                    },
                    {
                        "name": "Alfonso Snchez-Macin"
                    }
                ],
                "author_detail": {
                    "name": "Alfonso Snchez-Macin"
                },
                "author": "Alfonso Snchez-Macin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07967v2",
                "updated": "2025-07-03T07:29:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    29,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-11T01:46:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    1,
                    46,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex\n  Software Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex\n  Software Maintenance"
                },
                "summary": "While large language models (LLMs) have demonstrated promise in software\nengineering tasks like code completion and generation, their support for the\nmaintenance of complex software systems remains limited. These models often\nstruggle with understanding the tacit knowledge embedded in systems, such as\nresponsibility allocation and collaboration across different modules. To\naddress this gap, we introduce the concept and framework of \\textbf{Code\nDigital Twin}, a conceptual representation of tacit knowledge that captures the\nconcepts, functionalities, and design rationales behind code elements,\nco-evolving with the software. A code digital twin is constructed using a\nmethodology that combines knowledge extraction from both structured and\nunstructured sources--such as source code, documentation, and change\nhistories--leveraging LLMs, static analysis tools, and human expertise. This\nframework can empower LLMs for software maintenance tasks such as issue\nlocalization and repository-level code generation by providing tacit knowledge\nas contexts. Based on the proposed methodology, we explore the key challenges\nand opportunities involved in the continuous construction and refinement of\ncode digital twin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated promise in software\nengineering tasks like code completion and generation, their support for the\nmaintenance of complex software systems remains limited. These models often\nstruggle with understanding the tacit knowledge embedded in systems, such as\nresponsibility allocation and collaboration across different modules. To\naddress this gap, we introduce the concept and framework of \\textbf{Code\nDigital Twin}, a conceptual representation of tacit knowledge that captures the\nconcepts, functionalities, and design rationales behind code elements,\nco-evolving with the software. A code digital twin is constructed using a\nmethodology that combines knowledge extraction from both structured and\nunstructured sources--such as source code, documentation, and change\nhistories--leveraging LLMs, static analysis tools, and human expertise. This\nframework can empower LLMs for software maintenance tasks such as issue\nlocalization and repository-level code generation by providing tacit knowledge\nas contexts. Based on the proposed methodology, we explore the key challenges\nand opportunities involved in the continuous construction and refinement of\ncode digital twin."
                },
                "authors": [
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yiling Lou"
                    },
                    {
                        "name": "Yijian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yijian Wu"
                },
                "author": "Yijian Wu",
                "arxiv_comment": "A vision paper that will be continuously updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02380v1",
                "updated": "2025-07-03T07:22:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    22,
                    6,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:22:06Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    22,
                    6,
                    3,
                    184,
                    0
                ],
                "title": "JoyTTS: LLM-based Spoken Chatbot With Voice Cloning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyTTS: LLM-based Spoken Chatbot With Voice Cloning"
                },
                "summary": "JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git."
                },
                "authors": [
                    {
                        "name": "Fangru Zhou"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Guoxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoxin Wang"
                },
                "author": "Guoxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02378v1",
                "updated": "2025-07-03T07:19:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    19,
                    56,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:19:56Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    19,
                    56,
                    3,
                    184,
                    0
                ],
                "title": "Efficient Code LLM Training via Distribution-Consistent and\n  Diversity-Aware Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Code LLM Training via Distribution-Consistent and\n  Diversity-Aware Data Selection"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs."
                },
                "authors": [
                    {
                        "name": "Weijie Lyu"
                    },
                    {
                        "name": "Sheng-Jun Huang"
                    },
                    {
                        "name": "Xuan Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Xia"
                },
                "author": "Xuan Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02376v1",
                "updated": "2025-07-03T07:17:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    17,
                    49,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:17:49Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    17,
                    49,
                    3,
                    184,
                    0
                ],
                "title": "VeFIA: An Efficient Inference Auditing Framework for Vertical Federated\n  Collaborative Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeFIA: An Efficient Inference Auditing Framework for Vertical Federated\n  Collaborative Software"
                },
                "summary": "Vertical Federated Learning (VFL) is a distributed AI software deployment\nmechanism for cross-silo collaboration without accessing participants' data.\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\nof the inference software of the data party. To address this problem, we design\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\nparty to audit whether the data party's inference software is executed as\nexpected during large-scale inference without leaking the data privacy of the\ndata party or introducing additional latency to the inference system. The core\nof VeFIA is that the task party can use the inference results from a framework\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\ncorrectness of the data party's computation results. VeFIA guarantees that, as\nlong as the abnormal inference exceeds 5.4%, the task party can detect\nexecution anomalies in the inference software with a probability of 99.99%,\nwithout incurring any additional online inference latency. VeFIA's random\nsampling validation achieves 100% positive predictive value, negative\npredictive value, and true positive rate in detecting abnormal inference. To\nthe best of our knowledge, this is the first paper to discuss the correctness\nof inference software execution in VFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) is a distributed AI software deployment\nmechanism for cross-silo collaboration without accessing participants' data.\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\nof the inference software of the data party. To address this problem, we design\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\nparty to audit whether the data party's inference software is executed as\nexpected during large-scale inference without leaking the data privacy of the\ndata party or introducing additional latency to the inference system. The core\nof VeFIA is that the task party can use the inference results from a framework\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\ncorrectness of the data party's computation results. VeFIA guarantees that, as\nlong as the abnormal inference exceeds 5.4%, the task party can detect\nexecution anomalies in the inference software with a probability of 99.99%,\nwithout incurring any additional online inference latency. VeFIA's random\nsampling validation achieves 100% positive predictive value, negative\npredictive value, and true positive rate in detecting abnormal inference. To\nthe best of our knowledge, this is the first paper to discuss the correctness\nof inference software execution in VFL."
                },
                "authors": [
                    {
                        "name": "Chung-ju Huang"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Binghui Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Leye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Leye Wang"
                },
                "author": "Leye Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00307v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00307v3",
                "updated": "2025-07-03T07:13:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    13,
                    52,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-01T04:59:05Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    4,
                    59,
                    5,
                    3,
                    121,
                    0
                ],
                "title": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations"
                },
                "summary": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer."
                },
                "authors": [
                    {
                        "name": "Yu-Hsiang Lan"
                    },
                    {
                        "name": "Eric K. Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric K. Oermann"
                },
                "author": "Eric K. Oermann",
                "arxiv_comment": "Accepted at ICML Workshop on Foundation Models for Structured Data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00307v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00307v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02373v1",
                "updated": "2025-07-03T07:08:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    8,
                    38,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:08:38Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    8,
                    38,
                    3,
                    184,
                    0
                ],
                "title": "UVLM: Benchmarking Video Language Model for Underwater World\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UVLM: Benchmarking Video Language Model for Underwater World\n  Understanding"
                },
                "summary": "Recently, the remarkable success of large language models (LLMs) has achieved\na profound impact on the field of artificial intelligence. Numerous advanced\nworks based on LLMs have been proposed and applied in various scenarios. Among\nthem, video language models (VidLMs) are particularly widely used. However,\nexisting works primarily focus on terrestrial scenarios, overlooking the highly\ndemanding application needs of underwater observation. To overcome this gap, we\nintroduce UVLM, an under water observation benchmark which is build through a\ncollaborative approach combining human expertise and AI models. To ensure data\nquality, we have conducted in-depth considerations from multiple perspectives.\nFirst, to address the unique challenges of underwater environments, we selected\nvideos that represent typical underwater challenges including light variations,\nwater turbidity, and diverse viewing angles to construct the dataset. Second,\nto ensure data diversity, the dataset covers a wide range of frame rates,\nresolutions, 419 classes of marine animals, and various static plants and\nterrains. Next, for task diversity, we adopted a structured design where\nobservation targets are categorized into two major classes: biological and\nenvironmental. Each category includes content observation and change/action\nobservation, totaling 20 distinct task types. Finally, we designed several\nchallenging evaluation metrics to enable quantitative comparison and analysis\nof different methods. Experiments on two representative VidLMs demonstrate that\nfine-tuning VidLMs on UVLM significantly improves underwater world\nunderstanding while also showing potential for slight improvements on existing\nin-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and\nprompt engineering will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the remarkable success of large language models (LLMs) has achieved\na profound impact on the field of artificial intelligence. Numerous advanced\nworks based on LLMs have been proposed and applied in various scenarios. Among\nthem, video language models (VidLMs) are particularly widely used. However,\nexisting works primarily focus on terrestrial scenarios, overlooking the highly\ndemanding application needs of underwater observation. To overcome this gap, we\nintroduce UVLM, an under water observation benchmark which is build through a\ncollaborative approach combining human expertise and AI models. To ensure data\nquality, we have conducted in-depth considerations from multiple perspectives.\nFirst, to address the unique challenges of underwater environments, we selected\nvideos that represent typical underwater challenges including light variations,\nwater turbidity, and diverse viewing angles to construct the dataset. Second,\nto ensure data diversity, the dataset covers a wide range of frame rates,\nresolutions, 419 classes of marine animals, and various static plants and\nterrains. Next, for task diversity, we adopted a structured design where\nobservation targets are categorized into two major classes: biological and\nenvironmental. Each category includes content observation and change/action\nobservation, totaling 20 distinct task types. Finally, we designed several\nchallenging evaluation metrics to enable quantitative comparison and analysis\nof different methods. Experiments on two representative VidLMs demonstrate that\nfine-tuning VidLMs on UVLM significantly improves underwater world\nunderstanding while also showing potential for slight improvements on existing\nin-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and\nprompt engineering will be released publicly."
                },
                "authors": [
                    {
                        "name": "Xizhe Xue"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Dawei Yan"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Haokui Zhang"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "arxiv_comment": "13 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06416v2",
                "updated": "2025-07-03T07:06:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    6,
                    8,
                    3,
                    184,
                    0
                ],
                "published": "2024-09-10T10:55:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    55,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "Exploring the Integration of Large Language Models in Industrial Test\n  Maintenance Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Large Language Models in Industrial Test\n  Maintenance Processes"
                },
                "summary": "Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplore the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also propose and demonstrate a multi-agent\narchitecture that can predict which tests require maintenance following a\nchange to the source code. Collectively, these contributions advance our\ntheoretical and practical understanding of how LLMs can be deployed to benefit\nindustrial test maintenance processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplore the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also propose and demonstrate a multi-agent\narchitecture that can predict which tests require maintenance following a\nchange to the source code. Collectively, these contributions advance our\ntheoretical and practical understanding of how LLMs can be deployed to benefit\nindustrial test maintenance processes."
                },
                "authors": [
                    {
                        "name": "Jingxiong Liu"
                    },
                    {
                        "name": "Ludvig Lemner"
                    },
                    {
                        "name": "Linnea Wahlgren"
                    },
                    {
                        "name": "Gregory Gay"
                    },
                    {
                        "name": "Nasser Mohammadiha"
                    },
                    {
                        "name": "Joakim Wennerberg"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Wennerberg"
                },
                "author": "Joakim Wennerberg",
                "arxiv_comment": "Under submission to Journal of Systems and Software",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16236v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16236v3",
                "updated": "2025-07-03T06:59:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    59,
                    56,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-21T17:41:28Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    41,
                    28,
                    0,
                    295,
                    0
                ],
                "title": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models"
                },
                "summary": "The success of Large Language Models (LLMs) has inspired the development of\nMultimodal Large Language Models (MLLMs) for unified understanding of vision\nand language. However, the increasing model size and computational complexity\nof large-scale MLLMs (l-MLLMs) limit their use in resource-constrained\nscenarios. Although small-scale MLLMs (s-MLLMs) are designed to reduce\ncomputational costs, they typically suffer from performance degradation. To\nmitigate this limitation, we propose a novel LLaVA-KD framework to transfer\nknowledge from l-MLLMs to s-MLLMs. Specifically, we introduce Multimodal\nDistillation (MDist) to transfer teacher model's robust representations across\nboth visual and linguistic modalities, and Relation Distillation (RDist) to\ntransfer teacher model's ability to capture visual token relationships.\nAdditionally, we propose a three-stage training scheme to fully exploit the\npotential of the proposed distillation strategy: 1) Distilled Pre-Training to\nstrengthen the alignment between visual-linguistic representations in s-MLLMs,\n2) Supervised Fine-Tuning to equip the s-MLLMs with multimodal understanding\ncapacity, and 3) Distilled Fine-Tuning to refine s-MLLM's knowledge. Our\napproach significantly improves s-MLLMs performance without altering the model\narchitecture. Extensive experiments and ablation studies validate the\neffectiveness of each proposed component. Code will be available at\nhttps://github.com/Fantasyele/LLaVA-KD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) has inspired the development of\nMultimodal Large Language Models (MLLMs) for unified understanding of vision\nand language. However, the increasing model size and computational complexity\nof large-scale MLLMs (l-MLLMs) limit their use in resource-constrained\nscenarios. Although small-scale MLLMs (s-MLLMs) are designed to reduce\ncomputational costs, they typically suffer from performance degradation. To\nmitigate this limitation, we propose a novel LLaVA-KD framework to transfer\nknowledge from l-MLLMs to s-MLLMs. Specifically, we introduce Multimodal\nDistillation (MDist) to transfer teacher model's robust representations across\nboth visual and linguistic modalities, and Relation Distillation (RDist) to\ntransfer teacher model's ability to capture visual token relationships.\nAdditionally, we propose a three-stage training scheme to fully exploit the\npotential of the proposed distillation strategy: 1) Distilled Pre-Training to\nstrengthen the alignment between visual-linguistic representations in s-MLLMs,\n2) Supervised Fine-Tuning to equip the s-MLLMs with multimodal understanding\ncapacity, and 3) Distilled Fine-Tuning to refine s-MLLM's knowledge. Our\napproach significantly improves s-MLLMs performance without altering the model\narchitecture. Extensive experiments and ablation studies validate the\neffectiveness of each proposed component. Code will be available at\nhttps://github.com/Fantasyele/LLaVA-KD."
                },
                "authors": [
                    {
                        "name": "Yuxuan Cai"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Haoyang He"
                    },
                    {
                        "name": "Xinwei He"
                    },
                    {
                        "name": "Ao Tong"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Zhucun Xue"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "ICCV'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16236v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16236v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21880v2",
                "updated": "2025-07-03T06:38:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    38,
                    34,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T01:54:28Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    54,
                    28,
                    2,
                    148,
                    0
                ],
                "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation"
                },
                "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications."
                },
                "authors": [
                    {
                        "name": "Yu-Lun Song"
                    },
                    {
                        "name": "Chung-En Tsern"
                    },
                    {
                        "name": "Che-Cheng Wu"
                    },
                    {
                        "name": "Yu-Ming Chang"
                    },
                    {
                        "name": "Syuan-Bo Huang"
                    },
                    {
                        "name": "Wei-Chu Chen"
                    },
                    {
                        "name": "Michael Chia-Liang Lin"
                    },
                    {
                        "name": "Yu-Ta Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Ta Lin"
                },
                "author": "Yu-Ta Lin",
                "arxiv_doi": "10.17605/OSF.IO/ABYQH",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.17605/OSF.IO/ABYQH",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025",
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02353v1",
                "updated": "2025-07-03T06:37:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    37,
                    55,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T06:37:55Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    37,
                    55,
                    3,
                    184,
                    0
                ],
                "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation\n  via LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation\n  via LLM Agent"
                },
                "summary": "Keyword decision in Sponsored Search Advertising is critical to the success\nof ad campaigns. While LLM-based methods offer automated keyword generation,\nthey face three major limitations: reliance on large-scale query-keyword pair\ndata, lack of online multi-objective performance monitoring and optimization,\nand weak quality control in keyword selection. These issues hinder the agentic\nuse of LLMs in fully automating keyword decisions by monitoring and reasoning\nover key performance indicators such as impressions, clicks, conversions, and\nCTA effectiveness. To overcome these challenges, we propose OMS, a keyword\ngeneration framework that is On-the-fly (requires no training data, monitors\nonline performance, and adapts accordingly), Multi-objective (employs agentic\nreasoning to optimize keywords based on multiple performance metrics), and\nSelf-reflective (agentically evaluates keyword quality). Experiments on\nbenchmarks and real-world ad campaigns show that OMS outperforms existing\nmethods; ablation and human evaluations confirm the effectiveness of each\ncomponent and the quality of generated keywords.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyword decision in Sponsored Search Advertising is critical to the success\nof ad campaigns. While LLM-based methods offer automated keyword generation,\nthey face three major limitations: reliance on large-scale query-keyword pair\ndata, lack of online multi-objective performance monitoring and optimization,\nand weak quality control in keyword selection. These issues hinder the agentic\nuse of LLMs in fully automating keyword decisions by monitoring and reasoning\nover key performance indicators such as impressions, clicks, conversions, and\nCTA effectiveness. To overcome these challenges, we propose OMS, a keyword\ngeneration framework that is On-the-fly (requires no training data, monitors\nonline performance, and adapts accordingly), Multi-objective (employs agentic\nreasoning to optimize keywords based on multiple performance metrics), and\nSelf-reflective (agentically evaluates keyword quality). Experiments on\nbenchmarks and real-world ad campaigns show that OMS outperforms existing\nmethods; ablation and human evaluations confirm the effectiveness of each\ncomponent and the quality of generated keywords."
                },
                "authors": [
                    {
                        "name": "Bowen Chen"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Shingo Takamatsu"
                    }
                ],
                "author_detail": {
                    "name": "Shingo Takamatsu"
                },
                "author": "Shingo Takamatsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02351v1",
                "updated": "2025-07-03T06:34:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    34,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T06:34:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    34,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "Indoor thermal comfort management: A Bayesian machine-learning approach\n  to data denoising and dynamics prediction of HVAC systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor thermal comfort management: A Bayesian machine-learning approach\n  to data denoising and dynamics prediction of HVAC systems"
                },
                "summary": "The optimal management of a building's microclimate to satisfy the occupants'\nneeds and objectives in terms of comfort, energy efficiency, and costs is\nparticularly challenging. This complexity arises from the non-linear,\ntime-dependent interactions among all the variables of the control problem and\nthe changing internal and external constraints. Focusing on the accurate\nmodeling of the indoor temperature, we propose a data-driven approach to\naddress this challenge. We account for thermal inertia, non-linear effects,\nsmall perturbations of the indoor climate dynamics caused by ventilation and\nweather variations, as well as for the stochastic nature of the control system\ndue to the observed noise in the input signal. Since the prohibitive cost of\nquality data acquisition and processing limits the implementation of\ndata-driven approaches for real-life problems, we applied a method that merges\nseveral Bayesian machine learning and deep learning architectures that are\nsuitable for predicting complex system dynamics, while relaxing the dataset\nquality requirements. Our framework includes a built-in deep Kalman filter,\nwhich makes it deployable even with low-accuracy temperature sensors. It\nachieves state-of-the-art performance, best performing with a 150-minute\nprediction horizon with an RMSE of 0.2455, an MAE of 0.162, and an $R^2$ of\n0.926. The model's performance remains consistent even when exposed to highly\nnoisy data. Finally, we show how our approach can be extended to other\napplications including demand response event duration prediction and equipment\nfailure detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The optimal management of a building's microclimate to satisfy the occupants'\nneeds and objectives in terms of comfort, energy efficiency, and costs is\nparticularly challenging. This complexity arises from the non-linear,\ntime-dependent interactions among all the variables of the control problem and\nthe changing internal and external constraints. Focusing on the accurate\nmodeling of the indoor temperature, we propose a data-driven approach to\naddress this challenge. We account for thermal inertia, non-linear effects,\nsmall perturbations of the indoor climate dynamics caused by ventilation and\nweather variations, as well as for the stochastic nature of the control system\ndue to the observed noise in the input signal. Since the prohibitive cost of\nquality data acquisition and processing limits the implementation of\ndata-driven approaches for real-life problems, we applied a method that merges\nseveral Bayesian machine learning and deep learning architectures that are\nsuitable for predicting complex system dynamics, while relaxing the dataset\nquality requirements. Our framework includes a built-in deep Kalman filter,\nwhich makes it deployable even with low-accuracy temperature sensors. It\nachieves state-of-the-art performance, best performing with a 150-minute\nprediction horizon with an RMSE of 0.2455, an MAE of 0.162, and an $R^2$ of\n0.926. The model's performance remains consistent even when exposed to highly\nnoisy data. Finally, we show how our approach can be extended to other\napplications including demand response event duration prediction and equipment\nfailure detection."
                },
                "authors": [
                    {
                        "name": "Javier Penuela"
                    },
                    {
                        "name": "Sahar Moghimian Hoosh"
                    },
                    {
                        "name": "Ilia Kamyshev"
                    },
                    {
                        "name": "Aldo Bischi"
                    },
                    {
                        "name": "Henni Ouerdane"
                    }
                ],
                "author_detail": {
                    "name": "Henni Ouerdane"
                },
                "author": "Henni Ouerdane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01923v2",
                "updated": "2025-07-03T06:29:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    29,
                    26,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T17:32:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    32,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Decision-Oriented Text Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Oriented Text Evaluation"
                },
                "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics."
                },
                "authors": [
                    {
                        "name": "Yu-Shiang Huang"
                    },
                    {
                        "name": "Chuan-Ju Wang"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11556v2",
                "updated": "2025-07-03T06:24:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    24,
                    49,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-16T08:42:00Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    42,
                    0,
                    0,
                    351,
                    0
                ],
                "title": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence\n  Embeddings from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence\n  Embeddings from LLMs"
                },
                "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost."
                },
                "authors": [
                    {
                        "name": "Yuchen Fu"
                    },
                    {
                        "name": "Zifeng Cheng"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Zhonghui Wang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Zhengliang Li"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu",
                "arxiv_comment": "Accept to ACL 2025 (Oral). Code are available on\n  https://github.com/fuyuchenIfyw/token_prepending.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23486v2",
                "updated": "2025-07-03T06:03:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    3,
                    7,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-29T14:34:54Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    34,
                    54,
                    3,
                    149,
                    0
                ],
                "title": "Autoformalization in the Era of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization in the Era of Large Language Models: A Survey"
                },
                "summary": "Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field."
                },
                "authors": [
                    {
                        "name": "Ke Weng"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Sirui Li"
                    },
                    {
                        "name": "Wangyue Lu"
                    },
                    {
                        "name": "Haozhe Sun"
                    },
                    {
                        "name": "Hengyu Liu"
                    },
                    {
                        "name": "Tiancheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tiancheng Zhang"
                },
                "author": "Tiancheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02332v1",
                "updated": "2025-07-03T05:50:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    50,
                    50,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T05:50:50Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    50,
                    50,
                    3,
                    184,
                    0
                ],
                "title": "PII Jailbreaking in LLMs via Activation Steering Reveals Personal\n  Information Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PII Jailbreaking in LLMs via Activation Steering Reveals Personal\n  Information Leakage"
                },
                "summary": "This paper investigates privacy jailbreaking in LLMs via steering, focusing\non whether manipulating activations can bypass LLM alignment and alter response\nbehaviors to privacy related queries (e.g., a certain public figure's sexual\norientation). We begin by identifying attention heads predictive of refusal\nbehavior for private attributes (e.g., sexual orientation) using lightweight\nlinear probes trained with privacy evaluator labels. Next, we steer the\nactivations of a small subset of these attention heads guided by the trained\nprobes to induce the model to generate non-refusal responses. Our experiments\nshow that these steered responses often disclose sensitive attribute details,\nalong with other private information about data subjects such as life events,\nrelationships, and personal histories that the models would typically refuse to\nproduce. Evaluations across four LLMs reveal jailbreaking disclosure rates of\nat least 95%, with more than 50% on average of these responses revealing true\npersonal information. Our controlled study demonstrates that private\ninformation memorized in LLMs can be extracted through targeted manipulation of\ninternal activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates privacy jailbreaking in LLMs via steering, focusing\non whether manipulating activations can bypass LLM alignment and alter response\nbehaviors to privacy related queries (e.g., a certain public figure's sexual\norientation). We begin by identifying attention heads predictive of refusal\nbehavior for private attributes (e.g., sexual orientation) using lightweight\nlinear probes trained with privacy evaluator labels. Next, we steer the\nactivations of a small subset of these attention heads guided by the trained\nprobes to induce the model to generate non-refusal responses. Our experiments\nshow that these steered responses often disclose sensitive attribute details,\nalong with other private information about data subjects such as life events,\nrelationships, and personal histories that the models would typically refuse to\nproduce. Evaluations across four LLMs reveal jailbreaking disclosure rates of\nat least 95%, with more than 50% on average of these responses revealing true\npersonal information. Our controlled study demonstrates that private\ninformation memorized in LLMs can be extracted through targeted manipulation of\ninternal activations."
                },
                "authors": [
                    {
                        "name": "Krishna Kanth Nakka"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Xuebing Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Zhou"
                },
                "author": "Xuebing Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05451v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05451v3",
                "updated": "2025-07-03T05:45:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    45,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-07T19:34:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    19,
                    34,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "SecAlign: Defending Against Prompt Injection with Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecAlign: Defending Against Prompt Injection with Preference\n  Optimization"
                },
                "summary": "Large language models (LLMs) are becoming increasingly prevalent in modern\nsoftware systems, interfacing between the user and the Internet to assist with\ntasks that require advanced language understanding. To accomplish these tasks,\nthe LLM often uses external data sources such as user documents, web retrieval,\nresults from API calls, etc. This opens up new avenues for attackers to\nmanipulate the LLM via prompt injection. Adversarial prompts can be injected\ninto external data sources to override the system's intended instruction and\ninstead execute a malicious instruction. To mitigate this vulnerability, we\npropose a new defense called SecAlign based on the technique of preference\noptimization. Our defense first constructs a preference dataset with\nprompt-injected inputs, secure outputs (ones that respond to the legitimate\ninstruction), and insecure outputs (ones that respond to the injection). We\nthen perform preference optimization on this dataset to teach the LLM to prefer\nthe secure output over the insecure one. This provides the first known method\nthat reduces the success rates of various prompt injections to <10%, even\nagainst attacks much more sophisticated than ones seen during training. This\nindicates our defense generalizes well against unknown and yet-to-come attacks.\nAlso, SecAlign models are still practical with similar utility to the one\nbefore defensive training in our evaluations. Our code is at\nhttps://github.com/facebookresearch/SecAlign",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly prevalent in modern\nsoftware systems, interfacing between the user and the Internet to assist with\ntasks that require advanced language understanding. To accomplish these tasks,\nthe LLM often uses external data sources such as user documents, web retrieval,\nresults from API calls, etc. This opens up new avenues for attackers to\nmanipulate the LLM via prompt injection. Adversarial prompts can be injected\ninto external data sources to override the system's intended instruction and\ninstead execute a malicious instruction. To mitigate this vulnerability, we\npropose a new defense called SecAlign based on the technique of preference\noptimization. Our defense first constructs a preference dataset with\nprompt-injected inputs, secure outputs (ones that respond to the legitimate\ninstruction), and insecure outputs (ones that respond to the injection). We\nthen perform preference optimization on this dataset to teach the LLM to prefer\nthe secure output over the insecure one. This provides the first known method\nthat reduces the success rates of various prompt injections to <10%, even\nagainst attacks much more sophisticated than ones seen during training. This\nindicates our defense generalizes well against unknown and yet-to-come attacks.\nAlso, SecAlign models are still practical with similar utility to the one\nbefore defensive training in our evaluations. Our code is at\nhttps://github.com/facebookresearch/SecAlign"
                },
                "authors": [
                    {
                        "name": "Sizhe Chen"
                    },
                    {
                        "name": "Arman Zharmagambetov"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "David Wagner"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "arxiv_doi": "10.1145/3719027.3744836",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719027.3744836",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.05451v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05451v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM CCS 2025. Key words: prompt injection defense, LLM security,\n  LLM-integrated applications",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08723v3",
                "updated": "2025-07-03T05:25:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    25,
                    23,
                    3,
                    184,
                    0
                ],
                "published": "2024-10-11T11:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    11,
                    23,
                    26,
                    4,
                    285,
                    0
                ],
                "title": "Human-Computer Interaction and Visualization in Natural Language\n  Generation Models: Applications, Challenges, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Computer Interaction and Visualization in Natural Language\n  Generation Models: Applications, Challenges, and Opportunities"
                },
                "summary": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) models have emerged as a focal point of\nresearch within natural language processing (NLP), exhibiting remarkable\nperformance in tasks such as text composition and dialogue generation. However,\ntheir intricate architectures and extensive model parameters pose significant\nchallenges to interpretability, limiting their applicability in high-stakes\ndecision-making scenarios. To address this issue, human-computer interaction\n(HCI) and visualization techniques offer promising avenues to enhance the\ntransparency and usability of NLG models by making their decision-making\nprocesses more interpretable. In this paper, we provide a comprehensive\ninvestigation into the roles, limitations, and impact of HCI and visualization\nin facilitating human understanding and control over NLG systems. We introduce\na taxonomy of interaction methods and visualization techniques, categorizing\nthree major research domains and their corresponding six key tasks in the\napplication of NLG models. Finally, we summarize the shortcomings in the\nexisting work and investigate the key challenges and emerging opportunities in\nthe era of large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Yunchao Wang"
                    },
                    {
                        "name": "Guodao Sun"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Ronghua Liang"
                    }
                ],
                "author_detail": {
                    "name": "Ronghua Liang"
                },
                "author": "Ronghua Liang",
                "arxiv_doi": "10.1007/s11704-025-50356-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11704-025-50356-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.08723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-50356-6}",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17857v3",
                "updated": "2025-07-03T05:14:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    14,
                    46,
                    3,
                    184,
                    0
                ],
                "published": "2025-04-24T18:01:36Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    1,
                    36,
                    3,
                    114,
                    0
                ],
                "title": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation\n  Parameters with Distributional Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation\n  Parameters with Distributional Measures"
                },
                "summary": "This work presents an overview of the technical details behind a high\nperformance reinforcement learning policy deployment with the Spot RL\nResearcher Development Kit for low level motor access on Boston Dynamics Spot.\nThis represents the first public demonstration of an end to end end\nreinforcement learning policy deployed on Spot hardware with training code\npublicly available through Nvidia IsaacLab and deployment code available\nthrough Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean\nDiscrepancy to quantify the distributional dissimilarity of data collected on\nhardware and in simulation to measure our sim2real gap. We use these measures\nas a scoring function for the Covariance Matrix Adaptation Evolution Strategy\nto optimize simulated parameters that are unknown or difficult to measure from\nSpot. Our procedure for modeling and training produces high quality\nreinforcement learning policies capable of multiple gaits, including a flight\nphase. We deploy policies capable of over 5.2ms locomotion, more than triple\nSpots default controller maximum speed, robustness to slippery surfaces,\ndisturbance rejection, and overall agility previously unseen on Spot. We detail\nour method and release our code to support future work on Spot with the low\nlevel API.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents an overview of the technical details behind a high\nperformance reinforcement learning policy deployment with the Spot RL\nResearcher Development Kit for low level motor access on Boston Dynamics Spot.\nThis represents the first public demonstration of an end to end end\nreinforcement learning policy deployed on Spot hardware with training code\npublicly available through Nvidia IsaacLab and deployment code available\nthrough Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean\nDiscrepancy to quantify the distributional dissimilarity of data collected on\nhardware and in simulation to measure our sim2real gap. We use these measures\nas a scoring function for the Covariance Matrix Adaptation Evolution Strategy\nto optimize simulated parameters that are unknown or difficult to measure from\nSpot. Our procedure for modeling and training produces high quality\nreinforcement learning policies capable of multiple gaits, including a flight\nphase. We deploy policies capable of over 5.2ms locomotion, more than triple\nSpots default controller maximum speed, robustness to slippery surfaces,\ndisturbance rejection, and overall agility previously unseen on Spot. We detail\nour method and release our code to support future work on Spot with the low\nlevel API."
                },
                "authors": [
                    {
                        "name": "AJ Miller"
                    },
                    {
                        "name": "Fangzhou Yu"
                    },
                    {
                        "name": "Michael Brauckmann"
                    },
                    {
                        "name": "Farbod Farshidian"
                    }
                ],
                "author_detail": {
                    "name": "Farbod Farshidian"
                },
                "author": "Farbod Farshidian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17828v2",
                "updated": "2025-07-03T05:12:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    12,
                    51,
                    3,
                    184,
                    0
                ],
                "published": "2025-06-21T21:49:02Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    21,
                    49,
                    2,
                    5,
                    172,
                    0
                ],
                "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative\n  Reweight-then-Optimize Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative\n  Reweight-then-Optimize Approach"
                },
                "summary": "Aligning large language models (LLMs) with human preferences usually requires\nfine-tuning methods such as RLHF and DPO. These methods directly optimize the\nmodel parameters, so they cannot be used in test-time to improve model\nperformance, nor are they applicable when the model weights are not accessible.\nIn contrast, test-time methods sidestep weight updates by leveraging reward\nfunctions to guide and improve output quality. However, they incur high\ninference costs, and their one-shot guidance is often based on imperfect reward\nor value functions, leading to suboptimal outputs. In this work, we present a\nmethod named Iterative Reweight-then-Optimize (IRO), a reinforcement learning\n(RL) framework that performs RL-style alignment of the (frozen) base model\nwithout touching its parameters. During training, each iteration (i) samples\ncandidates from the base model, (ii) resamples using current value functions,\nand (iii) trains a new lightweight value function that guides the next decoding\npass. At test time, the value functions are used to guide the base model\ngeneration via a search-based optimization process. Notably, users can apply\nIRO to align a model on their own dataset, similar to OpenAI's reinforcement\nfine-tuning (RFT), but without requiring access to the model weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences usually requires\nfine-tuning methods such as RLHF and DPO. These methods directly optimize the\nmodel parameters, so they cannot be used in test-time to improve model\nperformance, nor are they applicable when the model weights are not accessible.\nIn contrast, test-time methods sidestep weight updates by leveraging reward\nfunctions to guide and improve output quality. However, they incur high\ninference costs, and their one-shot guidance is often based on imperfect reward\nor value functions, leading to suboptimal outputs. In this work, we present a\nmethod named Iterative Reweight-then-Optimize (IRO), a reinforcement learning\n(RL) framework that performs RL-style alignment of the (frozen) base model\nwithout touching its parameters. During training, each iteration (i) samples\ncandidates from the base model, (ii) resamples using current value functions,\nand (iii) trains a new lightweight value function that guides the next decoding\npass. At test time, the value functions are used to guide the base model\ngeneration via a search-based optimization process. Notably, users can apply\nIRO to align a model on their own dataset, similar to OpenAI's reinforcement\nfine-tuning (RFT), but without requiring access to the model weights."
                },
                "authors": [
                    {
                        "name": "Xinnan Zhang"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Siliang Zeng"
                    },
                    {
                        "name": "Jiaxiang Li"
                    },
                    {
                        "name": "Zhongruo Wang"
                    },
                    {
                        "name": "Kaixiang Lin"
                    },
                    {
                        "name": "Songtao Lu"
                    },
                    {
                        "name": "Alfredo Garcia"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02318v1",
                "updated": "2025-07-03T05:10:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    10,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T05:10:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    5,
                    10,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "Precisely Detecting Python Type Errors via LLM-based Unit Test\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precisely Detecting Python Type Errors via LLM-based Unit Test\n  Generation"
                },
                "summary": "Type errors in Python often lead to runtime failures, posing significant\nchallenges to software reliability and developer productivity. Existing static\nanalysis tools aim to detect such errors without execution but frequently\nsuffer from high false positive rates. Recently, unit test generation\ntechniques offer great promise in achieving high test coverage, but they often\nstruggle to produce bug-revealing tests without tailored guidance. To address\nthese limitations, we present RTED, a novel type-aware test generation\ntechnique for automatically detecting Python type errors. Specifically, RTED\ncombines step-by-step type constraint analysis with reflective validation to\nguide the test generation process and effectively suppress false positives. We\nevaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.\nExperimental results show that RTED can detect 22-29 more benchmarked type\nerrors than four state-of-the-art techniques. RTED is also capable of producing\nfewer false positives, achieving an improvement of 173.9%-245.9% in precision.\nFurthermore, RTED successfully discovered 12 previously unknown type errors\nfrom six real-world open-source Python projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type errors in Python often lead to runtime failures, posing significant\nchallenges to software reliability and developer productivity. Existing static\nanalysis tools aim to detect such errors without execution but frequently\nsuffer from high false positive rates. Recently, unit test generation\ntechniques offer great promise in achieving high test coverage, but they often\nstruggle to produce bug-revealing tests without tailored guidance. To address\nthese limitations, we present RTED, a novel type-aware test generation\ntechnique for automatically detecting Python type errors. Specifically, RTED\ncombines step-by-step type constraint analysis with reflective validation to\nguide the test generation process and effectively suppress false positives. We\nevaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.\nExperimental results show that RTED can detect 22-29 more benchmarked type\nerrors than four state-of-the-art techniques. RTED is also capable of producing\nfewer false positives, achieving an improvement of 173.9%-245.9% in precision.\nFurthermore, RTED successfully discovered 12 previously unknown type errors\nfrom six real-world open-source Python projects."
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Yanjie Jiang"
                    },
                    {
                        "name": "Lin Yang"
                    },
                    {
                        "name": "Yuteng Zheng"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Junjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Chen"
                },
                "author": "Junjie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]