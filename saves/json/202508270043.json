[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.18250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v1",
                "updated": "2025-08-25T17:41:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando Garc√≠a-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinst√§dtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v2",
                "updated": "2025-08-22T03:36:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    36,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v2",
                "updated": "2025-08-21T22:45:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    22,
                    45,
                    6,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "Added link to code repository. Fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v2",
                "updated": "2025-08-21T20:13:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    20,
                    13,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/"
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v1",
                "updated": "2025-08-21T18:40:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15717v1",
                "updated": "2025-08-21T16:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches."
                },
                "authors": [
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15694v1",
                "updated": "2025-08-21T16:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "title": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search"
                },
                "summary": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shengyuan Lin"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shuhao Fan"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "12 pages, 12 figures, this paper is the English version of our\n  Chinese paper accepted for publication in Journal of Software, Vol. 37, No.\n  3, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15647v1",
                "updated": "2025-08-21T15:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing"
                },
                "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shuai Mu"
                    },
                    {
                        "name": "Sebastian Angel"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu",
                "arxiv_doi": "10.14778/3704965.3704969",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3704965.3704969",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.15647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs",
                "arxiv_journal_ref": "PVLDB Volume 17, Issue 13, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v2",
                "updated": "2025-08-21T14:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    58,
                    12,
                    3,
                    233,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15601v1",
                "updated": "2025-08-21T14:24:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T14:24:52Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed-Precision Large Language Model Inference with TurboMind"
                },
                "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Qian Yao"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15583v1",
                "updated": "2025-08-21T13:57:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:57:09Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "title": "Time-Optimal Directed q-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Optimal Directed q-Analysis"
                },
                "summary": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Florian Unger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Unger"
                },
                "author": "Florian Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15545v1",
                "updated": "2025-08-21T13:24:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "title": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation"
                },
                "summary": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value."
                },
                "authors": [
                    {
                        "name": "Mingyang Yu"
                    },
                    {
                        "name": "Haorui Yang"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Desheng Kong"
                    },
                    {
                        "name": "Ji Du"
                    },
                    {
                        "name": "Yulong Fu"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v2",
                "updated": "2025-08-21T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    12,
                    52,
                    11,
                    3,
                    233,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas H√∂llein"
                    },
                    {
                        "name": "Alja≈æ Bo≈æiƒç"
                    },
                    {
                        "name": "Michael Zollh√∂fer"
                    },
                    {
                        "name": "Matthias Nie√üner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nie√üner"
                },
                "author": "Matthias Nie√üner",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://lukashoel.github.io/3DGS-LM, Video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, Code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14204v3",
                "updated": "2025-08-21T11:43:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    43,
                    48,
                    3,
                    233,
                    0
                ],
                "published": "2024-04-22T14:13:36Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    13,
                    36,
                    0,
                    113,
                    0
                ],
                "title": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading"
                },
                "summary": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "18 pages, 13 figures. Part of this work has been accepted by ICDCS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15290v1",
                "updated": "2025-08-21T06:26:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T06:26:18Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "title": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search"
                },
                "summary": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%."
                },
                "authors": [
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Xiaolu Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v1",
                "updated": "2025-08-21T03:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15036v1",
                "updated": "2025-08-20T20:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T20:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs"
                },
                "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."
                },
                "authors": [
                    {
                        "name": "Ruyi Ding"
                    },
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Xinyi Shen"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "This paper will appear in CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15033v1",
                "updated": "2025-08-20T19:54:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T19:54:41Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "title": "Rethinking the Potential of Layer Freezing for Efficient DNN Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Layer Freezing for Efficient DNN Training"
                },
                "summary": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training."
                },
                "authors": [
                    {
                        "name": "Chence Yang"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Xulong Tang"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Jinzhen Wang"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14468v1",
                "updated": "2025-08-20T06:48:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:48:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Diverse Negative Sampling for Implicit Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Negative Sampling for Implicit Collaborative Filtering"
                },
                "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yueqing Xuan"
                    },
                    {
                        "name": "Kacper Sokol"
                    },
                    {
                        "name": "Mark Sanderson"
                    },
                    {
                        "name": "Jeffrey Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Chan"
                },
                "author": "Jeffrey Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14420v1",
                "updated": "2025-08-20T04:36:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:36:25Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan"
                },
                "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform."
                },
                "authors": [
                    {
                        "name": "Shuli Wang"
                    },
                    {
                        "name": "Yinqiu Huang"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yonggang Liu"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Yinhua Zhu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang",
                "arxiv_doi": "10.1145/3746252.3761539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16653v1",
                "updated": "2025-08-20T03:42:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:42:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Peiyu Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "International Conference on Computer-Aided Design (ICCAD) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel M√°t√©"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel M√°t√©"
                },
                "author": "Tejfel M√°t√©",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. Le√≥n"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12357v1",
                "updated": "2025-08-17T13:05:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T13:05:52Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "title": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method"
                },
                "summary": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn."
                },
                "authors": [
                    {
                        "name": "S. Khardazi"
                    },
                    {
                        "name": "Z. Gargar"
                    },
                    {
                        "name": "A. Lyubchyk"
                    },
                    {
                        "name": "O. Zakir"
                    },
                    {
                        "name": "D. Mezzane"
                    },
                    {
                        "name": "M. Amjoud"
                    },
                    {
                        "name": "A. Alimoussa"
                    },
                    {
                        "name": "Z. Kutnjak"
                    }
                ],
                "author_detail": {
                    "name": "Z. Kutnjak"
                },
                "author": "Z. Kutnjak",
                "arxiv_doi": "10.1016/j.jssc.2025.125547",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jssc.2025.125547",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10963v1",
                "updated": "2025-08-14T14:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15806v1",
                "updated": "2025-08-14T14:08:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    8,
                    58,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:08:58Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    8,
                    58,
                    3,
                    226,
                    0
                ],
                "title": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need\n  for Robust KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need\n  for Robust KV Cache Compression"
                },
                "summary": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations"
                },
                "authors": [
                    {
                        "name": "Mengjie Li"
                    },
                    {
                        "name": "William J. Song"
                    }
                ],
                "author_detail": {
                    "name": "William J. Song"
                },
                "author": "William J. Song",
                "arxiv_comment": "18 pages, 9 tables, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10613v1",
                "updated": "2025-08-14T13:10:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:10:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks"
                },
                "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability."
                },
                "authors": [
                    {
                        "name": "Mengyao Li"
                    },
                    {
                        "name": "Qiaolun Zhang"
                    },
                    {
                        "name": "Zongshuai Yang"
                    },
                    {
                        "name": "Stefano Bregni"
                    },
                    {
                        "name": "Alberto Gatto"
                    },
                    {
                        "name": "Raouf Boutaba"
                    },
                    {
                        "name": "Massimo Tornatore"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Tornatore"
                },
                "author": "Massimo Tornatore",
                "arxiv_comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10424v1",
                "updated": "2025-08-14T07:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability."
                },
                "authors": [
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Junda Lu"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Liuzhuozheng Li"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v1",
                "updated": "2025-08-13T07:40:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09262v1",
                "updated": "2025-08-12T18:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T18:05:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Input-Adaptive Inference for Efficient VLN"
                },
                "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."
                },
                "authors": [
                    {
                        "name": "Dongwoo Kang"
                    },
                    {
                        "name": "Akhil Perincherry"
                    },
                    {
                        "name": "Zachary Coalson"
                    },
                    {
                        "name": "Aiden Gabriel"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Accepted to ICCV 2025 [Poster]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18265v1",
                "updated": "2025-08-25T17:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    58,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:58:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    58,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency"
                },
                "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released."
                },
                "authors": [
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhangwei Gao"
                    },
                    {
                        "name": "Lixin Gu"
                    },
                    {
                        "name": "Hengjun Pu"
                    },
                    {
                        "name": "Long Cui"
                    },
                    {
                        "name": "Xingguang Wei"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Linglin Jing"
                    },
                    {
                        "name": "Shenglong Ye"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Ganlin Yang"
                    },
                    {
                        "name": "Haomin Wang"
                    },
                    {
                        "name": "Qi Wei"
                    },
                    {
                        "name": "Jinhui Yin"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Guanzhou Chen"
                    },
                    {
                        "name": "Zichen Ding"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Zehao Li"
                    },
                    {
                        "name": "Bowen Yang"
                    },
                    {
                        "name": "Yuchen Duan"
                    },
                    {
                        "name": "Xuehui Wang"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Nianchen Deng"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yingtong Xiong"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Huipeng Deng"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Jiaye Ge"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Min Dou"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Gen Luo"
                    }
                ],
                "author_detail": {
                    "name": "Gen Luo"
                },
                "author": "Gen Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18264v1",
                "updated": "2025-08-25T17:57:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    57,
                    49,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:57:49Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    57,
                    49,
                    0,
                    237,
                    0
                ],
                "title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs"
                },
                "summary": "Vision-Language Models (VLMs) demonstrate impressive performance in\nunderstanding visual content with language instruction by converting visual\ninput to vision tokens. However, redundancy in vision tokens results in the\ndegenerated inference efficiency of VLMs. While many algorithms have been\nproposed to reduce the number of vision tokens, most of them apply only\nunimodal information (i.e., vision/text) for pruning and ignore the inherent\nmultimodal property of vision-language tasks. Moreover, it lacks a generic\ncriterion that can be applied to different modalities. To mitigate this\nlimitation, in this work, we propose to leverage both vision and text tokens to\nselect informative vision tokens by the criterion of coverage. We first\nformulate the subset selection problem as a maximum coverage problem.\nAfterward, a subset of vision tokens is optimized to cover the text tokens and\nthe original set of vision tokens, simultaneously. Finally, a VLM agent can be\nadopted to further improve the quality of text tokens for guiding vision\npruning. The proposed method MMTok is extensively evaluated on benchmark\ndatasets with different VLMs. The comparison illustrates that vision and text\ninformation are complementary, and combining multimodal information can surpass\nthe unimodal baseline with a clear margin. Moreover, under the maximum coverage\ncriterion on the POPE dataset, our method achieves a 1.87x speedup while\nmaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,\nwith only four vision tokens, it still preserves 87.7% of the original\nperformance on LLaVA-1.5-7B. These results highlight the effectiveness of\ncoverage in token selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demonstrate impressive performance in\nunderstanding visual content with language instruction by converting visual\ninput to vision tokens. However, redundancy in vision tokens results in the\ndegenerated inference efficiency of VLMs. While many algorithms have been\nproposed to reduce the number of vision tokens, most of them apply only\nunimodal information (i.e., vision/text) for pruning and ignore the inherent\nmultimodal property of vision-language tasks. Moreover, it lacks a generic\ncriterion that can be applied to different modalities. To mitigate this\nlimitation, in this work, we propose to leverage both vision and text tokens to\nselect informative vision tokens by the criterion of coverage. We first\nformulate the subset selection problem as a maximum coverage problem.\nAfterward, a subset of vision tokens is optimized to cover the text tokens and\nthe original set of vision tokens, simultaneously. Finally, a VLM agent can be\nadopted to further improve the quality of text tokens for guiding vision\npruning. The proposed method MMTok is extensively evaluated on benchmark\ndatasets with different VLMs. The comparison illustrates that vision and text\ninformation are complementary, and combining multimodal information can surpass\nthe unimodal baseline with a clear margin. Moreover, under the maximum coverage\ncriterion on the POPE dataset, our method achieves a 1.87x speedup while\nmaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,\nwith only four vision tokens, it still preserves 87.7% of the original\nperformance on LLaVA-1.5-7B. These results highlight the effectiveness of\ncoverage in token selection."
                },
                "authors": [
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Juhua Hu"
                    },
                    {
                        "name": "Mian Zhang"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Yanjie Fu"
                    },
                    {
                        "name": "Qi Qian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Qian"
                },
                "author": "Qi Qian",
                "arxiv_comment": "Project page: https://project.ironieser.cc/mmtok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18260v1",
                "updated": "2025-08-25T17:53:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    53,
                    22,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:53:22Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    53,
                    22,
                    0,
                    237,
                    0
                ],
                "title": "MIRAGE: Scaling Test-Time Inference with Parallel\n  Graph-Retrieval-Augmented Reasoning Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: Scaling Test-Time Inference with Parallel\n  Graph-Retrieval-Augmented Reasoning Chains"
                },
                "summary": "Large reasoning models (LRMs) have shown significant progress in test-time\nscaling through chain-of-thought prompting. Current approaches like search-o1\nintegrate retrieval augmented generation (RAG) into multi-step reasoning\nprocesses but rely on a single, linear reasoning chain while incorporating\nunstructured textual information in a flat, context-agnostic manner. As a\nresult, these approaches can lead to error accumulation throughout the\nreasoning chain, which significantly limits its effectiveness in medical\nquestion-answering (QA) tasks where both accuracy and traceability are critical\nrequirements. To address these challenges, we propose MIRAGE (Multi-chain\nInference with Retrieval-Augmented Graph Exploration), a novel test-time\nscalable reasoning framework that performs dynamic multi-chain inference over\nstructured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex\nqueries into entity-grounded sub-questions, 2) executes parallel inference\nchains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop\ntraversal, and 4) integrates answers using cross-chain verification to resolve\ncontradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,\nCMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,\nTree-of-Thought variants, and other retrieval-augmented baselines in both\nautomatic and human evaluations. Additionally, MIRAGE improves interpretability\nby generating explicit reasoning chains that trace each factual claim to\nconcrete chains within the knowledge graph, making it well-suited for complex\nmedical reasoning scenarios. The code will be available for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have shown significant progress in test-time\nscaling through chain-of-thought prompting. Current approaches like search-o1\nintegrate retrieval augmented generation (RAG) into multi-step reasoning\nprocesses but rely on a single, linear reasoning chain while incorporating\nunstructured textual information in a flat, context-agnostic manner. As a\nresult, these approaches can lead to error accumulation throughout the\nreasoning chain, which significantly limits its effectiveness in medical\nquestion-answering (QA) tasks where both accuracy and traceability are critical\nrequirements. To address these challenges, we propose MIRAGE (Multi-chain\nInference with Retrieval-Augmented Graph Exploration), a novel test-time\nscalable reasoning framework that performs dynamic multi-chain inference over\nstructured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex\nqueries into entity-grounded sub-questions, 2) executes parallel inference\nchains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop\ntraversal, and 4) integrates answers using cross-chain verification to resolve\ncontradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,\nCMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,\nTree-of-Thought variants, and other retrieval-augmented baselines in both\nautomatic and human evaluations. Additionally, MIRAGE improves interpretability\nby generating explicit reasoning chains that trace each factual claim to\nconcrete chains within the knowledge graph, making it well-suited for complex\nmedical reasoning scenarios. The code will be available for further research."
                },
                "authors": [
                    {
                        "name": "Kaiwen Wei"
                    },
                    {
                        "name": "Rui Shan"
                    },
                    {
                        "name": "Dongsheng Zou"
                    },
                    {
                        "name": "Jianzhong Yang"
                    },
                    {
                        "name": "Bi Zhao"
                    },
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Jiang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Zhong"
                },
                "author": "Jiang Zhong",
                "arxiv_comment": "10 pages, 8 figures (including tables), plus appendix. Submitted to\n  AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; I.2.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23840v3",
                "updated": "2025-08-26T02:19:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    2,
                    19,
                    29,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-28T14:05:46Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    5,
                    46,
                    2,
                    148,
                    0
                ],
                "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Sycophancy of Language Models in Multi-turn Dialogues"
                },
                "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench."
                },
                "authors": [
                    {
                        "name": "Jiseung Hong"
                    },
                    {
                        "name": "Grace Byun"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18253v1",
                "updated": "2025-08-25T17:41:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    46,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:41:46Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    46,
                    0,
                    237,
                    0
                ],
                "title": "From BERT to LLMs: Comparing and Understanding Chinese Classifier\n  Prediction in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From BERT to LLMs: Comparing and Understanding Chinese Classifier\n  Prediction in Language Models"
                },
                "summary": "Classifiers are an important and defining feature of the Chinese language,\nand their correct prediction is key to numerous educational applications. Yet,\nwhether the most popular Large Language Models (LLMs) possess proper knowledge\nthe Chinese classifiers is an issue that has largely remain unexplored in the\nNatural Language Processing (NLP) literature.\n  To address such a question, we employ various masking strategies to evaluate\nthe LLMs' intrinsic ability, the contribution of different sentence elements,\nand the working of the attention mechanisms during prediction. Besides, we\nexplore fine-tuning for LLMs to enhance the classifier performance.\n  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.\nThe prediction, as expected, greatly benefits from the information about the\nfollowing noun, which also explains the advantage of models with a\nbidirectional attention mechanism such as BERT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifiers are an important and defining feature of the Chinese language,\nand their correct prediction is key to numerous educational applications. Yet,\nwhether the most popular Large Language Models (LLMs) possess proper knowledge\nthe Chinese classifiers is an issue that has largely remain unexplored in the\nNatural Language Processing (NLP) literature.\n  To address such a question, we employ various masking strategies to evaluate\nthe LLMs' intrinsic ability, the contribution of different sentence elements,\nand the working of the attention mechanisms during prediction. Besides, we\nexplore fine-tuning for LLMs to enhance the classifier performance.\n  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.\nThe prediction, as expected, greatly benefits from the information about the\nfollowing noun, which also explains the advantage of models with a\nbidirectional attention mechanism such as BERT."
                },
                "authors": [
                    {
                        "name": "ZiqiZhang"
                    },
                    {
                        "name": "Jianfei Ma"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Jieshun You"
                    },
                    {
                        "name": "Zhaoxin Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxin Feng"
                },
                "author": "Zhaoxin Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11598v2",
                "updated": "2025-08-25T17:40:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    40,
                    24,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-16T18:03:48Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    3,
                    48,
                    4,
                    136,
                    0
                ],
                "title": "The Host Galaxies of PTA Sources: Converting Supermassive BH Binary\n  Parameters into EM Observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Host Galaxies of PTA Sources: Converting Supermassive BH Binary\n  Parameters into EM Observables"
                },
                "summary": "Pulsar timing arrays (PTAs) are approaching the sensitivity required to\nresolve gravitational waves (GWs) from individual supermassive black hole\n(SMBH) binaries. However, the large uncertainty in source localization will\nmake the identification of its host environment challenging. We show how to\nconvert the posterior probability function of binary parameters inferred by GW\nanalyses into distributions of apparent magnitudes of the host galaxy. We do so\nfor a scenario in which the host environment is a regular early-type galaxy,\nand one in which it is an active galactic nucleus. We estimate the reach of\nPTAs in the near and intermediate future, and estimate whether the binary hosts\nwill be detectable in all-sky electromagnetic (EM) surveys. A PTA with a\nbaseline of 20 yr and 116 pulsars, resembling the upcoming data release of the\nInternational Pulsar Timing Array, can detect binaries out to a luminosity\ndistance of 2 Gpc (corresponding to a redshift of $z\\sim0.36$), while a PTA\nwith a baseline of 30 yr and 200 pulsars can reach out to distances slightly\ngreater than 3 Gpc ($z\\sim0.53$). We find that the host galaxies of all\nbinaries detectable with a baseline of 20 yr are expected to be present in the\nWide-field Infrared Survey Explorer and SuperCOSMOS surveys, if they lie\noutside the plane of the Milky Way. The Two Micron All Sky Survey becomes\nincomplete for hosts of binaries more massive than $10^{9.8}{\\rm M}_\\odot$ at a\nluminosity distance greater than 1 Gpc. The EM surveys become slightly more\nincomplete when PTAs with longer baselines and therefore improved sensitivities\nare considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulsar timing arrays (PTAs) are approaching the sensitivity required to\nresolve gravitational waves (GWs) from individual supermassive black hole\n(SMBH) binaries. However, the large uncertainty in source localization will\nmake the identification of its host environment challenging. We show how to\nconvert the posterior probability function of binary parameters inferred by GW\nanalyses into distributions of apparent magnitudes of the host galaxy. We do so\nfor a scenario in which the host environment is a regular early-type galaxy,\nand one in which it is an active galactic nucleus. We estimate the reach of\nPTAs in the near and intermediate future, and estimate whether the binary hosts\nwill be detectable in all-sky electromagnetic (EM) surveys. A PTA with a\nbaseline of 20 yr and 116 pulsars, resembling the upcoming data release of the\nInternational Pulsar Timing Array, can detect binaries out to a luminosity\ndistance of 2 Gpc (corresponding to a redshift of $z\\sim0.36$), while a PTA\nwith a baseline of 30 yr and 200 pulsars can reach out to distances slightly\ngreater than 3 Gpc ($z\\sim0.53$). We find that the host galaxies of all\nbinaries detectable with a baseline of 20 yr are expected to be present in the\nWide-field Infrared Survey Explorer and SuperCOSMOS surveys, if they lie\noutside the plane of the Milky Way. The Two Micron All Sky Survey becomes\nincomplete for hosts of binaries more massive than $10^{9.8}{\\rm M}_\\odot$ at a\nluminosity distance greater than 1 Gpc. The EM surveys become slightly more\nincomplete when PTAs with longer baselines and therefore improved sensitivities\nare considered."
                },
                "authors": [
                    {
                        "name": "Niccolo Veronesi"
                    },
                    {
                        "name": "Maria Charisi"
                    },
                    {
                        "name": "Stephen R Taylor"
                    },
                    {
                        "name": "Jessie Runnoe"
                    },
                    {
                        "name": "Daniel J D'Orazio"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J D'Orazio"
                },
                "author": "Daniel J D'Orazio",
                "arxiv_doi": "10.3847/1538-4357/adf065",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adf065",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 7 figures",
                "arxiv_journal_ref": "The Astrophysical Journal, 990:46, 2025 September 1",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17390v2",
                "updated": "2025-08-25T17:39:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    39,
                    8,
                    0,
                    237,
                    0
                ],
                "published": "2025-06-20T18:00:02Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    18,
                    0,
                    2,
                    4,
                    171,
                    0
                ],
                "title": "JWST reveals cosmic ray dominated chemistry in the local ULIRG IRAS\n  07251$-$0248",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST reveals cosmic ray dominated chemistry in the local ULIRG IRAS\n  07251$-$0248"
                },
                "summary": "We analyse the ro-vibrational absorption bands of various molecular cations\n(HCO$^+$, HCNH$^+$, and N$_2$H$^+$) and neutral species (HCN, HNC, and HC$_3$N)\ndetected in the \\textit{James Webb Space Telescope}/Mid-Infrared Instrument\nMedium Resolution Spectrometer spectrum (4.9--27.9\\,$\\upmu$m) of the local\nultra luminous infrared galaxy IRAS~07251$-$0248. We find that the molecular\nabsorptions are blueshifted by 160\\,km\\,s$^{-1}$ relative to the systemic\nvelocity of the target. Using local thermal equilibrium (LTE) excitation\nmodels, we derive rotational temperatures ($T_{\\rm rot}$) from 42 to 185\\,K for\nthese absorption bands. This range of measured $T_{\\rm rot}$ can be explained\nby infrared (IR) radiative pumping as a by--product of the strength, effective\ncritical density, and opacity of each molecular band. Thus, these results\nsuggest that these absorptions originate in a warm expanding gas shell\n($\\dot{M}$$\\sim$90--330\\,$M_\\odot$\\,yr$^{-1}$), which might be the base of the\nlarger scale cold molecular outflow detected in this source. Finally, the\nelevated abundance of molecular cations can be explained by a high cosmic ray\nionization rate, with log($\\zeta_{\\text{H}_2}$/n$_{\\rm H}\\, [\\text{cm}^3\\,\n\\text{s}^{-1}])$ in the range of $-$18.2 (from H$_3^+$) to $-$19.1 (inferred\nfrom HCO$^+$ and N$_2$H$^+$, which are likely tracing denser gas), consistent\nwith a cosmic ray dominated chemistry as predicted by chemical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse the ro-vibrational absorption bands of various molecular cations\n(HCO$^+$, HCNH$^+$, and N$_2$H$^+$) and neutral species (HCN, HNC, and HC$_3$N)\ndetected in the \\textit{James Webb Space Telescope}/Mid-Infrared Instrument\nMedium Resolution Spectrometer spectrum (4.9--27.9\\,$\\upmu$m) of the local\nultra luminous infrared galaxy IRAS~07251$-$0248. We find that the molecular\nabsorptions are blueshifted by 160\\,km\\,s$^{-1}$ relative to the systemic\nvelocity of the target. Using local thermal equilibrium (LTE) excitation\nmodels, we derive rotational temperatures ($T_{\\rm rot}$) from 42 to 185\\,K for\nthese absorption bands. This range of measured $T_{\\rm rot}$ can be explained\nby infrared (IR) radiative pumping as a by--product of the strength, effective\ncritical density, and opacity of each molecular band. Thus, these results\nsuggest that these absorptions originate in a warm expanding gas shell\n($\\dot{M}$$\\sim$90--330\\,$M_\\odot$\\,yr$^{-1}$), which might be the base of the\nlarger scale cold molecular outflow detected in this source. Finally, the\nelevated abundance of molecular cations can be explained by a high cosmic ray\nionization rate, with log($\\zeta_{\\text{H}_2}$/n$_{\\rm H}\\, [\\text{cm}^3\\,\n\\text{s}^{-1}])$ in the range of $-$18.2 (from H$_3^+$) to $-$19.1 (inferred\nfrom HCO$^+$ and N$_2$H$^+$, which are likely tracing denser gas), consistent\nwith a cosmic ray dominated chemistry as predicted by chemical models."
                },
                "authors": [
                    {
                        "name": "G. Speranza"
                    },
                    {
                        "name": "M. Pereira-Santaella"
                    },
                    {
                        "name": "M. Ag√∫ndez"
                    },
                    {
                        "name": "E. Gonz√°lez-Alfonso"
                    },
                    {
                        "name": "I. Garc√≠a-Bernete"
                    },
                    {
                        "name": "J. R. Goicoechea"
                    },
                    {
                        "name": "M. Imanishi"
                    },
                    {
                        "name": "D. Rigopoulou"
                    },
                    {
                        "name": "M. G. Santa-Maria"
                    },
                    {
                        "name": "N. Thatte"
                    }
                ],
                "author_detail": {
                    "name": "N. Thatte"
                },
                "author": "N. Thatte",
                "arxiv_comment": "Accepted for publication in MNRAS Letters. 9 pages, 5 figures and 2\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04068v2",
                "updated": "2025-08-25T17:37:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    37,
                    37,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-06T13:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    25,
                    9,
                    3,
                    37,
                    0
                ],
                "title": "Detectability of Massive Boson Stars using Gravitational Waves from\n  Fundamental Oscillations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detectability of Massive Boson Stars using Gravitational Waves from\n  Fundamental Oscillations"
                },
                "summary": "Boson Stars are macroscopic self-gravitating configurations made of complex\nscalar fields. These exotic compact objects would manifest as dark Boson stars\nand, in the absence of electromagnetic signatures, could mimic properties of\ncompact stars in the gravitational wave spectrum. In a recent study, using the\nsimplest potential for massive Boson stars, we demonstrated that fundamental\nnon-radial oscillations ($f$-modes) obey scaling relations that allow them to\nbe distinguished from neutron stars and black holes. In this work, we provide\nanalytical fits for these scaling relations, valid for the dark matter\nparameter space compatible with current astrophysical and cosmological data\nthat can be directly incorporated into future studies of massive Boson stars in\nthe strong coupling regime, avoiding the need for numerical calculations. We\nalso provide analytical fits for empirical and universal relations, for\ngravitational wave asteroseismology, which can be used to infer microscopic\ndark matter properties following a successful detection. Further, we\ninvestigate the possibility of the detection of $f$-modes and the dark matter\nparameter space that can be probed with current and future gravitational wave\ndetectors across multiple frequency bands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boson Stars are macroscopic self-gravitating configurations made of complex\nscalar fields. These exotic compact objects would manifest as dark Boson stars\nand, in the absence of electromagnetic signatures, could mimic properties of\ncompact stars in the gravitational wave spectrum. In a recent study, using the\nsimplest potential for massive Boson stars, we demonstrated that fundamental\nnon-radial oscillations ($f$-modes) obey scaling relations that allow them to\nbe distinguished from neutron stars and black holes. In this work, we provide\nanalytical fits for these scaling relations, valid for the dark matter\nparameter space compatible with current astrophysical and cosmological data\nthat can be directly incorporated into future studies of massive Boson stars in\nthe strong coupling regime, avoiding the need for numerical calculations. We\nalso provide analytical fits for empirical and universal relations, for\ngravitational wave asteroseismology, which can be used to infer microscopic\ndark matter properties following a successful detection. Further, we\ninvestigate the possibility of the detection of $f$-modes and the dark matter\nparameter space that can be probed with current and future gravitational wave\ndetectors across multiple frequency bands."
                },
                "authors": [
                    {
                        "name": "Swarnim Shirke"
                    },
                    {
                        "name": "Bikram Keshari Pradhan"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Laura Sagunski"
                    },
                    {
                        "name": "J√ºrgen Schaffner-Bielich"
                    }
                ],
                "author_detail": {
                    "name": "J√ºrgen Schaffner-Bielich"
                },
                "author": "J√ºrgen Schaffner-Bielich",
                "arxiv_comment": "31 pages, 10 figures. Revised version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18245v1",
                "updated": "2025-08-25T17:36:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    58,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    58,
                    0,
                    237,
                    0
                ],
                "title": "Demographic Biases and Gaps in the Perception of Sexism in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographic Biases and Gaps in the Perception of Sexism in Large\n  Language Models"
                },
                "summary": "The use of Large Language Models (LLMs) has proven to be a tool that could\nhelp in the automatic detection of sexism. Previous studies have shown that\nthese models contain biases that do not accurately reflect reality, especially\nfor minority groups. Despite various efforts to improve the detection of sexist\ncontent, this task remains a significant challenge due to its subjective nature\nand the biases present in automated models. We explore the capabilities of\ndifferent LLMs to detect sexism in social media text using the EXIST 2024 tweet\ndataset. It includes annotations from six distinct profiles for each tweet,\nallowing us to evaluate to what extent LLMs can mimic these groups' perceptions\nin sexism detection. Additionally, we analyze the demographic biases present in\nthe models and conduct a statistical analysis to identify which demographic\ncharacteristics (age, gender) contribute most effectively to this task. Our\nresults show that, while LLMs can to some extent detect sexism when considering\nthe overall opinion of populations, they do not accurately replicate the\ndiversity of perceptions among different demographic groups. This highlights\nthe need for better-calibrated models that account for the diversity of\nperspectives across different populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) has proven to be a tool that could\nhelp in the automatic detection of sexism. Previous studies have shown that\nthese models contain biases that do not accurately reflect reality, especially\nfor minority groups. Despite various efforts to improve the detection of sexist\ncontent, this task remains a significant challenge due to its subjective nature\nand the biases present in automated models. We explore the capabilities of\ndifferent LLMs to detect sexism in social media text using the EXIST 2024 tweet\ndataset. It includes annotations from six distinct profiles for each tweet,\nallowing us to evaluate to what extent LLMs can mimic these groups' perceptions\nin sexism detection. Additionally, we analyze the demographic biases present in\nthe models and conduct a statistical analysis to identify which demographic\ncharacteristics (age, gender) contribute most effectively to this task. Our\nresults show that, while LLMs can to some extent detect sexism when considering\nthe overall opinion of populations, they do not accurately replicate the\ndiversity of perceptions among different demographic groups. This highlights\nthe need for better-calibrated models that account for the diversity of\nperspectives across different populations."
                },
                "authors": [
                    {
                        "name": "Judith Tavarez-Rodr√≠guez"
                    },
                    {
                        "name": "Fernando S√°nchez-Vega"
                    },
                    {
                        "name": "A. Pastor L√≥pez-Monroy"
                    }
                ],
                "author_detail": {
                    "name": "A. Pastor L√≥pez-Monroy"
                },
                "author": "A. Pastor L√≥pez-Monroy",
                "arxiv_comment": "This work was presented as a poster at the Latin American Meeting in\n  Artificial Intelligence KHIPU 2025, Santiago, Chile, March 10th - 14th 2025,\n  https://khipu.ai/khipu2025/poster-sessions-2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18244v1",
                "updated": "2025-08-25T17:36:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:36:21Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    21,
                    0,
                    237,
                    0
                ],
                "title": "Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows\n  to Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows\n  to Data"
                },
                "summary": "Reliably composing Large Language Models (LLMs) for complex, multi-step\nworkflows remains a significant challenge. The dominant paradigm-optimizing\ndiscrete prompts in a pipeline-is notoriously brittle and struggles to enforce\nthe formal compliance required for structured tasks. We introduce\nType-Compliant Adaptation Cascades (TACs), a framework that recasts workflow\nadaptation as learning typed probabilistic programs. TACs treats the entire\nworkflow, which is composed of parameter-efficiently adapted LLMs and\ndeterministic logic, as an unnormalized joint distribution. This enables\nprincipled, gradient-based training even with latent intermediate structures.\nWe provide theoretical justification for our tractable optimization objective,\nproving that the optimization bias vanishes as the model learns type\ncompliance. Empirically, TACs significantly outperforms state-of-the-art\nprompt-optimization baselines. Gains are particularly pronounced on structured\ntasks, improving MGSM-SymPy from $57.1\\%$ to $75.9\\%$ for a 27B model, MGSM\nfrom $1.6\\%$ to $27.3\\%$ for a 7B model. TACs offers a robust and theoretically\ngrounded paradigm for developing reliable, task-compliant LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliably composing Large Language Models (LLMs) for complex, multi-step\nworkflows remains a significant challenge. The dominant paradigm-optimizing\ndiscrete prompts in a pipeline-is notoriously brittle and struggles to enforce\nthe formal compliance required for structured tasks. We introduce\nType-Compliant Adaptation Cascades (TACs), a framework that recasts workflow\nadaptation as learning typed probabilistic programs. TACs treats the entire\nworkflow, which is composed of parameter-efficiently adapted LLMs and\ndeterministic logic, as an unnormalized joint distribution. This enables\nprincipled, gradient-based training even with latent intermediate structures.\nWe provide theoretical justification for our tractable optimization objective,\nproving that the optimization bias vanishes as the model learns type\ncompliance. Empirically, TACs significantly outperforms state-of-the-art\nprompt-optimization baselines. Gains are particularly pronounced on structured\ntasks, improving MGSM-SymPy from $57.1\\%$ to $75.9\\%$ for a 27B model, MGSM\nfrom $1.6\\%$ to $27.3\\%$ for a 7B model. TACs offers a robust and theoretically\ngrounded paradigm for developing reliable, task-compliant LLM systems."
                },
                "authors": [
                    {
                        "name": "Chu-Cheng Lin"
                    },
                    {
                        "name": "Daiyi Peng"
                    },
                    {
                        "name": "Yifeng Lu"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Eugene Ie"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Ie"
                },
                "author": "Eugene Ie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15650v2",
                "updated": "2025-08-25T17:25:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    25,
                    44,
                    0,
                    237,
                    0
                ],
                "published": "2025-04-22T07:16:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    16,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AffordanceSAM: Segment Anything Once More in Affordance Grounding"
                },
                "summary": "Building a generalized affordance grounding model to identify actionable\nregions on objects is vital for real-world applications. Existing methods to\ntrain the model can be divided into weakly and fully supervised ways. However,\nthe former method requires a complex training framework design and can not\ninfer new actions without an auxiliary prior. While the latter often struggle\nwith limited annotated data and components trained from scratch despite being\nsimpler. This study focuses on fully supervised affordance grounding and\novercomes its limitations by proposing AffordanceSAM, which extends SAM's\ngeneralization capacity in segmentation to affordance grounding. Specifically,\nwe design an affordance-adaption module and curate a coarse-to-fine annotated\ndataset called C2F-Aff to thoroughly transfer SAM's robust performance to\naffordance in a three-stage training manner. Experimental results confirm that\nAffordanceSAM achieves state-of-the-art (SOTA) performance on the AGD20K\nbenchmark and exhibits strong generalized capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a generalized affordance grounding model to identify actionable\nregions on objects is vital for real-world applications. Existing methods to\ntrain the model can be divided into weakly and fully supervised ways. However,\nthe former method requires a complex training framework design and can not\ninfer new actions without an auxiliary prior. While the latter often struggle\nwith limited annotated data and components trained from scratch despite being\nsimpler. This study focuses on fully supervised affordance grounding and\novercomes its limitations by proposing AffordanceSAM, which extends SAM's\ngeneralization capacity in segmentation to affordance grounding. Specifically,\nwe design an affordance-adaption module and curate a coarse-to-fine annotated\ndataset called C2F-Aff to thoroughly transfer SAM's robust performance to\naffordance in a three-stage training manner. Experimental results confirm that\nAffordanceSAM achieves state-of-the-art (SOTA) performance on the AGD20K\nbenchmark and exhibits strong generalized capacity."
                },
                "authors": [
                    {
                        "name": "Dengyang Jiang"
                    },
                    {
                        "name": "Zanyi Wang"
                    },
                    {
                        "name": "Hengzhuang Li"
                    },
                    {
                        "name": "Sizhe Dang"
                    },
                    {
                        "name": "Teli Ma"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Guang Dai"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Mengmeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengmeng Wang"
                },
                "author": "Mengmeng Wang",
                "arxiv_comment": "SAM Meets Affordance Grounding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18224v1",
                "updated": "2025-08-25T17:22:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    22,
                    15,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:22:15Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    22,
                    15,
                    0,
                    237,
                    0
                ],
                "title": "Flash Sparse Attention: An Alternative Efficient Implementation of\n  Native Sparse Attention Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Sparse Attention: An Alternative Efficient Implementation of\n  Native Sparse Attention Kernel"
                },
                "summary": "Recent progress in sparse attention mechanisms has demonstrated strong\npotential for reducing the computational cost of long-context training and\ninference in large language models (LLMs). Native Sparse Attention (NSA), a\nstate-of-the-art approach, introduces natively trainable, hardware-aligned\nsparse attention that delivers substantial system-level performance gains while\nmaintaining accuracy comparable to full attention. However, the kernel\nimplementation of NSA relies on a query-grouping strategy that is efficient\nonly with large Grouped Query Attention (GQA) sizes, whereas modern LLMs\ntypically adopt much smaller GQA groups, which limits the applicability of this\nsparse algorithmic advance. In this work, we propose Flash Sparse Attention\n(FSA), which includes an alternative kernel design that enables efficient NSA\ncomputation across a wide range of popular LLMs with varied smaller GQA group\nsizes on modern GPUs. Compared to vanilla NSA kernel implementation, our\nempirical evaluation demonstrates that FSA achieves (i) up to 3.5$\\times$ and\non average 1.6$\\times$ kernel-level latency reduction, (ii) up to 1.25$\\times$\nand 1.09$\\times$ on average end-to-end training speedup on state-of-the-art\nLLMs, and (iii) up to 1.36$\\times$ and 1.11$\\times$ on average end-to-end\nprefill speedup on state-of-the-art LLMs. The source code is open-sourced and\npublicly available at\nhttps://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in sparse attention mechanisms has demonstrated strong\npotential for reducing the computational cost of long-context training and\ninference in large language models (LLMs). Native Sparse Attention (NSA), a\nstate-of-the-art approach, introduces natively trainable, hardware-aligned\nsparse attention that delivers substantial system-level performance gains while\nmaintaining accuracy comparable to full attention. However, the kernel\nimplementation of NSA relies on a query-grouping strategy that is efficient\nonly with large Grouped Query Attention (GQA) sizes, whereas modern LLMs\ntypically adopt much smaller GQA groups, which limits the applicability of this\nsparse algorithmic advance. In this work, we propose Flash Sparse Attention\n(FSA), which includes an alternative kernel design that enables efficient NSA\ncomputation across a wide range of popular LLMs with varied smaller GQA group\nsizes on modern GPUs. Compared to vanilla NSA kernel implementation, our\nempirical evaluation demonstrates that FSA achieves (i) up to 3.5$\\times$ and\non average 1.6$\\times$ kernel-level latency reduction, (ii) up to 1.25$\\times$\nand 1.09$\\times$ on average end-to-end training speedup on state-of-the-art\nLLMs, and (iii) up to 1.36$\\times$ and 1.11$\\times$ on average end-to-end\nprefill speedup on state-of-the-art LLMs. The source code is open-sourced and\npublicly available at\nhttps://github.com/Relaxed-System-Lab/Flash-Sparse-Attention."
                },
                "authors": [
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15180v2",
                "updated": "2025-08-25T17:16:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    16,
                    0,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T02:36:16Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    2,
                    36,
                    16,
                    3,
                    233,
                    0
                ],
                "title": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data"
                },
                "summary": "High-quality mathematical and logical datasets with verifiable answers are\nessential for strengthening the reasoning capabilities of large language models\n(LLMs). While recent data augmentation techniques have facilitated the creation\nof large-scale benchmarks, existing LLM-generated datasets often suffer from\nlimited reliability, diversity, and scalability. To address these challenges,\nwe introduce PuzzleClone, a formal framework for synthesizing verifiable data\nat scale using Satisfiability Modulo Theories (SMT). Our approach features\nthree key innovations: (1) encoding seed puzzles into structured logical\nspecifications, (2) generating scalable variants through systematic variable\nand constraint randomization, and (3) ensuring validity via a reproduction\nmechanism. Applying PuzzleClone, we construct a curated benchmark comprising\nover 83K diverse and programmatically validated puzzles. The generated puzzles\nspan a wide spectrum of difficulty and formats, posing significant challenges\nto current state-of-the-art models. We conduct post training (SFT and RL) on\nPuzzleClone datasets. Experimental results show that training on PuzzleClone\nyields substantial improvements not only on PuzzleClone testset but also on\nlogic and mathematical benchmarks. Post training raises PuzzleClone average\nfrom 14.4 to 56.2 and delivers consistent improvements across 7 logic and\nmathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from\n52.5 to 65.0). Our code and data are available at\nhttps://github.com/HiThink-Research/PuzzleClone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality mathematical and logical datasets with verifiable answers are\nessential for strengthening the reasoning capabilities of large language models\n(LLMs). While recent data augmentation techniques have facilitated the creation\nof large-scale benchmarks, existing LLM-generated datasets often suffer from\nlimited reliability, diversity, and scalability. To address these challenges,\nwe introduce PuzzleClone, a formal framework for synthesizing verifiable data\nat scale using Satisfiability Modulo Theories (SMT). Our approach features\nthree key innovations: (1) encoding seed puzzles into structured logical\nspecifications, (2) generating scalable variants through systematic variable\nand constraint randomization, and (3) ensuring validity via a reproduction\nmechanism. Applying PuzzleClone, we construct a curated benchmark comprising\nover 83K diverse and programmatically validated puzzles. The generated puzzles\nspan a wide spectrum of difficulty and formats, posing significant challenges\nto current state-of-the-art models. We conduct post training (SFT and RL) on\nPuzzleClone datasets. Experimental results show that training on PuzzleClone\nyields substantial improvements not only on PuzzleClone testset but also on\nlogic and mathematical benchmarks. Post training raises PuzzleClone average\nfrom 14.4 to 56.2 and delivers consistent improvements across 7 logic and\nmathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from\n52.5 to 65.0). Our code and data are available at\nhttps://github.com/HiThink-Research/PuzzleClone."
                },
                "authors": [
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Yanwei Huang"
                    },
                    {
                        "name": "Rongjunchen Zhang"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Haipang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Haipang Wu"
                },
                "author": "Haipang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18213v1",
                "updated": "2025-08-25T17:11:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    11,
                    53,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:11:53Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    11,
                    53,
                    0,
                    237,
                    0
                ],
                "title": "Follow My Hold: Hand-Object Interaction Reconstruction through Geometric\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow My Hold: Hand-Object Interaction Reconstruction through Geometric\n  Guidance"
                },
                "summary": "We propose a novel diffusion-based framework for reconstructing 3D geometry\nof hand-held objects from monocular RGB images by leveraging hand-object\ninteraction as geometric guidance. Our method conditions a latent diffusion\nmodel on an inpainted object appearance and uses inference-time guidance to\noptimize the object reconstruction, while simultaneously ensuring plausible\nhand-object interactions. Unlike prior methods that rely on extensive\npost-processing or produce low-quality reconstructions, our approach directly\ngenerates high-quality object geometry during the diffusion process by\nintroducing guidance with an optimization-in-the-loop design. Specifically, we\nguide the diffusion model by applying supervision to the velocity field while\nsimultaneously optimizing the transformations of both the hand and the object\nbeing reconstructed. This optimization is driven by multi-modal geometric cues,\nincluding normal and depth alignment, silhouette consistency, and 2D keypoint\nreprojection. We further incorporate signed distance field supervision and\nenforce contact and non-intersection constraints to ensure physical\nplausibility of hand-object interaction. Our method yields accurate, robust and\ncoherent reconstructions under occlusion while generalizing well to in-the-wild\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel diffusion-based framework for reconstructing 3D geometry\nof hand-held objects from monocular RGB images by leveraging hand-object\ninteraction as geometric guidance. Our method conditions a latent diffusion\nmodel on an inpainted object appearance and uses inference-time guidance to\noptimize the object reconstruction, while simultaneously ensuring plausible\nhand-object interactions. Unlike prior methods that rely on extensive\npost-processing or produce low-quality reconstructions, our approach directly\ngenerates high-quality object geometry during the diffusion process by\nintroducing guidance with an optimization-in-the-loop design. Specifically, we\nguide the diffusion model by applying supervision to the velocity field while\nsimultaneously optimizing the transformations of both the hand and the object\nbeing reconstructed. This optimization is driven by multi-modal geometric cues,\nincluding normal and depth alignment, silhouette consistency, and 2D keypoint\nreprojection. We further incorporate signed distance field supervision and\nenforce contact and non-intersection constraints to ensure physical\nplausibility of hand-object interaction. Our method yields accurate, robust and\ncoherent reconstructions under occlusion while generalizing well to in-the-wild\nscenarios."
                },
                "authors": [
                    {
                        "name": "Ayce Idil Aytekin"
                    },
                    {
                        "name": "Helge Rhodin"
                    },
                    {
                        "name": "Rishabh Dabral"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "arxiv_comment": "Project page: https://aidilayce.github.io/FollowMyHold-page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18212v1",
                "updated": "2025-08-25T17:11:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    11,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:11:28Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    11,
                    28,
                    0,
                    237,
                    0
                ],
                "title": "Better Language Model-Based Judging Reward Modeling through Scaling\n  Comprehension Boundaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Language Model-Based Judging Reward Modeling through Scaling\n  Comprehension Boundaries"
                },
                "summary": "The emergence of LM-based judging reward modeling, represented by generative\nreward models, has successfully made reinforcement learning from AI feedback\n(RLAIF) efficient and scalable. To further advance this paradigm, we propose a\ncore insight: this form of reward modeling shares fundamental formal\nconsistency with natural language inference (NLI), a core task in natural\nlanguage understanding. This reframed perspective points to a key path for\nbuilding superior reward models: scaling the model's comprehension boundaries.\nPursuing this path, exploratory experiments on NLI tasks demonstrate that the\nslot prediction masked language models (MLMs) incorporating contextual\nexplanations achieve significantly better performance compared to mainstream\nautoregressive models. Based on this key finding, we propose ESFP-RM, a\ntwo-stage LM-based judging reward model that utilizes an explanation based slot\nframework for prediction to fully leverage the advantages of MLMs. Extensive\nexperiments demonstrate that in both reinforcement learning from human feedback\n(RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers\nmore stable and generalizable reward signals compared to generative reward\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of LM-based judging reward modeling, represented by generative\nreward models, has successfully made reinforcement learning from AI feedback\n(RLAIF) efficient and scalable. To further advance this paradigm, we propose a\ncore insight: this form of reward modeling shares fundamental formal\nconsistency with natural language inference (NLI), a core task in natural\nlanguage understanding. This reframed perspective points to a key path for\nbuilding superior reward models: scaling the model's comprehension boundaries.\nPursuing this path, exploratory experiments on NLI tasks demonstrate that the\nslot prediction masked language models (MLMs) incorporating contextual\nexplanations achieve significantly better performance compared to mainstream\nautoregressive models. Based on this key finding, we propose ESFP-RM, a\ntwo-stage LM-based judging reward model that utilizes an explanation based slot\nframework for prediction to fully leverage the advantages of MLMs. Extensive\nexperiments demonstrate that in both reinforcement learning from human feedback\n(RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers\nmore stable and generalizable reward signals compared to generative reward\nmodels."
                },
                "authors": [
                    {
                        "name": "Meiling Ning"
                    },
                    {
                        "name": "Zhongbao Zhang"
                    },
                    {
                        "name": "Junda Ye"
                    },
                    {
                        "name": "Jiabao Guo"
                    },
                    {
                        "name": "Qingyuan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Qingyuan Guan"
                },
                "author": "Qingyuan Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16315v2",
                "updated": "2025-08-25T17:04:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    4,
                    49,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T11:52:04Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    52,
                    4,
                    4,
                    234,
                    0
                ],
                "title": "OwkinZero: Accelerating Biological Discovery with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OwkinZero: Accelerating Biological Discovery with AI"
                },
                "summary": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Nathan Bigaud"
                    },
                    {
                        "name": "Vincent Cabeli"
                    },
                    {
                        "name": "Meltem G√ºrel"
                    },
                    {
                        "name": "Arthur Pignet"
                    },
                    {
                        "name": "John Klein"
                    },
                    {
                        "name": "Gilles Wainrib"
                    },
                    {
                        "name": "Eric Durand"
                    }
                ],
                "author_detail": {
                    "name": "Eric Durand"
                },
                "author": "Eric Durand",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v2",
                "updated": "2025-08-25T16:58:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    58,
                    23,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "TranSQL+: Serving Large Language Models with SQL on Low-Resource\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TranSQL+: Serving Large Language Models with SQL on Low-Resource\n  Hardware"
                },
                "summary": "Deploying Large Language Models (LLMs) on resource-constrained devices\nremains challenging due to limited memory, lack of GPUs, and the complexity of\nexisting runtimes. In this paper, we introduce TranSQL+, a template-based code\ngenerator that translates LLM computation graphs into pure SQL queries for\nexecution in relational databases. Without relying on external libraries,\nTranSQL+, leverages mature database features, such as vectorized execution and\nout-of-core processing, for efficient inference. We further propose a\nrow-to-column (ROW2COL) optimization that improves join efficiency in matrix\noperations. Evaluated on Llama3-8B and DeepSeekMoE models, TranSQL+ achieves up\nto 20x lower prefill latency and 4x higher decoding speed compared to DeepSpeed\nInference and Llama.cpp in low-memory and CPU-only configurations. Our results\nhighlight relational databases as a practical environment for LLMs on\nlow-resource hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) on resource-constrained devices\nremains challenging due to limited memory, lack of GPUs, and the complexity of\nexisting runtimes. In this paper, we introduce TranSQL+, a template-based code\ngenerator that translates LLM computation graphs into pure SQL queries for\nexecution in relational databases. Without relying on external libraries,\nTranSQL+, leverages mature database features, such as vectorized execution and\nout-of-core processing, for efficient inference. We further propose a\nrow-to-column (ROW2COL) optimization that improves join efficiency in matrix\noperations. Evaluated on Llama3-8B and DeepSeekMoE models, TranSQL+ achieves up\nto 20x lower prefill latency and 4x higher decoding speed compared to DeepSpeed\nInference and Llama.cpp in low-memory and CPU-only configurations. Our results\nhighlight relational databases as a practical environment for LLMs on\nlow-resource hardware."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "arxiv_comment": "Accepted by SIGMOD2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13386v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13386v3",
                "updated": "2025-08-25T16:51:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    51,
                    40,
                    0,
                    237,
                    0
                ],
                "published": "2025-04-18T00:24:52Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    24,
                    52,
                    4,
                    108,
                    0
                ],
                "title": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis"
                },
                "summary": "In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations. The code and models will be\navailable at https://thunder.is.tue.mpg.de/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations. The code and models will be\navailable at https://thunder.is.tue.mpg.de/"
                },
                "authors": [
                    {
                        "name": "Radek Danƒõƒçek"
                    },
                    {
                        "name": "Carolin Schmitt"
                    },
                    {
                        "name": "Senya Polikovsky"
                    },
                    {
                        "name": "Michael J. Black"
                    }
                ],
                "author_detail": {
                    "name": "Michael J. Black"
                },
                "author": "Michael J. Black",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13386v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13386v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18192v1",
                "updated": "2025-08-25T16:49:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    49,
                    38,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:49:38Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    49,
                    38,
                    0,
                    237,
                    0
                ],
                "title": "Unraveling the cognitive patterns of Large Language Models through\n  module communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the cognitive patterns of Large Language Models through\n  module communities"
                },
                "summary": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions."
                },
                "authors": [
                    {
                        "name": "Kushal Raj Bhandari"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Jianxi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Gao"
                },
                "author": "Jianxi Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07974v2",
                "updated": "2025-08-25T16:49:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    49,
                    10,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-10T17:51:05Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    51,
                    5,
                    3,
                    191,
                    0
                ],
                "title": "Defending Against Prompt Injection With a Few DefensiveTokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending Against Prompt Injection With a Few DefensiveTokens"
                },
                "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken."
                },
                "authors": [
                    {
                        "name": "Sizhe Chen"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Chawin Sitawarin"
                    },
                    {
                        "name": "David Wagner"
                    }
                ],
                "author_detail": {
                    "name": "David Wagner"
                },
                "author": "David Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18190v2",
                "updated": "2025-08-26T08:10:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    10,
                    55,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T16:48:51Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    48,
                    51,
                    0,
                    237,
                    0
                ],
                "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering"
                },
                "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor."
                },
                "authors": [
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Boyu Niu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Boxiu Li"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12964v2",
                "updated": "2025-08-25T16:47:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    47,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-18T15:46:31Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    46,
                    31,
                    1,
                    49,
                    0
                ],
                "title": "Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the\n  Answer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the\n  Answer"
                },
                "summary": "Prior work on large language model (LLM) hallucinations has associated them\nwith model uncertainty or inaccurate knowledge. In this work, we define and\ninvestigate a distinct type of hallucination, where a model can consistently\nanswer a question correctly, but a seemingly trivial perturbation, which can\nhappen in real-world settings, causes it to produce a hallucinated response\nwith high certainty. This phenomenon, which we dub CHOKE (Certain\nHallucinations Overriding Known Evidence), is particularly concerning in\nhigh-stakes domains such as medicine or law, where model certainty is often\nused as a proxy for reliability. We show that CHOKE examples are consistent\nacross prompts, occur in different models and datasets, and are fundamentally\ndistinct from other hallucinations. This difference leads existing mitigation\nmethods to perform worse on CHOKE examples than on general hallucinations.\nFinally, we introduce a probing-based mitigation that outperforms existing\nmethods on CHOKE hallucinations. These findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on large language model (LLM) hallucinations has associated them\nwith model uncertainty or inaccurate knowledge. In this work, we define and\ninvestigate a distinct type of hallucination, where a model can consistently\nanswer a question correctly, but a seemingly trivial perturbation, which can\nhappen in real-world settings, causes it to produce a hallucinated response\nwith high certainty. This phenomenon, which we dub CHOKE (Certain\nHallucinations Overriding Known Evidence), is particularly concerning in\nhigh-stakes domains such as medicine or law, where model certainty is often\nused as a proxy for reliability. We show that CHOKE examples are consistent\nacross prompts, occur in different models and datasets, and are fundamentally\ndistinct from other hallucinations. This difference leads existing mitigation\nmethods to perform worse on CHOKE examples than on general hallucinations.\nFinally, we introduce a probing-based mitigation that outperforms existing\nmethods on CHOKE hallucinations. These findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Itay Itzhak"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18183v1",
                "updated": "2025-08-25T16:36:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    36,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    36,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios"
                },
                "summary": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities."
                },
                "authors": [
                    {
                        "name": "Luana Bulla"
                    },
                    {
                        "name": "Gabriele Tuccio"
                    },
                    {
                        "name": "Misael Mongiov√¨"
                    },
                    {
                        "name": "Aldo Gangemi"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Gangemi"
                },
                "author": "Aldo Gangemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18182v1",
                "updated": "2025-08-25T16:35:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    35,
                    57,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:35:57Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    35,
                    57,
                    0,
                    237,
                    0
                ],
                "title": "AdLoCo: adaptive batching significantly improves communications\n  efficiency and convergence for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdLoCo: adaptive batching significantly improves communications\n  efficiency and convergence for Large Language Models"
                },
                "summary": "Scaling distributed training of Large Language Models (LLMs) requires not\nonly algorithmic advances but also efficient utilization of heterogeneous\nhardware resources. While existing methods such as DiLoCo have demonstrated\npromising results, they often fail to fully exploit computational clusters\nunder dynamic workloads. To address this limitation, we propose a three-stage\nmethod that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,\nand switch mode mechanism. MIT allows individual nodes to run multiple\nlightweight training streams with different model instances in parallel and\nmerge them to combine knowledge, increasing throughput and reducing idle time.\nAdaptive Batched DiLoCo dynamically adjusts local batch sizes to balance\ncomputation and communication, substantially lowering synchronization delays.\nSwitch mode further stabilizes training by seamlessly introducing gradient\naccumulation once adaptive batch sizes grow beyond hardware-friendly limits.\nTogether, these innovations improve both convergence speed and system\nefficiency. We also provide a theoretical estimate of the number of\ncommunications required for the full convergence of a model trained using our\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling distributed training of Large Language Models (LLMs) requires not\nonly algorithmic advances but also efficient utilization of heterogeneous\nhardware resources. While existing methods such as DiLoCo have demonstrated\npromising results, they often fail to fully exploit computational clusters\nunder dynamic workloads. To address this limitation, we propose a three-stage\nmethod that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,\nand switch mode mechanism. MIT allows individual nodes to run multiple\nlightweight training streams with different model instances in parallel and\nmerge them to combine knowledge, increasing throughput and reducing idle time.\nAdaptive Batched DiLoCo dynamically adjusts local batch sizes to balance\ncomputation and communication, substantially lowering synchronization delays.\nSwitch mode further stabilizes training by seamlessly introducing gradient\naccumulation once adaptive batch sizes grow beyond hardware-friendly limits.\nTogether, these innovations improve both convergence speed and system\nefficiency. We also provide a theoretical estimate of the number of\ncommunications required for the full convergence of a model trained using our\nmethod."
                },
                "authors": [
                    {
                        "name": "Nikolay Kutuzov"
                    },
                    {
                        "name": "Makar Baderko"
                    },
                    {
                        "name": "Stepan Kulibaba"
                    },
                    {
                        "name": "Artem Dzhalilov"
                    },
                    {
                        "name": "Daniel Bobrov"
                    },
                    {
                        "name": "Maxim Mashtaler"
                    },
                    {
                        "name": "Alexander Gasnikov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Gasnikov"
                },
                "author": "Alexander Gasnikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18175v1",
                "updated": "2025-08-25T16:28:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    28,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:28:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    28,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Amortized Sampling with Transferable Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Sampling with Transferable Normalizing Flows"
                },
                "summary": "Efficient equilibrium sampling of molecular conformations remains a core\nchallenge in computational chemistry and statistical inference. Classical\napproaches such as molecular dynamics or Markov chain Monte Carlo inherently\nlack amortization; the computational cost of sampling must be paid in-full for\neach system of interest. The widespread success of generative models has\ninspired interest into overcoming this limitation through learning sampling\nalgorithms. Despite performing on par with conventional methods when trained on\na single system, learned samplers have so far demonstrated limited ability to\ntransfer across systems. We prove that deep learning enables the design of\nscalable and transferable samplers by introducing Prose, a 280 million\nparameter all-atom transferable normalizing flow trained on a corpus of peptide\nmolecular dynamics trajectories up to 8 residues in length. Prose draws\nzero-shot uncorrelated proposal samples for arbitrary peptide systems,\nachieving the previously intractable transferability across sequence length,\nwhilst retaining the efficient likelihood evaluation of normalizing flows.\nThrough extensive empirical evaluation we demonstrate the efficacy of Prose as\na proposal for a variety of sampling algorithms, finding a simple importance\nsampling-based finetuning procedure to achieve superior performance to\nestablished methods such as sequential Monte Carlo on unseen tetrapeptides. We\nopen-source the Prose codebase, model weights, and training dataset, to further\nstimulate research into amortized sampling methods and finetuning objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient equilibrium sampling of molecular conformations remains a core\nchallenge in computational chemistry and statistical inference. Classical\napproaches such as molecular dynamics or Markov chain Monte Carlo inherently\nlack amortization; the computational cost of sampling must be paid in-full for\neach system of interest. The widespread success of generative models has\ninspired interest into overcoming this limitation through learning sampling\nalgorithms. Despite performing on par with conventional methods when trained on\na single system, learned samplers have so far demonstrated limited ability to\ntransfer across systems. We prove that deep learning enables the design of\nscalable and transferable samplers by introducing Prose, a 280 million\nparameter all-atom transferable normalizing flow trained on a corpus of peptide\nmolecular dynamics trajectories up to 8 residues in length. Prose draws\nzero-shot uncorrelated proposal samples for arbitrary peptide systems,\nachieving the previously intractable transferability across sequence length,\nwhilst retaining the efficient likelihood evaluation of normalizing flows.\nThrough extensive empirical evaluation we demonstrate the efficacy of Prose as\na proposal for a variety of sampling algorithms, finding a simple importance\nsampling-based finetuning procedure to achieve superior performance to\nestablished methods such as sequential Monte Carlo on unseen tetrapeptides. We\nopen-source the Prose codebase, model weights, and training dataset, to further\nstimulate research into amortized sampling methods and finetuning objectives."
                },
                "authors": [
                    {
                        "name": "Charlie B. Tan"
                    },
                    {
                        "name": "Majdi Hassan"
                    },
                    {
                        "name": "Leon Klein"
                    },
                    {
                        "name": "Saifuddin Syed"
                    },
                    {
                        "name": "Dominique Beaini"
                    },
                    {
                        "name": "Michael M. Bronstein"
                    },
                    {
                        "name": "Alexander Tong"
                    },
                    {
                        "name": "Kirill Neklyudov"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Neklyudov"
                },
                "author": "Kirill Neklyudov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18174v1",
                "updated": "2025-08-25T16:27:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    27,
                    1,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:27:01Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    27,
                    1,
                    0,
                    237,
                    0
                ],
                "title": "InReAcTable: LLM-Powered Interactive Visual Data Story Construction from\n  Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InReAcTable: LLM-Powered Interactive Visual Data Story Construction from\n  Tabular Data"
                },
                "summary": "Insights in tabular data capture valuable patterns that help analysts\nunderstand critical information. Organizing related insights into visual data\nstories is crucial for in-depth analysis. However, constructing such stories is\nchallenging because of the complexity of the inherent relations between\nextracted insights. Users face difficulty sifting through a vast number of\ndiscrete insights to integrate specific ones into a unified narrative that\nmeets their analytical goals. Existing methods either heavily rely on user\nexpertise, making the process inefficient, or employ automated approaches that\ncannot fully capture their evolving goals. In this paper, we introduce\nInReAcTable, a framework that enhances visual data story construction by\nestablishing both structural and semantic connections between data insights.\nEach user interaction triggers the Acting module, which utilizes an insight\ngraph for structural filtering to narrow the search space, followed by the\nReasoning module using the retrieval-augmented generation method based on large\nlanguage models for semantic filtering, ultimately providing insight\nrecommendations aligned with the user's analytical intent. Based on the\nInReAcTable framework, we develop an interactive prototype system that guides\nusers to construct visual data stories aligned with their analytical\nrequirements. We conducted a case study and a user experiment to demonstrate\nthe utility and effectiveness of the InReAcTable framework and the prototype\nsystem for interactively building visual data stories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights in tabular data capture valuable patterns that help analysts\nunderstand critical information. Organizing related insights into visual data\nstories is crucial for in-depth analysis. However, constructing such stories is\nchallenging because of the complexity of the inherent relations between\nextracted insights. Users face difficulty sifting through a vast number of\ndiscrete insights to integrate specific ones into a unified narrative that\nmeets their analytical goals. Existing methods either heavily rely on user\nexpertise, making the process inefficient, or employ automated approaches that\ncannot fully capture their evolving goals. In this paper, we introduce\nInReAcTable, a framework that enhances visual data story construction by\nestablishing both structural and semantic connections between data insights.\nEach user interaction triggers the Acting module, which utilizes an insight\ngraph for structural filtering to narrow the search space, followed by the\nReasoning module using the retrieval-augmented generation method based on large\nlanguage models for semantic filtering, ultimately providing insight\nrecommendations aligned with the user's analytical intent. Based on the\nInReAcTable framework, we develop an interactive prototype system that guides\nusers to construct visual data stories aligned with their analytical\nrequirements. We conducted a case study and a user experiment to demonstrate\nthe utility and effectiveness of the InReAcTable framework and the prototype\nsystem for interactively building visual data stories."
                },
                "authors": [
                    {
                        "name": "Gerile Aodeng"
                    },
                    {
                        "name": "Guozheng Li"
                    },
                    {
                        "name": "Yunshan Feng"
                    },
                    {
                        "name": "Qiyang Chen"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Chi Harold Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chi Harold Liu"
                },
                "author": "Chi Harold Liu",
                "arxiv_comment": "16 pages, 10 figures, accepted at ACM UIST 2025 (to appear)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12349v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12349v4",
                "updated": "2025-08-25T16:24:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    24,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-03-16T04:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    4,
                    10,
                    53,
                    6,
                    75,
                    0
                ],
                "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"
                },
                "summary": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/"
                },
                "authors": [
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Ryan Hsieh"
                    },
                    {
                        "name": "Haisu Zhou"
                    },
                    {
                        "name": "Tianqing Zou"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pramod Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Viswanath"
                },
                "author": "Pramod Viswanath",
                "arxiv_comment": "48 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12349v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17047v2",
                "updated": "2025-08-25T16:17:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    17,
                    48,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-22T22:09:00Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    22,
                    9,
                    0,
                    1,
                    203,
                    0
                ],
                "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Hybrid Captioner for Improved Long-form Video Understanding"
                },
                "summary": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video."
                },
                "authors": [
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Efsun Sarioglu Kayi"
                    },
                    {
                        "name": "Arun Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Arun Reddy"
                },
                "author": "Arun Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18167v1",
                "updated": "2025-08-25T16:16:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    16,
                    42,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:16:42Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    16,
                    42,
                    0,
                    237,
                    0
                ],
                "title": "DiscussLLM: Teaching Large Language Models When to Speak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiscussLLM: Teaching Large Language Models When to Speak"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nunderstanding and generating human-like text, yet they largely operate as\nreactive agents, responding only when directly prompted. This passivity creates\nan \"awareness gap,\" limiting their potential as truly collaborative partners in\ndynamic human discussions. We introduce $\\textit{DiscussLLM}$, a framework\ndesigned to bridge this gap by training models to proactively decide not just\n$\\textit{what}$ to say, but critically, $\\textit{when}$ to speak. Our primary\ncontribution is a scalable two-stage data generation pipeline that synthesizes\na large-scale dataset of realistic multi-turn human discussions. Each\ndiscussion is annotated with one of five intervention types (e.g., Factual\nCorrection, Concept Definition) and contains an explicit conversational trigger\nwhere an AI intervention adds value. By training models to predict a special\nsilent token when no intervention is needed, they learn to remain quiet until a\nhelpful contribution can be made. We explore two architectural baselines: an\nintegrated end-to-end model and a decoupled classifier-generator system\noptimized for low-latency inference. We evaluate these models on their ability\nto accurately time interventions and generate helpful responses, paving the way\nfor more situationally aware and proactive conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nunderstanding and generating human-like text, yet they largely operate as\nreactive agents, responding only when directly prompted. This passivity creates\nan \"awareness gap,\" limiting their potential as truly collaborative partners in\ndynamic human discussions. We introduce $\\textit{DiscussLLM}$, a framework\ndesigned to bridge this gap by training models to proactively decide not just\n$\\textit{what}$ to say, but critically, $\\textit{when}$ to speak. Our primary\ncontribution is a scalable two-stage data generation pipeline that synthesizes\na large-scale dataset of realistic multi-turn human discussions. Each\ndiscussion is annotated with one of five intervention types (e.g., Factual\nCorrection, Concept Definition) and contains an explicit conversational trigger\nwhere an AI intervention adds value. By training models to predict a special\nsilent token when no intervention is needed, they learn to remain quiet until a\nhelpful contribution can be made. We explore two architectural baselines: an\nintegrated end-to-end model and a decoupled classifier-generator system\noptimized for low-latency inference. We evaluate these models on their ability\nto accurately time interventions and generate helpful responses, paving the way\nfor more situationally aware and proactive conversational AI."
                },
                "authors": [
                    {
                        "name": "Deep Anil Patel"
                    },
                    {
                        "name": "Iain Melvin"
                    },
                    {
                        "name": "Christopher Malon"
                    },
                    {
                        "name": "Martin Renqiang Min"
                    }
                ],
                "author_detail": {
                    "name": "Martin Renqiang Min"
                },
                "author": "Martin Renqiang Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18164v1",
                "updated": "2025-08-25T16:13:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    13,
                    42,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:13:42Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    13,
                    42,
                    0,
                    237,
                    0
                ],
                "title": "S2Sent: Nested Selectivity Aware Sentence Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2Sent: Nested Selectivity Aware Sentence Representation Learning"
                },
                "summary": "The combination of Transformer-based encoders with contrastive learning\nrepresents the current mainstream paradigm for sentence representation\nlearning. This paradigm is typically based on the hidden states of the last\nTransformer block of the encoder. However, within Transformer-based encoders,\ndifferent blocks exhibit varying degrees of semantic perception ability. From\nthe perspective of interpretability, the semantic perception potential of\nknowledge neurons is modulated by stimuli, thus rational cross-block\nrepresentation fusion is a direction worth optimizing. To balance the semantic\nredundancy and loss across block fusion, we propose a sentence representation\nselection mechanism S\\textsuperscript{2}Sent, which integrates a parameterized\nnested selector downstream of the Transformer-based encoder. This selector\nperforms spatial selection (SS) and nested frequency selection (FS) from a\nmodular perspective. The SS innovatively employs a spatial squeeze based\nself-gating mechanism to obtain adaptive weights, which not only achieves\nfusion with low information redundancy but also captures the dependencies\nbetween embedding features. The nested FS replaces GAP with different DCT basis\nfunctions to achieve spatial squeeze with low semantic loss. Extensive\nexperiments have demonstrated that S\\textsuperscript{2}Sent achieves\nsignificant improvements over baseline methods with negligible additional\nparameters and inference latency, while highlighting high integrability and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of Transformer-based encoders with contrastive learning\nrepresents the current mainstream paradigm for sentence representation\nlearning. This paradigm is typically based on the hidden states of the last\nTransformer block of the encoder. However, within Transformer-based encoders,\ndifferent blocks exhibit varying degrees of semantic perception ability. From\nthe perspective of interpretability, the semantic perception potential of\nknowledge neurons is modulated by stimuli, thus rational cross-block\nrepresentation fusion is a direction worth optimizing. To balance the semantic\nredundancy and loss across block fusion, we propose a sentence representation\nselection mechanism S\\textsuperscript{2}Sent, which integrates a parameterized\nnested selector downstream of the Transformer-based encoder. This selector\nperforms spatial selection (SS) and nested frequency selection (FS) from a\nmodular perspective. The SS innovatively employs a spatial squeeze based\nself-gating mechanism to obtain adaptive weights, which not only achieves\nfusion with low information redundancy but also captures the dependencies\nbetween embedding features. The nested FS replaces GAP with different DCT basis\nfunctions to achieve spatial squeeze with low semantic loss. Extensive\nexperiments have demonstrated that S\\textsuperscript{2}Sent achieves\nsignificant improvements over baseline methods with negligible additional\nparameters and inference latency, while highlighting high integrability and\nscalability."
                },
                "authors": [
                    {
                        "name": "Jianxiang Zang"
                    },
                    {
                        "name": "Nijia Mo"
                    },
                    {
                        "name": "Yonda Wei"
                    },
                    {
                        "name": "Meiling Ning"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18160v1",
                "updated": "2025-08-25T16:09:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    9,
                    13,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:09:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    9,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "Shocks and complex chemodynamics in the metal-poor starburst galaxy CGCG\n  007-025 revealed through high-resolution echelle spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shocks and complex chemodynamics in the metal-poor starburst galaxy CGCG\n  007-025 revealed through high-resolution echelle spectroscopy"
                },
                "summary": "We use Magellan/MIKE echelle spectroscopy to conduct an in-depth\nchemodynamical analysis of the most luminous star-forming region within the\nmetal-poor starburst dwarf galaxy CGCG 007-025. Leveraging the exceptional high\nresolution (R$\\sim$50,000) and broad wavelength coverage, we apply Bayesian\ninference to simultaneously model the fluxes of 30 emission lines spanning the\nwavelength range 3400-9200\\AA. Employing a two-region ionisation model, we\ncharacterise various gas properties including electron temperature, electron\ndensity, and chemical abundances across different elements. Our direct-method\ninferred metallicity yields $\\rm 12+\\log(O/H)=7.77\\pm0.03$, placing the galaxy\nin the metal-poor regime. Furthermore, Metal-to-Oxygen ratios such as log(S/O),\nlog(Ne/O) or log(Ar/O) are in full agreement with the values derived for the\nMilky Way, consistent with expectations from stellar evolutionary models. The\nbrightest emission lines are kinematically complex, with modelling requiring up\nto four distinct components. The exceptional resolution and signal-to-noise\nratio of the data unveil asymmetric and wide ($\\sigma_{HeII} \\approx$ 35km/s)\nHeII$\\lambda$4686 emission. The flux ratio of this nebular line, together with\nthe absence of other high ionisation species such as [NeV]$\\lambda$3426,\nindicates the presence of fast radiative shocks. This dataset underscores the\ncapability of echelle spectroscopy in delivering comprehensive chemodynamical\nanalyses of starbursts in the Local Volume.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use Magellan/MIKE echelle spectroscopy to conduct an in-depth\nchemodynamical analysis of the most luminous star-forming region within the\nmetal-poor starburst dwarf galaxy CGCG 007-025. Leveraging the exceptional high\nresolution (R$\\sim$50,000) and broad wavelength coverage, we apply Bayesian\ninference to simultaneously model the fluxes of 30 emission lines spanning the\nwavelength range 3400-9200\\AA. Employing a two-region ionisation model, we\ncharacterise various gas properties including electron temperature, electron\ndensity, and chemical abundances across different elements. Our direct-method\ninferred metallicity yields $\\rm 12+\\log(O/H)=7.77\\pm0.03$, placing the galaxy\nin the metal-poor regime. Furthermore, Metal-to-Oxygen ratios such as log(S/O),\nlog(Ne/O) or log(Ar/O) are in full agreement with the values derived for the\nMilky Way, consistent with expectations from stellar evolutionary models. The\nbrightest emission lines are kinematically complex, with modelling requiring up\nto four distinct components. The exceptional resolution and signal-to-noise\nratio of the data unveil asymmetric and wide ($\\sigma_{HeII} \\approx$ 35km/s)\nHeII$\\lambda$4686 emission. The flux ratio of this nebular line, together with\nthe absence of other high ionisation species such as [NeV]$\\lambda$3426,\nindicates the presence of fast radiative shocks. This dataset underscores the\ncapability of echelle spectroscopy in delivering comprehensive chemodynamical\nanalyses of starbursts in the Local Volume."
                },
                "authors": [
                    {
                        "name": "Macarena G. del Valle-Espinosa"
                    },
                    {
                        "name": "Vital Fern√°ndez"
                    },
                    {
                        "name": "Rub√©n S√°nchez-Janssen"
                    },
                    {
                        "name": "Ricardo Amor√≠n"
                    },
                    {
                        "name": "Karla Z. Arrellano-C√≥rdova"
                    },
                    {
                        "name": "Konstantina Boutsia"
                    }
                ],
                "author_detail": {
                    "name": "Konstantina Boutsia"
                },
                "author": "Konstantina Boutsia",
                "arxiv_doi": "10.1093/mnras/staf1367",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1367",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 8 images, 5 tables, published in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07995v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07995v3",
                "updated": "2025-08-25T16:06:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    6,
                    32,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-11T13:57:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    57,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval"
                },
                "summary": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent DIVER, a retrieval pipeline designed for reasoning-intensive\ninformation retrieval. It consists of four components. The document\npreprocessing stage enhances readability and preserves content by cleaning\nnoisy texts and segmenting long documents. The query expansion stage leverages\nlarge language models to iteratively refine user queries with explicit\nreasoning and evidence from retrieved documents. The retrieval stage employs a\nmodel fine-tuned on synthetic data spanning medical and mathematical domains,\nalong with hard negatives, enabling effective handling of reasoning-intensive\nqueries. Finally, the reranking stage combines pointwise and listwise\nstrategies to produce both fine-grained and globally consistent rankings. On\nthe BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 45.8\noverall and 28.9 on original queries, consistently outperforming competitive\nreasoning-aware models. These results demonstrate the effectiveness of\nreasoning-aware retrieval strategies in complex real-world tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent DIVER, a retrieval pipeline designed for reasoning-intensive\ninformation retrieval. It consists of four components. The document\npreprocessing stage enhances readability and preserves content by cleaning\nnoisy texts and segmenting long documents. The query expansion stage leverages\nlarge language models to iteratively refine user queries with explicit\nreasoning and evidence from retrieved documents. The retrieval stage employs a\nmodel fine-tuned on synthetic data spanning medical and mathematical domains,\nalong with hard negatives, enabling effective handling of reasoning-intensive\nqueries. Finally, the reranking stage combines pointwise and listwise\nstrategies to produce both fine-grained and globally consistent rankings. On\nthe BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 45.8\noverall and 28.9 on original queries, consistently outperforming competitive\nreasoning-aware models. These results demonstrate the effectiveness of\nreasoning-aware retrieval strategies in complex real-world tasks."
                },
                "authors": [
                    {
                        "name": "Meixiu Long"
                    },
                    {
                        "name": "Duolin Sun"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07995v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07995v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08985v2",
                "updated": "2025-08-25T16:02:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    2,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-12T14:53:54Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    53,
                    54,
                    1,
                    224,
                    0
                ],
                "title": "Low-Regret and Low-Complexity Learning for Hierarchical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Regret and Low-Complexity Learning for Hierarchical Inference"
                },
                "summary": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods."
                },
                "authors": [
                    {
                        "name": "Sameep Chattopadhyay"
                    },
                    {
                        "name": "Vinay Sutar"
                    },
                    {
                        "name": "Jaya Prakash Champati"
                    },
                    {
                        "name": "Sharayu Moharir"
                    }
                ],
                "author_detail": {
                    "name": "Sharayu Moharir"
                },
                "author": "Sharayu Moharir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18148v1",
                "updated": "2025-08-25T15:55:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    55,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:55:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    55,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Few Samples: A Novel Approach for High-Quality Malcode\n  Generation"
                },
                "summary": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats."
                },
                "authors": [
                    {
                        "name": "Haijian Ma"
                    },
                    {
                        "name": "Daizong Liu"
                    },
                    {
                        "name": "Xiaowen Cai"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Yulai Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yulai Xie"
                },
                "author": "Yulai Xie",
                "arxiv_comment": "18pages,5 figures,emnlp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18142v1",
                "updated": "2025-08-25T15:51:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    51,
                    24,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:51:24Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    51,
                    24,
                    0,
                    237,
                    0
                ],
                "title": "Mirroring Users: Towards Building Preference-aligned User Simulator with\n  User Feedback in Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mirroring Users: Towards Building Preference-aligned User Simulator with\n  User Feedback in Recommendation"
                },
                "summary": "User simulation is increasingly vital to develop and evaluate recommender\nsystems (RSs). While Large Language Models (LLMs) offer promising avenues to\nsimulate user behavior, they often struggle with the absence of specific domain\nalignment required for RSs and the efficiency demands of large-scale\nsimulation. A vast yet underutilized resource for enhancing this alignment is\nthe extensive user feedback inherent in RSs. However, directly leveraging such\nfeedback presents two significant challenges. First, user feedback in RSs is\noften ambiguous and noisy, which negatively impacts effective preference\nalignment. Second, the massive volume of feedback largely hinders the\nefficiency of preference alignment, necessitating an efficient filtering\nmechanism to identify more informative samples. To overcome these hurdles, we\nintroduce a novel data construction framework that leverages user feedback in\nRSs with advanced LLM capabilities to generate high-quality simulation data.\nOur framework unfolds in two key phases: (1) employing LLMs to generate\ncognitive decision-making processes on constructed simulation samples, reducing\nambiguity in raw user feedback; (2) data distillation based on uncertainty\nestimation and behavior sampling to filter challenging yet denoised simulation\nsamples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using\nsuch high-quality dataset with corresponding decision-making processes.\nExtensive experiments verify that our framework significantly boosts the\nalignment with human preferences and in-domain reasoning capabilities of\nfine-tuned LLMs, and provides more insightful and interpretable signals when\ninteracting with RSs. We believe our work will advance the RS community and\noffer valuable insights for broader human-centric AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User simulation is increasingly vital to develop and evaluate recommender\nsystems (RSs). While Large Language Models (LLMs) offer promising avenues to\nsimulate user behavior, they often struggle with the absence of specific domain\nalignment required for RSs and the efficiency demands of large-scale\nsimulation. A vast yet underutilized resource for enhancing this alignment is\nthe extensive user feedback inherent in RSs. However, directly leveraging such\nfeedback presents two significant challenges. First, user feedback in RSs is\noften ambiguous and noisy, which negatively impacts effective preference\nalignment. Second, the massive volume of feedback largely hinders the\nefficiency of preference alignment, necessitating an efficient filtering\nmechanism to identify more informative samples. To overcome these hurdles, we\nintroduce a novel data construction framework that leverages user feedback in\nRSs with advanced LLM capabilities to generate high-quality simulation data.\nOur framework unfolds in two key phases: (1) employing LLMs to generate\ncognitive decision-making processes on constructed simulation samples, reducing\nambiguity in raw user feedback; (2) data distillation based on uncertainty\nestimation and behavior sampling to filter challenging yet denoised simulation\nsamples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using\nsuch high-quality dataset with corresponding decision-making processes.\nExtensive experiments verify that our framework significantly boosts the\nalignment with human preferences and in-domain reasoning capabilities of\nfine-tuned LLMs, and provides more insightful and interpretable signals when\ninteracting with RSs. We believe our work will advance the RS community and\noffer valuable insights for broader human-centric AI research."
                },
                "authors": [
                    {
                        "name": "Tianjun Wei"
                    },
                    {
                        "name": "Huizhong Guo"
                    },
                    {
                        "name": "Yingpeng Du"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Dongxia Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "arxiv_comment": "Github: https://github.com/UserMirrorer/UserMirrorer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19134v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19134v4",
                "updated": "2025-08-25T15:50:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    50,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-09-27T20:32:42Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    20,
                    32,
                    42,
                    4,
                    271,
                    0
                ],
                "title": "Confidential Prompting: Privacy-preserving LLM Inference on Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Prompting: Privacy-preserving LLM Inference on Cloud"
                },
                "summary": "This paper introduces a vision of confidential prompting: securing user\nprompts from untrusted, cloud-hosted large language model (LLM) provider while\npreserving model confidentiality, output invariance, and compute efficiency. As\na first step toward this vision, we present Obfuscated Secure Partitioned\nDecoding (OSPD), a system built on two key innovations. First, Secure\nPartitioned Decoding (SPD) isolates user prompts within per-user processes\nresiding in a confidential virtual machine (CVM) on the cloud, which are\ninaccessible for the cloud LLM while allowing it to generate tokens\nefficiently. Second, Prompt Obfuscation (PO) introduces a novel cryptographic\ntechnique that enhances SPD resilience against advanced prompt reconstruction\nattacks. Together, these innovations ensure OSPD protects both prompt and model\nconfidentiality while maintaining service functionality. OSPD enables\npractical, privacy-preserving cloud-hosted LLM inference for sensitive\napplications, such as processing personal data, clinical records, and financial\ndocuments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a vision of confidential prompting: securing user\nprompts from untrusted, cloud-hosted large language model (LLM) provider while\npreserving model confidentiality, output invariance, and compute efficiency. As\na first step toward this vision, we present Obfuscated Secure Partitioned\nDecoding (OSPD), a system built on two key innovations. First, Secure\nPartitioned Decoding (SPD) isolates user prompts within per-user processes\nresiding in a confidential virtual machine (CVM) on the cloud, which are\ninaccessible for the cloud LLM while allowing it to generate tokens\nefficiently. Second, Prompt Obfuscation (PO) introduces a novel cryptographic\ntechnique that enhances SPD resilience against advanced prompt reconstruction\nattacks. Together, these innovations ensure OSPD protects both prompt and model\nconfidentiality while maintaining service functionality. OSPD enables\npractical, privacy-preserving cloud-hosted LLM inference for sensitive\napplications, such as processing personal data, clinical records, and financial\ndocuments."
                },
                "authors": [
                    {
                        "name": "Caihua Li"
                    },
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19134v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19134v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00187v2",
                "updated": "2025-08-25T15:49:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    49,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-28T21:10:03Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    21,
                    10,
                    3,
                    4,
                    59,
                    0
                ],
                "title": "Steering Dialogue Dynamics for Robustness against Multi-turn\n  Jailbreaking Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Dialogue Dynamics for Robustness against Multi-turn\n  Jailbreaking Attacks"
                },
                "summary": "Large language models (LLMs) are shown to be vulnerable to jailbreaking\nattacks where adversarial prompts are designed to elicit harmful responses.\nWhile existing defenses effectively mitigate single-turn attacks by detecting\nand filtering unsafe inputs, they fail against multi-turn jailbreaks that\nexploit contextual drift over multiple interactions, gradually leading LLMs\naway from safe behavior. To address this challenge, we propose a safety\nsteering framework grounded in safe control theory, ensuring invariant safety\nin multi-turn dialogues. Our approach models the dialogue with LLMs using\nstate-space representations and introduces a novel neural barrier function\n(NBF) to detect and filter harmful queries emerging from evolving contexts\nproactively. Our method achieves invariant safety at each turn of dialogue by\nlearning a safety predictor that accounts for adversarial queries, preventing\npotential context drift toward jailbreaks. Extensive experiments under multiple\nLLMs show that our NBF-based safety steering outperforms safety alignment,\nprompt-based steering and lightweight LLM guardrails baselines, offering\nstronger defenses against multi-turn jailbreaks while maintaining a better\ntrade-off among safety, helpfulness and over-refusal. Check out the website\nhere https://sites.google.com/view/llm-nbf/home . Our code is available on\nhttps://github.com/HanjiangHu/NBF-LLM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are shown to be vulnerable to jailbreaking\nattacks where adversarial prompts are designed to elicit harmful responses.\nWhile existing defenses effectively mitigate single-turn attacks by detecting\nand filtering unsafe inputs, they fail against multi-turn jailbreaks that\nexploit contextual drift over multiple interactions, gradually leading LLMs\naway from safe behavior. To address this challenge, we propose a safety\nsteering framework grounded in safe control theory, ensuring invariant safety\nin multi-turn dialogues. Our approach models the dialogue with LLMs using\nstate-space representations and introduces a novel neural barrier function\n(NBF) to detect and filter harmful queries emerging from evolving contexts\nproactively. Our method achieves invariant safety at each turn of dialogue by\nlearning a safety predictor that accounts for adversarial queries, preventing\npotential context drift toward jailbreaks. Extensive experiments under multiple\nLLMs show that our NBF-based safety steering outperforms safety alignment,\nprompt-based steering and lightweight LLM guardrails baselines, offering\nstronger defenses against multi-turn jailbreaks while maintaining a better\ntrade-off among safety, helpfulness and over-refusal. Check out the website\nhere https://sites.google.com/view/llm-nbf/home . Our code is available on\nhttps://github.com/HanjiangHu/NBF-LLM ."
                },
                "authors": [
                    {
                        "name": "Hanjiang Hu"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "arxiv_comment": "23 pages, 10 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18137v2",
                "updated": "2025-08-26T13:00:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    0,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T15:41:40Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    41,
                    40,
                    0,
                    237,
                    0
                ],
                "title": "Estimating the average treatment effect in cluster-randomized trials\n  with misclassified outcomes and non-random validation subsets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the average treatment effect in cluster-randomized trials\n  with misclassified outcomes and non-random validation subsets"
                },
                "summary": "Randomized trials are viewed as the benchmark for assessing causal effects of\ntreatments on outcomes of interest. Nonetheless, challenges such as measurement\nerror can undermine the standard causal assumptions for randomized trials. In\nASPIRE, a cluster-randomized trial, pediatric primary care clinics were\nassigned to one of two treatments aimed at promoting clinician delivery of a\nsecure firearm program to parents during well-child visits. A key outcome of\ninterest is thus parent receipt of the program at each visit. Clinicians\ndocumented program delivery in patients' electronic health records for all\nvisits, but their reporting is a proxy measure for the parent receipt outcome.\nParents were also surveyed to report directly on program receipt after their\nchild's visit; however, only a small subset of them completed the survey. Here,\nwe develop a causal inference framework for a binary outcome that is subject to\nmisclassification through silver-standard measures (clinician reports), but\ngold-standard measures (parent reports) are only available for a non-random\ninternal validation subset. We propose a method for identifying the average\ntreatment effect (ATE) that addresses the risk of bias due to misclassification\nand non-random validation selection, even when the outcome (parent receipt) may\ndirectly impact selection propensity (survey responsiveness). We show that ATE\nestimation relies on specifying the relationship between the gold- and\nsilver-standard outcome measures in the validation subset, which may depend on\ntreatment and covariates. Additionally, the clustered design is reflected in\nour causal assumptions and in our cluster-robust approach to estimation of the\nATE. Simulation studies demonstrate acceptable finite-sample operating\ncharacteristics of our ATE estimator, supporting its application to ASPIRE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized trials are viewed as the benchmark for assessing causal effects of\ntreatments on outcomes of interest. Nonetheless, challenges such as measurement\nerror can undermine the standard causal assumptions for randomized trials. In\nASPIRE, a cluster-randomized trial, pediatric primary care clinics were\nassigned to one of two treatments aimed at promoting clinician delivery of a\nsecure firearm program to parents during well-child visits. A key outcome of\ninterest is thus parent receipt of the program at each visit. Clinicians\ndocumented program delivery in patients' electronic health records for all\nvisits, but their reporting is a proxy measure for the parent receipt outcome.\nParents were also surveyed to report directly on program receipt after their\nchild's visit; however, only a small subset of them completed the survey. Here,\nwe develop a causal inference framework for a binary outcome that is subject to\nmisclassification through silver-standard measures (clinician reports), but\ngold-standard measures (parent reports) are only available for a non-random\ninternal validation subset. We propose a method for identifying the average\ntreatment effect (ATE) that addresses the risk of bias due to misclassification\nand non-random validation selection, even when the outcome (parent receipt) may\ndirectly impact selection propensity (survey responsiveness). We show that ATE\nestimation relies on specifying the relationship between the gold- and\nsilver-standard outcome measures in the validation subset, which may depend on\ntreatment and covariates. Additionally, the clustered design is reflected in\nour causal assumptions and in our cluster-robust approach to estimation of the\nATE. Simulation studies demonstrate acceptable finite-sample operating\ncharacteristics of our ATE estimator, supporting its application to ASPIRE."
                },
                "authors": [
                    {
                        "name": "Dane Isenberg"
                    },
                    {
                        "name": "Nandita Mitra"
                    },
                    {
                        "name": "Steven C. Marcus"
                    },
                    {
                        "name": "Rinad S. Beidas"
                    },
                    {
                        "name": "Kristin A. Linn"
                    }
                ],
                "author_detail": {
                    "name": "Kristin A. Linn"
                },
                "author": "Kristin A. Linn",
                "arxiv_comment": "corrected very minor typos in tables/figs and one small change to\n  abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18132v1",
                "updated": "2025-08-25T15:38:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    38,
                    56,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:38:56Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    38,
                    56,
                    0,
                    237,
                    0
                ],
                "title": "Test-Time Scaling Strategies for Generative Retrieval in Multimodal\n  Conversational Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling Strategies for Generative Retrieval in Multimodal\n  Conversational Recommendations"
                },
                "summary": "The rapid evolution of e-commerce has exposed the limitations of traditional\nproduct retrieval systems in managing complex, multi-turn user interactions.\nRecent advances in multimodal generative retrieval -- particularly those\nleveraging multimodal large language models (MLLMs) as retrievers -- have shown\npromise. However, most existing methods are tailored to single-turn scenarios\nand struggle to model the evolving intent and iterative nature of multi-turn\ndialogues when applied naively. Concurrently, test-time scaling has emerged as\na powerful paradigm for improving large language model (LLM) performance\nthrough iterative inference-time refinement. Yet, its effectiveness typically\nrelies on two conditions: (1) a well-defined problem space (e.g., mathematical\nreasoning), and (2) the model's ability to self-correct -- conditions that are\nrarely met in conversational product search. In this setting, user queries are\noften ambiguous and evolving, and MLLMs alone have difficulty grounding\nresponses in a fixed product corpus. Motivated by these challenges, we propose\na novel framework that introduces test-time scaling into conversational\nmultimodal product retrieval. Our approach builds on a generative retriever,\nfurther augmented with a test-time reranking (TTR) mechanism that improves\nretrieval accuracy and better aligns results with evolving user intent\nthroughout the dialogue. Experiments across multiple benchmarks show consistent\nimprovements, with average gains of 14.5 points in MRR and 10.6 points in\nnDCG@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of e-commerce has exposed the limitations of traditional\nproduct retrieval systems in managing complex, multi-turn user interactions.\nRecent advances in multimodal generative retrieval -- particularly those\nleveraging multimodal large language models (MLLMs) as retrievers -- have shown\npromise. However, most existing methods are tailored to single-turn scenarios\nand struggle to model the evolving intent and iterative nature of multi-turn\ndialogues when applied naively. Concurrently, test-time scaling has emerged as\na powerful paradigm for improving large language model (LLM) performance\nthrough iterative inference-time refinement. Yet, its effectiveness typically\nrelies on two conditions: (1) a well-defined problem space (e.g., mathematical\nreasoning), and (2) the model's ability to self-correct -- conditions that are\nrarely met in conversational product search. In this setting, user queries are\noften ambiguous and evolving, and MLLMs alone have difficulty grounding\nresponses in a fixed product corpus. Motivated by these challenges, we propose\na novel framework that introduces test-time scaling into conversational\nmultimodal product retrieval. Our approach builds on a generative retriever,\nfurther augmented with a test-time reranking (TTR) mechanism that improves\nretrieval accuracy and better aligns results with evolving user intent\nthroughout the dialogue. Experiments across multiple benchmarks show consistent\nimprovements, with average gains of 14.5 points in MRR and 10.6 points in\nnDCG@1."
                },
                "authors": [
                    {
                        "name": "Hung-Chun Hsu"
                    },
                    {
                        "name": "Yuan-Ching Kuo"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Ming-Feng Tsai"
                    },
                    {
                        "name": "Chuan-Ju Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chuan-Ju Wang"
                },
                "author": "Chuan-Ju Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18130v1",
                "updated": "2025-08-25T15:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    38,
                    23,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:38:23Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    38,
                    23,
                    0,
                    237,
                    0
                ],
                "title": "Frozen in Time: Parameter-Efficient Time Series Transformers via\n  Reservoir-Induced Feature Expansion and Fixed Random Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frozen in Time: Parameter-Efficient Time Series Transformers via\n  Reservoir-Induced Feature Expansion and Fixed Random Dynamics"
                },
                "summary": "Transformers are the de-facto choice for sequence modelling, yet their\nquadratic self-attention and weak temporal bias can make long-range forecasting\nboth expensive and brittle. We introduce FreezeTST, a lightweight hybrid that\ninterleaves frozen random-feature (reservoir) blocks with standard trainable\nTransformer layers. The frozen blocks endow the network with rich nonlinear\nmemory at no optimisation cost; the trainable layers learn to query this memory\nthrough self-attention. The design cuts trainable parameters and also lowers\nwall-clock training time, while leaving inference complexity unchanged. On\nseven standard long-term forecasting benchmarks, FreezeTST consistently matches\nor surpasses specialised variants such as Informer, Autoformer, and PatchTST;\nwith substantially lower compute. Our results show that embedding reservoir\nprinciples within Transformers offers a simple, principled route to efficient\nlong-term time-series prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are the de-facto choice for sequence modelling, yet their\nquadratic self-attention and weak temporal bias can make long-range forecasting\nboth expensive and brittle. We introduce FreezeTST, a lightweight hybrid that\ninterleaves frozen random-feature (reservoir) blocks with standard trainable\nTransformer layers. The frozen blocks endow the network with rich nonlinear\nmemory at no optimisation cost; the trainable layers learn to query this memory\nthrough self-attention. The design cuts trainable parameters and also lowers\nwall-clock training time, while leaving inference complexity unchanged. On\nseven standard long-term forecasting benchmarks, FreezeTST consistently matches\nor surpasses specialised variants such as Informer, Autoformer, and PatchTST;\nwith substantially lower compute. Our results show that embedding reservoir\nprinciples within Transformers offers a simple, principled route to efficient\nlong-term time-series prediction."
                },
                "authors": [
                    {
                        "name": "Pradeep Singh"
                    },
                    {
                        "name": "Mehak Sharma"
                    },
                    {
                        "name": "Anupriya Dey"
                    },
                    {
                        "name": "Balasubramanian Raman"
                    }
                ],
                "author_detail": {
                    "name": "Balasubramanian Raman"
                },
                "author": "Balasubramanian Raman",
                "arxiv_comment": "8 pages, 5 tables, 3 figures, accepted at ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01710v2",
                "updated": "2025-08-25T15:35:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    35,
                    12,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-03T10:35:05Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    35,
                    5,
                    6,
                    215,
                    0
                ],
                "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for\n  Multilingual Safety Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for\n  Multilingual Safety Applications"
                },
                "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models."
                },
                "authors": [
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Wartikar"
                },
                "author": "Niranjan Wartikar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18126v1",
                "updated": "2025-08-25T15:34:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    34,
                    10,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:34:10Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    34,
                    10,
                    0,
                    237,
                    0
                ],
                "title": "Inferring Mbh-Mbulge Evolution from the Gravitational Wave Background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Mbh-Mbulge Evolution from the Gravitational Wave Background"
                },
                "summary": "We test the impact of an evolving supermassive black hole (SMBH) mass scaling\nrelation (Mbh-Mbulge) on the predictions for the gravitational wave background\n(GWB). The observed GWB amplitude is 2-3 times higher than predicted by\nastrophysically informed models which suggests the need to revise the\nassumptions in those models. We compare a semi-analytic model's ability to\nreproduce the observed GWB spectrum with a static versus evolving-amplitude\nMbh-Mbulge relation. We additionally consider the influence of the choice of\ngalaxy stellar mass function on the modeled GWB spectra. Our models are able to\nreproduce the GWB amplitude with either a large number density of massive\ngalaxies or a positively evolving Mbh-Mbulge amplitude (i.e., the Mbh / Mbulge\nratio was higher in the past). If we assume that the Mbh-Mbulge amplitude does\nnot evolve, our models require a galaxy stellar mass function that implies an\nundetected population of massive galaxies (Mstellar > 10^11 Msun at z > 1).\nWhen the Mbh-Mbulge amplitude is allowed to evolve, we can model the GWB\nspectrum with all fiducial values and an Mbh-Mbulge amplitude that evolves as\nalpha(z) = alpha_0 (1 + z)^(1.04 +/- 0.5).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We test the impact of an evolving supermassive black hole (SMBH) mass scaling\nrelation (Mbh-Mbulge) on the predictions for the gravitational wave background\n(GWB). The observed GWB amplitude is 2-3 times higher than predicted by\nastrophysically informed models which suggests the need to revise the\nassumptions in those models. We compare a semi-analytic model's ability to\nreproduce the observed GWB spectrum with a static versus evolving-amplitude\nMbh-Mbulge relation. We additionally consider the influence of the choice of\ngalaxy stellar mass function on the modeled GWB spectra. Our models are able to\nreproduce the GWB amplitude with either a large number density of massive\ngalaxies or a positively evolving Mbh-Mbulge amplitude (i.e., the Mbh / Mbulge\nratio was higher in the past). If we assume that the Mbh-Mbulge amplitude does\nnot evolve, our models require a galaxy stellar mass function that implies an\nundetected population of massive galaxies (Mstellar > 10^11 Msun at z > 1).\nWhen the Mbh-Mbulge amplitude is allowed to evolve, we can model the GWB\nspectrum with all fiducial values and an Mbh-Mbulge amplitude that evolves as\nalpha(z) = alpha_0 (1 + z)^(1.04 +/- 0.5)."
                },
                "authors": [
                    {
                        "name": "Cayenne Matt"
                    },
                    {
                        "name": "Kayhan Gultekin"
                    },
                    {
                        "name": "Luke Kelley"
                    },
                    {
                        "name": "Laura Blecha"
                    },
                    {
                        "name": "Joseph Simon"
                    },
                    {
                        "name": "Gabriella Agazie"
                    },
                    {
                        "name": "Akash Anumarlapudi"
                    },
                    {
                        "name": "Anne Archibald"
                    },
                    {
                        "name": "Zaven Arzoumanian"
                    },
                    {
                        "name": "Jeremy Baier"
                    },
                    {
                        "name": "Paul Baker"
                    },
                    {
                        "name": "Bence B√©csy"
                    },
                    {
                        "name": "Adam Brazier"
                    },
                    {
                        "name": "Paul Brook"
                    },
                    {
                        "name": "Sarah Burke-Spolaor"
                    },
                    {
                        "name": "Rand Burnette"
                    },
                    {
                        "name": "Robin Case"
                    },
                    {
                        "name": "James Casey-Clyde"
                    },
                    {
                        "name": "Maria Charisi"
                    },
                    {
                        "name": "Shami Chatterjee"
                    },
                    {
                        "name": "Tyler Cohen"
                    },
                    {
                        "name": "James Cordes"
                    },
                    {
                        "name": "Neil Cornish"
                    },
                    {
                        "name": "Fronefield Crawford"
                    },
                    {
                        "name": "H. Thankful Cromartie"
                    },
                    {
                        "name": "Kathryn Crowter"
                    },
                    {
                        "name": "Megan DeCesar"
                    },
                    {
                        "name": "Paul Demorest"
                    },
                    {
                        "name": "Heling Deng"
                    },
                    {
                        "name": "Lankeswar Dey"
                    },
                    {
                        "name": "Timothy Dolch"
                    },
                    {
                        "name": "Elizabeth Ferrara"
                    },
                    {
                        "name": "William Fiore"
                    },
                    {
                        "name": "Emmanuel Fonseca"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Emiko Gardiner"
                    },
                    {
                        "name": "Nate Garver-Daniels"
                    },
                    {
                        "name": "Peter Gentile"
                    },
                    {
                        "name": "Kyle Gersbach"
                    },
                    {
                        "name": "Joseph Glaser"
                    },
                    {
                        "name": "Deborah Good"
                    },
                    {
                        "name": "C. Harris"
                    },
                    {
                        "name": "Jeffrey Hazboun"
                    },
                    {
                        "name": "Ross Jennings"
                    },
                    {
                        "name": "Aaron Johnson"
                    },
                    {
                        "name": "Megan Jones"
                    },
                    {
                        "name": "David Kaplan"
                    },
                    {
                        "name": "Matthew Kerr"
                    },
                    {
                        "name": "Joey Key"
                    },
                    {
                        "name": "Nima Laal"
                    },
                    {
                        "name": "Michael Lam"
                    },
                    {
                        "name": "William Lamb"
                    },
                    {
                        "name": "Bjorn Larsen"
                    },
                    {
                        "name": "T. Lazio"
                    },
                    {
                        "name": "Natalia Lewandowska"
                    },
                    {
                        "name": "Tingting Liu"
                    },
                    {
                        "name": "Duncan Lorimer"
                    },
                    {
                        "name": "Jing Luo"
                    },
                    {
                        "name": "Ryan Lynch"
                    },
                    {
                        "name": "Chung-Pei Ma"
                    },
                    {
                        "name": "Dustin Madison"
                    },
                    {
                        "name": "Alexander McEwen"
                    },
                    {
                        "name": "James McKee"
                    },
                    {
                        "name": "Maura McLaughlin"
                    },
                    {
                        "name": "Natasha McMann"
                    },
                    {
                        "name": "Bradley Meyers"
                    },
                    {
                        "name": "Patrick Meyers"
                    },
                    {
                        "name": "Chiara Mingarelli"
                    },
                    {
                        "name": "Andrea Mitridate"
                    },
                    {
                        "name": "Cherry Ng"
                    },
                    {
                        "name": "David Nice"
                    },
                    {
                        "name": "Stella Ocker"
                    },
                    {
                        "name": "Ken Olum"
                    },
                    {
                        "name": "Timothy Pennucci"
                    },
                    {
                        "name": "Benetge Perera"
                    },
                    {
                        "name": "Polina Petrov"
                    },
                    {
                        "name": "Nihan Pol"
                    },
                    {
                        "name": "Henri Radovan"
                    },
                    {
                        "name": "Scott Ransom"
                    },
                    {
                        "name": "Paul Ray"
                    },
                    {
                        "name": "Joseph Romano"
                    },
                    {
                        "name": "Jessie Runnoe"
                    },
                    {
                        "name": "Alexander Saffer"
                    },
                    {
                        "name": "Shashwat Sardesai"
                    },
                    {
                        "name": "A. Schmiedekamp"
                    },
                    {
                        "name": "Carl Schmiedekamp"
                    },
                    {
                        "name": "Kai Schmitz"
                    },
                    {
                        "name": "Brent Shapiro-Albert"
                    },
                    {
                        "name": "Xavier Siemens"
                    },
                    {
                        "name": "Sophia Sosa Fiscella"
                    },
                    {
                        "name": "Ingrid Stairs"
                    },
                    {
                        "name": "Daniel Stinebring"
                    },
                    {
                        "name": "Kevin Stovall"
                    },
                    {
                        "name": "Abhimanyu Susobhanan"
                    },
                    {
                        "name": "Joseph Swiggum"
                    },
                    {
                        "name": "Jacob Taylor"
                    },
                    {
                        "name": "Stephen Taylor"
                    },
                    {
                        "name": "Mercedes Thompson"
                    },
                    {
                        "name": "Jacob Turner"
                    },
                    {
                        "name": "Michele Vallisneri"
                    },
                    {
                        "name": "Rutger van Haasteren"
                    },
                    {
                        "name": "Sarah Vigeland"
                    },
                    {
                        "name": "Haley Wahl"
                    },
                    {
                        "name": "Kevin Wilson"
                    },
                    {
                        "name": "Caitlin Witt"
                    },
                    {
                        "name": "David Wright"
                    },
                    {
                        "name": "Olivia Young"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Young"
                },
                "author": "Olivia Young",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18124v2",
                "updated": "2025-08-26T04:21:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    4,
                    21,
                    37,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T15:32:22Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    32,
                    22,
                    0,
                    237,
                    0
                ],
                "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics"
                },
                "summary": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench."
                },
                "authors": [
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Dongchen Huang"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Tengchao Yang"
                    },
                    {
                        "name": "Ziyang Zheng"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dong Han"
                    },
                    {
                        "name": "Benteng Chen"
                    },
                    {
                        "name": "Binzhao Luo"
                    },
                    {
                        "name": "Zhiyu Liu"
                    },
                    {
                        "name": "Kunling Liu"
                    },
                    {
                        "name": "Zhiyuan Gao"
                    },
                    {
                        "name": "Shiqi Geng"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Jiaming Su"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Shuchen Pu"
                    },
                    {
                        "name": "Yuhan Shui"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Dongfei Cui"
                    },
                    {
                        "name": "Changyong He"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Zeke Xie"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yunqi Cai"
                    },
                    {
                        "name": "Xi Dai"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jinguang Cheng"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    }
                ],
                "author_detail": {
                    "name": "Hongming Weng"
                },
                "author": "Hongming Weng",
                "arxiv_comment": "29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23163v2",
                "updated": "2025-08-25T15:30:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    30,
                    19,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-30T23:58:37Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    23,
                    58,
                    37,
                    2,
                    211,
                    0
                ],
                "title": "Argumentatively Coherent Judgmental Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argumentatively Coherent Judgmental Forecasting"
                },
                "summary": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions."
                },
                "authors": [
                    {
                        "name": "Deniz Gorur"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "17 pages, 18 figures, ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18122v1",
                "updated": "2025-08-25T15:30:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    30,
                    12,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:30:12Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    30,
                    12,
                    0,
                    237,
                    0
                ],
                "title": "Provable Mixed-Noise Learning with Flow-Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Mixed-Noise Learning with Flow-Matching"
                },
                "summary": "We study Bayesian inverse problems with mixed noise, modeled as a combination\nof additive and multiplicative Gaussian components. While traditional inference\nmethods often assume fixed or known noise characteristics, real-world\napplications, particularly in physics and chemistry, frequently involve noise\nwith unknown and heterogeneous structure. Motivated by recent advances in\nflow-based generative modeling, we propose a novel inference framework based on\nconditional flow matching embedded within an Expectation-Maximization (EM)\nalgorithm to jointly estimate posterior samplers and noise parameters. To\nenable high-dimensional inference and improve scalability, we use\nsimulation-free ODE-based flow matching as the generative model in the E-step\nof the EM algorithm. We prove that, under suitable assumptions, the EM updates\nconverge to the true noise parameters in the population limit of infinite\nobservations. Our numerical results illustrate the effectiveness of combining\nEM inference with flow matching for mixed-noise Bayesian inverse problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study Bayesian inverse problems with mixed noise, modeled as a combination\nof additive and multiplicative Gaussian components. While traditional inference\nmethods often assume fixed or known noise characteristics, real-world\napplications, particularly in physics and chemistry, frequently involve noise\nwith unknown and heterogeneous structure. Motivated by recent advances in\nflow-based generative modeling, we propose a novel inference framework based on\nconditional flow matching embedded within an Expectation-Maximization (EM)\nalgorithm to jointly estimate posterior samplers and noise parameters. To\nenable high-dimensional inference and improve scalability, we use\nsimulation-free ODE-based flow matching as the generative model in the E-step\nof the EM algorithm. We prove that, under suitable assumptions, the EM updates\nconverge to the true noise parameters in the population limit of infinite\nobservations. Our numerical results illustrate the effectiveness of combining\nEM inference with flow matching for mixed-noise Bayesian inverse problems."
                },
                "authors": [
                    {
                        "name": "Paul Hagemann"
                    },
                    {
                        "name": "Robert Gruhlke"
                    },
                    {
                        "name": "Bernhard Stankewitz"
                    },
                    {
                        "name": "Claudia Schillings"
                    },
                    {
                        "name": "Gabriele Steidl"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Steidl"
                },
                "author": "Gabriele Steidl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18118v1",
                "updated": "2025-08-25T15:23:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    23,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:23:21Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    23,
                    21,
                    0,
                    237,
                    0
                ],
                "title": "HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation"
                },
                "summary": "AI-generated content technologies are widely used in content creation.\nHowever, current AIGC systems rely heavily on creators' inspiration, rarely\ngenerating truly user-personalized content. In real-world applications such as\nonline advertising, a single product may have multiple selling points, with\ndifferent users focusing on different features. This underscores the\nsignificant value of personalized, user-centric creative generation. Effective\npersonalized content generation faces two main challenges: (1) accurately\nmodeling user interests and integrating them into the content generation\nprocess while adhering to factual constraints, and (2) ensuring high efficiency\nand scalability to handle the massive user base in industrial scenarios.\nAdditionally, the scarcity of personalized creative data in practice\ncomplicates model training, making data construction another key hurdle. We\npropose HLLM-Creator, a hierarchical LLM framework for efficient user interest\nmodeling and personalized content generation. During inference, a combination\nof user clustering and a user-ad-matching-prediction based pruning strategy is\nemployed to significantly enhance generation efficiency and reduce\ncomputational overhead, making the approach suitable for large-scale\ndeployment. Moreover, we design a data construction pipeline based on\nchain-of-thought reasoning, which generates high-quality, user-specific\ncreative titles and ensures factual consistency despite limited personalized\ndata. This pipeline serves as a critical foundation for the effectiveness of\nour model. Extensive experiments on personalized title generation for Douyin\nSearch Ads show the effectiveness of HLLM-Creator. Online A/B test shows a\n0.476% increase on Adss, paving the way for more effective and efficient\npersonalized generation in industrial scenarios. Codes for academic dataset are\navailable at https://github.com/bytedance/HLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-generated content technologies are widely used in content creation.\nHowever, current AIGC systems rely heavily on creators' inspiration, rarely\ngenerating truly user-personalized content. In real-world applications such as\nonline advertising, a single product may have multiple selling points, with\ndifferent users focusing on different features. This underscores the\nsignificant value of personalized, user-centric creative generation. Effective\npersonalized content generation faces two main challenges: (1) accurately\nmodeling user interests and integrating them into the content generation\nprocess while adhering to factual constraints, and (2) ensuring high efficiency\nand scalability to handle the massive user base in industrial scenarios.\nAdditionally, the scarcity of personalized creative data in practice\ncomplicates model training, making data construction another key hurdle. We\npropose HLLM-Creator, a hierarchical LLM framework for efficient user interest\nmodeling and personalized content generation. During inference, a combination\nof user clustering and a user-ad-matching-prediction based pruning strategy is\nemployed to significantly enhance generation efficiency and reduce\ncomputational overhead, making the approach suitable for large-scale\ndeployment. Moreover, we design a data construction pipeline based on\nchain-of-thought reasoning, which generates high-quality, user-specific\ncreative titles and ensures factual consistency despite limited personalized\ndata. This pipeline serves as a critical foundation for the effectiveness of\nour model. Extensive experiments on personalized title generation for Douyin\nSearch Ads show the effectiveness of HLLM-Creator. Online A/B test shows a\n0.476% increase on Adss, paving the way for more effective and efficient\npersonalized generation in industrial scenarios. Codes for academic dataset are\navailable at https://github.com/bytedance/HLLM."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Lu Chi"
                    },
                    {
                        "name": "Siliang Xu"
                    },
                    {
                        "name": "Shiwei Ran"
                    },
                    {
                        "name": "Bingyue Peng"
                    },
                    {
                        "name": "Zehuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zehuan Yuan"
                },
                "author": "Zehuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02531v2",
                "updated": "2025-08-25T15:23:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    23,
                    8,
                    0,
                    237,
                    0
                ],
                "published": "2025-01-05T13:18:13Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    18,
                    13,
                    6,
                    5,
                    0
                ],
                "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially\n  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially\n  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI"
                },
                "summary": "As general-purpose artificial intelligence systems become increasingly\nintegrated into society and are used for information seeking, content\ngeneration, problem solving, textual analysis, coding, and running processes,\nit is crucial to assess their long-term impact on humans. This research\nexplores the sentiment of large language models (LLMs) and humans toward\nartificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared with sentiment data from\nthree independent human sample populations. Temporal variations in sentiment\nwere also evaluated over three consecutive days. The results show a diversity\nin sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4\nrecorded the most positive sentiment toward AGI, while Bard leaned toward a\nneutral sentiment. In contrast, the human samples showed a lower average\nsentiment of 2.97. The analysis outlines potential conflicts of interest and\nbiases in the sentiment formation of LLMs, and indicates that LLMs could subtly\ninfluence societal perceptions. To address the need for regulatory oversight\nand culturally grounded assessments of AI systems, we introduce the Societal AI\nAlignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional\nprompts and empirically validated societal value frameworks to evaluate\nlanguage model outputs across temporal, model, and multilingual axes. This\nbenchmark is designed to guide policymakers and AI agencies, including within\nframeworks such as the EU AI Act, by providing robust, actionable insights into\nAI alignment with human values, public sentiment, and ethical norms at both\nnational and international levels. Future research should further refine the\noperationalization of the SAAS-AI benchmark and systematically evaluate its\neffectiveness through comprehensive empirical testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As general-purpose artificial intelligence systems become increasingly\nintegrated into society and are used for information seeking, content\ngeneration, problem solving, textual analysis, coding, and running processes,\nit is crucial to assess their long-term impact on humans. This research\nexplores the sentiment of large language models (LLMs) and humans toward\nartificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared with sentiment data from\nthree independent human sample populations. Temporal variations in sentiment\nwere also evaluated over three consecutive days. The results show a diversity\nin sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4\nrecorded the most positive sentiment toward AGI, while Bard leaned toward a\nneutral sentiment. In contrast, the human samples showed a lower average\nsentiment of 2.97. The analysis outlines potential conflicts of interest and\nbiases in the sentiment formation of LLMs, and indicates that LLMs could subtly\ninfluence societal perceptions. To address the need for regulatory oversight\nand culturally grounded assessments of AI systems, we introduce the Societal AI\nAlignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional\nprompts and empirically validated societal value frameworks to evaluate\nlanguage model outputs across temporal, model, and multilingual axes. This\nbenchmark is designed to guide policymakers and AI agencies, including within\nframeworks such as the EU AI Act, by providing robust, actionable insights into\nAI alignment with human values, public sentiment, and ethical norms at both\nnational and international levels. Future research should further refine the\noperationalization of the SAAS-AI benchmark and systematically evaluate its\neffectiveness through comprehensive empirical testing."
                },
                "authors": [
                    {
                        "name": "Ljubisa Bojic"
                    },
                    {
                        "name": "Dylan Seychell"
                    },
                    {
                        "name": "Milan Cabarkapa"
                    }
                ],
                "author_detail": {
                    "name": "Milan Cabarkapa"
                },
                "author": "Milan Cabarkapa",
                "arxiv_comment": "20 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18115v1",
                "updated": "2025-08-25T15:22:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    22,
                    42,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:22:42Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    22,
                    42,
                    0,
                    237,
                    0
                ],
                "title": "Compositional Verification in Concurrent Separation Logic with\n  Permissions Regions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Verification in Concurrent Separation Logic with\n  Permissions Regions"
                },
                "summary": "Concurrent separation logic with fractional permissions (CSLPerm) provides a\npromising reasoning system to verify most complex sequential and concurrent\nfine-grained programs. The logic with strong and weak separating conjunctions\noffers a solid foundation for producing concise and precise proofs. However, it\nlacks automation and compositionality support. This paper addresses this\nlimitation by introducing a compositional verification system for concurrent\nprograms that manipulate regions of shared memory. The centre of our system is\nnovel logical principles and an entailment procedure that can infer the\nresidual heaps in the frame rule for a fragment of CSL-Perm with explicit\narithmetical constraints for memory heaps' disjointness. This procedure enables\nthe compositional reasoning for concurrent threads and function calls. We have\nimplemented the proposal in a prototype tool called CoSl, tested it with 10\nchallenging concurrent programs, including those beyond the state-of-the-art,\nand confirmed the advantage of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent separation logic with fractional permissions (CSLPerm) provides a\npromising reasoning system to verify most complex sequential and concurrent\nfine-grained programs. The logic with strong and weak separating conjunctions\noffers a solid foundation for producing concise and precise proofs. However, it\nlacks automation and compositionality support. This paper addresses this\nlimitation by introducing a compositional verification system for concurrent\nprograms that manipulate regions of shared memory. The centre of our system is\nnovel logical principles and an entailment procedure that can infer the\nresidual heaps in the frame rule for a fragment of CSL-Perm with explicit\narithmetical constraints for memory heaps' disjointness. This procedure enables\nthe compositional reasoning for concurrent threads and function calls. We have\nimplemented the proposal in a prototype tool called CoSl, tested it with 10\nchallenging concurrent programs, including those beyond the state-of-the-art,\nand confirmed the advantage of our approach."
                },
                "authors": [
                    {
                        "name": "Quang Loc Le"
                    }
                ],
                "author_detail": {
                    "name": "Quang Loc Le"
                },
                "author": "Quang Loc Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18113v1",
                "updated": "2025-08-25T15:21:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    21,
                    49,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:21:49Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    21,
                    49,
                    0,
                    237,
                    0
                ],
                "title": "The AI Data Scientist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Data Scientist"
                },
                "summary": "Imagine decision-makers uploading data and, within minutes, receiving clear,\nactionable insights delivered straight to their fingertips. That is the promise\nof the AI Data Scientist, an autonomous Agent powered by large language models\n(LLMs) that closes the gap between evidence and action. Rather than simply\nwriting code or responding to prompts, it reasons through questions, tests\nideas, and delivers end-to-end insights at a pace far beyond traditional\nworkflows. Guided by the scientific tenet of the hypothesis, this Agent\nuncovers explanatory patterns in data, evaluates their statistical\nsignificance, and uses them to inform predictive modeling. It then translates\nthese results into recommendations that are both rigorous and accessible. At\nthe core of the AI Data Scientist is a team of specialized LLM Subagents, each\nresponsible for a distinct task such as data cleaning, statistical testing,\nvalidation, and plain-language communication. These Subagents write their own\ncode, reason about causality, and identify when additional data is needed to\nsupport sound conclusions. Together, they achieve in minutes what might\notherwise take days or weeks, enabling a new kind of interaction that makes\ndeep data science both accessible and actionable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine decision-makers uploading data and, within minutes, receiving clear,\nactionable insights delivered straight to their fingertips. That is the promise\nof the AI Data Scientist, an autonomous Agent powered by large language models\n(LLMs) that closes the gap between evidence and action. Rather than simply\nwriting code or responding to prompts, it reasons through questions, tests\nideas, and delivers end-to-end insights at a pace far beyond traditional\nworkflows. Guided by the scientific tenet of the hypothesis, this Agent\nuncovers explanatory patterns in data, evaluates their statistical\nsignificance, and uses them to inform predictive modeling. It then translates\nthese results into recommendations that are both rigorous and accessible. At\nthe core of the AI Data Scientist is a team of specialized LLM Subagents, each\nresponsible for a distinct task such as data cleaning, statistical testing,\nvalidation, and plain-language communication. These Subagents write their own\ncode, reason about causality, and identify when additional data is needed to\nsupport sound conclusions. Together, they achieve in minutes what might\notherwise take days or weeks, enabling a new kind of interaction that makes\ndeep data science both accessible and actionable."
                },
                "authors": [
                    {
                        "name": "Farkhad Akimov"
                    },
                    {
                        "name": "Munachiso Samuel Nwadike"
                    },
                    {
                        "name": "Zangir Iklassov"
                    },
                    {
                        "name": "Martin Tak√°ƒç"
                    }
                ],
                "author_detail": {
                    "name": "Martin Tak√°ƒç"
                },
                "author": "Martin Tak√°ƒç",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18106v1",
                "updated": "2025-08-25T15:11:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    11,
                    11,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:11:11Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    11,
                    11,
                    0,
                    237,
                    0
                ],
                "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code"
                },
                "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching."
                },
                "authors": [
                    {
                        "name": "Keke Lian"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Libo Chen"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ziming Zhao"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Haotong Duan"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Shuang Liao"
                    },
                    {
                        "name": "Mingda Guo"
                    },
                    {
                        "name": "Jiazheng Quan"
                    },
                    {
                        "name": "Yilu Zhong"
                    },
                    {
                        "name": "Chenhao He"
                    },
                    {
                        "name": "Zichuan Chen"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Zhaoxuan Li"
                    },
                    {
                        "name": "Jiongchi Yu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Dong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Zhang"
                },
                "author": "Dong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06941v2",
                "updated": "2025-08-25T15:03:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    3,
                    0,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-09T11:26:10Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    26,
                    10,
                    5,
                    221,
                    0
                ],
                "title": "CLAP: Coreference-Linked Augmentation for Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAP: Coreference-Linked Augmentation for Passage Retrieval"
                },
                "summary": "Large Language Model (LLM)-based passage expansion has shown promise for\nenhancing first-stage retrieval, but often underperforms with dense retrievers\ndue to semantic drift and misalignment with their pretrained semantic space.\nBeyond this, only a portion of a passage is typically relevant to a query,\nwhile the rest introduces noise--an issue compounded by chunking techniques\nthat break coreference continuity. We propose Coreference-Linked Augmentation\nfor Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that\nsegments passages into coherent chunks, resolves coreference chains, and\ngenerates localized pseudo-queries aligned with dense retriever\nrepresentations. A simple fusion of global topical signals and fine-grained\nsubtopic signals achieves robust performance across domains. CLAP yields\nconsistent gains even as retriever strength increases, enabling dense\nretrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,\nwith up to 20.68% absolute nDCG@10 improvement. These improvements are\nespecially notable in out-of-domain settings, where conventional LLM-based\nexpansion methods relying on domain knowledge often falter. CLAP instead adopts\na logic-centric pipeline that enables robust, domain-agnostic generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based passage expansion has shown promise for\nenhancing first-stage retrieval, but often underperforms with dense retrievers\ndue to semantic drift and misalignment with their pretrained semantic space.\nBeyond this, only a portion of a passage is typically relevant to a query,\nwhile the rest introduces noise--an issue compounded by chunking techniques\nthat break coreference continuity. We propose Coreference-Linked Augmentation\nfor Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that\nsegments passages into coherent chunks, resolves coreference chains, and\ngenerates localized pseudo-queries aligned with dense retriever\nrepresentations. A simple fusion of global topical signals and fine-grained\nsubtopic signals achieves robust performance across domains. CLAP yields\nconsistent gains even as retriever strength increases, enabling dense\nretrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,\nwith up to 20.68% absolute nDCG@10 improvement. These improvements are\nespecially notable in out-of-domain settings, where conventional LLM-based\nexpansion methods relying on domain knowledge often falter. CLAP instead adopts\na logic-centric pipeline that enables robust, domain-agnostic generalization."
                },
                "authors": [
                    {
                        "name": "Huanwei Xu"
                    },
                    {
                        "name": "Lin Xu"
                    },
                    {
                        "name": "Liang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yuan"
                },
                "author": "Liang Yuan",
                "arxiv_doi": "10.1145/3746252.3761113",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761113",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18098v1",
                "updated": "2025-08-25T14:59:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    59,
                    46,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:59:46Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    59,
                    46,
                    0,
                    237,
                    0
                ],
                "title": "Detecting and Characterizing Planning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Characterizing Planning in Language Models"
                },
                "summary": "Modern large language models (LLMs) have demonstrated impressive performance\nacross a wide range of multi-step reasoning tasks. Recent work suggests that\nLLMs may perform planning - selecting a future target token in advance and\ngenerating intermediate tokens that lead towards it - rather than merely\nimprovising one token at a time. However, existing studies assume fixed\nplanning horizons and often focus on single prompts or narrow domains. To\ndistinguish planning from improvisation across models and tasks, we present\nformal and causally grounded criteria for detecting planning and operationalize\nthem as a semi-automated annotation pipeline. We apply this pipeline to both\nbase and instruction-tuned Gemma-2-2B models on the MBPP code generation\nbenchmark and a poem generation task where Claude 3.5 Haiku was previously\nshown to plan. Our findings show that planning is not universal: unlike Haiku,\nGemma-2-2B solves the same poem generation task through improvisation, and on\nMBPP it switches between planning and improvisation across similar tasks and\neven successive token predictions. We further show that instruction tuning\nrefines existing planning behaviors in the base model rather than creating them\nfrom scratch. Together, these studies provide a reproducible and scalable\nfoundation for mechanistic studies of planning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) have demonstrated impressive performance\nacross a wide range of multi-step reasoning tasks. Recent work suggests that\nLLMs may perform planning - selecting a future target token in advance and\ngenerating intermediate tokens that lead towards it - rather than merely\nimprovising one token at a time. However, existing studies assume fixed\nplanning horizons and often focus on single prompts or narrow domains. To\ndistinguish planning from improvisation across models and tasks, we present\nformal and causally grounded criteria for detecting planning and operationalize\nthem as a semi-automated annotation pipeline. We apply this pipeline to both\nbase and instruction-tuned Gemma-2-2B models on the MBPP code generation\nbenchmark and a poem generation task where Claude 3.5 Haiku was previously\nshown to plan. Our findings show that planning is not universal: unlike Haiku,\nGemma-2-2B solves the same poem generation task through improvisation, and on\nMBPP it switches between planning and improvisation across similar tasks and\neven successive token predictions. We further show that instruction tuning\nrefines existing planning behaviors in the base model rather than creating them\nfrom scratch. Together, these studies provide a reproducible and scalable\nfoundation for mechanistic studies of planning in LLMs."
                },
                "authors": [
                    {
                        "name": "Jatin Nainani"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "Connor Watts"
                    },
                    {
                        "name": "Andre N. Assis"
                    },
                    {
                        "name": "Alice Rigg"
                    }
                ],
                "author_detail": {
                    "name": "Alice Rigg"
                },
                "author": "Alice Rigg",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18097v1",
                "updated": "2025-08-25T14:57:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    57,
                    13,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:57:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    57,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "A$^3$COSMOS: The dust content of massive quiescent galaxies and its\n  evolution with cosmic time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^3$COSMOS: The dust content of massive quiescent galaxies and its\n  evolution with cosmic time"
                },
                "summary": "We study the dust content of massive ($\\log(M_*/M_{\\odot})\\geq10.8$)\nquiescent galaxies (QGs) at redshift $z=0.5-3$ to place constraints on the\nevolution of their cold interstellar medium (ISM), and thereby obtain insights\non the processes of galaxy quenching throughout cosmic time. We use a robust\nsample of 458 colour-selected QGs covered by the A$^3$COSMOS+A$^3$GOODSS\ndatabase to perform a stacking analysis in the $uv$-domain and measure their\nmean dust masses from their stacked submillimetre luminosities. We use the\nCIGALE SED-fitting code to obtain star formation histories and infer the time\nsince quenching for all QGs in our sample. We use this information to gain\ninsight on the time evolution of the dust content after quenching. Most QGs in\nour sample quenched around a redshift of $z\\sim1.3$, following the peak of\ncosmic star formation. The majority of QGs observed at $z>1$ are recently\nquenched (i.e., quenched for no longer than $500\\,$Myr), whereas the majority\nof QGs observed at $z<1$ have already been quenched for a significant amount of\ntime ($\\gtrsim1\\,$Gyr). This implies that high-redshift galaxies ($z\\gtrsim2$)\nare ideal for studying the mechanisms of quenching and its effects on the ISM,\nwhile lower-redshift galaxies are more suitable for studying the long-term\neffects of the QG environment on their ISM. We obtain upper limits on the dust\nmass fraction of the QG population, pointing towards lower dust content in\nhigh-redshift massive QGs than found by earlier stacking studies, and\nsignificantly lower (by a factor $\\sim2-6$) than that of normal star forming\ngalaxies. We also place constraints on the initial gas fraction right after\nquenching. We find that within the first $\\sim600\\,$Myr after quenching, QGs\nalready lose on average $\\gtrsim70\\%$ of their cold ISM. Our findings support a\ngas consumption or removal scenario acting on short timescales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the dust content of massive ($\\log(M_*/M_{\\odot})\\geq10.8$)\nquiescent galaxies (QGs) at redshift $z=0.5-3$ to place constraints on the\nevolution of their cold interstellar medium (ISM), and thereby obtain insights\non the processes of galaxy quenching throughout cosmic time. We use a robust\nsample of 458 colour-selected QGs covered by the A$^3$COSMOS+A$^3$GOODSS\ndatabase to perform a stacking analysis in the $uv$-domain and measure their\nmean dust masses from their stacked submillimetre luminosities. We use the\nCIGALE SED-fitting code to obtain star formation histories and infer the time\nsince quenching for all QGs in our sample. We use this information to gain\ninsight on the time evolution of the dust content after quenching. Most QGs in\nour sample quenched around a redshift of $z\\sim1.3$, following the peak of\ncosmic star formation. The majority of QGs observed at $z>1$ are recently\nquenched (i.e., quenched for no longer than $500\\,$Myr), whereas the majority\nof QGs observed at $z<1$ have already been quenched for a significant amount of\ntime ($\\gtrsim1\\,$Gyr). This implies that high-redshift galaxies ($z\\gtrsim2$)\nare ideal for studying the mechanisms of quenching and its effects on the ISM,\nwhile lower-redshift galaxies are more suitable for studying the long-term\neffects of the QG environment on their ISM. We obtain upper limits on the dust\nmass fraction of the QG population, pointing towards lower dust content in\nhigh-redshift massive QGs than found by earlier stacking studies, and\nsignificantly lower (by a factor $\\sim2-6$) than that of normal star forming\ngalaxies. We also place constraints on the initial gas fraction right after\nquenching. We find that within the first $\\sim600\\,$Myr after quenching, QGs\nalready lose on average $\\gtrsim70\\%$ of their cold ISM. Our findings support a\ngas consumption or removal scenario acting on short timescales."
                },
                "authors": [
                    {
                        "name": "Sylvia Adscheid"
                    },
                    {
                        "name": "Benjamin Magnelli"
                    },
                    {
                        "name": "Laure Ciesla"
                    },
                    {
                        "name": "Daizhong Liu"
                    },
                    {
                        "name": "Eva Schinnerer"
                    },
                    {
                        "name": "Frank Bertoldi"
                    }
                ],
                "author_detail": {
                    "name": "Frank Bertoldi"
                },
                "author": "Frank Bertoldi",
                "arxiv_comment": "Accepted for publication in A&A; 10 pages, 5 pages appendix, 10\n  figures; Acknowledgements can be found in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18096v2",
                "updated": "2025-08-26T06:59:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    6,
                    59,
                    36,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T14:57:10Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    57,
                    10,
                    0,
                    237,
                    0
                ],
                "title": "Realizing Reduced and Sparse Biochemical Reaction Networks from Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realizing Reduced and Sparse Biochemical Reaction Networks from Dynamics"
                },
                "summary": "We propose a direct optimization framework for learning reduced and sparse\nchemical reaction networks (CRNs) from time-series trajectory data. In contrast\nto widely used indirect methods-such as those based on sparse identification of\nnonlinear dynamics (SINDy)-which infer reaction dynamics by fitting numerically\nestimated derivatives, our approach fits entire trajectories by solving a\ndynamically constrained optimization problem. This formulation enables the\nconstruction of reduced CRNs that are both low-dimensional and sparse, while\npreserving key dynamical behaviors of the original system. We develop an\naccelerated proximal gradient algorithm to efficiently solve the resulting\nnon-convex optimization problem. Through illustrative examples, including a\nDrosophila circadian oscillator and a glycolytic oscillator, we demonstrate the\nability of our method to recover accurate and interpretable reduced-order CRNs.\nNotably, the direct approach avoids the derivative estimation step and\nmitigates error accumulation issues inherent in indirect methods, making it a\nrobust alternative for data-driven CRN realizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a direct optimization framework for learning reduced and sparse\nchemical reaction networks (CRNs) from time-series trajectory data. In contrast\nto widely used indirect methods-such as those based on sparse identification of\nnonlinear dynamics (SINDy)-which infer reaction dynamics by fitting numerically\nestimated derivatives, our approach fits entire trajectories by solving a\ndynamically constrained optimization problem. This formulation enables the\nconstruction of reduced CRNs that are both low-dimensional and sparse, while\npreserving key dynamical behaviors of the original system. We develop an\naccelerated proximal gradient algorithm to efficiently solve the resulting\nnon-convex optimization problem. Through illustrative examples, including a\nDrosophila circadian oscillator and a glycolytic oscillator, we demonstrate the\nability of our method to recover accurate and interpretable reduced-order CRNs.\nNotably, the direct approach avoids the derivative estimation step and\nmitigates error accumulation issues inherent in indirect methods, making it a\nrobust alternative for data-driven CRN realizations."
                },
                "authors": [
                    {
                        "name": "Maurice Filo"
                    },
                    {
                        "name": "Mustafa Khammash"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa Khammash"
                },
                "author": "Mustafa Khammash",
                "arxiv_comment": "Accepted to IEEE CDC 2025. Author-accepted version; supplementary\n  material in ancillary files (In this version, supplementary PDF is moved to\n  ancillary files; no content changes to main article)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05520v2",
                "updated": "2025-08-25T14:55:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    55,
                    38,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-07T22:31:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    22,
                    31,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for\n  Multimodal Medical VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for\n  Multimodal Medical VQA"
                },
                "summary": "Dermatological care via telemedicine often lacks the rich context of\nin-person visits. Clinicians must make diagnoses based on a handful of images\nand brief descriptions, without the benefit of physical exams, second opinions,\nor reference materials. While many medical AI systems attempt to bridge these\ngaps with domain-specific fine-tuning, this work hypothesized that mimicking\nclinical reasoning processes could offer a more effective path forward. This\nstudy tested seven vision-language models on medical visual question answering\nacross six configurations: baseline models, fine-tuned variants, and both\naugmented with either reasoning layers that combine multiple model\nperspectives, analogous to peer consultation, or retrieval-augmented generation\nthat incorporates medical literature at inference time, serving a role similar\nto reference-checking. While fine-tuning degraded performance in four of seven\nmodels with an average 30\\% decrease, baseline models collapsed on test data.\nClinical-inspired architectures, meanwhile, achieved up to 70\\% accuracy,\nmaintaining performance on unseen data while generating explainable,\nliterature-grounded outputs critical for clinical adoption. These findings\ndemonstrate that medical AI succeeds by reconstructing the collaborative and\nevidence-based practices fundamental to clinical diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dermatological care via telemedicine often lacks the rich context of\nin-person visits. Clinicians must make diagnoses based on a handful of images\nand brief descriptions, without the benefit of physical exams, second opinions,\nor reference materials. While many medical AI systems attempt to bridge these\ngaps with domain-specific fine-tuning, this work hypothesized that mimicking\nclinical reasoning processes could offer a more effective path forward. This\nstudy tested seven vision-language models on medical visual question answering\nacross six configurations: baseline models, fine-tuned variants, and both\naugmented with either reasoning layers that combine multiple model\nperspectives, analogous to peer consultation, or retrieval-augmented generation\nthat incorporates medical literature at inference time, serving a role similar\nto reference-checking. While fine-tuning degraded performance in four of seven\nmodels with an average 30\\% decrease, baseline models collapsed on test data.\nClinical-inspired architectures, meanwhile, achieved up to 70\\% accuracy,\nmaintaining performance on unseen data while generating explainable,\nliterature-grounded outputs critical for clinical adoption. These findings\ndemonstrate that medical AI succeeds by reconstructing the collaborative and\nevidence-based practices fundamental to clinical diagnosis."
                },
                "authors": [
                    {
                        "name": "Karishma Thakrar"
                    },
                    {
                        "name": "Shreyas Basavatia"
                    },
                    {
                        "name": "Akshay Daftardar"
                    }
                ],
                "author_detail": {
                    "name": "Akshay Daftardar"
                },
                "author": "Akshay Daftardar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18093v1",
                "updated": "2025-08-25T14:54:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    54,
                    46,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:54:46Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    54,
                    46,
                    0,
                    237,
                    0
                ],
                "title": "Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual\n  Technical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual\n  Technical Question Answering"
                },
                "summary": "We present a case study evaluating large language models (LLMs) with\n128K-token context windows on a technical question answering (QA) task. Our\nbenchmark is built on a user manual for an agricultural machine, available in\nEnglish, French, and German. It simulates a cross-lingual information retrieval\nscenario where questions are posed in English against all three language\nversions of the manual. The evaluation focuses on realistic\n\"needle-in-a-haystack\" challenges and includes unanswerable questions to test\nfor hallucinations. We compare nine long-context LLMs using direct prompting\nagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,\nsemantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this\nspecific manual show that Hybrid RAG consistently outperforms direct\nlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5\n7B achieve high accuracy (over 85%) across all languages with RAG. This paper\ncontributes a detailed analysis of LLM performance in a specialized industrial\ndomain and an open framework for similar evaluations, highlighting practical\ntrade-offs and challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a case study evaluating large language models (LLMs) with\n128K-token context windows on a technical question answering (QA) task. Our\nbenchmark is built on a user manual for an agricultural machine, available in\nEnglish, French, and German. It simulates a cross-lingual information retrieval\nscenario where questions are posed in English against all three language\nversions of the manual. The evaluation focuses on realistic\n\"needle-in-a-haystack\" challenges and includes unanswerable questions to test\nfor hallucinations. We compare nine long-context LLMs using direct prompting\nagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,\nsemantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this\nspecific manual show that Hybrid RAG consistently outperforms direct\nlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5\n7B achieve high accuracy (over 85%) across all languages with RAG. This paper\ncontributes a detailed analysis of LLM performance in a specialized industrial\ndomain and an open framework for similar evaluations, highlighting practical\ntrade-offs and challenges."
                },
                "authors": [
                    {
                        "name": "Julius Gun"
                    },
                    {
                        "name": "Timo Oksanen"
                    }
                ],
                "author_detail": {
                    "name": "Timo Oksanen"
                },
                "author": "Timo Oksanen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18091v1",
                "updated": "2025-08-25T14:52:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    56,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:52:56Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    56,
                    0,
                    237,
                    0
                ],
                "title": "Teaching LLMs to Think Mathematically: A Critical Study of\n  Decision-Making via Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs to Think Mathematically: A Critical Study of\n  Decision-Making via Optimization"
                },
                "summary": "This paper investigates the capabilities of large language models (LLMs) in\nformulating and solving decision-making problems using mathematical\nprogramming. We first conduct a systematic review and meta-analysis of recent\nliterature to assess how well LLMs understand, structure, and solve\noptimization problems across domains. The analysis is guided by critical review\nquestions focusing on learning approaches, dataset designs, evaluation metrics,\nand prompting strategies. Our systematic evidence is complemented by targeted\nexperiments designed to evaluate the performance of state-of-the-art LLMs in\nautomatically generating optimization models for problems in computer networks.\nUsing a newly constructed dataset, we apply three prompting strategies:\nAct-as-expert, chain-of-thought, and self-consistency, and evaluate the\nobtained outputs based on optimality gap, token-level F1 score, and compilation\naccuracy. Results show promising progress in LLMs' ability to parse natural\nlanguage and represent symbolic formulations, but also reveal key limitations\nin accuracy, scalability, and interpretability. These empirical gaps motivate\nseveral future research directions, including structured datasets,\ndomain-specific fine-tuning, hybrid neuro-symbolic approaches, modular\nmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper\ncontributes a structured roadmap for advancing LLM capabilities in mathematical\nprogramming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the capabilities of large language models (LLMs) in\nformulating and solving decision-making problems using mathematical\nprogramming. We first conduct a systematic review and meta-analysis of recent\nliterature to assess how well LLMs understand, structure, and solve\noptimization problems across domains. The analysis is guided by critical review\nquestions focusing on learning approaches, dataset designs, evaluation metrics,\nand prompting strategies. Our systematic evidence is complemented by targeted\nexperiments designed to evaluate the performance of state-of-the-art LLMs in\nautomatically generating optimization models for problems in computer networks.\nUsing a newly constructed dataset, we apply three prompting strategies:\nAct-as-expert, chain-of-thought, and self-consistency, and evaluate the\nobtained outputs based on optimality gap, token-level F1 score, and compilation\naccuracy. Results show promising progress in LLMs' ability to parse natural\nlanguage and represent symbolic formulations, but also reveal key limitations\nin accuracy, scalability, and interpretability. These empirical gaps motivate\nseveral future research directions, including structured datasets,\ndomain-specific fine-tuning, hybrid neuro-symbolic approaches, modular\nmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper\ncontributes a structured roadmap for advancing LLM capabilities in mathematical\nprogramming."
                },
                "authors": [
                    {
                        "name": "Mohammad J. Abdel-Rahman"
                    },
                    {
                        "name": "Yasmeen Alslman"
                    },
                    {
                        "name": "Dania Refai"
                    },
                    {
                        "name": "Amro Saleh"
                    },
                    {
                        "name": "Malik A. Abu Loha"
                    },
                    {
                        "name": "Mohammad Yahya Hamed"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Yahya Hamed"
                },
                "author": "Mohammad Yahya Hamed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18090v1",
                "updated": "2025-08-25T14:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    11,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:52:11Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    11,
                    0,
                    237,
                    0
                ],
                "title": "Named Entity Recognition of Historical Text via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition of Historical Text via Large Language Model"
                },
                "summary": "Large language models have demonstrated remarkable versatility across a wide\nrange of natural language processing tasks and domains. One such task is Named\nEntity Recognition (NER), which involves identifying and classifying proper\nnames in text, such as people, organizations, locations, dates, and other\nspecific entities. NER plays a crucial role in extracting information from\nunstructured textual data, enabling downstream applications such as information\nretrieval from unstructured text.\n  Traditionally, NER is addressed using supervised machine learning approaches,\nwhich require large amounts of annotated training data. However, historical\ntexts present a unique challenge, as the annotated datasets are often scarce or\nnonexistent, due to the high cost and expertise required for manual labeling.\nIn addition, the variability and noise inherent in historical language, such as\ninconsistent spelling and archaic vocabulary, further complicate the\ndevelopment of reliable NER systems for these sources.\n  In this study, we explore the feasibility of applying LLMs to NER in\nhistorical documents using zero-shot and few-shot prompting strategies, which\nrequire little to no task-specific training data. Our experiments, conducted on\nthe HIPE-2022 (Identifying Historical People, Places and other Entities)\ndataset, show that LLMs can achieve reasonably strong performance on NER tasks\nin this setting. While their performance falls short of fully supervised models\ntrained on domain-specific annotations, the results are nevertheless promising.\nThese findings suggest that LLMs offer a viable and efficient alternative for\ninformation extraction in low-resource or historically significant corpora,\nwhere traditional supervised methods are infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable versatility across a wide\nrange of natural language processing tasks and domains. One such task is Named\nEntity Recognition (NER), which involves identifying and classifying proper\nnames in text, such as people, organizations, locations, dates, and other\nspecific entities. NER plays a crucial role in extracting information from\nunstructured textual data, enabling downstream applications such as information\nretrieval from unstructured text.\n  Traditionally, NER is addressed using supervised machine learning approaches,\nwhich require large amounts of annotated training data. However, historical\ntexts present a unique challenge, as the annotated datasets are often scarce or\nnonexistent, due to the high cost and expertise required for manual labeling.\nIn addition, the variability and noise inherent in historical language, such as\ninconsistent spelling and archaic vocabulary, further complicate the\ndevelopment of reliable NER systems for these sources.\n  In this study, we explore the feasibility of applying LLMs to NER in\nhistorical documents using zero-shot and few-shot prompting strategies, which\nrequire little to no task-specific training data. Our experiments, conducted on\nthe HIPE-2022 (Identifying Historical People, Places and other Entities)\ndataset, show that LLMs can achieve reasonably strong performance on NER tasks\nin this setting. While their performance falls short of fully supervised models\ntrained on domain-specific annotations, the results are nevertheless promising.\nThese findings suggest that LLMs offer a viable and efficient alternative for\ninformation extraction in low-resource or historically significant corpora,\nwhere traditional supervised methods are infeasible."
                },
                "authors": [
                    {
                        "name": "Shibingfeng Zhang"
                    },
                    {
                        "name": "Giovanni Colavizza"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Colavizza"
                },
                "author": "Giovanni Colavizza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18089v1",
                "updated": "2025-08-25T14:49:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    49,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:49:29Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    49,
                    29,
                    0,
                    237,
                    0
                ],
                "title": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated\n  Software Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated\n  Software Evolution"
                },
                "summary": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals."
                },
                "authors": [
                    {
                        "name": "Karine Even-Mendoza"
                    },
                    {
                        "name": "Alexander Brownlee"
                    },
                    {
                        "name": "Alina Geiger"
                    },
                    {
                        "name": "Carol Hanna"
                    },
                    {
                        "name": "Justyna Petke"
                    },
                    {
                        "name": "Federica Sarro"
                    },
                    {
                        "name": "Dominik Sobania"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Sobania"
                },
                "author": "Dominik Sobania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18083v1",
                "updated": "2025-08-25T14:46:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    10,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:46:10Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    10,
                    0,
                    237,
                    0
                ],
                "title": "GWTC-4.0: Population Properties of Merging Compact Binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: Population Properties of Merging Compact Binaries"
                },
                "summary": "We detail the population properties of merging compact objects using 158\nmergers from the cumulative Gravitational-Wave Transient Catalog 4.0, which\nincludes three types of binary mergers: binary neutron star, neutron\nstar--black hole binary, and binary black hole mergers. We resolve multiple\nover- and under-densities in the black hole mass distribution: features persist\nat primary masses of $10\\,M_\\odot$ and $35\\,M_\\odot$ with a possible third\nfeature at $\\sim 20\\,M_\\odot$. These are departures from an otherwise\npower-law-like continuum that steepens above $35\\,M_\\odot$. Binary black holes\nwith primary masses near $10\\,M_\\odot$ are more likely to have less massive\nsecondaries, with a mass ratio distribution peaking at $q =\n0.74^{+0.13}_{-0.13}$, potentially a signature of stable mass transfer during\nbinary evolution. Black hole spins are inferred to be non-extremal, with 90\\%\nof black holes having $\\chi < 0.57$, and preferentially aligned with binary\norbits, implying many merging binaries form in isolation. However, we find a\nsignificant fraction, 0.24--0.42, of binaries have negative effective inspiral\nspins, suggesting many could be formed dynamically in gas-free environments. We\nfind evidence for correlation between effective inspiral spin and mass ratio,\nthough it is unclear if this is driven by variation in the mode of the\ndistribution or the width. (Abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the population properties of merging compact objects using 158\nmergers from the cumulative Gravitational-Wave Transient Catalog 4.0, which\nincludes three types of binary mergers: binary neutron star, neutron\nstar--black hole binary, and binary black hole mergers. We resolve multiple\nover- and under-densities in the black hole mass distribution: features persist\nat primary masses of $10\\,M_\\odot$ and $35\\,M_\\odot$ with a possible third\nfeature at $\\sim 20\\,M_\\odot$. These are departures from an otherwise\npower-law-like continuum that steepens above $35\\,M_\\odot$. Binary black holes\nwith primary masses near $10\\,M_\\odot$ are more likely to have less massive\nsecondaries, with a mass ratio distribution peaking at $q =\n0.74^{+0.13}_{-0.13}$, potentially a signature of stable mass transfer during\nbinary evolution. Black hole spins are inferred to be non-extremal, with 90\\%\nof black holes having $\\chi < 0.57$, and preferentially aligned with binary\norbits, implying many merging binaries form in isolation. However, we find a\nsignificant fraction, 0.24--0.42, of binaries have negative effective inspiral\nspins, suggesting many could be formed dynamically in gas-free environments. We\nfind evidence for correlation between effective inspiral spin and mass ratio,\nthough it is unclear if this is driven by variation in the mode of the\ndistribution or the width. (Abridged)"
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "the KAGRA Collaboration"
                },
                "author": "the KAGRA Collaboration",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18082v1",
                "updated": "2025-08-25T14:46:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    9,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:46:09Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    9,
                    0,
                    237,
                    0
                ],
                "title": "GWTC-4.0: Updating the Gravitational-Wave Transient Catalog with\n  Observations from the First Part of the Fourth LIGO-Virgo-KAGRA Observing Run",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: Updating the Gravitational-Wave Transient Catalog with\n  Observations from the First Part of the Fourth LIGO-Virgo-KAGRA Observing Run"
                },
                "summary": "Version 4.0 of the Gravitational-Wave Transient Catalog (GWTC-4.0) adds new\ncandidates detected by the LIGO, Virgo, and KAGRA observatories through the\nfirst part of the fourth observing run (O4a: 2023 May 24 15:00:00 to 2024\nJanuary 16 16:00:00 UTC) and a preceding engineering run. In this new data, we\nfind 128 new compact binary coalescence candidates that are identified by at\nleast one of our search algorithms with a probability of astrophysical origin\n$p_{\\rm astro} \\geq 0.5$ and that are not vetoed during event validation. We\nalso provide detailed source property measurements for 86 of these that have a\nfalse alarm rate $< 1 \\rm{yr}^{-1}$. Based on the inferred component masses,\nthese new candidates are consistent with signals from binary black holes and\nneutron star-black hole binaries (GW230518_125908 and GW230529_181500). Median\ninferred component masses of binary black holes in the catalog now range from\n$5.79\\,M_\\odot$ (GW230627_015337) to $137\\,M_\\odot$ (GW231123_135430), while\nGW231123_135430 was probably produced by the most massive binary observed in\nthe catalog. For the first time we have discovered binary black hole signals\nwith network signal-to-noise ratio exceeding 30, GW230814_230901 and\nGW231226_01520, enabling high-fidelity studies of the waveforms and\nastrophysical properties of these systems. Combined with the 90 candidates\nincluded in GWTC-3.0, the catalog now contains 218 candidates with $p_{\\rm\nastro} \\geq 0.5$ and not otherwise vetoed, doubling the size of the catalog and\nfurther opening our view of the gravitational-wave Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Version 4.0 of the Gravitational-Wave Transient Catalog (GWTC-4.0) adds new\ncandidates detected by the LIGO, Virgo, and KAGRA observatories through the\nfirst part of the fourth observing run (O4a: 2023 May 24 15:00:00 to 2024\nJanuary 16 16:00:00 UTC) and a preceding engineering run. In this new data, we\nfind 128 new compact binary coalescence candidates that are identified by at\nleast one of our search algorithms with a probability of astrophysical origin\n$p_{\\rm astro} \\geq 0.5$ and that are not vetoed during event validation. We\nalso provide detailed source property measurements for 86 of these that have a\nfalse alarm rate $< 1 \\rm{yr}^{-1}$. Based on the inferred component masses,\nthese new candidates are consistent with signals from binary black holes and\nneutron star-black hole binaries (GW230518_125908 and GW230529_181500). Median\ninferred component masses of binary black holes in the catalog now range from\n$5.79\\,M_\\odot$ (GW230627_015337) to $137\\,M_\\odot$ (GW231123_135430), while\nGW231123_135430 was probably produced by the most massive binary observed in\nthe catalog. For the first time we have discovered binary black hole signals\nwith network signal-to-noise ratio exceeding 30, GW230814_230901 and\nGW231226_01520, enabling high-fidelity studies of the waveforms and\nastrophysical properties of these systems. Combined with the 90 candidates\nincluded in GWTC-3.0, the catalog now contains 218 candidates with $p_{\\rm\nastro} \\geq 0.5$ and not otherwise vetoed, doubling the size of the catalog and\nfurther opening our view of the gravitational-wave Universe."
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "The Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "the KAGRA Collaboration"
                },
                "author": "the KAGRA Collaboration",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18081v1",
                "updated": "2025-08-25T14:46:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    8,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:46:08Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    8,
                    0,
                    237,
                    0
                ],
                "title": "GWTC-4.0: Methods for Identifying and Characterizing Gravitational-wave\n  Transients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: Methods for Identifying and Characterizing Gravitational-wave\n  Transients"
                },
                "summary": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of candidate\ngravitational-wave transient signals identified and characterized by the\nLIGO-Virgo-KAGRA Collaboration. Producing the contents of the GWTC from\ndetector data requires complex analysis methods. These comprise techniques to\nmodel the signal; identify the transients in the data; evaluate the quality of\nthe data and mitigate possible instrumental issues; infer the parameters of\neach transient; compare the data with the waveform models for compact binary\ncoalescences; and handle the large amount of results associated with all these\ndifferent analyses. In this paper, we describe the methods employed to produce\nthe catalog's fourth release, GWTC-4.0, focusing on the analysis of the first\npart of the fourth observing run of Advanced LIGO, Advanced Virgo and KAGRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of candidate\ngravitational-wave transient signals identified and characterized by the\nLIGO-Virgo-KAGRA Collaboration. Producing the contents of the GWTC from\ndetector data requires complex analysis methods. These comprise techniques to\nmodel the signal; identify the transients in the data; evaluate the quality of\nthe data and mitigate possible instrumental issues; infer the parameters of\neach transient; compare the data with the waveform models for compact binary\ncoalescences; and handle the large amount of results associated with all these\ndifferent analyses. In this paper, we describe the methods employed to produce\nthe catalog's fourth release, GWTC-4.0, focusing on the analysis of the first\npart of the fourth observing run of Advanced LIGO, Advanced Virgo and KAGRA."
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "A. G. Abac"
                    },
                    {
                        "name": "I. Abouelfettouh"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "S. Adhicary"
                    },
                    {
                        "name": "D. Adhikari"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. K. Adkins"
                    },
                    {
                        "name": "S. Afroz"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "M. Aghaei Abchouyeh"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "S. Ahmadzadeh"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "S. Akcay"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "R. A. Alfaidi"
                    },
                    {
                        "name": "A. Al-Jodah"
                    },
                    {
                        "name": "C. All√©n√©"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "S. Al-Shammari"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "S. Alvarez-Lopez"
                    },
                    {
                        "name": "O. Amarasinghe"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "C. Amra"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Andia"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "T. Andrade"
                    },
                    {
                        "name": "M. Andr√©s-Carcasona"
                    },
                    {
                        "name": "T. Andriƒá"
                    },
                    {
                        "name": "J. Anglin"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "M. Aoumi"
                    },
                    {
                        "name": "E. Z. Appavuravther"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "S. K. Apple"
                    },
                    {
                        "name": "K. Arai"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "M. Arca Sedda"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "L. Argianas"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "F. Armato"
                    },
                    {
                        "name": "S. Armstrong"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "M. Arogeti"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. Assis de Souza Melo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Attadio"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "K. AultONeal"
                    },
                    {
                        "name": "G. Avallone"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "F. Badaracco"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "E. Bagui"
                    },
                    {
                        "name": "L. Baiotti"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "T. Baka"
                    },
                    {
                        "name": "T. Baker"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "B. Banerjee"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "T. M. Baptiste"
                    },
                    {
                        "name": "P. Baral"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "N. Barman"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "A. M. Bartoletti"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "S. Basak"
                    },
                    {
                        "name": "A. Basalaev"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "D. E. Bates"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "P. Baxi"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "A. C. Baylor"
                    },
                    {
                        "name": "P. A. Baynard II"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "F. Beirnaert"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "D. Belardinelli"
                    },
                    {
                        "name": "A. S. Bell"
                    },
                    {
                        "name": "D. S. Bellie"
                    },
                    {
                        "name": "L. Bellizzi"
                    },
                    {
                        "name": "W. Benoit"
                    },
                    {
                        "name": "I. Bentara"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. Ben Yaala"
                    },
                    {
                        "name": "S. Bera"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "M. Beroiz"
                    },
                    {
                        "name": "C. P. L. Berry"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "G. Bevilacqua"
                    },
                    {
                        "name": "N. Bevins"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "S. A. Bhat"
                    },
                    {
                        "name": "R. Bhatt"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "S. Bhowmick"
                    },
                    {
                        "name": "V. Biancalana"
                    },
                    {
                        "name": "A. Bianchi"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "A. Binetti"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "C. Binu"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "S. Blaber"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "L. A. Blagg"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "F. Bobba"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "G. Boileau"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "G. N. Bolingbroke"
                    },
                    {
                        "name": "A. Bolliand"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "R. Bondarescu"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "M. S. Bonilla"
                    },
                    {
                        "name": "A. Bonino"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "P. Booker"
                    },
                    {
                        "name": "A. Borchers"
                    },
                    {
                        "name": "S. Borhanian"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "A. Boudon"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "I. Braun"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "E. Brockmueller"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "B. C. Brown"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "M. L. Brozzetti"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "Y. Bu"
                    },
                    {
                        "name": "F. Bucci"
                    },
                    {
                        "name": "J. Buchanan"
                    },
                    {
                        "name": "O. Bulashenko"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "K. Burtnyk"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "G. S. Cabourn Davies"
                    },
                    {
                        "name": "G. Cabras"
                    },
                    {
                        "name": "R. Cabrita"
                    },
                    {
                        "name": "V. C√°ceres-Barbosa"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "A. Calafat"
                    },
                    {
                        "name": "J. Calder√≥n Bustillo"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "G. Caneva Santoro"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "L. A. Capistran"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Capurri"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "M. Carlassara"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "T. K. Carlson"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "J. J. Carter"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "S. Y. Castro-Lucas"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. Cavagli√†"
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. Cerd√°-Dur√°n"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "P. Chakraborty"
                    },
                    {
                        "name": "S. Chakraborty"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "J. C. L. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "R. -J. Chang"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "E. L. Charlton"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "C. Checchia"
                    },
                    {
                        "name": "A. Chen"
                    },
                    {
                        "name": "A. H. -Y. Chen"
                    },
                    {
                        "name": "D. Chen"
                    },
                    {
                        "name": "H. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Y. Chen"
                    },
                    {
                        "name": "Yanbei Chen"
                    },
                    {
                        "name": "Yitian Chen"
                    },
                    {
                        "name": "H. P. Cheng"
                    },
                    {
                        "name": "P. Chessa"
                    },
                    {
                        "name": "H. T. Cheung"
                    },
                    {
                        "name": "S. Y. Cheung"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "R. Chierici"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "C. Chou"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "S. S. Y. Chua"
                    },
                    {
                        "name": "P. Chugh"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. Cie≈õlar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "R. Ciolfi"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "J. Clarke"
                    },
                    {
                        "name": "T. A. Clarke"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "S. M. Clyne"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "S. Colace"
                    },
                    {
                        "name": "E. Colangeli"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "J. Collins"
                    },
                    {
                        "name": "S. Colloms"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "G. Connolly"
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-Carri√≥n"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "N. J. Cornish"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "S. Cortese"
                    },
                    {
                        "name": "R. Cottingham"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "A. Couineaux"
                    },
                    {
                        "name": "J. -P. Coulon"
                    },
                    {
                        "name": "J. -F. Coupechoux"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "K. Craig"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "P. Cremonese"
                    },
                    {
                        "name": "A. W. Criswell"
                    },
                    {
                        "name": "S. Crook"
                    },
                    {
                        "name": "R. Crouch"
                    },
                    {
                        "name": "J. Csizmazia"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Cusinato"
                    },
                    {
                        "name": "P. Dabadie"
                    },
                    {
                        "name": "L. V. Da Concei√ß√£o"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dall'Osso"
                    },
                    {
                        "name": "S. Dal Pra"
                    },
                    {
                        "name": "G. D√°lya"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "K. E. Darroch"
                    },
                    {
                        "name": "L. P. Dartez"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "S. Datta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "A. Daumas"
                    },
                    {
                        "name": "N. Davari"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "A. Davenport"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "T. F. Davies"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "L. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "P. Davis"
                    },
                    {
                        "name": "M. Dax"
                    },
                    {
                        "name": "J. De Bolle"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "U. Deka"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "S. Del√©glise"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "D. Dell'Aquila"
                    },
                    {
                        "name": "F. Della Valle"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "F. De Marco"
                    },
                    {
                        "name": "G. Demasi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "V. D'Emilio"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "T. Dent"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "N. DePergola"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "M. Desai"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "A. DeSimone"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "A. Dhani"
                    },
                    {
                        "name": "R. Diab"
                    },
                    {
                        "name": "M. C. D√≠az"
                    },
                    {
                        "name": "M. Di Cesare"
                    },
                    {
                        "name": "G. Dideron"
                    },
                    {
                        "name": "N. A. Didio"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "D. Diksha"
                    },
                    {
                        "name": "A. Di Michele"
                    },
                    {
                        "name": "J. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "N. Doerksen"
                    },
                    {
                        "name": "E. Dohmen"
                    },
                    {
                        "name": "D. Dominguez"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "T. Dooney"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "O. Dorosh"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "J. -G. Ducoin"
                    },
                    {
                        "name": "L. Dunn"
                    },
                    {
                        "name": "U. Dupletsa"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "H. Duval"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "B. Edelman"
                    },
                    {
                        "name": "T. B. Edo"
                    },
                    {
                        "name": "O. Edy"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "H. Einsle"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "R. A. Eisenstein"
                    },
                    {
                        "name": "A. Ejlli"
                    },
                    {
                        "name": "M. Emma"
                    },
                    {
                        "name": "K. Endo"
                    },
                    {
                        "name": "R. Enficiaud"
                    },
                    {
                        "name": "A. J. Engl"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. Espinosa"
                    },
                    {
                        "name": "M. Esposito"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. Estell√©s"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. Evstafyeva"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "J. M. Ezquiaga"
                    },
                    {
                        "name": "F. Fabrizi"
                    },
                    {
                        "name": "F. Faedi"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "R. Felicetti"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "D. L. Ferguson"
                    },
                    {
                        "name": "T. Fernandes"
                    },
                    {
                        "name": "D. Fernando"
                    },
                    {
                        "name": "S. Ferraiuolo"
                    },
                    {
                        "name": "I. Ferrante"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "A. Fiori"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "S. M. Fleischer"
                    },
                    {
                        "name": "L. S. Fleming"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "C. Foo"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "P. W. F. Forsyth"
                    },
                    {
                        "name": "K. Franceschetti"
                    },
                    {
                        "name": "N. Franchini"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "A. Frattale Mascioli"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "O. Freitas"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "W. Frischhertz"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. Fronz√©"
                    },
                    {
                        "name": "M. Fuentes-Garcia"
                    },
                    {
                        "name": "S. Fujii"
                    },
                    {
                        "name": "T. Fujimori"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "B. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "V. Galdi"
                    },
                    {
                        "name": "H. Gallagher"
                    },
                    {
                        "name": "B. Gallego"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "A. Gamboa"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "J. Garc√≠a-Bellido"
                    },
                    {
                        "name": "C. Garc√≠a N√∫√±ez"
                    },
                    {
                        "name": "C. Garc√≠a-Quir√≥s"
                    },
                    {
                        "name": "J. W. Gardner"
                    },
                    {
                        "name": "K. A. Gardner"
                    },
                    {
                        "name": "J. Gargiulo"
                    },
                    {
                        "name": "A. Garron"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "P. A. Garver"
                    },
                    {
                        "name": "C. Gasbarra"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "F. Gautier"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "T. Gayer"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "V. Gennari"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "R. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Sayantan Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "Suprovo Ghosh"
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "D. T. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "S. Gkaitatzis"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "F. Glotin"
                    },
                    {
                        "name": "J. Godfrey"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "A. S. Goettel"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "S. Gomez Lopez"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "Y. Gong"
                    },
                    {
                        "name": "G. Gonz√°lez"
                    },
                    {
                        "name": "P. Goodarzi"
                    },
                    {
                        "name": "S. Goode"
                    },
                    {
                        "name": "A. W. Goodwin-Jones"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "K. Govorkova"
                    },
                    {
                        "name": "S. Goyal"
                    },
                    {
                        "name": "B. Grace"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "V. Graham"
                    },
                    {
                        "name": "A. E. Granados"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "A. Gray"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "S. M. Green"
                    },
                    {
                        "name": "S. R. Green"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "E. M. Gretarsson"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "W. L. Griffiths"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "C. Grimaud"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "D. Guetta"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "F. Gulminelli"
                    },
                    {
                        "name": "A. M. Gunny"
                    },
                    {
                        "name": "H. Guo"
                    },
                    {
                        "name": "W. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anchal Gupta"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "I. Gupta"
                    },
                    {
                        "name": "N. C. Gupta"
                    },
                    {
                        "name": "P. Gupta"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "T. Gupta"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "N. Gupte"
                    },
                    {
                        "name": "J. Gurs"
                    },
                    {
                        "name": "N. Gutierrez"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "D. Haba"
                    },
                    {
                        "name": "M. Haberland"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "R. Hamburg"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "W. -B. Han"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. A. Hannuksela"
                    },
                    {
                        "name": "A. G. Hanselman"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "R. Harada"
                    },
                    {
                        "name": "A. R. Hardison"
                    },
                    {
                        "name": "S. Harikumar"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "T. Harmark"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "J. Hart"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "C. -J. Haster"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "R. Hayes"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "A. Heffernan"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "O. Henderson-Sapir"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "C. Henshaw"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "J. Heyns"
                    },
                    {
                        "name": "S. Higginbotham"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "S. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "C. Hirose"
                    },
                    {
                        "name": "S. Hochheim"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "L. Honet"
                    },
                    {
                        "name": "C. Hong"
                    },
                    {
                        "name": "S. Hoshino"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "N. T. Howard"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "C. A. Hrishikesh"
                    },
                    {
                        "name": "H. -F. Hsieh"
                    },
                    {
                        "name": "H. -Y. Hsieh"
                    },
                    {
                        "name": "C. Hsiung"
                    },
                    {
                        "name": "W. -F. Hsu"
                    },
                    {
                        "name": "Q. Hu"
                    },
                    {
                        "name": "H. Y. Huang"
                    },
                    {
                        "name": "Y. Huang"
                    },
                    {
                        "name": "Y. T. Huang"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "D. C. Y. Hui"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "L. Iampieri"
                    },
                    {
                        "name": "G. A. Iandolo"
                    },
                    {
                        "name": "M. Ianni"
                    },
                    {
                        "name": "A. Ierardi"
                    },
                    {
                        "name": "A. Iess"
                    },
                    {
                        "name": "H. Imafuku"
                    },
                    {
                        "name": "K. Inayoshi"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "G. Iorio"
                    },
                    {
                        "name": "P. Iosif"
                    },
                    {
                        "name": "M. H. Iqbal"
                    },
                    {
                        "name": "J. Irwin"
                    },
                    {
                        "name": "R. Ishikawa"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "H. Iwanaga"
                    },
                    {
                        "name": "M. Iwaya"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "C. Jacquet"
                    },
                    {
                        "name": "P. -E. Jacquet"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "T. Jain"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "P. A. James"
                    },
                    {
                        "name": "R. Jamshidi"
                    },
                    {
                        "name": "A. Jan"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "K. Janssens"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "S. Jaraba"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "W. Javed"
                    },
                    {
                        "name": "A. Jennings"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "S. J. Jin"
                    },
                    {
                        "name": "C. Johanson"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "N. A. Johnson"
                    },
                    {
                        "name": "N. K. Johnson-McDaniel"
                    },
                    {
                        "name": "M. C. Johnston"
                    },
                    {
                        "name": "R. Johnston"
                    },
                    {
                        "name": "N. Johny"
                    },
                    {
                        "name": "D. H. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "E. J. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "S. Jose"
                    },
                    {
                        "name": "P. Joshi"
                    },
                    {
                        "name": "S. K. Joshi"
                    },
                    {
                        "name": "J. Ju"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "K. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "H. B. Kabagoz"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "I. Kaku"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "M. Kalomenopoulos"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "N. C. Kannachel"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "S. Karat"
                    },
                    {
                        "name": "R. Kashyap"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "W. Kastaun"
                    },
                    {
                        "name": "T. Kato"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "R. Kaushik"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "R. Kawamoto"
                    },
                    {
                        "name": "A. Kazemi"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "J. Kennington"
                    },
                    {
                        "name": "R. Kesharwani"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "R. Khadela"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "F. Khan"
                    },
                    {
                        "name": "I. Khan"
                    },
                    {
                        "name": "T. Khanam"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. M. Khusid"
                    },
                    {
                        "name": "W. Kiendrebeogo"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "M. H. Kim"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "M. Kinley-Hanlon"
                    },
                    {
                        "name": "M. Kinnear"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "N. Knust"
                    },
                    {
                        "name": "K. Kobayashi"
                    },
                    {
                        "name": "P. Koch"
                    },
                    {
                        "name": "S. M. Koehlenbeck"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "K. Kohri"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "R. V. Kossak"
                    },
                    {
                        "name": "X. Kou"
                    },
                    {
                        "name": "A. Koushik"
                    },
                    {
                        "name": "N. Kouvatsos"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "S. L. Kranzhoff"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "A. Kr√≥lak"
                    },
                    {
                        "name": "K. Kruska"
                    },
                    {
                        "name": "J. Kubisz"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "S. Kulkarni"
                    },
                    {
                        "name": "A. Kulur Ramamohan"
                    },
                    {
                        "name": "A. Kumar"
                    },
                    {
                        "name": "Praveen Kumar"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "N. Kuntimaddi"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "S. Kuwahara"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "K. Kwan"
                    },
                    {
                        "name": "J. Kwok"
                    },
                    {
                        "name": "G. Lacaille"
                    },
                    {
                        "name": "P. Lagabbe"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "S. Lai"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "M. Lalleman"
                    },
                    {
                        "name": "P. C. Lalremruati"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "R. Langgin"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "A. La Rana"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "J. Larsen"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "J. Lawrence"
                    },
                    {
                        "name": "M. N. Lawrence"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "C. Lazarte"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "L. Leali"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. -K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "Sungho Lee"
                    },
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Y. Lee"
                    },
                    {
                        "name": "I. N. Legred"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "L. Lehner"
                    },
                    {
                        "name": "M. Le Jean"
                    },
                    {
                        "name": "A. Lema{√Æ"
                    },
                    {
                        "name": "M. Lenti"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "M. Lequime"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "M. Lesovsky"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "M. Lethuillier"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Z. Li"
                    },
                    {
                        "name": "A. Lihos"
                    },
                    {
                        "name": "C-Y. Lin"
                    },
                    {
                        "name": "E. T. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "Y. -C. Lin"
                    },
                    {
                        "name": "C. Lindsay"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "T. B. Littenberg"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "F. Llamas Villarreal"
                    },
                    {
                        "name": "J. Llobera-Querol"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "J. -P. Locquet"
                    },
                    {
                        "name": "M. R. Loizou"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "A. Lorenzo-Medina"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "E. Lotti"
                    },
                    {
                        "name": "T. P. Lott IV"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "H. A. Loughlin"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "N. Low"
                    },
                    {
                        "name": "M. J. Lowry"
                    },
                    {
                        "name": "N. Lu"
                    },
                    {
                        "name": "L. Lucchesi"
                    },
                    {
                        "name": "H. L√ºck"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "A. W. Lussier"
                    },
                    {
                        "name": "L. -T. Ma"
                    },
                    {
                        "name": "S. Ma"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "A. Macedo"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "R. R. Maciy"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "D. Macri"
                    },
                    {
                        "name": "K. Maeda"
                    },
                    {
                        "name": "S. Maenaut"
                    },
                    {
                        "name": "S. S. Magare"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "E. Maggio"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "M. Mahesh"
                    },
                    {
                        "name": "M. Maini"
                    },
                    {
                        "name": "S. Majhi"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "C. N. Makarem"
                    },
                    {
                        "name": "D. Malakar"
                    },
                    {
                        "name": "J. A. Malaquias-Reis"
                    },
                    {
                        "name": "U. Mali"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "L. Mallick"
                    },
                    {
                        "name": "A. Malz"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "B. Mannix"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "G. Mansingh"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "F. Marchesoni"
                    },
                    {
                        "name": "C. Marinelli"
                    },
                    {
                        "name": "D. Mar√≠n Pina"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "S. M√°rka"
                    },
                    {
                        "name": "Z. M√°rka"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "B. B. Martinez"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "A. Martini"
                    },
                    {
                        "name": "J. C. Martins"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "L. Massaro"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "M. Mastrodicasa"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "T. Matcovich"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "M. Matsuyama"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "N. Maxwell"
                    },
                    {
                        "name": "G. McCarrol"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "S. McEachin"
                    },
                    {
                        "name": "C. McElhenny"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "J. McGinn"
                    },
                    {
                        "name": "K. B. M. McGowan"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "A. McLeod"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "M. Melching"
                    },
                    {
                        "name": "S. Mellaerts"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "F. Mera"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. R. M√©rou"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "C. Messenger"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "B. Mestichelli"
                    },
                    {
                        "name": "M. Meyer-Conde"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "I. Michaloliakos"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "S. J. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "V. Milotti"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "N. Mio"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "L. Mirasola"
                    },
                    {
                        "name": "M. Miravet-Ten√©s"
                    },
                    {
                        "name": "C. -A. Miritescu"
                    },
                    {
                        "name": "A. K. Mishra"
                    },
                    {
                        "name": "A. Mishra"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "A. L. Mitchell"
                    },
                    {
                        "name": "J. G. Mitchell"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "S. Miyamoto"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "G. Mo"
                    },
                    {
                        "name": "L. Mobilia"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "C. Mondal"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "S. More"
                    },
                    {
                        "name": "E. A. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "G. Morras"
                    },
                    {
                        "name": "A. Moscatello"
                    },
                    {
                        "name": "M. Mould"
                    },
                    {
                        "name": "P. Mourier"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Samanwaya Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "H. Mullock"
                    },
                    {
                        "name": "J. Munch"
                    },
                    {
                        "name": "J. Mundi"
                    },
                    {
                        "name": "C. L. Mungioli"
                    },
                    {
                        "name": "Y. Murakami"
                    },
                    {
                        "name": "M. Murakoshi"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "S. Muusse"
                    },
                    {
                        "name": "D. Nabari"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "N. Nagarajan"
                    },
                    {
                        "name": "K. Nakagaki"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "D. Nanadoumgar-Lacroze"
                    },
                    {
                        "name": "D. Nandi"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "P. Narayan"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "H. Narola"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "A. Nela"
                    },
                    {
                        "name": "A. Nelson"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "S. Ng"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "A. B. Nielsen"
                    },
                    {
                        "name": "G. Nieradka"
                    },
                    {
                        "name": "Y. Nishino"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "E. Nitoglia"
                    },
                    {
                        "name": "W. Niu"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "J. Novak"
                    },
                    {
                        "name": "J. F. Nu√±o Siles"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "K. Obayashi"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "M. Oertel"
                    },
                    {
                        "name": "A. Offermans"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "J. J. Oh"
                    },
                    {
                        "name": "K. Oh"
                    },
                    {
                        "name": "T. O'Hanlon"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "M. Ohkawa"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "R. Oliveri"
                    },
                    {
                        "name": "R. Omer"
                    },
                    {
                        "name": "B. O'Neal"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "R. Oram"
                    },
                    {
                        "name": "N. D. Ormsby"
                    },
                    {
                        "name": "M. Orselli"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "S. O'Shea"
                    },
                    {
                        "name": "Y. Oshima"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "I. Ota"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "A. Ouzriat"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "B. J. Owen"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "R. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "L. Paiella"
                    },
                    {
                        "name": "A. Pal"
                    },
                    {
                        "name": "S. Pal"
                    },
                    {
                        "name": "M. A. Palaia"
                    },
                    {
                        "name": "M. P√°lfi"
                    },
                    {
                        "name": "P. P. Palma"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "P. Palud"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "K. C. Pan"
                    },
                    {
                        "name": "R. Panai"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "Shiksha Pandey"
                    },
                    {
                        "name": "Swadha Pandey"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "K. A. Pannone"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "A. Papadopoulos"
                    },
                    {
                        "name": "E. E. Papalexakis"
                    },
                    {
                        "name": "L. Papalini"
                    },
                    {
                        "name": "G. Papigkiotis"
                    },
                    {
                        "name": "A. Paquis"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "B. -J. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "G. Pascale"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "L. Passenger"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "O. Patane"
                    },
                    {
                        "name": "D. Pathak"
                    },
                    {
                        "name": "L. Pathak"
                    },
                    {
                        "name": "A. Patra"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "A. S. Patron"
                    },
                    {
                        "name": "B. G. Patterson"
                    },
                    {
                        "name": "K. Paul"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "T. Pearce"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. Pe√±a Arellano"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "M. D. Penuliar"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "Z. Pereira"
                    },
                    {
                        "name": "J. J. Perez"
                    },
                    {
                        "name": "C. P√©rigois"
                    },
                    {
                        "name": "G. Perna"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "J. Perret"
                    },
                    {
                        "name": "S. Perri√®s"
                    },
                    {
                        "name": "J. W. Perry"
                    },
                    {
                        "name": "D. Pesios"
                    },
                    {
                        "name": "S. Petracca"
                    },
                    {
                        "name": "C. Petrillo"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "H. Phurailatpam"
                    },
                    {
                        "name": "M. Piarulli"
                    },
                    {
                        "name": "L. Piccari"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "G. Pierra"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "M. Pietrzak"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "B. J. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "A. Placidi"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "M. L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Plunkett"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "L. Pompili"
                    },
                    {
                        "name": "J. Poon"
                    },
                    {
                        "name": "E. Porcelli"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "C. Posnansky"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "B. K. Pradhan"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "P. Prasia"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "G. Principe"
                    },
                    {
                        "name": "M. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "L. Prokhorov"
                    },
                    {
                        "name": "P. Prosperi"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "A. C. Providence"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "J. Pullin"
                    },
                    {
                        "name": "M. Punturo"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. P√ºrrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "J. Qin"
                    },
                    {
                        "name": "G. Qu√©m√©ner"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "P. J. Quinonez"
                    },
                    {
                        "name": "F. J. Raab"
                    },
                    {
                        "name": "I. Rainho"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "B. Rajbhandari"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "F. A. Ramis Vidal"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "D. Rana"
                    },
                    {
                        "name": "S. Ranjan"
                    },
                    {
                        "name": "K. Ransom"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "B. Ratto"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "M. Recaman Payo"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "L. Rei"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "P. Relton"
                    },
                    {
                        "name": "A. I. Renzini"
                    },
                    {
                        "name": "A. Renzini"
                    },
                    {
                        "name": "B. Revenu"
                    },
                    {
                        "name": "R. Reyes"
                    },
                    {
                        "name": "A. S. Rezaei"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "M. Ricci"
                    },
                    {
                        "name": "A. Ricciardone"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "M. Richardson"
                    },
                    {
                        "name": "A. Rijal"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "H. K. Riley"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "J. Rittmeyer"
                    },
                    {
                        "name": "C. Robertson"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "M. Robinson"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "A. E. Romano"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "A. Romero"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "T. J. Roocke"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "T. J. Rosauer"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. Rosi≈Ñska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "M. Rossello-Sastre"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "S. K. Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "N. Ruhama"
                    },
                    {
                        "name": "E. Ruiz Morales"
                    },
                    {
                        "name": "K. Ruiz-Rocha"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "J. Sadiq"
                    },
                    {
                        "name": "P. Saffarieh"
                    },
                    {
                        "name": "S. Safi-Harb"
                    },
                    {
                        "name": "M. R. Sah"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "T. Sainrat"
                    },
                    {
                        "name": "S. Sajith Menon"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "S. Sakon"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "F. Salces-Carcoba"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "M. Sall√©"
                    },
                    {
                        "name": "S. U. Salunkhe"
                    },
                    {
                        "name": "S. Salvador"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "A. Sanchez"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "J. H. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "E. M. S√§nger"
                    },
                    {
                        "name": "F. Santoliquido"
                    },
                    {
                        "name": "F. Sarandrea"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "P. Sarkar"
                    },
                    {
                        "name": "S. Sasaoka"
                    },
                    {
                        "name": "A. Sasli"
                    },
                    {
                        "name": "P. Sassi"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "Y. Sato"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "V. Scacco"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "A. Schiebelbein"
                    },
                    {
                        "name": "M. G. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "K. Schouteden"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "M. Scialpi"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "R. M. Sedas"
                    },
                    {
                        "name": "T. C. Seetharamu"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "D. Sentenac"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "J. W. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "M. Serra"
                    },
                    {
                        "name": "G. Servignat"
                    },
                    {
                        "name": "A. Sevrin"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "U. S. Shah"
                    },
                    {
                        "name": "M. S. Shahriar"
                    },
                    {
                        "name": "M. A. Shaikh"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. Sharma"
                    },
                    {
                        "name": "A. K. Sharma"
                    },
                    {
                        "name": "P. Sharma"
                    },
                    {
                        "name": "S. Sharma Chaudhary"
                    },
                    {
                        "name": "M. R. Shaw"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "Y. Shikano"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "K. Shimode"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "J. Shiota"
                    },
                    {
                        "name": "S. Shirke"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "R. W. Short"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "A. Sider"
                    },
                    {
                        "name": "H. Siegel"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. Silenzi"
                    },
                    {
                        "name": "M. Simmonds"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "A. Singh"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "M. K. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "S. Singh"
                    },
                    {
                        "name": "A. Singha"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "D. A. Slater"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "L. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "W. J. Smith"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "I. Song"
                    },
                    {
                        "name": "K. Soni"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "H. Sotani"
                    },
                    {
                        "name": "A. Southgate"
                    },
                    {
                        "name": "F. Spada"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "M. Spera"
                    },
                    {
                        "name": "P. Spinicelli"
                    },
                    {
                        "name": "C. A. Sprague"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "F. Stachurski"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "N. Steinle"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "N. Stergioulas"
                    },
                    {
                        "name": "P. Stevens"
                    },
                    {
                        "name": "S. P. Stevenson"
                    },
                    {
                        "name": "F. Stolzi"
                    },
                    {
                        "name": "M. StPierre"
                    },
                    {
                        "name": "G. Stratta"
                    },
                    {
                        "name": "M. D. Strong"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "R. Sturani"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "M. Suchenek"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "N. Sueltmann"
                    },
                    {
                        "name": "L. Suleiman"
                    },
                    {
                        "name": "J. M. Sullivan"
                    },
                    {
                        "name": "K. D. Sullivan"
                    },
                    {
                        "name": "J. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "B. J. Sutton"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "T. Suzuki"
                    },
                    {
                        "name": "Y. Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "A. Syx"
                    },
                    {
                        "name": "M. J. Szczepa≈Ñczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "S. C. Tait"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "A. Takamori"
                    },
                    {
                        "name": "T. Takase"
                    },
                    {
                        "name": "K. Takatani"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "K. Takeshita"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "M. Tamaki"
                    },
                    {
                        "name": "N. Tamanini"
                    },
                    {
                        "name": "D. Tanabe"
                    },
                    {
                        "name": "K. Tanaka"
                    },
                    {
                        "name": "S. J. Tanaka"
                    },
                    {
                        "name": "T. Tanaka"
                    },
                    {
                        "name": "D. Tang"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "W. Tanner"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "R. D. Tapia"
                    },
                    {
                        "name": "E. N. Tapia San Mart√≠n"
                    },
                    {
                        "name": "R. Tarafder"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "A. Taruya"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "J. G. Tau"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "H. Themann"
                    },
                    {
                        "name": "A. Theodoropoulos"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "L. M. Thomas"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "S. Tibrewal"
                    },
                    {
                        "name": "J. Tissino"
                    },
                    {
                        "name": "A. Tiwari"
                    },
                    {
                        "name": "P. Tiwari"
                    },
                    {
                        "name": "S. Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "M. R. Todd"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "K. Toland"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "K. Tomita"
                    },
                    {
                        "name": "V. Tommasini"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "H. Tong"
                    },
                    {
                        "name": "C. Tong-Yu"
                    },
                    {
                        "name": "A. Toriyama"
                    },
                    {
                        "name": "N. Toropov"
                    },
                    {
                        "name": "A. Torres-Forn√©"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "M. Toscani"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "E. Tournefier"
                    },
                    {
                        "name": "M. Trad Nery"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "C. Trejo"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "G. Troian"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "T. T. L. Tsang"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "C. Turski"
                    },
                    {
                        "name": "H. Ubach"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "M. Uematsu"
                    },
                    {
                        "name": "S. Ueno"
                    },
                    {
                        "name": "V. Undheim"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "M. Vacatello"
                    },
                    {
                        "name": "H. Vahlbruch"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "G. Valdes"
                    },
                    {
                        "name": "J. Valencia"
                    },
                    {
                        "name": "A. F. Valentini"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "S. A. Vallejo-Pe√±a"
                    },
                    {
                        "name": "S. Vallero"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "N. van Bakel"
                    },
                    {
                        "name": "M. van Beuzekom"
                    },
                    {
                        "name": "M. van Dael"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "D. C. Vander-Hyde"
                    },
                    {
                        "name": "M. van der Sluys"
                    },
                    {
                        "name": "A. Van de Walle"
                    },
                    {
                        "name": "J. van Dongen"
                    },
                    {
                        "name": "K. Vandra"
                    },
                    {
                        "name": "H. van Haevermaet"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "P. Van Hove"
                    },
                    {
                        "name": "J. Vanier"
                    },
                    {
                        "name": "M. VanKeuren"
                    },
                    {
                        "name": "J. Vanosky"
                    },
                    {
                        "name": "M. H. P. M. van Putten"
                    },
                    {
                        "name": "Z. Van Ranst"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "J. J. Varghese"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "A. N. Vazquez"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "S. Venikoudis"
                    },
                    {
                        "name": "J. Venneberg"
                    },
                    {
                        "name": "P. Verdier"
                    },
                    {
                        "name": "M. Vereecken"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "B. Verma"
                    },
                    {
                        "name": "P. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "S. M. Vermeulen"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Veutro"
                    },
                    {
                        "name": "A. M. Vibhute"
                    },
                    {
                        "name": "A. Vicer√©"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "A. Vilkha"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "E. T. Vincent"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "S. Viret"
                    },
                    {
                        "name": "A. Virtuoso"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "A. Vives"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "D. Voigt"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "L. Vujeva"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "J. Wack"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "A. Wajid"
                    },
                    {
                        "name": "M. Walker"
                    },
                    {
                        "name": "G. S. Wallace"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "E. J. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "Y. F. Wang"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "G. Waratkar"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "D. Watarai"
                    },
                    {
                        "name": "K. E. Wayt"
                    },
                    {
                        "name": "B. R. Weaver"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "C. R. Weaving"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "N. L. Weickhardt"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "F. Wellmann"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "P. We√üels"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "E. G. Wickens"
                    },
                    {
                        "name": "J. B. Wildberger"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "D. J. Willadsen"
                    },
                    {
                        "name": "K. Willetts"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "N. S. Williams"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "M. Wils"
                    },
                    {
                        "name": "C. W. Winborn"
                    },
                    {
                        "name": "J. Winterflood"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "N. E. Wolfe"
                    },
                    {
                        "name": "H. T. Wong"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "J. L. Wright"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "E. Wuchner"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "V. A. Xu"
                    },
                    {
                        "name": "Y. Xu"
                    },
                    {
                        "name": "N. Yadav"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "K. Yamamoto"
                    },
                    {
                        "name": "T. S. Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "S. Yamamura"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "T. Yan"
                    },
                    {
                        "name": "F. W. Yang"
                    },
                    {
                        "name": "F. Yang"
                    },
                    {
                        "name": "K. Z. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Z. Yarbrough"
                    },
                    {
                        "name": "H. Yasui"
                    },
                    {
                        "name": "S. -W. Yeh"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "X. Yin"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "J. Yoo"
                    },
                    {
                        "name": "H. Yu"
                    },
                    {
                        "name": "S. Yuan"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "A. Zadro≈ºny"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "M. Zeeshan"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zeoli"
                    },
                    {
                        "name": "M. Zerrad"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "A. C. Zhang"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "R. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Y. Zheng"
                    },
                    {
                        "name": "H. Zhong"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "X. -J. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18080v1",
                "updated": "2025-08-25T14:46:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    7,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:46:07Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    7,
                    0,
                    237,
                    0
                ],
                "title": "GWTC-4.0: An Introduction to Version 4.0 of the Gravitational-Wave\n  Transient Catalog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: An Introduction to Version 4.0 of the Gravitational-Wave\n  Transient Catalog"
                },
                "summary": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of\nshort-duration (transient) gravitational wave signals identified by the\nLIGO-Virgo-KAGRA Collaboration in gravitational-wave data produced by the\neponymous detectors. The catalog provides information about the identified\ncandidates, such as the arrival time and amplitude of the signal and properties\nof the signal's source as inferred from the observational data. GWTC is the\ndata release of this dataset and version 4.0 extends the catalog to include\nobservations made during the first part of the fourth LIGO-Virgo-KAGRA\nobserving run up until 2024 January 31. This paper marks an introduction to a\ncollection of articles related to this version of the catalog, GWTC-4.0. The\ncollection of articles accompanying the catalog provides documentation of the\nmethods used to analyze the data, summaries of the catalog of events,\nobservational measurements drawn from the population, and detailed discussions\nof selected candidates",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of\nshort-duration (transient) gravitational wave signals identified by the\nLIGO-Virgo-KAGRA Collaboration in gravitational-wave data produced by the\neponymous detectors. The catalog provides information about the identified\ncandidates, such as the arrival time and amplitude of the signal and properties\nof the signal's source as inferred from the observational data. GWTC is the\ndata release of this dataset and version 4.0 extends the catalog to include\nobservations made during the first part of the fourth LIGO-Virgo-KAGRA\nobserving run up until 2024 January 31. This paper marks an introduction to a\ncollection of articles related to this version of the catalog, GWTC-4.0. The\ncollection of articles accompanying the catalog provides documentation of the\nmethods used to analyze the data, summaries of the catalog of events,\nobservational measurements drawn from the population, and detailed discussions\nof selected candidates"
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "A. G. Abac"
                    },
                    {
                        "name": "I. Abouelfettouh"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "S. Adhicary"
                    },
                    {
                        "name": "D. Adhikari"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. K. Adkins"
                    },
                    {
                        "name": "S. Afroz"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "M. Aghaei Abchouyeh"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "S. Ahmadzadeh"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "S. Akcay"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "R. A. Alfaidi"
                    },
                    {
                        "name": "A. Al-Jodah"
                    },
                    {
                        "name": "C. All√©n√©"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "S. Al-Shammari"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "S. Alvarez-Lopez"
                    },
                    {
                        "name": "O. Amarasinghe"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "C. Amra"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Andia"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "T. Andrade"
                    },
                    {
                        "name": "M. Andr√©s-Carcasona"
                    },
                    {
                        "name": "T. Andriƒá"
                    },
                    {
                        "name": "J. Anglin"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "M. Aoumi"
                    },
                    {
                        "name": "E. Z. Appavuravther"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "S. K. Apple"
                    },
                    {
                        "name": "K. Arai"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "M. Arca Sedda"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "L. Argianas"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "F. Armato"
                    },
                    {
                        "name": "S. Armstrong"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "M. Arogeti"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. Assis de Souza Melo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Attadio"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "K. AultONeal"
                    },
                    {
                        "name": "G. Avallone"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "F. Badaracco"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "E. Bagui"
                    },
                    {
                        "name": "L. Baiotti"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "T. Baka"
                    },
                    {
                        "name": "T. Baker"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "B. Banerjee"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "T. M. Baptiste"
                    },
                    {
                        "name": "P. Baral"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "N. Barman"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "A. M. Bartoletti"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "S. Basak"
                    },
                    {
                        "name": "A. Basalaev"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "D. E. Bates"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "P. Baxi"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "A. C. Baylor"
                    },
                    {
                        "name": "P. A. Baynard II"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "F. Beirnaert"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "D. Belardinelli"
                    },
                    {
                        "name": "A. S. Bell"
                    },
                    {
                        "name": "D. S. Bellie"
                    },
                    {
                        "name": "L. Bellizzi"
                    },
                    {
                        "name": "W. Benoit"
                    },
                    {
                        "name": "I. Bentara"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. Ben Yaala"
                    },
                    {
                        "name": "S. Bera"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "M. Beroiz"
                    },
                    {
                        "name": "C. P. L. Berry"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "G. Bevilacqua"
                    },
                    {
                        "name": "N. Bevins"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "R. Bhatt"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "S. Bhowmick"
                    },
                    {
                        "name": "V. Biancalana"
                    },
                    {
                        "name": "A. Bianchi"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "A. Binetti"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "C. Binu"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "S. Blaber"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "L. A. Blagg"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "F. Bobba"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "G. Boileau"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "G. N. Bolingbroke"
                    },
                    {
                        "name": "A. Bolliand"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "R. Bondarescu"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "M. S. Bonilla"
                    },
                    {
                        "name": "A. Bonino"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "P. Booker"
                    },
                    {
                        "name": "A. Borchers"
                    },
                    {
                        "name": "S. Borhanian"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "A. Boudon"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "I. Braun"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "E. Brockmueller"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "B. C. Brown"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "M. L. Brozzetti"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "Y. Bu"
                    },
                    {
                        "name": "F. Bucci"
                    },
                    {
                        "name": "J. Buchanan"
                    },
                    {
                        "name": "O. Bulashenko"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "K. Burtnyk"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "G. S. Cabourn Davies"
                    },
                    {
                        "name": "G. Cabras"
                    },
                    {
                        "name": "R. Cabrita"
                    },
                    {
                        "name": "V. C√°ceres-Barbosa"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "A. Calafat"
                    },
                    {
                        "name": "J. Calder√≥n Bustillo"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "M. Canepa"
                    },
                    {
                        "name": "G. Caneva Santoro"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "L. A. Capistran"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Capurri"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "M. Carlassara"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "T. K. Carlson"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "J. J. Carter"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "S. Y. Castro-Lucas"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. Cavagli√†"
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. Cerd√°-Dur√°n"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "P. Chakraborty"
                    },
                    {
                        "name": "S. Chakraborty"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "J. C. L. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "R. -J. Chang"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "E. L. Charlton"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "C. Checchia"
                    },
                    {
                        "name": "A. Chen"
                    },
                    {
                        "name": "A. H. -Y. Chen"
                    },
                    {
                        "name": "D. Chen"
                    },
                    {
                        "name": "H. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Y. Chen"
                    },
                    {
                        "name": "Yanbei Chen"
                    },
                    {
                        "name": "Yitian Chen"
                    },
                    {
                        "name": "H. P. Cheng"
                    },
                    {
                        "name": "P. Chessa"
                    },
                    {
                        "name": "H. T. Cheung"
                    },
                    {
                        "name": "S. Y. Cheung"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "R. Chierici"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "C. Chou"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "S. S. Y. Chua"
                    },
                    {
                        "name": "P. Chugh"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. Cie≈õlar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "R. Ciolfi"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "J. Clarke"
                    },
                    {
                        "name": "T. A. Clarke"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "S. M. Clyne"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "S. Colace"
                    },
                    {
                        "name": "E. Colangeli"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "J. Collins"
                    },
                    {
                        "name": "S. Colloms"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "G. Connolly"
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-Carri√≥n"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "N. J. Cornish"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "S. Cortese"
                    },
                    {
                        "name": "R. Cottingham"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "A. Couineaux"
                    },
                    {
                        "name": "J. -P. Coulon"
                    },
                    {
                        "name": "J. -F. Coupechoux"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "K. Craig"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "P. Cremonese"
                    },
                    {
                        "name": "A. W. Criswell"
                    },
                    {
                        "name": "S. Crook"
                    },
                    {
                        "name": "R. Crouch"
                    },
                    {
                        "name": "J. Csizmazia"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Cusinato"
                    },
                    {
                        "name": "P. Dabadie"
                    },
                    {
                        "name": "L. V. Da Concei√ß√£o"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dall'Osso"
                    },
                    {
                        "name": "S. Dal Pra"
                    },
                    {
                        "name": "G. D√°lya"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "K. E. Darroch"
                    },
                    {
                        "name": "L. P. Dartez"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "S. Datta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "A. Daumas"
                    },
                    {
                        "name": "N. Davari"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "A. Davenport"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "T. F. Davies"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "L. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "P. Davis"
                    },
                    {
                        "name": "M. Dax"
                    },
                    {
                        "name": "J. De Bolle"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "U. Deka"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "S. Del√©glise"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "D. Dell'Aquila"
                    },
                    {
                        "name": "F. Della Valle"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "F. De Marco"
                    },
                    {
                        "name": "G. Demasi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "V. D'Emilio"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "N. DePergola"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "M. Desai"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "A. DeSimone"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "A. Dhani"
                    },
                    {
                        "name": "R. Diab"
                    },
                    {
                        "name": "M. C. D√≠az"
                    },
                    {
                        "name": "M. Di Cesare"
                    },
                    {
                        "name": "G. Dideron"
                    },
                    {
                        "name": "N. A. Didio"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "D. Diksha"
                    },
                    {
                        "name": "A. Di Michele"
                    },
                    {
                        "name": "J. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "N. Doerksen"
                    },
                    {
                        "name": "E. Dohmen"
                    },
                    {
                        "name": "D. Dominguez"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "T. Dooney"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "O. Dorosh"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "J. -G. Ducoin"
                    },
                    {
                        "name": "L. Dunn"
                    },
                    {
                        "name": "U. Dupletsa"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "H. Duval"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "B. Edelman"
                    },
                    {
                        "name": "T. B. Edo"
                    },
                    {
                        "name": "O. Edy"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "H. Einsle"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "R. A. Eisenstein"
                    },
                    {
                        "name": "A. Ejlli"
                    },
                    {
                        "name": "M. Emma"
                    },
                    {
                        "name": "K. Endo"
                    },
                    {
                        "name": "R. Enficiaud"
                    },
                    {
                        "name": "A. J. Engl"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. Espinosa"
                    },
                    {
                        "name": "M. Esposito"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. Estell√©s"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. Evstafyeva"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "J. M. Ezquiaga"
                    },
                    {
                        "name": "F. Fabrizi"
                    },
                    {
                        "name": "F. Faedi"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "R. Felicetti"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "D. L. Ferguson"
                    },
                    {
                        "name": "T. Fernandes"
                    },
                    {
                        "name": "D. Fernando"
                    },
                    {
                        "name": "S. Ferraiuolo"
                    },
                    {
                        "name": "I. Ferrante"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "A. Fiori"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "S. M. Fleischer"
                    },
                    {
                        "name": "L. S. Fleming"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "C. Foo"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "P. W. F. Forsyth"
                    },
                    {
                        "name": "K. Franceschetti"
                    },
                    {
                        "name": "N. Franchini"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "A. Frattale Mascioli"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "O. Freitas"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "W. Frischhertz"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. Fronz√©"
                    },
                    {
                        "name": "M. Fuentes-Garcia"
                    },
                    {
                        "name": "S. Fujii"
                    },
                    {
                        "name": "T. Fujimori"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "B. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "V. Galdi"
                    },
                    {
                        "name": "H. Gallagher"
                    },
                    {
                        "name": "B. Gallego"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "A. Gamboa"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "J. Garc√≠a-Bellido"
                    },
                    {
                        "name": "C. Garc√≠a N√∫√±ez"
                    },
                    {
                        "name": "C. Garc√≠a-Quir√≥s"
                    },
                    {
                        "name": "J. W. Gardner"
                    },
                    {
                        "name": "K. A. Gardner"
                    },
                    {
                        "name": "J. Gargiulo"
                    },
                    {
                        "name": "A. Garron"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "P. A. Garver"
                    },
                    {
                        "name": "C. Gasbarra"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "F. Gautier"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "T. Gayer"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "V. Gennari"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "R. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Sayantan Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "Suprovo Ghosh"
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "D. T. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "S. Gkaitatzis"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "F. Glotin"
                    },
                    {
                        "name": "J. Godfrey"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "A. S. Goettel"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "S. Gomez Lopez"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "Y. Gong"
                    },
                    {
                        "name": "G. Gonz√°lez"
                    },
                    {
                        "name": "P. Goodarzi"
                    },
                    {
                        "name": "S. Goode"
                    },
                    {
                        "name": "A. W. Goodwin-Jones"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "K. Govorkova"
                    },
                    {
                        "name": "S. Goyal"
                    },
                    {
                        "name": "B. Grace"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "V. Graham"
                    },
                    {
                        "name": "A. E. Granados"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "A. Gray"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "S. M. Green"
                    },
                    {
                        "name": "S. R. Green"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "E. M. Gretarsson"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "W. L. Griffiths"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "C. Grimaud"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "D. Guetta"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "F. Gulminelli"
                    },
                    {
                        "name": "A. M. Gunny"
                    },
                    {
                        "name": "H. Guo"
                    },
                    {
                        "name": "W. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anchal Gupta"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "I. Gupta"
                    },
                    {
                        "name": "N. C. Gupta"
                    },
                    {
                        "name": "P. Gupta"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "T. Gupta"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "N. Gupte"
                    },
                    {
                        "name": "J. Gurs"
                    },
                    {
                        "name": "N. Gutierrez"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "D. Haba"
                    },
                    {
                        "name": "M. Haberland"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "R. Hamburg"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "W. -B. Han"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. A. Hannuksela"
                    },
                    {
                        "name": "A. G. Hanselman"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "R. Harada"
                    },
                    {
                        "name": "A. R. Hardison"
                    },
                    {
                        "name": "S. Harikumar"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "T. Harmark"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "J. Hart"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "C. -J. Haster"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "R. Hayes"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "A. Heffernan"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "O. Henderson-Sapir"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "C. Henshaw"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "J. Heyns"
                    },
                    {
                        "name": "S. Higginbotham"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "S. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "C. Hirose"
                    },
                    {
                        "name": "S. Hochheim"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "L. Honet"
                    },
                    {
                        "name": "C. Hong"
                    },
                    {
                        "name": "S. Hoshino"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "N. T. Howard"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "C. A. Hrishikesh"
                    },
                    {
                        "name": "H. -F. Hsieh"
                    },
                    {
                        "name": "H. -Y. Hsieh"
                    },
                    {
                        "name": "C. Hsiung"
                    },
                    {
                        "name": "W. -F. Hsu"
                    },
                    {
                        "name": "Q. Hu"
                    },
                    {
                        "name": "H. Y. Huang"
                    },
                    {
                        "name": "Y. Huang"
                    },
                    {
                        "name": "Y. T. Huang"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "D. C. Y. Hui"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "L. Iampieri"
                    },
                    {
                        "name": "G. A. Iandolo"
                    },
                    {
                        "name": "M. Ianni"
                    },
                    {
                        "name": "A. Ierardi"
                    },
                    {
                        "name": "A. Iess"
                    },
                    {
                        "name": "H. Imafuku"
                    },
                    {
                        "name": "K. Inayoshi"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "G. Iorio"
                    },
                    {
                        "name": "P. Iosif"
                    },
                    {
                        "name": "M. H. Iqbal"
                    },
                    {
                        "name": "J. Irwin"
                    },
                    {
                        "name": "R. Ishikawa"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "H. Iwanaga"
                    },
                    {
                        "name": "M. Iwaya"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "C. Jacquet"
                    },
                    {
                        "name": "P. -E. Jacquet"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "T. Jain"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "P. A. James"
                    },
                    {
                        "name": "R. Jamshidi"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "K. Janssens"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "S. Jaraba"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "W. Javed"
                    },
                    {
                        "name": "A. Jennings"
                    },
                    {
                        "name": "S. J. Jin"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "C. Johanson"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "N. A. Johnson"
                    },
                    {
                        "name": "N. K. Johnson-McDaniel"
                    },
                    {
                        "name": "M. C. Johnston"
                    },
                    {
                        "name": "R. Johnston"
                    },
                    {
                        "name": "N. Johny"
                    },
                    {
                        "name": "D. H. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "E. J. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "S. Jose"
                    },
                    {
                        "name": "P. Joshi"
                    },
                    {
                        "name": "S. K. Joshi"
                    },
                    {
                        "name": "J. Ju"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "K. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "H. B. Kabagoz"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "I. Kaku"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "M. Kalomenopoulos"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "N. C. Kannachel"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "S. Karat"
                    },
                    {
                        "name": "R. Kashyap"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "W. Kastaun"
                    },
                    {
                        "name": "T. Kato"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "R. Kaushik"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "R. Kawamoto"
                    },
                    {
                        "name": "A. Kazemi"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "J. Kennington"
                    },
                    {
                        "name": "R. Kesharwani"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "R. Khadela"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "F. Khan"
                    },
                    {
                        "name": "I. Khan"
                    },
                    {
                        "name": "T. Khanam"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. M. Khusid"
                    },
                    {
                        "name": "W. Kiendrebeogo"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "M. H. Kim"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "M. Kinley-Hanlon"
                    },
                    {
                        "name": "M. Kinnear"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "N. Knust"
                    },
                    {
                        "name": "K. Kobayashi"
                    },
                    {
                        "name": "P. Koch"
                    },
                    {
                        "name": "S. M. Koehlenbeck"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "K. Kohri"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "R. V. Kossak"
                    },
                    {
                        "name": "X. Kou"
                    },
                    {
                        "name": "A. Koushik"
                    },
                    {
                        "name": "N. Kouvatsos"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "S. L. Kranzhoff"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "A. Kr√≥lak"
                    },
                    {
                        "name": "K. Kruska"
                    },
                    {
                        "name": "J. Kubisz"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "S. Kulkarni"
                    },
                    {
                        "name": "A. Kulur Ramamohan"
                    },
                    {
                        "name": "A. Kumar"
                    },
                    {
                        "name": "Praveen Kumar"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "N. Kuntimaddi"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "S. Kuwahara"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "K. Kwan"
                    },
                    {
                        "name": "J. Kwok"
                    },
                    {
                        "name": "G. Lacaille"
                    },
                    {
                        "name": "P. Lagabbe"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "S. Lai"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "M. Lalleman"
                    },
                    {
                        "name": "P. C. Lalremruati"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "R. Langgin"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "A. La Rana"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "J. Larsen"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "J. Lawrence"
                    },
                    {
                        "name": "M. N. Lawrence"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "C. Lazarte"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "L. Leali"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. -K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "Sungho Lee"
                    },
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Y. Lee"
                    },
                    {
                        "name": "I. N. Legred"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "L. Lehner"
                    },
                    {
                        "name": "M. Le Jean"
                    },
                    {
                        "name": "A. Lema√Ætre"
                    },
                    {
                        "name": "M. Lenti"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "M. Lequime"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "M. Lesovsky"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "M. Lethuillier"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Z. Li"
                    },
                    {
                        "name": "A. Lihos"
                    },
                    {
                        "name": "C-Y. Lin"
                    },
                    {
                        "name": "E. T. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "Y. -C. Lin"
                    },
                    {
                        "name": "C. Lindsay"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "T. B. Littenberg"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "F. Llamas Villarreal"
                    },
                    {
                        "name": "J. Llobera-Querol"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "J. -P. Locquet"
                    },
                    {
                        "name": "M. R. Loizou"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "A. Lorenzo-Medina"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "E. Lotti"
                    },
                    {
                        "name": "T. P. Lott IV"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "H. A. Loughlin"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "N. Low"
                    },
                    {
                        "name": "M. J. Lowry"
                    },
                    {
                        "name": "N. Lu"
                    },
                    {
                        "name": "L. Lucchesi"
                    },
                    {
                        "name": "H. L√ºck"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "A. W. Lussier"
                    },
                    {
                        "name": "L. -T. Ma"
                    },
                    {
                        "name": "S. Ma"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "A. Macedo"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "R. R. Maciy"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "D. Macri"
                    },
                    {
                        "name": "K. Maeda"
                    },
                    {
                        "name": "S. Maenaut"
                    },
                    {
                        "name": "S. S. Magare"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "E. Maggio"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "M. Mahesh"
                    },
                    {
                        "name": "M. Maini"
                    },
                    {
                        "name": "S. Majhi"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "C. N. Makarem"
                    },
                    {
                        "name": "D. Malakar"
                    },
                    {
                        "name": "J. A. Malaquias-Reis"
                    },
                    {
                        "name": "U. Mali"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "L. Mallick"
                    },
                    {
                        "name": "A. Malz"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "B. Mannix"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "G. Mansingh"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "F. Marchesoni"
                    },
                    {
                        "name": "C. Marinelli"
                    },
                    {
                        "name": "D. Mar√≠n Pina"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "S. M√°rka"
                    },
                    {
                        "name": "Z. M√°rka"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "B. B. Martinez"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "A. Martini"
                    },
                    {
                        "name": "J. C. Martins"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "L. Massaro"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "M. Mastrodicasa"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "T. Matcovich"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "M. Matsuyama"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "N. Maxwell"
                    },
                    {
                        "name": "G. McCarrol"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "S. McEachin"
                    },
                    {
                        "name": "C. McElhenny"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "J. McGinn"
                    },
                    {
                        "name": "K. B. M. McGowan"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "A. McLeod"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "M. Melching"
                    },
                    {
                        "name": "S. Mellaerts"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "F. Mera"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. R. M√©rou"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "C. Messenger"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "B. Mestichelli"
                    },
                    {
                        "name": "M. Meyer-Conde"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "I. Michaloliakos"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "S. J. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "V. Milotti"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "N. Mio"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "L. Mirasola"
                    },
                    {
                        "name": "M. Miravet-Ten√©s"
                    },
                    {
                        "name": "C. -A. Miritescu"
                    },
                    {
                        "name": "A. K. Mishra"
                    },
                    {
                        "name": "A. Mishra"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "A. L. Mitchell"
                    },
                    {
                        "name": "J. G. Mitchell"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "S. Miyamoto"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "G. Mo"
                    },
                    {
                        "name": "L. Mobilia"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "C. Mondal"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "S. More"
                    },
                    {
                        "name": "E. A. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "G. Morras"
                    },
                    {
                        "name": "A. Moscatello"
                    },
                    {
                        "name": "M. Mould"
                    },
                    {
                        "name": "P. Mourier"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Samanwaya Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "H. Mullock"
                    },
                    {
                        "name": "J. Munch"
                    },
                    {
                        "name": "J. Mundi"
                    },
                    {
                        "name": "C. L. Mungioli"
                    },
                    {
                        "name": "Y. Murakami"
                    },
                    {
                        "name": "M. Murakoshi"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "S. Muusse"
                    },
                    {
                        "name": "D. Nabari"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "N. Nagarajan"
                    },
                    {
                        "name": "K. Nakagaki"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "D. Nanadoumgar-Lacroze"
                    },
                    {
                        "name": "D. Nandi"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "P. Narayan"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "H. Narola"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "A. Nela"
                    },
                    {
                        "name": "A. Nelson"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "S. Ng"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "A. B. Nielsen"
                    },
                    {
                        "name": "G. Nieradka"
                    },
                    {
                        "name": "Y. Nishino"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "E. Nitoglia"
                    },
                    {
                        "name": "W. Niu"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "J. Novak"
                    },
                    {
                        "name": "J. F. Nu√±o Siles"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "K. Obayashi"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "M. Oertel"
                    },
                    {
                        "name": "A. Offermans"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "J. J. Oh"
                    },
                    {
                        "name": "K. Oh"
                    },
                    {
                        "name": "T. O'Hanlon"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "M. Ohkawa"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "R. Oliveri"
                    },
                    {
                        "name": "R. Omer"
                    },
                    {
                        "name": "B. O'Neal"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "R. Oram"
                    },
                    {
                        "name": "N. D. Ormsby"
                    },
                    {
                        "name": "M. Orselli"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "S. O'Shea"
                    },
                    {
                        "name": "Y. Oshima"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "I. Ota"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "A. Ouzriat"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "B. J. Owen"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "R. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "L. Paiella"
                    },
                    {
                        "name": "A. Pal"
                    },
                    {
                        "name": "S. Pal"
                    },
                    {
                        "name": "M. A. Palaia"
                    },
                    {
                        "name": "M. P√°lfi"
                    },
                    {
                        "name": "P. P. Palma"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "P. Palud"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "K. C. Pan"
                    },
                    {
                        "name": "R. Panai"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "Shiksha Pandey"
                    },
                    {
                        "name": "Swadha Pandey"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "K. A. Pannone"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "A. Papadopoulos"
                    },
                    {
                        "name": "E. E. Papalexakis"
                    },
                    {
                        "name": "L. Papalini"
                    },
                    {
                        "name": "G. Papigkiotis"
                    },
                    {
                        "name": "A. Paquis"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "B. -J. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "G. Pascale"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "L. Passenger"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "O. Patane"
                    },
                    {
                        "name": "D. Pathak"
                    },
                    {
                        "name": "L. Pathak"
                    },
                    {
                        "name": "A. Patra"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "A. S. Patron"
                    },
                    {
                        "name": "B. G. Patterson"
                    },
                    {
                        "name": "K. Paul"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "T. Pearce"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. Pe√±a Arellano"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "M. D. Penuliar"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "Z. Pereira"
                    },
                    {
                        "name": "J. J. Perez"
                    },
                    {
                        "name": "C. P√©rigois"
                    },
                    {
                        "name": "G. Perna"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "J. Perret"
                    },
                    {
                        "name": "S. Perri√®s"
                    },
                    {
                        "name": "J. W. Perry"
                    },
                    {
                        "name": "D. Pesios"
                    },
                    {
                        "name": "S. Petracca"
                    },
                    {
                        "name": "C. Petrillo"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "H. Phurailatpam"
                    },
                    {
                        "name": "M. Piarulli"
                    },
                    {
                        "name": "L. Piccari"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "G. Pierra"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "M. Pietrzak"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "B. J. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "A. Placidi"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "M. L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Plunkett"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "L. Pompili"
                    },
                    {
                        "name": "J. Poon"
                    },
                    {
                        "name": "E. Porcelli"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "C. Posnansky"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "B. K. Pradhan"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "P. Prasia"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "G. Principe"
                    },
                    {
                        "name": "M. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "L. Prokhorov"
                    },
                    {
                        "name": "P. Prosperi"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "A. C. Providence"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "J. Pullin"
                    },
                    {
                        "name": "M. Punturo"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. P√ºrrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "J. Qin"
                    },
                    {
                        "name": "G. Qu√©m√©ner"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "P. J. Quinonez"
                    },
                    {
                        "name": "F. J. Raab"
                    },
                    {
                        "name": "I. Rainho"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "B. Rajbhandari"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "F. A. Ramis Vidal"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "D. Rana"
                    },
                    {
                        "name": "S. Ranjan"
                    },
                    {
                        "name": "K. Ransom"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "B. Ratto"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "M. Recaman Payo"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "L. Rei"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "P. Relton"
                    },
                    {
                        "name": "A. I. Renzini"
                    },
                    {
                        "name": "A. Renzini"
                    },
                    {
                        "name": "B. Revenu"
                    },
                    {
                        "name": "R. Reyes"
                    },
                    {
                        "name": "A. S. Rezaei"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "M. Ricci"
                    },
                    {
                        "name": "A. Ricciardone"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "M. Richardson"
                    },
                    {
                        "name": "A. Rijal"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "H. K. Riley"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "J. Rittmeyer"
                    },
                    {
                        "name": "C. Robertson"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "M. Robinson"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "A. E. Romano"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "A. Romero"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "T. J. Roocke"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "T. J. Rosauer"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. Rosi≈Ñska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "M. Rossello-Sastre"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "S. K. Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "N. Ruhama"
                    },
                    {
                        "name": "E. Ruiz Morales"
                    },
                    {
                        "name": "K. Ruiz-Rocha"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "J. Sadiq"
                    },
                    {
                        "name": "P. Saffarieh"
                    },
                    {
                        "name": "S. Safi-Harb"
                    },
                    {
                        "name": "M. R. Sah"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "T. Sainrat"
                    },
                    {
                        "name": "S. Sajith Menon"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "S. Sakon"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "F. Salces-Carcoba"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "M. Sall√©"
                    },
                    {
                        "name": "S. U. Salunkhe"
                    },
                    {
                        "name": "S. Salvador"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "A. Sanchez"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "J. H. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "E. M. S√§nger"
                    },
                    {
                        "name": "F. Santoliquido"
                    },
                    {
                        "name": "F. Sarandrea"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "P. Sarkar"
                    },
                    {
                        "name": "S. Sasaoka"
                    },
                    {
                        "name": "A. Sasli"
                    },
                    {
                        "name": "P. Sassi"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "Y. Sato"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "V. Scacco"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "A. Schiebelbein"
                    },
                    {
                        "name": "M. G. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "K. Schouteden"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "M. Scialpi"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "R. M. Sedas"
                    },
                    {
                        "name": "T. C. Seetharamu"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "D. Sentenac"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "J. W. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "M. Serra"
                    },
                    {
                        "name": "G. Servignat"
                    },
                    {
                        "name": "A. Sevrin"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "U. S. Shah"
                    },
                    {
                        "name": "M. S. Shahriar"
                    },
                    {
                        "name": "M. A. Shaikh"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. Sharma"
                    },
                    {
                        "name": "A. K. Sharma"
                    },
                    {
                        "name": "P. Sharma"
                    },
                    {
                        "name": "S. Sharma Chaudhary"
                    },
                    {
                        "name": "M. R. Shaw"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "Y. Shikano"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "K. Shimode"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "J. Shiota"
                    },
                    {
                        "name": "S. Shirke"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "R. W. Short"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "A. Sider"
                    },
                    {
                        "name": "H. Siegel"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. Silenzi"
                    },
                    {
                        "name": "M. Simmonds"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "A. Singh"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "M. K. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "S. Singh"
                    },
                    {
                        "name": "A. Singha"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "D. A. Slater"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "L. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "W. J. Smith"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "I. Song"
                    },
                    {
                        "name": "K. Soni"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "H. Sotani"
                    },
                    {
                        "name": "A. Southgate"
                    },
                    {
                        "name": "F. Spada"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "M. Spera"
                    },
                    {
                        "name": "P. Spinicelli"
                    },
                    {
                        "name": "C. A. Sprague"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "F. Stachurski"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "N. Steinle"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "N. Stergioulas"
                    },
                    {
                        "name": "P. Stevens"
                    },
                    {
                        "name": "S. P. Stevenson"
                    },
                    {
                        "name": "F. Stolzi"
                    },
                    {
                        "name": "M. StPierre"
                    },
                    {
                        "name": "G. Stratta"
                    },
                    {
                        "name": "M. D. Strong"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "R. Sturani"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "M. Suchenek"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "N. Sueltmann"
                    },
                    {
                        "name": "L. Suleiman"
                    },
                    {
                        "name": "J. M. Sullivan"
                    },
                    {
                        "name": "K. D. Sullivan"
                    },
                    {
                        "name": "J. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "B. J. Sutton"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "T. Suzuki"
                    },
                    {
                        "name": "Y. Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "A. Syx"
                    },
                    {
                        "name": "M. J. Szczepa≈Ñczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "S. C. Tait"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "A. Takamori"
                    },
                    {
                        "name": "T. Takase"
                    },
                    {
                        "name": "K. Takatani"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "K. Takeshita"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "M. Tamaki"
                    },
                    {
                        "name": "N. Tamanini"
                    },
                    {
                        "name": "D. Tanabe"
                    },
                    {
                        "name": "K. Tanaka"
                    },
                    {
                        "name": "S. J. Tanaka"
                    },
                    {
                        "name": "T. Tanaka"
                    },
                    {
                        "name": "D. Tang"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "W. Tanner"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "R. D. Tapia"
                    },
                    {
                        "name": "E. N. Tapia San Mart√≠n"
                    },
                    {
                        "name": "R. Tarafder"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "A. Taruya"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "J. G. Tau"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "H. Themann"
                    },
                    {
                        "name": "A. Theodoropoulos"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "L. M. Thomas"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "S. Tibrewal"
                    },
                    {
                        "name": "J. Tissino"
                    },
                    {
                        "name": "A. Tiwari"
                    },
                    {
                        "name": "P. Tiwari"
                    },
                    {
                        "name": "S. Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "M. R. Todd"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "K. Toland"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "K. Tomita"
                    },
                    {
                        "name": "V. Tommasini"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "H. Tong"
                    },
                    {
                        "name": "C. Tong-Yu"
                    },
                    {
                        "name": "A. Toriyama"
                    },
                    {
                        "name": "N. Toropov"
                    },
                    {
                        "name": "A. Torres-Forn√©"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "M. Toscani"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "E. Tournefier"
                    },
                    {
                        "name": "M. Trad Nery"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "C. Trejo"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "G. Troian"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "T. T. L. Tsang"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "C. Turski"
                    },
                    {
                        "name": "H. Ubach"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "M. Uematsu"
                    },
                    {
                        "name": "S. Ueno"
                    },
                    {
                        "name": "V. Undheim"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "M. Vacatello"
                    },
                    {
                        "name": "H. Vahlbruch"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "G. Valdes"
                    },
                    {
                        "name": "J. Valencia"
                    },
                    {
                        "name": "A. F. Valentini"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "S. A. Vallejo-Pe√±a"
                    },
                    {
                        "name": "S. Vallero"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "N. van Bakel"
                    },
                    {
                        "name": "M. van Beuzekom"
                    },
                    {
                        "name": "M. van Dael"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "D. C. Vander-Hyde"
                    },
                    {
                        "name": "M. van der Sluys"
                    },
                    {
                        "name": "A. Van de Walle"
                    },
                    {
                        "name": "J. van Dongen"
                    },
                    {
                        "name": "K. Vandra"
                    },
                    {
                        "name": "H. van Haevermaet"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "P. Van Hove"
                    },
                    {
                        "name": "J. Vanier"
                    },
                    {
                        "name": "M. VanKeuren"
                    },
                    {
                        "name": "J. Vanosky"
                    },
                    {
                        "name": "M. H. P. M. van Putten"
                    },
                    {
                        "name": "Z. Van Ranst"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "J. J. Varghese"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "A. N. Vazquez"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "S. Venikoudis"
                    },
                    {
                        "name": "J. Venneberg"
                    },
                    {
                        "name": "P. Verdier"
                    },
                    {
                        "name": "M. Vereecken"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "B. Verma"
                    },
                    {
                        "name": "P. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "S. M. Vermeulen"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Veutro"
                    },
                    {
                        "name": "A. M. Vibhute"
                    },
                    {
                        "name": "A. Vicer√©"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "A. Vilkha"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "E. T. Vincent"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "S. Viret"
                    },
                    {
                        "name": "A. Virtuoso"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "A. Vives"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "D. Voigt"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "L. Vujeva"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "J. Wack"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "A. Wajid"
                    },
                    {
                        "name": "M. Walker"
                    },
                    {
                        "name": "G. S. Wallace"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "E. J. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "Y. F. Wang"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "G. Waratkar"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "D. Watarai"
                    },
                    {
                        "name": "K. E. Wayt"
                    },
                    {
                        "name": "B. R. Weaver"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "C. R. Weaving"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "N. L. Weickhardt"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "F. Wellmann"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "P. We√üels"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "E. G. Wickens"
                    },
                    {
                        "name": "J. B. Wildberger"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "D. J. Willadsen"
                    },
                    {
                        "name": "K. Willetts"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "N. S. Williams"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "M. Wils"
                    },
                    {
                        "name": "C. W. Winborn"
                    },
                    {
                        "name": "J. Winterflood"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "N. E. Wolfe"
                    },
                    {
                        "name": "H. T. Wong"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "J. L. Wright"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "E. Wuchner"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "V. A. Xu"
                    },
                    {
                        "name": "Y. Xu"
                    },
                    {
                        "name": "N. Yadav"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "K. Yamamoto"
                    },
                    {
                        "name": "T. S. Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "S. Yamamura"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "T. Yan"
                    },
                    {
                        "name": "F. W. Yang"
                    },
                    {
                        "name": "F. Yang"
                    },
                    {
                        "name": "K. Z. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Z. Yarbrough"
                    },
                    {
                        "name": "H. Yasui"
                    },
                    {
                        "name": "S. -W. Yeh"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "X. Yin"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "J. Yoo"
                    },
                    {
                        "name": "H. Yu"
                    },
                    {
                        "name": "S. Yuan"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "A. Zadro≈ºny"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "M. Zeeshan"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zeoli"
                    },
                    {
                        "name": "M. Zerrad"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "A. C. Zhang"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "R. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Y. Zheng"
                    },
                    {
                        "name": "H. Zhong"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "X. -J. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.15022v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.15022v4",
                "updated": "2025-08-25T14:43:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    43,
                    13,
                    0,
                    237,
                    0
                ],
                "published": "2023-08-29T04:59:53Z",
                "published_parsed": [
                    2023,
                    8,
                    29,
                    4,
                    59,
                    53,
                    1,
                    241,
                    0
                ],
                "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models"
                },
                "summary": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts are released."
                },
                "authors": [
                    {
                        "name": "Qingyue Wang"
                    },
                    {
                        "name": "Yanhe Fu"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "arxiv_comment": "This paper has been accepted by Neurocomputing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.15022v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.15022v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18076v1",
                "updated": "2025-08-25T14:43:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    43,
                    10,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:43:10Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    43,
                    10,
                    0,
                    237,
                    0
                ],
                "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges"
                },
                "summary": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG."
                },
                "authors": [
                    {
                        "name": "Khaoula Chehbouni"
                    },
                    {
                        "name": "Mohammed Haddou"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "Prepared for conference submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18075v1",
                "updated": "2025-08-25T14:40:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    40,
                    6,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:40:06Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    40,
                    6,
                    0,
                    237,
                    0
                ],
                "title": "Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype\n  Learning and Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype\n  Learning and Clustering"
                },
                "summary": "Open-set few-shot hyperspectral image (HSI) classification aims to classify\nimage pixels by using few labeled pixels per class, where the pixels to be\nclassified may be not all from the classes that have been seen. To address the\nopen-set HSI classification challenge, current methods focus mainly on\ndistinguishing the unknown class samples from the known class samples and\nrejecting them to increase the accuracy of identifying known class samples.\nThey fails to further identify or discovery the unknow classes among the\nsamples. This paper proposes a prototype learning and clustering method for\ndiscoverying unknown classes in HSIs under the few-shot environment. Using few\nlabeled samples, it strives to develop the ability of infering the prototypes\nof unknown classes while distinguishing unknown classes from known classes.\nOnce the unknown class samples are rejected by the learned known class\nclassifier, the proposed method can further cluster the unknown class samples\ninto different classes according to their distance to the inferred unknown\nclass prototypes. Compared to existing state-of-the-art methods, extensive\nexperiments on four benchmark HSI datasets demonstrate that our proposed method\nexhibits competitive performance in open-set few-shot HSI classification tasks.\nAll the codes are available at \\href{https://github.com/KOBEN-ff/OpenFUCD-main}\n{https://github.com/KOBEN-ff/OpenFUCD-main}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-set few-shot hyperspectral image (HSI) classification aims to classify\nimage pixels by using few labeled pixels per class, where the pixels to be\nclassified may be not all from the classes that have been seen. To address the\nopen-set HSI classification challenge, current methods focus mainly on\ndistinguishing the unknown class samples from the known class samples and\nrejecting them to increase the accuracy of identifying known class samples.\nThey fails to further identify or discovery the unknow classes among the\nsamples. This paper proposes a prototype learning and clustering method for\ndiscoverying unknown classes in HSIs under the few-shot environment. Using few\nlabeled samples, it strives to develop the ability of infering the prototypes\nof unknown classes while distinguishing unknown classes from known classes.\nOnce the unknown class samples are rejected by the learned known class\nclassifier, the proposed method can further cluster the unknown class samples\ninto different classes according to their distance to the inferred unknown\nclass prototypes. Compared to existing state-of-the-art methods, extensive\nexperiments on four benchmark HSI datasets demonstrate that our proposed method\nexhibits competitive performance in open-set few-shot HSI classification tasks.\nAll the codes are available at \\href{https://github.com/KOBEN-ff/OpenFUCD-main}\n{https://github.com/KOBEN-ff/OpenFUCD-main}"
                },
                "authors": [
                    {
                        "name": "Chun Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18048v1",
                "updated": "2025-08-25T14:06:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    6,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:06:27Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    6,
                    27,
                    0,
                    237,
                    0
                ],
                "title": "HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data"
                },
                "summary": "User queries in real-world recommendation systems often combine structured\nconstraints (e.g., category, attributes) with unstructured preferences (e.g.,\nproduct descriptions or reviews). We introduce HyST (Hybrid retrieval over\nSemi-structured Tabular data), a hybrid retrieval framework that combines\nLLM-powered structured filtering with semantic embedding search to support\ncomplex information needs over semi-structured tabular data. HyST extracts\nattribute-level constraints from natural language using large language models\n(LLMs) and applies them as metadata filters, while processing the remaining\nunstructured query components via embedding-based retrieval. Experiments on a\nsemi-structured benchmark show that HyST consistently outperforms tradtional\nbaselines, highlighting the importance of structured filtering in improving\nretrieval precision, offering a scalable and accurate solution for real-world\nuser queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User queries in real-world recommendation systems often combine structured\nconstraints (e.g., category, attributes) with unstructured preferences (e.g.,\nproduct descriptions or reviews). We introduce HyST (Hybrid retrieval over\nSemi-structured Tabular data), a hybrid retrieval framework that combines\nLLM-powered structured filtering with semantic embedding search to support\ncomplex information needs over semi-structured tabular data. HyST extracts\nattribute-level constraints from natural language using large language models\n(LLMs) and applies them as metadata filters, while processing the remaining\nunstructured query components via embedding-based retrieval. Experiments on a\nsemi-structured benchmark show that HyST consistently outperforms tradtional\nbaselines, highlighting the importance of structured filtering in improving\nretrieval precision, offering a scalable and accurate solution for real-world\nuser queries."
                },
                "authors": [
                    {
                        "name": "Jiyoon Myung"
                    },
                    {
                        "name": "Jihyeon Park"
                    },
                    {
                        "name": "Joohyung Han"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Han"
                },
                "author": "Joohyung Han",
                "arxiv_comment": "Accepted at the 2nd EARL Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models (RecSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03001v3",
                "updated": "2025-08-25T14:06:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    6,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2024-08-06T07:19:51Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    19,
                    51,
                    1,
                    219,
                    0
                ],
                "title": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning"
                },
                "summary": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yu Song"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Jihong Hu"
                    },
                    {
                        "name": "Yen-Wei Chen"
                    },
                    {
                        "name": "Lanfen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lanfen Lin"
                },
                "author": "Lanfen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18596v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18596v4",
                "updated": "2025-08-26T10:08:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    8,
                    51,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-24T08:44:33Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    8,
                    44,
                    33,
                    5,
                    144,
                    0
                ],
                "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models"
                },
                "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication."
                },
                "authors": [
                    {
                        "name": "Chen Han"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xijin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xijin Tang"
                },
                "author": "Xijin Tang",
                "arxiv_comment": "This paper has been accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18596v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05214v3",
                "updated": "2025-08-25T14:03:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    3,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-04-07T16:01:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "Post-Training Language Models for Continual Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Language Models for Continual Relation Extraction"
                },
                "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction."
                },
                "authors": [
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Adrian Paschke"
                    },
                    {
                        "name": "Sonja Schimmler"
                    }
                ],
                "author_detail": {
                    "name": "Sonja Schimmler"
                },
                "author": "Sonja Schimmler",
                "arxiv_comment": "17 pages, Initial Results and Reporting of the work. This work has\n  been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18046v1",
                "updated": "2025-08-25T14:02:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    2,
                    55,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:02:55Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    2,
                    55,
                    0,
                    237,
                    0
                ],
                "title": "All-Sky Imaging with Vector Sensor Interferometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-Sky Imaging with Vector Sensor Interferometry"
                },
                "summary": "Radio astronomy observations at frequencies below 10~MHz could provide\nvaluable science, such as measuring the cosmic dark age signal in the\nredshifted 21~cm hydrogen absorption line, detecting exoplanetary auroral\nemissions which lead to inferences about magnetic fields and atmospheres, and\ncharacterizing the effects of solar wind and coronal mass ejections on the\nmagnetospheres of solar system planets. Despite their value, few resolved\nmeasurements in the sub-10~MHz band have been made. At frequencies below\n10~MHz, the Earth's ionosphere reflects, attenuates, and distorts radio waves,\nmaking radio astronomy in this band possible only from space. However, a\nspace-borne array would need thousands of electrically-small antennas to reach\nthe sensitivity required for detecting faint astronomical signals, and it would\nneed to be positioned far from the Earth to reduce the impact of Earth-based\nradio interference. Using more efficient antennas would minimize the number\nneeded, and using antennas that are more resilient to interference would reduce\nthe required distance from Earth. To this end, we consider constructing a low\nfrequency array out of vector sensor antennas. These advanced antennas consist\nof three orthogonal dipole and three orthogonal loop antennas with a common\nphase centre. Their benefits include direction-finding and polarimetric\ncapabilities, but they have not been considered for this application\npreviously. We show that vector sensors can provide four times more Fisher\ninformation during interferometry than tripoles, simpler antennas that are\ncommonly considered for space applications. We also present an all-sky imaging\nsimulation to demonstrate a spherical harmonic imaging technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio astronomy observations at frequencies below 10~MHz could provide\nvaluable science, such as measuring the cosmic dark age signal in the\nredshifted 21~cm hydrogen absorption line, detecting exoplanetary auroral\nemissions which lead to inferences about magnetic fields and atmospheres, and\ncharacterizing the effects of solar wind and coronal mass ejections on the\nmagnetospheres of solar system planets. Despite their value, few resolved\nmeasurements in the sub-10~MHz band have been made. At frequencies below\n10~MHz, the Earth's ionosphere reflects, attenuates, and distorts radio waves,\nmaking radio astronomy in this band possible only from space. However, a\nspace-borne array would need thousands of electrically-small antennas to reach\nthe sensitivity required for detecting faint astronomical signals, and it would\nneed to be positioned far from the Earth to reduce the impact of Earth-based\nradio interference. Using more efficient antennas would minimize the number\nneeded, and using antennas that are more resilient to interference would reduce\nthe required distance from Earth. To this end, we consider constructing a low\nfrequency array out of vector sensor antennas. These advanced antennas consist\nof three orthogonal dipole and three orthogonal loop antennas with a common\nphase centre. Their benefits include direction-finding and polarimetric\ncapabilities, but they have not been considered for this application\npreviously. We show that vector sensors can provide four times more Fisher\ninformation during interferometry than tripoles, simpler antennas that are\ncommonly considered for space applications. We also present an all-sky imaging\nsimulation to demonstrate a spherical harmonic imaging technique."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kononov"
                    },
                    {
                        "name": "Mary Knapp"
                    }
                ],
                "author_detail": {
                    "name": "Mary Knapp"
                },
                "arxiv_affiliation": "MIT Haystack Observatory",
                "author": "Mary Knapp",
                "arxiv_doi": "10.1093/rasti/rzaf038",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/rasti/rzaf038",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "RAS Techniques and Instruments (RASTI), accepted; 9 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18688v2",
                "updated": "2025-08-25T14:01:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    1,
                    55,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-24T13:19:03Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    19,
                    3,
                    5,
                    144,
                    0
                ],
                "title": "Large Language Models in the Task of Automatic Validation of Text\n  Classifier Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in the Task of Automatic Validation of Text\n  Classifier Predictions"
                },
                "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning."
                },
                "authors": [
                    {
                        "name": "Aleksandr Tsymbalov"
                    },
                    {
                        "name": "Mikhail Khovrichev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Khovrichev"
                },
                "author": "Mikhail Khovrichev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07165v2",
                "updated": "2025-08-25T14:01:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    1,
                    22,
                    0,
                    237,
                    0
                ],
                "published": "2025-01-13T09:51:23Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    51,
                    23,
                    0,
                    13,
                    0
                ],
                "title": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study"
                },
                "summary": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field."
                },
                "authors": [
                    {
                        "name": "Huashan Chen"
                    },
                    {
                        "name": "Zisheng Huang"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Jinfu Chen"
                    },
                    {
                        "name": "Haotang Li"
                    },
                    {
                        "name": "Kebin Peng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Sen He"
                    }
                ],
                "author_detail": {
                    "name": "Sen He"
                },
                "author": "Sen He",
                "arxiv_doi": "10.1007/s10515-025-00536-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10515-025-00536-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18040v1",
                "updated": "2025-08-25T13:57:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    57,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:57:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    57,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and\n  Exploration"
                },
                "summary": "Vision language model (VLM)-based mobile agents show great potential for\nassisting users in performing instruction-driven tasks. However, these agents\ntypically struggle with personalized instructions -- those containing\nambiguous, user-specific context -- a challenge that has been largely\noverlooked in previous research. In this paper, we define personalized\ninstructions and introduce PerInstruct, a novel human-annotated dataset\ncovering diverse personalized instructions across various mobile scenarios.\nFurthermore, given the limited personalization capabilities of existing mobile\nagents, we propose PerPilot, a plug-and-play framework powered by large\nlanguage models (LLMs) that enables mobile agents to autonomously perceive,\nunderstand, and execute personalized user instructions. PerPilot identifies\npersonalized elements and autonomously completes instructions via two\ncomplementary approaches: memory-based retrieval and reasoning-based\nexploration. Experimental results demonstrate that PerPilot effectively handles\npersonalized tasks with minimal user intervention and progressively improves\nits performance with continued use, underscoring the importance of\npersonalization-aware reasoning for next-generation mobile agents. The dataset\nand code are available at: https://github.com/xinwang-nwpu/PerPilot",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language model (VLM)-based mobile agents show great potential for\nassisting users in performing instruction-driven tasks. However, these agents\ntypically struggle with personalized instructions -- those containing\nambiguous, user-specific context -- a challenge that has been largely\noverlooked in previous research. In this paper, we define personalized\ninstructions and introduce PerInstruct, a novel human-annotated dataset\ncovering diverse personalized instructions across various mobile scenarios.\nFurthermore, given the limited personalization capabilities of existing mobile\nagents, we propose PerPilot, a plug-and-play framework powered by large\nlanguage models (LLMs) that enables mobile agents to autonomously perceive,\nunderstand, and execute personalized user instructions. PerPilot identifies\npersonalized elements and autonomously completes instructions via two\ncomplementary approaches: memory-based retrieval and reasoning-based\nexploration. Experimental results demonstrate that PerPilot effectively handles\npersonalized tasks with minimal user intervention and progressively improves\nits performance with continued use, underscoring the importance of\npersonalization-aware reasoning for next-generation mobile agents. The dataset\nand code are available at: https://github.com/xinwang-nwpu/PerPilot"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zhiyao Cui"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Ya Zeng"
                    },
                    {
                        "name": "Chenxu Wang"
                    },
                    {
                        "name": "Ruiqi Song"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Jinzhuo Liu"
                    },
                    {
                        "name": "Siyue Ren"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Wang"
                },
                "author": "Zhen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05544v2",
                "updated": "2025-08-25T13:56:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    56,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-08T17:07:00Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    7,
                    0,
                    3,
                    128,
                    0
                ],
                "title": "Unbinned inclusive cross-section measurements with machine-learned\n  systematic uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbinned inclusive cross-section measurements with machine-learned\n  systematic uncertainties"
                },
                "summary": "We introduce a novel methodology for addressing systematic uncertainties in\nunbinned inclusive cross-section measurements and related collider-based\ninference problems. Our approach incorporates known analytic dependencies on\nparameters of interest, including signal strengths and nuisance parameters.\nWhen these dependencies are unknown, as is frequently the case for systematic\nuncertainties, dedicated neural network parametrizations provide an\napproximation that is trained on simulated data. The resulting machine-learned\nsurrogate captures the complete parameter dependence of the likelihood ratio,\nproviding a near-optimal test statistic. As a case study, we perform a\nfirst-principles inclusive cross-section measurement of\n$\\textrm{H}\\rightarrow\\tau\\tau$ in the single-lepton channel, utilizing\nsimulated data from the FAIR Universe Higgs Uncertainty Challenge. Results in\nAsimov data, from large-scale toy studies, and using the Fisher information\ndemonstrate significant improvements over traditional binned methods. Our\ncomputer code ``Guaranteed Optimal Log-Likelihood-based Unbinned Method''\n(GOLLUM) for machine-learning and inference is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel methodology for addressing systematic uncertainties in\nunbinned inclusive cross-section measurements and related collider-based\ninference problems. Our approach incorporates known analytic dependencies on\nparameters of interest, including signal strengths and nuisance parameters.\nWhen these dependencies are unknown, as is frequently the case for systematic\nuncertainties, dedicated neural network parametrizations provide an\napproximation that is trained on simulated data. The resulting machine-learned\nsurrogate captures the complete parameter dependence of the likelihood ratio,\nproviding a near-optimal test statistic. As a case study, we perform a\nfirst-principles inclusive cross-section measurement of\n$\\textrm{H}\\rightarrow\\tau\\tau$ in the single-lepton channel, utilizing\nsimulated data from the FAIR Universe Higgs Uncertainty Challenge. Results in\nAsimov data, from large-scale toy studies, and using the Fisher information\ndemonstrate significant improvements over traditional binned methods. Our\ncomputer code ``Guaranteed Optimal Log-Likelihood-based Unbinned Method''\n(GOLLUM) for machine-learning and inference is publicly available."
                },
                "authors": [
                    {
                        "name": "Lisa Benato"
                    },
                    {
                        "name": "Cristina Giordano"
                    },
                    {
                        "name": "Claudius Krause"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Robert Sch√∂fbeck"
                    },
                    {
                        "name": "Dennis Schwarz"
                    },
                    {
                        "name": "Maryam Shooshtari"
                    },
                    {
                        "name": "Daohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Daohan Wang"
                },
                "author": "Daohan Wang",
                "arxiv_comment": "23 pages, 16 figures, 2 tables. Contribution to the FAIR Universe\n  Higgs Uncertainty Challenge, winning first place ex aequo; v2: accepted for\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18025v1",
                "updated": "2025-08-25T13:44:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    44,
                    0,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:44:00Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    44,
                    0,
                    0,
                    237,
                    0
                ],
                "title": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for\n  Autonomous Space Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for\n  Autonomous Space Exploration"
                },
                "summary": "Autonomous planetary exploration missions are critically dependent on\nreal-time, accurate environmental perception for navigation and hazard\navoidance. However, deploying deep learning models on the resource-constrained\ncomputational hardware of planetary exploration platforms remains a significant\nchallenge. This paper introduces the Adaptive Quantized Planetary Crater\nDetection System (AQ-PCDSys), a novel framework specifically engineered for\nreal-time, onboard deployment in the computationally constrained environments\nof space exploration missions. AQ-PCDSys synergistically integrates a Quantized\nNeural Network (QNN) architecture, trained using Quantization-Aware Training\n(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture\nsignificantly optimizes model size and inference latency suitable for real-time\nonboard deployment in space exploration missions, while preserving high\naccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and\nDigital Elevation Models (DEMs) at the feature level, utilizing an Adaptive\nWeighting Mechanism (AWM) to dynamically prioritize the most relevant and\nreliable sensor modality based on planetary ambient conditions. This approach\nenhances detection robustness across diverse planetary landscapes. Paired with\nMulti-Scale Detection Heads specifically designed for robust and efficient\ndetection of craters across a wide range of sizes, AQ-PCDSys provides a\ncomputationally efficient, reliable and accurate solution for planetary crater\ndetection, a critical capability for enabling the next generation of autonomous\nplanetary landing, navigation, and scientific exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous planetary exploration missions are critically dependent on\nreal-time, accurate environmental perception for navigation and hazard\navoidance. However, deploying deep learning models on the resource-constrained\ncomputational hardware of planetary exploration platforms remains a significant\nchallenge. This paper introduces the Adaptive Quantized Planetary Crater\nDetection System (AQ-PCDSys), a novel framework specifically engineered for\nreal-time, onboard deployment in the computationally constrained environments\nof space exploration missions. AQ-PCDSys synergistically integrates a Quantized\nNeural Network (QNN) architecture, trained using Quantization-Aware Training\n(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture\nsignificantly optimizes model size and inference latency suitable for real-time\nonboard deployment in space exploration missions, while preserving high\naccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and\nDigital Elevation Models (DEMs) at the feature level, utilizing an Adaptive\nWeighting Mechanism (AWM) to dynamically prioritize the most relevant and\nreliable sensor modality based on planetary ambient conditions. This approach\nenhances detection robustness across diverse planetary landscapes. Paired with\nMulti-Scale Detection Heads specifically designed for robust and efficient\ndetection of craters across a wide range of sizes, AQ-PCDSys provides a\ncomputationally efficient, reliable and accurate solution for planetary crater\ndetection, a critical capability for enabling the next generation of autonomous\nplanetary landing, navigation, and scientific exploration."
                },
                "authors": [
                    {
                        "name": "Aditri Paul"
                    },
                    {
                        "name": "Archan Paul"
                    }
                ],
                "author_detail": {
                    "name": "Archan Paul"
                },
                "author": "Archan Paul",
                "arxiv_comment": "17 pages, 6 figures. A research paper on a novel deep learning\n  framework for planetary crater detection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07(2020), 68T45(2020), 68T10(2020), 90C90(2020)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.6; I.2.9; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21577v3",
                "updated": "2025-08-25T13:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    40,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-27T08:35:05Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    8,
                    35,
                    5,
                    1,
                    147,
                    0
                ],
                "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub\n  Repositories for Complex Task Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMaster: Autonomous Exploration and Understanding of GitHub\n  Repositories for Complex Task Solving"
                },
                "summary": "The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/QuantaAlpha/RepoMaster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/QuantaAlpha/RepoMaster."
                },
                "authors": [
                    {
                        "name": "Huacan Wang"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Shuo Lu"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyang He"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Pin Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Pin Lyu"
                },
                "author": "Pin Lyu",
                "arxiv_comment": "A novel approach; Very practical",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18018v1",
                "updated": "2025-08-25T13:33:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    33,
                    23,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:33:23Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    33,
                    23,
                    0,
                    237,
                    0
                ],
                "title": "Learning from nature: insights into GraphDOP's representations of the\n  Earth System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from nature: insights into GraphDOP's representations of the\n  Earth System"
                },
                "summary": "Through a series of experiments, we provide evidence that the GraphDOP model\n- trained solely on meteorological observations, using no prior knowledge -\ndevelops internal representations of the Earth System state, structure and\ndynamics as well as the characteristics of different observing systems.\nFirstly, we demonstrate that the network constructs a unified latent\nrepresentation of the Earth System state which is common across different\nobservation types. For example, cloud structures maintain physical consistency\nwhether viewed in predictions for satellite radiances from different sensors,\nor for direct in-situ measurements of the cloud fraction. Secondly, we show\nexamples that suggest that the network learns to emulate viewing effects -\nlearned observation operators that map from the unified state representation to\nobserved properties. Microwave sounder limb effects and geometric viewing\neffects, such as sunglint in visible imagery, are both well captured. Finally,\nwe demonstrate that the model develops rich internal representations of the\nstructure of meteorological systems and their dynamics. For instance, when the\nnetwork is only provided with observations from a single infrared instrument,\nit is able to infer unobserved, non-local structures such as jet streams,\nsurface pressure patterns and warm and cold air masses associated with synoptic\nsystems. This work provides insights into how neural networks trained solely on\nobservations of the Earth System spontaneously develop coherent internal\nrepresentations of the physical world in order to meet the training objective -\nenhancing our understanding and guiding future development of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through a series of experiments, we provide evidence that the GraphDOP model\n- trained solely on meteorological observations, using no prior knowledge -\ndevelops internal representations of the Earth System state, structure and\ndynamics as well as the characteristics of different observing systems.\nFirstly, we demonstrate that the network constructs a unified latent\nrepresentation of the Earth System state which is common across different\nobservation types. For example, cloud structures maintain physical consistency\nwhether viewed in predictions for satellite radiances from different sensors,\nor for direct in-situ measurements of the cloud fraction. Secondly, we show\nexamples that suggest that the network learns to emulate viewing effects -\nlearned observation operators that map from the unified state representation to\nobserved properties. Microwave sounder limb effects and geometric viewing\neffects, such as sunglint in visible imagery, are both well captured. Finally,\nwe demonstrate that the model develops rich internal representations of the\nstructure of meteorological systems and their dynamics. For instance, when the\nnetwork is only provided with observations from a single infrared instrument,\nit is able to infer unobserved, non-local structures such as jet streams,\nsurface pressure patterns and warm and cold air masses associated with synoptic\nsystems. This work provides insights into how neural networks trained solely on\nobservations of the Earth System spontaneously develop coherent internal\nrepresentations of the physical world in order to meet the training objective -\nenhancing our understanding and guiding future development of these models."
                },
                "authors": [
                    {
                        "name": "Peter Lean"
                    },
                    {
                        "name": "Mihai Alexe"
                    },
                    {
                        "name": "Eulalie Boucher"
                    },
                    {
                        "name": "Ewan Pinnington"
                    },
                    {
                        "name": "Simon Lang"
                    },
                    {
                        "name": "Patrick Laloyaux"
                    },
                    {
                        "name": "Niels Bormann"
                    },
                    {
                        "name": "Anthony McNally"
                    }
                ],
                "author_detail": {
                    "name": "Anthony McNally"
                },
                "author": "Anthony McNally",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16153v2",
                "updated": "2025-08-25T13:32:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    32,
                    12,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T07:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    25,
                    30,
                    4,
                    234,
                    0
                ],
                "title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"
                },
                "summary": "In this paper, we introduce a novel learning paradigm for Adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely \\emph{Memento}, which attains top-1\non GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It\nreaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset,\noutperforming the state-of-the-art training-based method, while case-based\nmemory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks.\nOur approach offers a scalable and efficient pathway for developing generalist\nLLM agents capable of continuous, real-time learning without gradient updates,\nadvancing machine learning towards open-ended skill acquisition and deep\nresearch scenarios. The code is available at\nhttps://github.com/Agent-on-the-Fly/Memento.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel learning paradigm for Adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely \\emph{Memento}, which attains top-1\non GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It\nreaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset,\noutperforming the state-of-the-art training-based method, while case-based\nmemory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks.\nOur approach offers a scalable and efficient pathway for developing generalist\nLLM agents capable of continuous, real-time learning without gradient updates,\nadvancing machine learning towards open-ended skill acquisition and deep\nresearch scenarios. The code is available at\nhttps://github.com/Agent-on-the-Fly/Memento."
                },
                "authors": [
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Siyuan Guo"
                    },
                    {
                        "name": "Xue Yan"
                    },
                    {
                        "name": "Kin Hei Lee"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Ka Yiu Lee"
                    },
                    {
                        "name": "Guchun Zhang"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18023v3",
                "updated": "2025-08-25T13:17:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    17,
                    9,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-25T09:32:08Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    32,
                    8,
                    1,
                    56,
                    0
                ],
                "title": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference"
                },
                "summary": "Despite the advancements made in Vision Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tune a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary, based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advancements made in Vision Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tune a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary, based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary"
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Geng"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "EMNLP2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18560v2",
                "updated": "2025-08-25T13:14:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    14,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-03-24T11:14:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    14,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Simultaneous Inference Bands for Autocorrelations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Inference Bands for Autocorrelations"
                },
                "summary": "Sample autocorrelograms typically come with significance bands (non-rejection\nregions) for the null hypothesis of no temporal correlation. These bands have\ntwo shortcomings. First, they build on pointwise intervals and suffer from\njoint undercoverage (overrejection) under the null hypothesis. Second, if this\nnull is clearly violated one would rather prefer to see confidence bands to\nquantify estimation uncertainty. We propose and discuss both simultaneous\nsignificance bands and simultaneous confidence bands for time series and series\nof regression residuals. They are as easy to construct as their pointwise\ncounterparts and at the same time provide an intuitive and visual\nquantification of sampling uncertainty as well as valid statistical inference.\nFor regression residuals, we show that for static regressions the asymptotic\nvariances underlying the construction of the bands are the same as those for\nobserved time series, and for dynamic regressions (with lagged endogenous\nregressors) we show how they need to be adjusted. We study theoretical\nproperties of simultaneous significance bands and two types of simultaneous\nconfidence bands (sup-t and Bonferroni) and analyse their finite-sample\nperformance in a simulation study. Finally, we illustrate the use of the bands\nin an application to monthly US inflation and residuals from Phillips curve\nregressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample autocorrelograms typically come with significance bands (non-rejection\nregions) for the null hypothesis of no temporal correlation. These bands have\ntwo shortcomings. First, they build on pointwise intervals and suffer from\njoint undercoverage (overrejection) under the null hypothesis. Second, if this\nnull is clearly violated one would rather prefer to see confidence bands to\nquantify estimation uncertainty. We propose and discuss both simultaneous\nsignificance bands and simultaneous confidence bands for time series and series\nof regression residuals. They are as easy to construct as their pointwise\ncounterparts and at the same time provide an intuitive and visual\nquantification of sampling uncertainty as well as valid statistical inference.\nFor regression residuals, we show that for static regressions the asymptotic\nvariances underlying the construction of the bands are the same as those for\nobserved time series, and for dynamic regressions (with lagged endogenous\nregressors) we show how they need to be adjusted. We study theoretical\nproperties of simultaneous significance bands and two types of simultaneous\nconfidence bands (sup-t and Bonferroni) and analyse their finite-sample\nperformance in a simulation study. Finally, we illustrate the use of the bands\nin an application to monthly US inflation and residuals from Phillips curve\nregressions."
                },
                "authors": [
                    {
                        "name": "Uwe Hassler"
                    },
                    {
                        "name": "Marc-Oliver Pohle"
                    },
                    {
                        "name": "Tanja Zahn"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Zahn"
                },
                "author": "Tanja Zahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11703v2",
                "updated": "2025-08-25T13:14:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    14,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-13T17:56:59Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    56,
                    59,
                    2,
                    225,
                    0
                ],
                "title": "Data-Driven Discovery of Interpretable Kalman Filter Variants through\n  Large Language Models and Genetic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Discovery of Interpretable Kalman Filter Variants through\n  Large Language Models and Genetic Programming"
                },
                "summary": "Algorithmic discovery has traditionally relied on human ingenuity and\nextensive experimentation. Here we investigate whether a prominent scientific\ncomputing algorithm, the Kalman Filter, can be discovered through an automated,\ndata-driven, evolutionary process that relies on Cartesian Genetic Programming\n(CGP) and Large Language Models (LLM). We evaluate the contributions of both\nmodalities (CGP and LLM) in discovering the Kalman filter under varying\nconditions. Our results demonstrate that our framework of CGP and LLM-assisted\nevolution converges to near-optimal solutions when Kalman optimality\nassumptions hold. When these assumptions are violated, our framework evolves\ninterpretable alternatives that outperform the Kalman filter. These results\ndemonstrate that combining evolutionary algorithms and generative models for\ninterpretable, data-driven synthesis of simple computational modules is a\npotent approach for algorithmic discovery in scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic discovery has traditionally relied on human ingenuity and\nextensive experimentation. Here we investigate whether a prominent scientific\ncomputing algorithm, the Kalman Filter, can be discovered through an automated,\ndata-driven, evolutionary process that relies on Cartesian Genetic Programming\n(CGP) and Large Language Models (LLM). We evaluate the contributions of both\nmodalities (CGP and LLM) in discovering the Kalman filter under varying\nconditions. Our results demonstrate that our framework of CGP and LLM-assisted\nevolution converges to near-optimal solutions when Kalman optimality\nassumptions hold. When these assumptions are violated, our framework evolves\ninterpretable alternatives that outperform the Kalman filter. These results\ndemonstrate that combining evolutionary algorithms and generative models for\ninterpretable, data-driven synthesis of simple computational modules is a\npotent approach for algorithmic discovery in scientific computing."
                },
                "authors": [
                    {
                        "name": "Vasileios Saketos"
                    },
                    {
                        "name": "Sebastian Kaltenbach"
                    },
                    {
                        "name": "Sergey Litvinov"
                    },
                    {
                        "name": "Petros Koumoutsakos"
                    }
                ],
                "author_detail": {
                    "name": "Petros Koumoutsakos"
                },
                "author": "Petros Koumoutsakos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18004v1",
                "updated": "2025-08-25T13:13:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    13,
                    15,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:13:15Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    13,
                    15,
                    0,
                    237,
                    0
                ],
                "title": "Outlier-robust Bayesian Multivariate Analysis with Correlation-intact\n  Sandwich Mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier-robust Bayesian Multivariate Analysis with Correlation-intact\n  Sandwich Mixture"
                },
                "summary": "Handling outliers is a fundamental challenge in multivariate data analysis,\nas outliers may distort structures of correlation or conditional independence.\nAlthough robust Bayesian inference has been extensively studied for univariate\nsettings, theoretical results ensuring posterior robustness in multivariate\nmodels are scarce. We propose a novel scale mixture of multivariate normals\ncalled correlation-intact sandwich mixture, where the scale parameters are\nreal-valued and follow the unfolded log-Pareto distribution. Our theoretical\nresults on posterior robustness in multivariate settings emphasizes that the\nuse of a symmetric, super heavy-tailed distribution for the scale parameters is\nessential in achieving posterior robustness against element-wise contamination.\nPosterior inference for the proposed model is feasible by an efficient Gibbs\nsampling algorithm we developed. The superiority of the proposed method is\nillustrated further in simulation and empirical studies using graphical models\nand multivariate regression in the presence of complex outlier structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling outliers is a fundamental challenge in multivariate data analysis,\nas outliers may distort structures of correlation or conditional independence.\nAlthough robust Bayesian inference has been extensively studied for univariate\nsettings, theoretical results ensuring posterior robustness in multivariate\nmodels are scarce. We propose a novel scale mixture of multivariate normals\ncalled correlation-intact sandwich mixture, where the scale parameters are\nreal-valued and follow the unfolded log-Pareto distribution. Our theoretical\nresults on posterior robustness in multivariate settings emphasizes that the\nuse of a symmetric, super heavy-tailed distribution for the scale parameters is\nessential in achieving posterior robustness against element-wise contamination.\nPosterior inference for the proposed model is feasible by an efficient Gibbs\nsampling algorithm we developed. The superiority of the proposed method is\nillustrated further in simulation and empirical studies using graphical models\nand multivariate regression in the presence of complex outlier structures."
                },
                "authors": [
                    {
                        "name": "Yasuyuki Hamura"
                    },
                    {
                        "name": "Kaoru Irie"
                    },
                    {
                        "name": "Shonosuke Sugasawa"
                    }
                ],
                "author_detail": {
                    "name": "Shonosuke Sugasawa"
                },
                "author": "Shonosuke Sugasawa",
                "arxiv_comment": "78 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17990v1",
                "updated": "2025-08-25T13:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    0,
                    41,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    0,
                    41,
                    0,
                    237,
                    0
                ],
                "title": "Automating Conflict-Aware ACL Configurations with Natural Language\n  Intents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Conflict-Aware ACL Configurations with Natural Language\n  Intents"
                },
                "summary": "ACL configuration is essential for managing network flow reachability, yet\nits complexity grows significantly with topologies and pre-existing rules. To\ncarry out ACL configuration, the operator needs to (1) understand the new\nconfiguration policies or intents and translate them into concrete ACL rules,\n(2) check and resolve any conflicts between the new and existing rules, and (3)\ndeploy them across the network. Existing systems rely heavily on manual efforts\nfor these tasks, especially for the first two, which are tedious, error-prone,\nand impractical to scale.\n  We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge\nof the target network, Xumi automatically and accurately translates the natural\nlanguage intents into complete ACL rules to reduce operators' manual efforts.\nXumi then detects all potential conflicts between new and existing rules and\ngenerates resolved intents for deployment with operators' guidance, and finally\nidentifies the best deployment plan that minimizes the rule additions while\nsatisfying all intents. Evaluation shows that Xumi accelerates the entire\nconfiguration pipeline by over 10x compared to current practices, addresses\nO(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACL configuration is essential for managing network flow reachability, yet\nits complexity grows significantly with topologies and pre-existing rules. To\ncarry out ACL configuration, the operator needs to (1) understand the new\nconfiguration policies or intents and translate them into concrete ACL rules,\n(2) check and resolve any conflicts between the new and existing rules, and (3)\ndeploy them across the network. Existing systems rely heavily on manual efforts\nfor these tasks, especially for the first two, which are tedious, error-prone,\nand impractical to scale.\n  We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge\nof the target network, Xumi automatically and accurately translates the natural\nlanguage intents into complete ACL rules to reduce operators' manual efforts.\nXumi then detects all potential conflicts between new and existing rules and\ngenerates resolved intents for deployment with operators' guidance, and finally\nidentifies the best deployment plan that minimizes the rule additions while\nsatisfying all intents. Evaluation shows that Xumi accelerates the entire\nconfiguration pipeline by over 10x compared to current practices, addresses\nO(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud\nnetwork."
                },
                "authors": [
                    {
                        "name": "Wenlong Ding"
                    },
                    {
                        "name": "Jianqiang Li"
                    },
                    {
                        "name": "Zhixiong Niu"
                    },
                    {
                        "name": "Huangxun Chen"
                    },
                    {
                        "name": "Yongqiang Xiong"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17974v1",
                "updated": "2025-08-25T12:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    41,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    41,
                    28,
                    0,
                    237,
                    0
                ],
                "title": "Cellular Flow Architecture Exposes the Hidden Mechanics of Biological\n  Matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular Flow Architecture Exposes the Hidden Mechanics of Biological\n  Matter"
                },
                "summary": "Understanding how biomechanical reorganization governs key biological\nprocesses, such as morphogenesis and development, requires predictive insights\ninto stress distributions and cellular behavior. While traditional approaches\nfocused on cell motion as a response to stress, we demonstrate that Lagrangian\ncoherent structures (LCSs) -- robust attractors and repellers in cellular flows\n-- precede and drive long-term intercellular stress reorganization, physically\ngoverned by the mechanical properties of intercellular junctions. We show that\nthis hidden flow skeleton correlates strongly with biomechanical metrics,\nbridging microscopic cell motion with mesoscopic biomechanics. Specifically,\nattractors and repellers mark hotspots of compressive and tensile stress\nenrichment (exceeding tenfold), alongside heterogeneities in cell packing.\nNotably, these connections remain robust across varying strengths of cell-cell\nand cell-substrate force transmission. Finally, by linking the attracting\nregions in the flow skeleton to future cell extrusion spots, we establish a\ndirect link between cell motion and biologically significant outcomes.\nTogether, these findings establish a framework for using cell motion to\nindependently infer biomechanical metrics and bridge the scale mismatch between\ncell motion and biomechanics, potentially offering a new route to interpret\nmechanosensitive biological processes directly from cell trajectories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how biomechanical reorganization governs key biological\nprocesses, such as morphogenesis and development, requires predictive insights\ninto stress distributions and cellular behavior. While traditional approaches\nfocused on cell motion as a response to stress, we demonstrate that Lagrangian\ncoherent structures (LCSs) -- robust attractors and repellers in cellular flows\n-- precede and drive long-term intercellular stress reorganization, physically\ngoverned by the mechanical properties of intercellular junctions. We show that\nthis hidden flow skeleton correlates strongly with biomechanical metrics,\nbridging microscopic cell motion with mesoscopic biomechanics. Specifically,\nattractors and repellers mark hotspots of compressive and tensile stress\nenrichment (exceeding tenfold), alongside heterogeneities in cell packing.\nNotably, these connections remain robust across varying strengths of cell-cell\nand cell-substrate force transmission. Finally, by linking the attracting\nregions in the flow skeleton to future cell extrusion spots, we establish a\ndirect link between cell motion and biologically significant outcomes.\nTogether, these findings establish a framework for using cell motion to\nindependently infer biomechanical metrics and bridge the scale mismatch between\ncell motion and biomechanics, potentially offering a new route to interpret\nmechanosensitive biological processes directly from cell trajectories."
                },
                "authors": [
                    {
                        "name": "Tianxiang Ma"
                    },
                    {
                        "name": "Valeriia Grudtsyna"
                    },
                    {
                        "name": "Robin V. B√∂lsterli"
                    },
                    {
                        "name": "Amin Doostmohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Doostmohammadi"
                },
                "author": "Amin Doostmohammadi",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17973v1",
                "updated": "2025-08-25T12:40:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    40,
                    32,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:40:32Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    40,
                    32,
                    0,
                    237,
                    0
                ],
                "title": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German"
                },
                "summary": "The ability to paraphrase texts across different complexity levels is\nessential for creating accessible texts that can be tailored toward diverse\nreader groups. Thus, we introduce German4All, the first large-scale German\ndataset of aligned readability-controlled, paragraph-level paraphrases. It\nspans five readability levels and comprises over 25,000 samples. The dataset is\nautomatically synthesized using GPT-4 and rigorously evaluated through both\nhuman and LLM-based judgments. Using German4All, we train an open-source,\nreadability-controlled paraphrasing model that achieves state-of-the-art\nperformance in German text simplification, enabling more nuanced and\nreader-specific adaptations. We opensource both the dataset and the model to\nencourage further research on multi-level paraphrasing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to paraphrase texts across different complexity levels is\nessential for creating accessible texts that can be tailored toward diverse\nreader groups. Thus, we introduce German4All, the first large-scale German\ndataset of aligned readability-controlled, paragraph-level paraphrases. It\nspans five readability levels and comprises over 25,000 samples. The dataset is\nautomatically synthesized using GPT-4 and rigorously evaluated through both\nhuman and LLM-based judgments. Using German4All, we train an open-source,\nreadability-controlled paraphrasing model that achieves state-of-the-art\nperformance in German text simplification, enabling more nuanced and\nreader-specific adaptations. We opensource both the dataset and the model to\nencourage further research on multi-level paraphrasing"
                },
                "authors": [
                    {
                        "name": "Miriam Ansch√ºtz"
                    },
                    {
                        "name": "Thanh Mai Pham"
                    },
                    {
                        "name": "Eslam Nasrallah"
                    },
                    {
                        "name": "Maximilian M√ºller"
                    },
                    {
                        "name": "Cristian-George Craciun"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh",
                "arxiv_comment": "Accepted to INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17971v1",
                "updated": "2025-08-25T12:38:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    38,
                    8,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:38:08Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    38,
                    8,
                    0,
                    237,
                    0
                ],
                "title": "Neural Algorithmic Reasoners informed Large Language Model for\n  Multi-Agent Path Finding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Algorithmic Reasoners informed Large Language Model for\n  Multi-Agent Path Finding"
                },
                "summary": "The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems."
                },
                "authors": [
                    {
                        "name": "Pu Feng"
                    },
                    {
                        "name": "Size Wang"
                    },
                    {
                        "name": "Yuhong Cao"
                    },
                    {
                        "name": "Junkang Liang"
                    },
                    {
                        "name": "Rongye Shi"
                    },
                    {
                        "name": "Wenjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Wu"
                },
                "author": "Wenjun Wu",
                "arxiv_comment": "Accepted by IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17960v1",
                "updated": "2025-08-25T12:20:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    20,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:20:29Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    20,
                    29,
                    0,
                    237,
                    0
                ],
                "title": "A Unified Transformer Architecture for Low-Latency and Scalable Wireless\n  Signal Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Transformer Architecture for Low-Latency and Scalable Wireless\n  Signal Processing"
                },
                "summary": "We propose a unified Transformer-based architecture for wireless signal\nprocessing tasks, offering a low-latency, task-adaptive alternative to\nconventional receiver pipelines. Unlike traditional modular designs, our model\nintegrates channel estimation, interpolation, and demapping into a single,\ncompact attention-driven architecture designed for real-time deployment. The\nmodel's structure allows dynamic adaptation to diverse output formats by simply\nmodifying the final projection layer, enabling consistent reuse across receiver\nsubsystems. Experimental results demonstrate strong generalization to varying\nuser counts, modulation schemes, and pilot configurations, while satisfying\nlatency constraints imposed by practical systems. The architecture is evaluated\nacross three core use cases: (1) an End-to-End Receiver, which replaces the\nentire baseband processing pipeline from pilot symbols to bit-level decisions;\n(2) Channel Frequency Interpolation, implemented and tested within a\n3GPP-compliant OAI+Aerial system; and (3) Channel Estimation, where the model\ninfers full-band channel responses from sparse pilot observations. In all\ncases, our approach outperforms classical baselines in terms of accuracy,\nrobustness, and computational efficiency. This work presents a deployable,\ndata-driven alternative to hand-engineered PHY-layer blocks, and lays the\nfoundation for intelligent, software-defined signal processing in\nnext-generation wireless communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unified Transformer-based architecture for wireless signal\nprocessing tasks, offering a low-latency, task-adaptive alternative to\nconventional receiver pipelines. Unlike traditional modular designs, our model\nintegrates channel estimation, interpolation, and demapping into a single,\ncompact attention-driven architecture designed for real-time deployment. The\nmodel's structure allows dynamic adaptation to diverse output formats by simply\nmodifying the final projection layer, enabling consistent reuse across receiver\nsubsystems. Experimental results demonstrate strong generalization to varying\nuser counts, modulation schemes, and pilot configurations, while satisfying\nlatency constraints imposed by practical systems. The architecture is evaluated\nacross three core use cases: (1) an End-to-End Receiver, which replaces the\nentire baseband processing pipeline from pilot symbols to bit-level decisions;\n(2) Channel Frequency Interpolation, implemented and tested within a\n3GPP-compliant OAI+Aerial system; and (3) Channel Estimation, where the model\ninfers full-band channel responses from sparse pilot observations. In all\ncases, our approach outperforms classical baselines in terms of accuracy,\nrobustness, and computational efficiency. This work presents a deployable,\ndata-driven alternative to hand-engineered PHY-layer blocks, and lays the\nfoundation for intelligent, software-defined signal processing in\nnext-generation wireless communication systems."
                },
                "authors": [
                    {
                        "name": "Yuto Kawai"
                    },
                    {
                        "name": "Rajeev Koodli"
                    }
                ],
                "author_detail": {
                    "name": "Rajeev Koodli"
                },
                "author": "Rajeev Koodli",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17959v1",
                "updated": "2025-08-25T12:19:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    19,
                    57,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:19:57Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    19,
                    57,
                    0,
                    237,
                    0
                ],
                "title": "Language Models Coupled with Metacognition Can Outperform Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Coupled with Metacognition Can Outperform Reasoning\n  Models"
                },
                "summary": "Large language models (LLMs) excel in speed and adaptability across various\nreasoning tasks, but they often struggle when strict logic or constraint\nenforcement is required. In contrast, Large Reasoning Models (LRMs) are\nspecifically designed for complex, step-by-step reasoning, although they come\nwith significant computational costs and slower inference times. To address\nthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)\ncognitive architecture into SOFAI-LM, which coordinates a fast LLM with a\nslower but more powerful LRM through metacognition. The metacognitive module\nactively monitors the LLM's performance and provides targeted, iterative\nfeedback with relevant examples. This enables the LLM to progressively refine\nits solutions without requiring the need for additional model fine-tuning.\nExtensive experiments on graph coloring and code debugging problems demonstrate\nthat our feedback-driven approach significantly enhances the problem-solving\ncapabilities of the LLM. In many instances, it achieves performance levels that\nmatch or even exceed those of standalone LRMs while requiring considerably less\ntime. Additionally, when the LLM and feedback mechanism alone are insufficient,\nwe engage the LRM by providing appropriate information collected during the\nLLM's feedback loop, tailored to the specific characteristics of the problem\ndomain and leads to improved overall performance. Evaluations on two\ncontrasting domains: graph coloring, requiring globally consistent solutions,\nand code debugging, demanding localized fixes, demonstrate that SOFAI-LM\nenables LLMs to match or outperform standalone LRMs in accuracy while\nmaintaining significantly lower inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in speed and adaptability across various\nreasoning tasks, but they often struggle when strict logic or constraint\nenforcement is required. In contrast, Large Reasoning Models (LRMs) are\nspecifically designed for complex, step-by-step reasoning, although they come\nwith significant computational costs and slower inference times. To address\nthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)\ncognitive architecture into SOFAI-LM, which coordinates a fast LLM with a\nslower but more powerful LRM through metacognition. The metacognitive module\nactively monitors the LLM's performance and provides targeted, iterative\nfeedback with relevant examples. This enables the LLM to progressively refine\nits solutions without requiring the need for additional model fine-tuning.\nExtensive experiments on graph coloring and code debugging problems demonstrate\nthat our feedback-driven approach significantly enhances the problem-solving\ncapabilities of the LLM. In many instances, it achieves performance levels that\nmatch or even exceed those of standalone LRMs while requiring considerably less\ntime. Additionally, when the LLM and feedback mechanism alone are insufficient,\nwe engage the LRM by providing appropriate information collected during the\nLLM's feedback loop, tailored to the specific characteristics of the problem\ndomain and leads to improved overall performance. Evaluations on two\ncontrasting domains: graph coloring, requiring globally consistent solutions,\nand code debugging, demanding localized fixes, demonstrate that SOFAI-LM\nenables LLMs to match or outperform standalone LRMs in accuracy while\nmaintaining significantly lower inference time."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Francesca Rossi"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Murray Campbell"
                    },
                    {
                        "name": "Karthikeyan Natesan Ramamurthy"
                    },
                    {
                        "name": "Lior Horesh"
                    }
                ],
                "author_detail": {
                    "name": "Lior Horesh"
                },
                "author": "Lior Horesh",
                "arxiv_comment": "37 Pages, 95 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17953v1",
                "updated": "2025-08-25T12:16:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    16,
                    56,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:16:56Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    16,
                    56,
                    0,
                    237,
                    0
                ],
                "title": "Understanding Subword Compositionality of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Subword Compositionality of Large Language Models"
                },
                "summary": "Large language models (LLMs) take sequences of subwords as input, requiring\nthem to effective compose subword representations into meaningful word-level\nrepresentations. In this paper, we present a comprehensive set of experiments\nto probe how LLMs compose subword information, focusing on three key aspects:\nstructural similarity, semantic decomposability, and form retention. Our\nanalysis of the experiments suggests that these five LLM families can be\nclassified into three distinct groups, likely reflecting difference in their\nunderlying composition strategies. Specifically, we observe (i) three distinct\npatterns in the evolution of structural similarity between subword compositions\nand whole-word representations across layers; (ii) great performance when\nprobing layer by layer their sensitivity to semantic decompositionality; and\n(iii) three distinct patterns when probing sensitivity to formal features,\ne.g., character sequence length. These findings provide valuable insights into\nthe compositional dynamics of LLMs and highlight different compositional\npattens in how LLMs encode and integrate subword information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) take sequences of subwords as input, requiring\nthem to effective compose subword representations into meaningful word-level\nrepresentations. In this paper, we present a comprehensive set of experiments\nto probe how LLMs compose subword information, focusing on three key aspects:\nstructural similarity, semantic decomposability, and form retention. Our\nanalysis of the experiments suggests that these five LLM families can be\nclassified into three distinct groups, likely reflecting difference in their\nunderlying composition strategies. Specifically, we observe (i) three distinct\npatterns in the evolution of structural similarity between subword compositions\nand whole-word representations across layers; (ii) great performance when\nprobing layer by layer their sensitivity to semantic decompositionality; and\n(iii) three distinct patterns when probing sensitivity to formal features,\ne.g., character sequence length. These findings provide valuable insights into\nthe compositional dynamics of LLMs and highlight different compositional\npattens in how LLMs encode and integrate subword information."
                },
                "authors": [
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Anders S√∏gaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders S√∏gaard"
                },
                "author": "Anders S√∏gaard",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17948v1",
                "updated": "2025-08-25T12:13:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    13,
                    37,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:13:37Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    13,
                    37,
                    0,
                    237,
                    0
                ],
                "title": "Debiasing Multilingual LLMs in Cross-lingual Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Multilingual LLMs in Cross-lingual Latent Space"
                },
                "summary": "Debiasing techniques such as SentDebias aim to reduce bias in large language\nmodels (LLMs). Previous studies have evaluated their cross-lingual\ntransferability by directly applying these methods to LLM representations,\nrevealing their limited effectiveness across languages. In this work, we\ntherefore propose to perform debiasing in a joint latent space rather than\ndirectly on LLM representations. We construct a well-aligned cross-lingual\nlatent space using an autoencoder trained on parallel TED talk scripts. Our\nexperiments with Aya-expanse and two debiasing techniques across four languages\n(English, French, German, Dutch) demonstrate that a) autoencoders effectively\nconstruct a well-aligned cross-lingual latent space, and b) applying debiasing\ntechniques in the learned cross-lingual latent space significantly improves\nboth the overall debiasing performance and cross-lingual transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing techniques such as SentDebias aim to reduce bias in large language\nmodels (LLMs). Previous studies have evaluated their cross-lingual\ntransferability by directly applying these methods to LLM representations,\nrevealing their limited effectiveness across languages. In this work, we\ntherefore propose to perform debiasing in a joint latent space rather than\ndirectly on LLM representations. We construct a well-aligned cross-lingual\nlatent space using an autoencoder trained on parallel TED talk scripts. Our\nexperiments with Aya-expanse and two debiasing techniques across four languages\n(English, French, German, Dutch) demonstrate that a) autoencoders effectively\nconstruct a well-aligned cross-lingual latent space, and b) applying debiasing\ntechniques in the learned cross-lingual latent space significantly improves\nboth the overall debiasing performance and cross-lingual transferability."
                },
                "authors": [
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Anders S√∏gaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders S√∏gaard"
                },
                "author": "Anders S√∏gaard",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00026v2",
                "updated": "2025-08-25T12:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    0,
                    55,
                    0,
                    237,
                    0
                ],
                "published": "2025-04-26T10:17:48Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    10,
                    17,
                    48,
                    5,
                    116,
                    0
                ],
                "title": "Theory of Mind in Large Language Models: Assessment and Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind in Large Language Models: Assessment and Enhancement"
                },
                "summary": "Theory of Mind (ToM)-the ability to reason about the mental states of oneself\nand others-is a cornerstone of human social intelligence. As Large Language\nModels (LLMs) become increasingly integrated into daily life, understanding\ntheir ability to interpret and respond to human mental states is crucial for\nenabling effective interactions. In this paper, we review LLMs' ToM\ncapabilities by analyzing both evaluation benchmarks and enhancement\nstrategies. For evaluation, we focus on recently proposed and widely used\nstory-based benchmarks. For enhancement, we provide an in-depth analysis of\nrecent methods aimed at improving LLMs' ToM abilities. Furthermore, we outline\npromising directions for future research to further advance these capabilities\nand better adapt LLMs to more realistic and diverse scenarios. Our survey\nserves as a valuable resource for researchers interested in evaluating and\nadvancing LLMs' ToM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM)-the ability to reason about the mental states of oneself\nand others-is a cornerstone of human social intelligence. As Large Language\nModels (LLMs) become increasingly integrated into daily life, understanding\ntheir ability to interpret and respond to human mental states is crucial for\nenabling effective interactions. In this paper, we review LLMs' ToM\ncapabilities by analyzing both evaluation benchmarks and enhancement\nstrategies. For evaluation, we focus on recently proposed and widely used\nstory-based benchmarks. For enhancement, we provide an in-depth analysis of\nrecent methods aimed at improving LLMs' ToM abilities. Furthermore, we outline\npromising directions for future research to further advance these capabilities\nand better adapt LLMs to more realistic and diverse scenarios. Our survey\nserves as a valuable resource for researchers interested in evaluating and\nadvancing LLMs' ToM capabilities."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Cheston Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheston Tan"
                },
                "author": "Cheston Tan",
                "arxiv_comment": "Accepted to ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10811v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10811v3",
                "updated": "2025-08-25T11:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    41,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-01-19T16:56:11Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    16,
                    56,
                    11,
                    4,
                    19,
                    0
                ],
                "title": "Simulation Based Bayesian Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation Based Bayesian Optimization"
                },
                "summary": "Bayesian Optimization (BO) is a powerful method for optimizing black-box\nfunctions by combining prior knowledge with ongoing function evaluations. BO\nconstructs a probabilistic surrogate model of the objective function given the\ncovariates, which is in turn used to inform the selection of future evaluation\npoints through an acquisition function. For smooth continuous search spaces,\nGaussian Processes (GPs) are commonly used as the surrogate model as they offer\nanalytical access to posterior predictive distributions, thus facilitating the\ncomputation and optimization of acquisition functions. However, in complex\nscenarios involving optimization over categorical or mixed covariate spaces,\nGPs may not be ideal. This paper introduces Simulation Based Bayesian\nOptimization (SBBO) as a novel approach to optimizing acquisition functions\nthat only requires sampling-based access to posterior predictive distributions.\nSBBO allows the use of surrogate probabilistic models tailored for\ncombinatorial spaces with discrete variables. Any Bayesian model in which\nposterior inference is carried out through Markov chain Monte Carlo can be\nselected as the surrogate model in SBBO. We demonstrate empirically the\neffectiveness of SBBO using various choices of surrogate models in applications\ninvolving combinatorial optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Optimization (BO) is a powerful method for optimizing black-box\nfunctions by combining prior knowledge with ongoing function evaluations. BO\nconstructs a probabilistic surrogate model of the objective function given the\ncovariates, which is in turn used to inform the selection of future evaluation\npoints through an acquisition function. For smooth continuous search spaces,\nGaussian Processes (GPs) are commonly used as the surrogate model as they offer\nanalytical access to posterior predictive distributions, thus facilitating the\ncomputation and optimization of acquisition functions. However, in complex\nscenarios involving optimization over categorical or mixed covariate spaces,\nGPs may not be ideal. This paper introduces Simulation Based Bayesian\nOptimization (SBBO) as a novel approach to optimizing acquisition functions\nthat only requires sampling-based access to posterior predictive distributions.\nSBBO allows the use of surrogate probabilistic models tailored for\ncombinatorial spaces with discrete variables. Any Bayesian model in which\nposterior inference is carried out through Markov chain Monte Carlo can be\nselected as the surrogate model in SBBO. We demonstrate empirically the\neffectiveness of SBBO using various choices of surrogate models in applications\ninvolving combinatorial optimization."
                },
                "authors": [
                    {
                        "name": "Roi Naveiro"
                    },
                    {
                        "name": "Becky Tang"
                    }
                ],
                "author_detail": {
                    "name": "Becky Tang"
                },
                "author": "Becky Tang",
                "arxiv_comment": "Accepted in Statistics and Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10811v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10811v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17910v1",
                "updated": "2025-08-25T11:29:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    29,
                    9,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:29:09Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    29,
                    9,
                    0,
                    237,
                    0
                ],
                "title": "Quasi-likelihood inference for SDE with mixed-effects observed at high\n  frequency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-likelihood inference for SDE with mixed-effects observed at high\n  frequency"
                },
                "summary": "We consider statistical inference for a class of dynamic mixed-effect models\ndescribed by stochastic differential equations whose drift and diffusion\ncoefficients simultaneously depend on fixed- and random-effect parameters.\nAssuming that each process is observed at high frequency and the number of\nindividuals goes to infinity, we propose a stepwise inference procedure and\nprove its theoretical properties. The methodology is based on suitable\nquasi-likelihood functions by profiling the random effect in the diffusion\ncoefficient at the first stage, and then taking the marginal distribution in\nthe drift coefficient in the second stage, resulting in a fully explicit and\ncomputationally convenient method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider statistical inference for a class of dynamic mixed-effect models\ndescribed by stochastic differential equations whose drift and diffusion\ncoefficients simultaneously depend on fixed- and random-effect parameters.\nAssuming that each process is observed at high frequency and the number of\nindividuals goes to infinity, we propose a stepwise inference procedure and\nprove its theoretical properties. The methodology is based on suitable\nquasi-likelihood functions by profiling the random effect in the diffusion\ncoefficient at the first stage, and then taking the marginal distribution in\nthe drift coefficient in the second stage, resulting in a fully explicit and\ncomputationally convenient method."
                },
                "authors": [
                    {
                        "name": "Maud Delattre"
                    },
                    {
                        "name": "Hiroki Masuda"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Masuda"
                },
                "author": "Hiroki Masuda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05914v3",
                "updated": "2025-08-25T11:26:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    26,
                    22,
                    0,
                    237,
                    0
                ],
                "published": "2024-06-09T20:56:38Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    20,
                    56,
                    38,
                    6,
                    161,
                    0
                ],
                "title": "Soundscape Captioning using Sound Affective Quality Network and Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soundscape Captioning using Sound Affective Quality Network and Large\n  Language Model"
                },
                "summary": "We live in a rich and varied acoustic world, which is experienced by\nindividuals or communities as a soundscape. Computational auditory scene\nanalysis, disentangling acoustic scenes by detecting and classifying events,\nfocuses on objective attributes of sounds, such as their category and temporal\ncharacteristics, ignoring their effects on people, such as the emotions they\nevoke within a context. To fill this gap, we propose the affective soundscape\ncaptioning (ASSC) task, which enables automated soundscape analysis, thus\navoiding labour-intensive subjective ratings and surveys in conventional\nmethods. With soundscape captioning, context-aware descriptions are generated\nfor soundscape by capturing the acoustic scenes (ASs), audio events (AEs)\ninformation, and the corresponding human affective qualities (AQs). To this\nend, we propose an automatic soundscape captioner (SoundSCaper) system composed\nof an acoustic model, i.e. SoundAQnet, and a large language model (LLM).\nSoundAQnet simultaneously models multi-scale information about ASs, AEs, and\nperceived AQs, while the LLM describes the soundscape with captions by parsing\nthe information captured with SoundAQnet. SoundSCaper is assessed by two juries\nof 32 people. In expert evaluation, the average score of SoundSCaper-generated\ncaptions is slightly lower than that of two soundscape experts on the\nevaluation set D1 and the external mixed dataset D2, but not statistically\nsignificant. In layperson evaluation, SoundSCaper outperforms soundscape\nexperts in several metrics. In addition to human evaluation, compared to other\nautomated audio captioning systems with and without LLM, SoundSCaper performs\nbetter on the ASSC task in several NLP-based metrics. Overall, SoundSCaper\nperforms well in human subjective evaluation and various objective captioning\nmetrics, and the generated captions are comparable to those annotated by\nsoundscape experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a rich and varied acoustic world, which is experienced by\nindividuals or communities as a soundscape. Computational auditory scene\nanalysis, disentangling acoustic scenes by detecting and classifying events,\nfocuses on objective attributes of sounds, such as their category and temporal\ncharacteristics, ignoring their effects on people, such as the emotions they\nevoke within a context. To fill this gap, we propose the affective soundscape\ncaptioning (ASSC) task, which enables automated soundscape analysis, thus\navoiding labour-intensive subjective ratings and surveys in conventional\nmethods. With soundscape captioning, context-aware descriptions are generated\nfor soundscape by capturing the acoustic scenes (ASs), audio events (AEs)\ninformation, and the corresponding human affective qualities (AQs). To this\nend, we propose an automatic soundscape captioner (SoundSCaper) system composed\nof an acoustic model, i.e. SoundAQnet, and a large language model (LLM).\nSoundAQnet simultaneously models multi-scale information about ASs, AEs, and\nperceived AQs, while the LLM describes the soundscape with captions by parsing\nthe information captured with SoundAQnet. SoundSCaper is assessed by two juries\nof 32 people. In expert evaluation, the average score of SoundSCaper-generated\ncaptions is slightly lower than that of two soundscape experts on the\nevaluation set D1 and the external mixed dataset D2, but not statistically\nsignificant. In layperson evaluation, SoundSCaper outperforms soundscape\nexperts in several metrics. In addition to human evaluation, compared to other\nautomated audio captioning systems with and without LLM, SoundSCaper performs\nbetter on the ASSC task in several NLP-based metrics. Overall, SoundSCaper\nperforms well in human subjective evaluation and various objective captioning\nmetrics, and the generated captions are comparable to those annotated by\nsoundscape experts."
                },
                "authors": [
                    {
                        "name": "Yuanbo Hou"
                    },
                    {
                        "name": "Qiaoqiao Ren"
                    },
                    {
                        "name": "Andrew Mitchell"
                    },
                    {
                        "name": "Wenwu Wang"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Tony Belpaeme"
                    },
                    {
                        "name": "Dick Botteldooren"
                    }
                ],
                "author_detail": {
                    "name": "Dick Botteldooren"
                },
                "author": "Dick Botteldooren",
                "arxiv_comment": "IEEE Transactions on Multimedia, Code:\n  https://github.com/Yuanbo2020/SoundSCaper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17906v1",
                "updated": "2025-08-25T11:24:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    24,
                    55,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:24:55Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    24,
                    55,
                    0,
                    237,
                    0
                ],
                "title": "FinReflectKG: Agentic Construction and Evaluation of Financial Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinReflectKG: Agentic Construction and Evaluation of Financial Knowledge\n  Graphs"
                },
                "summary": "The financial domain poses unique challenges for knowledge graph (KG)\nconstruction at scale due to the complexity and regulatory nature of financial\ndocuments. Despite the critical importance of structured financial knowledge,\nthe field lacks large-scale, open-source datasets capturing rich semantic\nrelationships from corporate disclosures. We introduce an open-source,\nlarge-scale financial knowledge graph dataset built from the latest annual SEC\n10-K filings of all S and P 100 companies - a comprehensive resource designed\nto catalyze research in financial AI. We propose a robust and generalizable\nknowledge graph (KG) construction framework that integrates intelligent\ndocument parsing, table-aware chunking, and schema-guided iterative extraction\nwith a reflection-driven feedback loop. Our system incorporates a comprehensive\nevaluation pipeline, combining rule-based checks, statistical validation, and\nLLM-as-a-Judge assessments to holistically measure extraction quality. We\nsupport three extraction modes - single-pass, multi-pass, and\nreflection-agent-based - allowing flexible trade-offs between efficiency,\naccuracy, and reliability based on user requirements. Empirical evaluations\ndemonstrate that the reflection-agent-based mode consistently achieves the best\nbalance, attaining a 64.8 percent compliance score against all rule-based\npolicies (CheckRules) and outperforming baseline methods (single-pass and\nmulti-pass) across key metrics such as precision, comprehensiveness, and\nrelevance in LLM-guided evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The financial domain poses unique challenges for knowledge graph (KG)\nconstruction at scale due to the complexity and regulatory nature of financial\ndocuments. Despite the critical importance of structured financial knowledge,\nthe field lacks large-scale, open-source datasets capturing rich semantic\nrelationships from corporate disclosures. We introduce an open-source,\nlarge-scale financial knowledge graph dataset built from the latest annual SEC\n10-K filings of all S and P 100 companies - a comprehensive resource designed\nto catalyze research in financial AI. We propose a robust and generalizable\nknowledge graph (KG) construction framework that integrates intelligent\ndocument parsing, table-aware chunking, and schema-guided iterative extraction\nwith a reflection-driven feedback loop. Our system incorporates a comprehensive\nevaluation pipeline, combining rule-based checks, statistical validation, and\nLLM-as-a-Judge assessments to holistically measure extraction quality. We\nsupport three extraction modes - single-pass, multi-pass, and\nreflection-agent-based - allowing flexible trade-offs between efficiency,\naccuracy, and reliability based on user requirements. Empirical evaluations\ndemonstrate that the reflection-agent-based mode consistently achieves the best\nbalance, attaining a 64.8 percent compliance score against all rule-based\npolicies (CheckRules) and outperforming baseline methods (single-pass and\nmulti-pass) across key metrics such as precision, comprehensiveness, and\nrelevance in LLM-guided evaluations."
                },
                "authors": [
                    {
                        "name": "Abhinav Arun"
                    },
                    {
                        "name": "Fabrizio Dimino"
                    },
                    {
                        "name": "Tejas Prakash Agarwal"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17905v1",
                "updated": "2025-08-25T11:22:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    22,
                    58,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:22:58Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    22,
                    58,
                    0,
                    237,
                    0
                ],
                "title": "Pandora: Leveraging Code-driven Knowledge Transfer for Unified\n  Structured Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pandora: Leveraging Code-driven Knowledge Transfer for Unified\n  Structured Knowledge Reasoning"
                },
                "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions by using structured sources such as tables, databases, and knowledge\ngraphs in a unified way. Existing USKR methods rely on task-specific strategies\nor bespoke representations, which hinder their ability to dismantle barriers\nbetween different SKR tasks, thereby constraining their overall performance in\ncross-task scenarios. In this paper, we introduce \\textsc{Pandora}, a novel\nUSKR framework that addresses the limitations of existing methods by leveraging\ntwo key innovations. First, we propose a code-based unified knowledge\nrepresentation using \\textsc{Python}'s \\textsc{Pandas} API, which aligns\nseamlessly with the pre-training of LLMs. This representation facilitates a\ncohesive approach to handling different structured knowledge sources. Building\non this foundation, we employ knowledge transfer to bolster the unified\nreasoning process of LLMs by automatically building cross-task memory. By\nadaptively correcting reasoning using feedback from code execution,\n\\textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive\nexperiments on six widely used benchmarks across three SKR tasks demonstrate\nthat \\textsc{Pandora} outperforms existing unified reasoning frameworks and\ncompetes effectively with task-specific methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions by using structured sources such as tables, databases, and knowledge\ngraphs in a unified way. Existing USKR methods rely on task-specific strategies\nor bespoke representations, which hinder their ability to dismantle barriers\nbetween different SKR tasks, thereby constraining their overall performance in\ncross-task scenarios. In this paper, we introduce \\textsc{Pandora}, a novel\nUSKR framework that addresses the limitations of existing methods by leveraging\ntwo key innovations. First, we propose a code-based unified knowledge\nrepresentation using \\textsc{Python}'s \\textsc{Pandas} API, which aligns\nseamlessly with the pre-training of LLMs. This representation facilitates a\ncohesive approach to handling different structured knowledge sources. Building\non this foundation, we employ knowledge transfer to bolster the unified\nreasoning process of LLMs by automatically building cross-task memory. By\nadaptively correcting reasoning using feedback from code execution,\n\\textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive\nexperiments on six widely used benchmarks across three SKR tasks demonstrate\nthat \\textsc{Pandora} outperforms existing unified reasoning frameworks and\ncompetes effectively with task-specific methods."
                },
                "authors": [
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Junhao He"
                    },
                    {
                        "name": "Linbo Fu"
                    },
                    {
                        "name": "Shenyu Zhang"
                    },
                    {
                        "name": "Rihui Jin"
                    },
                    {
                        "name": "Xinbang Dai"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Dehai Min"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Tongtong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongtong Wu"
                },
                "author": "Tongtong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16390v2",
                "updated": "2025-08-25T11:17:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    17,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T13:48:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    48,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "MedQARo: A Large-Scale Benchmark for Medical Question Answering in\n  Romanian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedQARo: A Large-Scale Benchmark for Medical Question Answering in\n  Romanian"
                },
                "summary": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nMedQARo, the first large-scale medical QA benchmark in Romanian, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. MedQARo is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on MedQARo. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on MedQARo. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/MedQARo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nMedQARo, the first large-scale medical QA benchmark in Romanian, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. MedQARo is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on MedQARo. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on MedQARo. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/MedQARo."
                },
                "authors": [
                    {
                        "name": "Ana-Cristina Rogoz"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Alexandra-Valentina Anghel"
                    },
                    {
                        "name": "Ionut-Lucian Antone-Iordache"
                    },
                    {
                        "name": "Simona Coniac"
                    },
                    {
                        "name": "Andreea Iuliana Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Iuliana Ionescu"
                },
                "author": "Andreea Iuliana Ionescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17901v1",
                "updated": "2025-08-25T11:15:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    15,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:15:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    15,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "Riemannian Optimization for LoRA on the Stiefel Manifold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riemannian Optimization for LoRA on the Stiefel Manifold"
                },
                "summary": "While powerful, large language models (LLMs) present significant fine-tuning\nchallenges due to their size. Parameter-efficient fine-tuning (PEFT) methods\nlike LoRA provide solutions, yet suffer from critical optimizer inefficiencies;\nnotably basis redundancy in LoRA's $B$ matrix when using AdamW, which\nfundamentally limits performance. We address this by optimizing the $B$ matrix\non the Stiefel manifold, imposing explicit orthogonality constraints that\nachieve near-perfect orthogonality and full effective rank. This geometric\napproach dramatically enhances parameter efficiency and representational\ncapacity. Our Stiefel optimizer consistently outperforms AdamW across\nbenchmarks with both LoRA and DoRA, demonstrating that geometric constraints\nare the key to unlocking LoRA's full potential for effective LLM fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While powerful, large language models (LLMs) present significant fine-tuning\nchallenges due to their size. Parameter-efficient fine-tuning (PEFT) methods\nlike LoRA provide solutions, yet suffer from critical optimizer inefficiencies;\nnotably basis redundancy in LoRA's $B$ matrix when using AdamW, which\nfundamentally limits performance. We address this by optimizing the $B$ matrix\non the Stiefel manifold, imposing explicit orthogonality constraints that\nachieve near-perfect orthogonality and full effective rank. This geometric\napproach dramatically enhances parameter efficiency and representational\ncapacity. Our Stiefel optimizer consistently outperforms AdamW across\nbenchmarks with both LoRA and DoRA, demonstrating that geometric constraints\nare the key to unlocking LoRA's full potential for effective LLM fine-tuning."
                },
                "authors": [
                    {
                        "name": "Juneyoung Park"
                    },
                    {
                        "name": "Minjae Kang"
                    },
                    {
                        "name": "Seongbae Lee"
                    },
                    {
                        "name": "Haegang Lee"
                    },
                    {
                        "name": "Seongwan Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.18271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18271v1",
                "updated": "2025-08-25T17:59:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    59,
                    40,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:59:40Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    59,
                    40,
                    0,
                    237,
                    0
                ],
                "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion\n  Models"
                },
                "summary": "3D inpainting often relies on multi-view 2D image inpainting, where the\ninherent inconsistencies across different inpainted views can result in blurred\ntextures, spatial discontinuities, and distracting visual artifacts. These\ninconsistencies pose significant challenges when striving for accurate and\nrealistic 3D object completion, particularly in applications that demand high\nfidelity and structural coherence. To overcome these limitations, we propose\nObjFiller-3D, a novel method designed for the completion and editing of\nhigh-quality and consistent 3D objects. Instead of employing a conventional 2D\nimage inpainting model, our approach leverages a curated selection of\nstate-of-the-art video editing model to fill in the masked regions of 3D\nobjects. We analyze the representation gap between 3D and videos, and propose\nan adaptation of a video inpainting model for 3D scene inpainting. In addition,\nwe introduce a reference-based 3D inpainting method to further enhance the\nquality of reconstruction. Experiments across diverse datasets show that\ncompared to previous methods, ObjFiller-3D produces more faithful and\nfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of\n0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for\npractical deployment in real-world 3D editing applications. Project page:\nhttps://objfiller3d.github.io/ Code:\nhttps://github.com/objfiller3d/ObjFiller-3D .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D inpainting often relies on multi-view 2D image inpainting, where the\ninherent inconsistencies across different inpainted views can result in blurred\ntextures, spatial discontinuities, and distracting visual artifacts. These\ninconsistencies pose significant challenges when striving for accurate and\nrealistic 3D object completion, particularly in applications that demand high\nfidelity and structural coherence. To overcome these limitations, we propose\nObjFiller-3D, a novel method designed for the completion and editing of\nhigh-quality and consistent 3D objects. Instead of employing a conventional 2D\nimage inpainting model, our approach leverages a curated selection of\nstate-of-the-art video editing model to fill in the masked regions of 3D\nobjects. We analyze the representation gap between 3D and videos, and propose\nan adaptation of a video inpainting model for 3D scene inpainting. In addition,\nwe introduce a reference-based 3D inpainting method to further enhance the\nquality of reconstruction. Experiments across diverse datasets show that\ncompared to previous methods, ObjFiller-3D produces more faithful and\nfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of\n0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for\npractical deployment in real-world 3D editing applications. Project page:\nhttps://objfiller3d.github.io/ Code:\nhttps://github.com/objfiller3d/ObjFiller-3D ."
                },
                "authors": [
                    {
                        "name": "Haitang Feng"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Gangshan Wu"
                    },
                    {
                        "name": "Beiqi Chen"
                    },
                    {
                        "name": "Jianhuang Lai"
                    },
                    {
                        "name": "Guangcong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangcong Wang"
                },
                "author": "Guangcong Wang",
                "arxiv_comment": "Project page: https://objfiller3d.github.io/ Code:\n  https://github.com/objfiller3d/ObjFiller-3D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18265v1",
                "updated": "2025-08-25T17:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    58,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:58:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    58,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency"
                },
                "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released."
                },
                "authors": [
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhangwei Gao"
                    },
                    {
                        "name": "Lixin Gu"
                    },
                    {
                        "name": "Hengjun Pu"
                    },
                    {
                        "name": "Long Cui"
                    },
                    {
                        "name": "Xingguang Wei"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Linglin Jing"
                    },
                    {
                        "name": "Shenglong Ye"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Ganlin Yang"
                    },
                    {
                        "name": "Haomin Wang"
                    },
                    {
                        "name": "Qi Wei"
                    },
                    {
                        "name": "Jinhui Yin"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Guanzhou Chen"
                    },
                    {
                        "name": "Zichen Ding"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Zehao Li"
                    },
                    {
                        "name": "Bowen Yang"
                    },
                    {
                        "name": "Yuchen Duan"
                    },
                    {
                        "name": "Xuehui Wang"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Nianchen Deng"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yingtong Xiong"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Huipeng Deng"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Jiaye Ge"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Min Dou"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Gen Luo"
                    }
                ],
                "author_detail": {
                    "name": "Gen Luo"
                },
                "author": "Gen Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23840v3",
                "updated": "2025-08-26T02:19:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    2,
                    19,
                    29,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-28T14:05:46Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    5,
                    46,
                    2,
                    148,
                    0
                ],
                "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Sycophancy of Language Models in Multi-turn Dialogues"
                },
                "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench."
                },
                "authors": [
                    {
                        "name": "Jiseung Hong"
                    },
                    {
                        "name": "Grace Byun"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18253v1",
                "updated": "2025-08-25T17:41:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    46,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:41:46Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    46,
                    0,
                    237,
                    0
                ],
                "title": "From BERT to LLMs: Comparing and Understanding Chinese Classifier\n  Prediction in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From BERT to LLMs: Comparing and Understanding Chinese Classifier\n  Prediction in Language Models"
                },
                "summary": "Classifiers are an important and defining feature of the Chinese language,\nand their correct prediction is key to numerous educational applications. Yet,\nwhether the most popular Large Language Models (LLMs) possess proper knowledge\nthe Chinese classifiers is an issue that has largely remain unexplored in the\nNatural Language Processing (NLP) literature.\n  To address such a question, we employ various masking strategies to evaluate\nthe LLMs' intrinsic ability, the contribution of different sentence elements,\nand the working of the attention mechanisms during prediction. Besides, we\nexplore fine-tuning for LLMs to enhance the classifier performance.\n  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.\nThe prediction, as expected, greatly benefits from the information about the\nfollowing noun, which also explains the advantage of models with a\nbidirectional attention mechanism such as BERT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifiers are an important and defining feature of the Chinese language,\nand their correct prediction is key to numerous educational applications. Yet,\nwhether the most popular Large Language Models (LLMs) possess proper knowledge\nthe Chinese classifiers is an issue that has largely remain unexplored in the\nNatural Language Processing (NLP) literature.\n  To address such a question, we employ various masking strategies to evaluate\nthe LLMs' intrinsic ability, the contribution of different sentence elements,\nand the working of the attention mechanisms during prediction. Besides, we\nexplore fine-tuning for LLMs to enhance the classifier performance.\n  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.\nThe prediction, as expected, greatly benefits from the information about the\nfollowing noun, which also explains the advantage of models with a\nbidirectional attention mechanism such as BERT."
                },
                "authors": [
                    {
                        "name": "ZiqiZhang"
                    },
                    {
                        "name": "Jianfei Ma"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Jieshun You"
                    },
                    {
                        "name": "Zhaoxin Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxin Feng"
                },
                "author": "Zhaoxin Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18245v1",
                "updated": "2025-08-25T17:36:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    58,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    58,
                    0,
                    237,
                    0
                ],
                "title": "Demographic Biases and Gaps in the Perception of Sexism in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographic Biases and Gaps in the Perception of Sexism in Large\n  Language Models"
                },
                "summary": "The use of Large Language Models (LLMs) has proven to be a tool that could\nhelp in the automatic detection of sexism. Previous studies have shown that\nthese models contain biases that do not accurately reflect reality, especially\nfor minority groups. Despite various efforts to improve the detection of sexist\ncontent, this task remains a significant challenge due to its subjective nature\nand the biases present in automated models. We explore the capabilities of\ndifferent LLMs to detect sexism in social media text using the EXIST 2024 tweet\ndataset. It includes annotations from six distinct profiles for each tweet,\nallowing us to evaluate to what extent LLMs can mimic these groups' perceptions\nin sexism detection. Additionally, we analyze the demographic biases present in\nthe models and conduct a statistical analysis to identify which demographic\ncharacteristics (age, gender) contribute most effectively to this task. Our\nresults show that, while LLMs can to some extent detect sexism when considering\nthe overall opinion of populations, they do not accurately replicate the\ndiversity of perceptions among different demographic groups. This highlights\nthe need for better-calibrated models that account for the diversity of\nperspectives across different populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) has proven to be a tool that could\nhelp in the automatic detection of sexism. Previous studies have shown that\nthese models contain biases that do not accurately reflect reality, especially\nfor minority groups. Despite various efforts to improve the detection of sexist\ncontent, this task remains a significant challenge due to its subjective nature\nand the biases present in automated models. We explore the capabilities of\ndifferent LLMs to detect sexism in social media text using the EXIST 2024 tweet\ndataset. It includes annotations from six distinct profiles for each tweet,\nallowing us to evaluate to what extent LLMs can mimic these groups' perceptions\nin sexism detection. Additionally, we analyze the demographic biases present in\nthe models and conduct a statistical analysis to identify which demographic\ncharacteristics (age, gender) contribute most effectively to this task. Our\nresults show that, while LLMs can to some extent detect sexism when considering\nthe overall opinion of populations, they do not accurately replicate the\ndiversity of perceptions among different demographic groups. This highlights\nthe need for better-calibrated models that account for the diversity of\nperspectives across different populations."
                },
                "authors": [
                    {
                        "name": "Judith Tavarez-Rodr√≠guez"
                    },
                    {
                        "name": "Fernando S√°nchez-Vega"
                    },
                    {
                        "name": "A. Pastor L√≥pez-Monroy"
                    }
                ],
                "author_detail": {
                    "name": "A. Pastor L√≥pez-Monroy"
                },
                "author": "A. Pastor L√≥pez-Monroy",
                "arxiv_comment": "This work was presented as a poster at the Latin American Meeting in\n  Artificial Intelligence KHIPU 2025, Santiago, Chile, March 10th - 14th 2025,\n  https://khipu.ai/khipu2025/poster-sessions-2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18244v1",
                "updated": "2025-08-25T17:36:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:36:21Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    36,
                    21,
                    0,
                    237,
                    0
                ],
                "title": "Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows\n  to Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows\n  to Data"
                },
                "summary": "Reliably composing Large Language Models (LLMs) for complex, multi-step\nworkflows remains a significant challenge. The dominant paradigm-optimizing\ndiscrete prompts in a pipeline-is notoriously brittle and struggles to enforce\nthe formal compliance required for structured tasks. We introduce\nType-Compliant Adaptation Cascades (TACs), a framework that recasts workflow\nadaptation as learning typed probabilistic programs. TACs treats the entire\nworkflow, which is composed of parameter-efficiently adapted LLMs and\ndeterministic logic, as an unnormalized joint distribution. This enables\nprincipled, gradient-based training even with latent intermediate structures.\nWe provide theoretical justification for our tractable optimization objective,\nproving that the optimization bias vanishes as the model learns type\ncompliance. Empirically, TACs significantly outperforms state-of-the-art\nprompt-optimization baselines. Gains are particularly pronounced on structured\ntasks, improving MGSM-SymPy from $57.1\\%$ to $75.9\\%$ for a 27B model, MGSM\nfrom $1.6\\%$ to $27.3\\%$ for a 7B model. TACs offers a robust and theoretically\ngrounded paradigm for developing reliable, task-compliant LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliably composing Large Language Models (LLMs) for complex, multi-step\nworkflows remains a significant challenge. The dominant paradigm-optimizing\ndiscrete prompts in a pipeline-is notoriously brittle and struggles to enforce\nthe formal compliance required for structured tasks. We introduce\nType-Compliant Adaptation Cascades (TACs), a framework that recasts workflow\nadaptation as learning typed probabilistic programs. TACs treats the entire\nworkflow, which is composed of parameter-efficiently adapted LLMs and\ndeterministic logic, as an unnormalized joint distribution. This enables\nprincipled, gradient-based training even with latent intermediate structures.\nWe provide theoretical justification for our tractable optimization objective,\nproving that the optimization bias vanishes as the model learns type\ncompliance. Empirically, TACs significantly outperforms state-of-the-art\nprompt-optimization baselines. Gains are particularly pronounced on structured\ntasks, improving MGSM-SymPy from $57.1\\%$ to $75.9\\%$ for a 27B model, MGSM\nfrom $1.6\\%$ to $27.3\\%$ for a 7B model. TACs offers a robust and theoretically\ngrounded paradigm for developing reliable, task-compliant LLM systems."
                },
                "authors": [
                    {
                        "name": "Chu-Cheng Lin"
                    },
                    {
                        "name": "Daiyi Peng"
                    },
                    {
                        "name": "Yifeng Lu"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Eugene Ie"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Ie"
                },
                "author": "Eugene Ie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18224v1",
                "updated": "2025-08-25T17:22:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    22,
                    15,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:22:15Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    22,
                    15,
                    0,
                    237,
                    0
                ],
                "title": "Flash Sparse Attention: An Alternative Efficient Implementation of\n  Native Sparse Attention Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Sparse Attention: An Alternative Efficient Implementation of\n  Native Sparse Attention Kernel"
                },
                "summary": "Recent progress in sparse attention mechanisms has demonstrated strong\npotential for reducing the computational cost of long-context training and\ninference in large language models (LLMs). Native Sparse Attention (NSA), a\nstate-of-the-art approach, introduces natively trainable, hardware-aligned\nsparse attention that delivers substantial system-level performance gains while\nmaintaining accuracy comparable to full attention. However, the kernel\nimplementation of NSA relies on a query-grouping strategy that is efficient\nonly with large Grouped Query Attention (GQA) sizes, whereas modern LLMs\ntypically adopt much smaller GQA groups, which limits the applicability of this\nsparse algorithmic advance. In this work, we propose Flash Sparse Attention\n(FSA), which includes an alternative kernel design that enables efficient NSA\ncomputation across a wide range of popular LLMs with varied smaller GQA group\nsizes on modern GPUs. Compared to vanilla NSA kernel implementation, our\nempirical evaluation demonstrates that FSA achieves (i) up to 3.5$\\times$ and\non average 1.6$\\times$ kernel-level latency reduction, (ii) up to 1.25$\\times$\nand 1.09$\\times$ on average end-to-end training speedup on state-of-the-art\nLLMs, and (iii) up to 1.36$\\times$ and 1.11$\\times$ on average end-to-end\nprefill speedup on state-of-the-art LLMs. The source code is open-sourced and\npublicly available at\nhttps://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in sparse attention mechanisms has demonstrated strong\npotential for reducing the computational cost of long-context training and\ninference in large language models (LLMs). Native Sparse Attention (NSA), a\nstate-of-the-art approach, introduces natively trainable, hardware-aligned\nsparse attention that delivers substantial system-level performance gains while\nmaintaining accuracy comparable to full attention. However, the kernel\nimplementation of NSA relies on a query-grouping strategy that is efficient\nonly with large Grouped Query Attention (GQA) sizes, whereas modern LLMs\ntypically adopt much smaller GQA groups, which limits the applicability of this\nsparse algorithmic advance. In this work, we propose Flash Sparse Attention\n(FSA), which includes an alternative kernel design that enables efficient NSA\ncomputation across a wide range of popular LLMs with varied smaller GQA group\nsizes on modern GPUs. Compared to vanilla NSA kernel implementation, our\nempirical evaluation demonstrates that FSA achieves (i) up to 3.5$\\times$ and\non average 1.6$\\times$ kernel-level latency reduction, (ii) up to 1.25$\\times$\nand 1.09$\\times$ on average end-to-end training speedup on state-of-the-art\nLLMs, and (iii) up to 1.36$\\times$ and 1.11$\\times$ on average end-to-end\nprefill speedup on state-of-the-art LLMs. The source code is open-sourced and\npublicly available at\nhttps://github.com/Relaxed-System-Lab/Flash-Sparse-Attention."
                },
                "authors": [
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15180v2",
                "updated": "2025-08-25T17:16:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    16,
                    0,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T02:36:16Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    2,
                    36,
                    16,
                    3,
                    233,
                    0
                ],
                "title": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data"
                },
                "summary": "High-quality mathematical and logical datasets with verifiable answers are\nessential for strengthening the reasoning capabilities of large language models\n(LLMs). While recent data augmentation techniques have facilitated the creation\nof large-scale benchmarks, existing LLM-generated datasets often suffer from\nlimited reliability, diversity, and scalability. To address these challenges,\nwe introduce PuzzleClone, a formal framework for synthesizing verifiable data\nat scale using Satisfiability Modulo Theories (SMT). Our approach features\nthree key innovations: (1) encoding seed puzzles into structured logical\nspecifications, (2) generating scalable variants through systematic variable\nand constraint randomization, and (3) ensuring validity via a reproduction\nmechanism. Applying PuzzleClone, we construct a curated benchmark comprising\nover 83K diverse and programmatically validated puzzles. The generated puzzles\nspan a wide spectrum of difficulty and formats, posing significant challenges\nto current state-of-the-art models. We conduct post training (SFT and RL) on\nPuzzleClone datasets. Experimental results show that training on PuzzleClone\nyields substantial improvements not only on PuzzleClone testset but also on\nlogic and mathematical benchmarks. Post training raises PuzzleClone average\nfrom 14.4 to 56.2 and delivers consistent improvements across 7 logic and\nmathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from\n52.5 to 65.0). Our code and data are available at\nhttps://github.com/HiThink-Research/PuzzleClone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality mathematical and logical datasets with verifiable answers are\nessential for strengthening the reasoning capabilities of large language models\n(LLMs). While recent data augmentation techniques have facilitated the creation\nof large-scale benchmarks, existing LLM-generated datasets often suffer from\nlimited reliability, diversity, and scalability. To address these challenges,\nwe introduce PuzzleClone, a formal framework for synthesizing verifiable data\nat scale using Satisfiability Modulo Theories (SMT). Our approach features\nthree key innovations: (1) encoding seed puzzles into structured logical\nspecifications, (2) generating scalable variants through systematic variable\nand constraint randomization, and (3) ensuring validity via a reproduction\nmechanism. Applying PuzzleClone, we construct a curated benchmark comprising\nover 83K diverse and programmatically validated puzzles. The generated puzzles\nspan a wide spectrum of difficulty and formats, posing significant challenges\nto current state-of-the-art models. We conduct post training (SFT and RL) on\nPuzzleClone datasets. Experimental results show that training on PuzzleClone\nyields substantial improvements not only on PuzzleClone testset but also on\nlogic and mathematical benchmarks. Post training raises PuzzleClone average\nfrom 14.4 to 56.2 and delivers consistent improvements across 7 logic and\nmathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from\n52.5 to 65.0). Our code and data are available at\nhttps://github.com/HiThink-Research/PuzzleClone."
                },
                "authors": [
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Yanwei Huang"
                    },
                    {
                        "name": "Rongjunchen Zhang"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Haipang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Haipang Wu"
                },
                "author": "Haipang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18214v1",
                "updated": "2025-08-25T17:13:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    13,
                    30,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:13:30Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    13,
                    30,
                    0,
                    237,
                    0
                ],
                "title": "AI Data Centers Need Pioneers to Deliver Scalable Power via Offgrid AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Data Centers Need Pioneers to Deliver Scalable Power via Offgrid AI"
                },
                "summary": "The scalable computing revolution of the late '80s through mid- '00s forged a\nnew technical and economic model for computing that delivered massive societal\nimpact, but its economic benefit has driven scalability to sizes that are now\nexhausting the energy grid's capacity. Our time demands a new revolution in\nscalable energy, mirroring in key ways the scalable computing revolution; e.g.,\ncompelling economic forces, use of mass-market components, overcoming foibles\nof those components, judicious use of physical locality, and the the difficult\nintegration into an effective system. The offgrid AI approach closely fits this\nmold, combining local mostly renewable generation and storage to power an AI\ndata center, starting offgrid. Obstacles to delivering this approach are\nsocial, technical, and project, but the potential is massive. I argue that the\noffgrid-AI approach needs pioneers among both system developers and\nAI-data-center operators to move it quickly from concept to large-scale\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalable computing revolution of the late '80s through mid- '00s forged a\nnew technical and economic model for computing that delivered massive societal\nimpact, but its economic benefit has driven scalability to sizes that are now\nexhausting the energy grid's capacity. Our time demands a new revolution in\nscalable energy, mirroring in key ways the scalable computing revolution; e.g.,\ncompelling economic forces, use of mass-market components, overcoming foibles\nof those components, judicious use of physical locality, and the the difficult\nintegration into an effective system. The offgrid AI approach closely fits this\nmold, combining local mostly renewable generation and storage to power an AI\ndata center, starting offgrid. Obstacles to delivering this approach are\nsocial, technical, and project, but the potential is massive. I argue that the\noffgrid-AI approach needs pioneers among both system developers and\nAI-data-center operators to move it quickly from concept to large-scale\ndeployment."
                },
                "authors": [
                    {
                        "name": "Steven P. Reinhardt"
                    }
                ],
                "author_detail": {
                    "name": "Steven P. Reinhardt"
                },
                "author": "Steven P. Reinhardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18210v1",
                "updated": "2025-08-25T17:10:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    10,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:10:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    10,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center\n  Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center\n  Dialogue Generation"
                },
                "summary": "Synthetic transcript generation is critical in contact center domains, where\nprivacy and data scarcity limit model training and evaluation. Unlike prior\nsynthetic dialogue generation work on open-domain or medical dialogues, contact\ncenter conversations are goal-oriented, role-asymmetric, and behaviorally\ncomplex, featuring disfluencies, ASR noise, and compliance-driven agent\nactions. In deployments where transcripts are unavailable, standard pipelines\nstill yield derived call attributes such as Intent Summaries, Topic Flow, and\nQA Evaluation Forms. We leverage these as supervision signals to guide\ngeneration. To assess the quality of such outputs, we introduce a diagnostic\nframework of 18 linguistically and behaviorally grounded metrics for comparing\nreal and synthetic transcripts. We benchmark four language-agnostic generation\nstrategies, from simple prompting to characteristic-aware multi-stage\napproaches, alongside reference-free baselines. Results reveal persistent\nchallenges: no method excels across all traits, with notable deficits in\ndisfluency, sentiment, and behavioral realism. Our diagnostic tool exposes\nthese gaps, enabling fine-grained evaluation and stress testing of synthetic\ndialogue across languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic transcript generation is critical in contact center domains, where\nprivacy and data scarcity limit model training and evaluation. Unlike prior\nsynthetic dialogue generation work on open-domain or medical dialogues, contact\ncenter conversations are goal-oriented, role-asymmetric, and behaviorally\ncomplex, featuring disfluencies, ASR noise, and compliance-driven agent\nactions. In deployments where transcripts are unavailable, standard pipelines\nstill yield derived call attributes such as Intent Summaries, Topic Flow, and\nQA Evaluation Forms. We leverage these as supervision signals to guide\ngeneration. To assess the quality of such outputs, we introduce a diagnostic\nframework of 18 linguistically and behaviorally grounded metrics for comparing\nreal and synthetic transcripts. We benchmark four language-agnostic generation\nstrategies, from simple prompting to characteristic-aware multi-stage\napproaches, alongside reference-free baselines. Results reveal persistent\nchallenges: no method excels across all traits, with notable deficits in\ndisfluency, sentiment, and behavioral realism. Our diagnostic tool exposes\nthese gaps, enabling fine-grained evaluation and stress testing of synthetic\ndialogue across languages."
                },
                "authors": [
                    {
                        "name": "Rishikesh Devanathan"
                    },
                    {
                        "name": "Varun Nathan"
                    },
                    {
                        "name": "Ayush Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Kumar"
                },
                "author": "Ayush Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16315v2",
                "updated": "2025-08-25T17:04:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    4,
                    49,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T11:52:04Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    52,
                    4,
                    4,
                    234,
                    0
                ],
                "title": "OwkinZero: Accelerating Biological Discovery with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OwkinZero: Accelerating Biological Discovery with AI"
                },
                "summary": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Nathan Bigaud"
                    },
                    {
                        "name": "Vincent Cabeli"
                    },
                    {
                        "name": "Meltem G√ºrel"
                    },
                    {
                        "name": "Arthur Pignet"
                    },
                    {
                        "name": "John Klein"
                    },
                    {
                        "name": "Gilles Wainrib"
                    },
                    {
                        "name": "Eric Durand"
                    }
                ],
                "author_detail": {
                    "name": "Eric Durand"
                },
                "author": "Eric Durand",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v2",
                "updated": "2025-08-25T16:58:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    58,
                    23,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "TranSQL+: Serving Large Language Models with SQL on Low-Resource\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TranSQL+: Serving Large Language Models with SQL on Low-Resource\n  Hardware"
                },
                "summary": "Deploying Large Language Models (LLMs) on resource-constrained devices\nremains challenging due to limited memory, lack of GPUs, and the complexity of\nexisting runtimes. In this paper, we introduce TranSQL+, a template-based code\ngenerator that translates LLM computation graphs into pure SQL queries for\nexecution in relational databases. Without relying on external libraries,\nTranSQL+, leverages mature database features, such as vectorized execution and\nout-of-core processing, for efficient inference. We further propose a\nrow-to-column (ROW2COL) optimization that improves join efficiency in matrix\noperations. Evaluated on Llama3-8B and DeepSeekMoE models, TranSQL+ achieves up\nto 20x lower prefill latency and 4x higher decoding speed compared to DeepSpeed\nInference and Llama.cpp in low-memory and CPU-only configurations. Our results\nhighlight relational databases as a practical environment for LLMs on\nlow-resource hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) on resource-constrained devices\nremains challenging due to limited memory, lack of GPUs, and the complexity of\nexisting runtimes. In this paper, we introduce TranSQL+, a template-based code\ngenerator that translates LLM computation graphs into pure SQL queries for\nexecution in relational databases. Without relying on external libraries,\nTranSQL+, leverages mature database features, such as vectorized execution and\nout-of-core processing, for efficient inference. We further propose a\nrow-to-column (ROW2COL) optimization that improves join efficiency in matrix\noperations. Evaluated on Llama3-8B and DeepSeekMoE models, TranSQL+ achieves up\nto 20x lower prefill latency and 4x higher decoding speed compared to DeepSpeed\nInference and Llama.cpp in low-memory and CPU-only configurations. Our results\nhighlight relational databases as a practical environment for LLMs on\nlow-resource hardware."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "arxiv_comment": "Accepted by SIGMOD2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18192v1",
                "updated": "2025-08-25T16:49:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    49,
                    38,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:49:38Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    49,
                    38,
                    0,
                    237,
                    0
                ],
                "title": "Unraveling the cognitive patterns of Large Language Models through\n  module communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the cognitive patterns of Large Language Models through\n  module communities"
                },
                "summary": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions."
                },
                "authors": [
                    {
                        "name": "Kushal Raj Bhandari"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Jianxi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Gao"
                },
                "author": "Jianxi Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07974v2",
                "updated": "2025-08-25T16:49:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    49,
                    10,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-10T17:51:05Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    51,
                    5,
                    3,
                    191,
                    0
                ],
                "title": "Defending Against Prompt Injection With a Few DefensiveTokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending Against Prompt Injection With a Few DefensiveTokens"
                },
                "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken."
                },
                "authors": [
                    {
                        "name": "Sizhe Chen"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Chawin Sitawarin"
                    },
                    {
                        "name": "David Wagner"
                    }
                ],
                "author_detail": {
                    "name": "David Wagner"
                },
                "author": "David Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18190v2",
                "updated": "2025-08-26T08:10:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    10,
                    55,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T16:48:51Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    48,
                    51,
                    0,
                    237,
                    0
                ],
                "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering"
                },
                "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor."
                },
                "authors": [
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Boyu Niu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Boxiu Li"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12964v2",
                "updated": "2025-08-25T16:47:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    47,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-18T15:46:31Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    46,
                    31,
                    1,
                    49,
                    0
                ],
                "title": "Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the\n  Answer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the\n  Answer"
                },
                "summary": "Prior work on large language model (LLM) hallucinations has associated them\nwith model uncertainty or inaccurate knowledge. In this work, we define and\ninvestigate a distinct type of hallucination, where a model can consistently\nanswer a question correctly, but a seemingly trivial perturbation, which can\nhappen in real-world settings, causes it to produce a hallucinated response\nwith high certainty. This phenomenon, which we dub CHOKE (Certain\nHallucinations Overriding Known Evidence), is particularly concerning in\nhigh-stakes domains such as medicine or law, where model certainty is often\nused as a proxy for reliability. We show that CHOKE examples are consistent\nacross prompts, occur in different models and datasets, and are fundamentally\ndistinct from other hallucinations. This difference leads existing mitigation\nmethods to perform worse on CHOKE examples than on general hallucinations.\nFinally, we introduce a probing-based mitigation that outperforms existing\nmethods on CHOKE hallucinations. These findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on large language model (LLM) hallucinations has associated them\nwith model uncertainty or inaccurate knowledge. In this work, we define and\ninvestigate a distinct type of hallucination, where a model can consistently\nanswer a question correctly, but a seemingly trivial perturbation, which can\nhappen in real-world settings, causes it to produce a hallucinated response\nwith high certainty. This phenomenon, which we dub CHOKE (Certain\nHallucinations Overriding Known Evidence), is particularly concerning in\nhigh-stakes domains such as medicine or law, where model certainty is often\nused as a proxy for reliability. We show that CHOKE examples are consistent\nacross prompts, occur in different models and datasets, and are fundamentally\ndistinct from other hallucinations. This difference leads existing mitigation\nmethods to perform worse on CHOKE examples than on general hallucinations.\nFinally, we introduce a probing-based mitigation that outperforms existing\nmethods on CHOKE hallucinations. These findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Itay Itzhak"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18188v1",
                "updated": "2025-08-25T16:46:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    46,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:46:21Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    46,
                    21,
                    0,
                    237,
                    0
                ],
                "title": "Explain and Monitor Deep Learning Models for Computer Vision using Obz\n  AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain and Monitor Deep Learning Models for Computer Vision using Obz\n  AI"
                },
                "summary": "Deep learning has transformed computer vision (CV), achieving outstanding\nperformance in classification, segmentation, and related tasks. Such AI-based\nCV systems are becoming prevalent, with applications spanning from medical\nimaging to surveillance. State of the art models such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs) are often regarded as ``black\nboxes,'' offering limited transparency into their decision-making processes.\nDespite a recent advancement in explainable AI (XAI), explainability remains\nunderutilized in practical CV deployments. A primary obstacle is the absence of\nintegrated software solutions that connect XAI techniques with robust knowledge\nmanagement and monitoring frameworks. To close this gap, we have developed Obz\nAI, a comprehensive software ecosystem designed to facilitate state-of-the-art\nexplainability and observability for vision AI systems. Obz AI provides a\nseamless integration pipeline, from a Python client library to a full-stack\nanalytics dashboard. With Obz AI, a machine learning engineer can easily\nincorporate advanced XAI methodologies, extract and analyze features for\noutlier detection, and continuously monitor AI models in real time. By making\nthe decision-making mechanisms of deep models interpretable, Obz AI promotes\nobservability and responsible deployment of computer vision systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has transformed computer vision (CV), achieving outstanding\nperformance in classification, segmentation, and related tasks. Such AI-based\nCV systems are becoming prevalent, with applications spanning from medical\nimaging to surveillance. State of the art models such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs) are often regarded as ``black\nboxes,'' offering limited transparency into their decision-making processes.\nDespite a recent advancement in explainable AI (XAI), explainability remains\nunderutilized in practical CV deployments. A primary obstacle is the absence of\nintegrated software solutions that connect XAI techniques with robust knowledge\nmanagement and monitoring frameworks. To close this gap, we have developed Obz\nAI, a comprehensive software ecosystem designed to facilitate state-of-the-art\nexplainability and observability for vision AI systems. Obz AI provides a\nseamless integration pipeline, from a Python client library to a full-stack\nanalytics dashboard. With Obz AI, a machine learning engineer can easily\nincorporate advanced XAI methodologies, extract and analyze features for\noutlier detection, and continuously monitor AI models in real time. By making\nthe decision-making mechanisms of deep models interpretable, Obz AI promotes\nobservability and responsible deployment of computer vision systems."
                },
                "authors": [
                    {
                        "name": "Neo Christopher Chung"
                    },
                    {
                        "name": "Jakub Binda"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Binda"
                },
                "author": "Jakub Binda",
                "arxiv_journal_ref": "2025 Conference on Information and Knowledge Management (CIKM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18183v1",
                "updated": "2025-08-25T16:36:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    36,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    36,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios"
                },
                "summary": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities."
                },
                "authors": [
                    {
                        "name": "Luana Bulla"
                    },
                    {
                        "name": "Gabriele Tuccio"
                    },
                    {
                        "name": "Misael Mongiov√¨"
                    },
                    {
                        "name": "Aldo Gangemi"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Gangemi"
                },
                "author": "Aldo Gangemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18182v1",
                "updated": "2025-08-25T16:35:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    35,
                    57,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:35:57Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    35,
                    57,
                    0,
                    237,
                    0
                ],
                "title": "AdLoCo: adaptive batching significantly improves communications\n  efficiency and convergence for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdLoCo: adaptive batching significantly improves communications\n  efficiency and convergence for Large Language Models"
                },
                "summary": "Scaling distributed training of Large Language Models (LLMs) requires not\nonly algorithmic advances but also efficient utilization of heterogeneous\nhardware resources. While existing methods such as DiLoCo have demonstrated\npromising results, they often fail to fully exploit computational clusters\nunder dynamic workloads. To address this limitation, we propose a three-stage\nmethod that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,\nand switch mode mechanism. MIT allows individual nodes to run multiple\nlightweight training streams with different model instances in parallel and\nmerge them to combine knowledge, increasing throughput and reducing idle time.\nAdaptive Batched DiLoCo dynamically adjusts local batch sizes to balance\ncomputation and communication, substantially lowering synchronization delays.\nSwitch mode further stabilizes training by seamlessly introducing gradient\naccumulation once adaptive batch sizes grow beyond hardware-friendly limits.\nTogether, these innovations improve both convergence speed and system\nefficiency. We also provide a theoretical estimate of the number of\ncommunications required for the full convergence of a model trained using our\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling distributed training of Large Language Models (LLMs) requires not\nonly algorithmic advances but also efficient utilization of heterogeneous\nhardware resources. While existing methods such as DiLoCo have demonstrated\npromising results, they often fail to fully exploit computational clusters\nunder dynamic workloads. To address this limitation, we propose a three-stage\nmethod that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,\nand switch mode mechanism. MIT allows individual nodes to run multiple\nlightweight training streams with different model instances in parallel and\nmerge them to combine knowledge, increasing throughput and reducing idle time.\nAdaptive Batched DiLoCo dynamically adjusts local batch sizes to balance\ncomputation and communication, substantially lowering synchronization delays.\nSwitch mode further stabilizes training by seamlessly introducing gradient\naccumulation once adaptive batch sizes grow beyond hardware-friendly limits.\nTogether, these innovations improve both convergence speed and system\nefficiency. We also provide a theoretical estimate of the number of\ncommunications required for the full convergence of a model trained using our\nmethod."
                },
                "authors": [
                    {
                        "name": "Nikolay Kutuzov"
                    },
                    {
                        "name": "Makar Baderko"
                    },
                    {
                        "name": "Stepan Kulibaba"
                    },
                    {
                        "name": "Artem Dzhalilov"
                    },
                    {
                        "name": "Daniel Bobrov"
                    },
                    {
                        "name": "Maxim Mashtaler"
                    },
                    {
                        "name": "Alexander Gasnikov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Gasnikov"
                },
                "author": "Alexander Gasnikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16796v2",
                "updated": "2025-08-25T16:33:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    33,
                    35,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-25T09:58:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    58,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "HeteroTune: Efficient Federated Learning for Large Heterogeneous Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeteroTune: Efficient Federated Learning for Large Heterogeneous Models"
                },
                "summary": "While large pre-trained models have achieved impressive performance across AI\ntasks, their deployment in privacy-sensitive and distributed environments\nremains challenging. Federated learning (FL) offers a viable solution by\nenabling decentralized fine-tuning without data sharing, but real-world\napplications face significant obstacles due to heterogeneous client resources\nin compute and memory. To address this, we propose HeteroTune, a novel\nfederated fine-tuning paradigm for large, heterogeneous models operating under\nlimited communication and computation budgets. The core of our method lies in a\nnovel architecture, DeMA (Dense Mixture of Adapters), which enables flexible\nand efficient aggregation of heterogeneous models by preserving their full\nrepresentational capacity while facilitating seamless cross-model knowledge\nfusion. We further introduce CMGA (Cross-Model Gradient Alignment), a\nlightweight yet effective mechanism that enhances training stability by\nharmonizing gradient directions across heterogeneous client models during\naggregation, mitigating update conflicts and promoting more consistent\nconvergence in federated settings. We provide both theoretical analysis and\nempirical evidence showing that HeteroTune achieves state-of-the-art\nperformance and efficiency across diverse tasks and model architectures. For\nexample, on LLaMA models, it reduces communication overhead by 99.5%, cuts peak\nmemory usage by ~50%, and improves performance by 4.61%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large pre-trained models have achieved impressive performance across AI\ntasks, their deployment in privacy-sensitive and distributed environments\nremains challenging. Federated learning (FL) offers a viable solution by\nenabling decentralized fine-tuning without data sharing, but real-world\napplications face significant obstacles due to heterogeneous client resources\nin compute and memory. To address this, we propose HeteroTune, a novel\nfederated fine-tuning paradigm for large, heterogeneous models operating under\nlimited communication and computation budgets. The core of our method lies in a\nnovel architecture, DeMA (Dense Mixture of Adapters), which enables flexible\nand efficient aggregation of heterogeneous models by preserving their full\nrepresentational capacity while facilitating seamless cross-model knowledge\nfusion. We further introduce CMGA (Cross-Model Gradient Alignment), a\nlightweight yet effective mechanism that enhances training stability by\nharmonizing gradient directions across heterogeneous client models during\naggregation, mitigating update conflicts and promoting more consistent\nconvergence in federated settings. We provide both theoretical analysis and\nempirical evidence showing that HeteroTune achieves state-of-the-art\nperformance and efficiency across diverse tasks and model architectures. For\nexample, on LLaMA models, it reduces communication overhead by 99.5%, cuts peak\nmemory usage by ~50%, and improves performance by 4.61%."
                },
                "authors": [
                    {
                        "name": "Ruofan Jia"
                    },
                    {
                        "name": "Weiying Xie"
                    },
                    {
                        "name": "Jie Lei"
                    },
                    {
                        "name": "Jitao Ma"
                    },
                    {
                        "name": "Haonan Qin"
                    },
                    {
                        "name": "Leyuan Fang"
                    }
                ],
                "author_detail": {
                    "name": "Leyuan Fang"
                },
                "author": "Leyuan Fang",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18174v1",
                "updated": "2025-08-25T16:27:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    27,
                    1,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:27:01Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    27,
                    1,
                    0,
                    237,
                    0
                ],
                "title": "InReAcTable: LLM-Powered Interactive Visual Data Story Construction from\n  Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InReAcTable: LLM-Powered Interactive Visual Data Story Construction from\n  Tabular Data"
                },
                "summary": "Insights in tabular data capture valuable patterns that help analysts\nunderstand critical information. Organizing related insights into visual data\nstories is crucial for in-depth analysis. However, constructing such stories is\nchallenging because of the complexity of the inherent relations between\nextracted insights. Users face difficulty sifting through a vast number of\ndiscrete insights to integrate specific ones into a unified narrative that\nmeets their analytical goals. Existing methods either heavily rely on user\nexpertise, making the process inefficient, or employ automated approaches that\ncannot fully capture their evolving goals. In this paper, we introduce\nInReAcTable, a framework that enhances visual data story construction by\nestablishing both structural and semantic connections between data insights.\nEach user interaction triggers the Acting module, which utilizes an insight\ngraph for structural filtering to narrow the search space, followed by the\nReasoning module using the retrieval-augmented generation method based on large\nlanguage models for semantic filtering, ultimately providing insight\nrecommendations aligned with the user's analytical intent. Based on the\nInReAcTable framework, we develop an interactive prototype system that guides\nusers to construct visual data stories aligned with their analytical\nrequirements. We conducted a case study and a user experiment to demonstrate\nthe utility and effectiveness of the InReAcTable framework and the prototype\nsystem for interactively building visual data stories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights in tabular data capture valuable patterns that help analysts\nunderstand critical information. Organizing related insights into visual data\nstories is crucial for in-depth analysis. However, constructing such stories is\nchallenging because of the complexity of the inherent relations between\nextracted insights. Users face difficulty sifting through a vast number of\ndiscrete insights to integrate specific ones into a unified narrative that\nmeets their analytical goals. Existing methods either heavily rely on user\nexpertise, making the process inefficient, or employ automated approaches that\ncannot fully capture their evolving goals. In this paper, we introduce\nInReAcTable, a framework that enhances visual data story construction by\nestablishing both structural and semantic connections between data insights.\nEach user interaction triggers the Acting module, which utilizes an insight\ngraph for structural filtering to narrow the search space, followed by the\nReasoning module using the retrieval-augmented generation method based on large\nlanguage models for semantic filtering, ultimately providing insight\nrecommendations aligned with the user's analytical intent. Based on the\nInReAcTable framework, we develop an interactive prototype system that guides\nusers to construct visual data stories aligned with their analytical\nrequirements. We conducted a case study and a user experiment to demonstrate\nthe utility and effectiveness of the InReAcTable framework and the prototype\nsystem for interactively building visual data stories."
                },
                "authors": [
                    {
                        "name": "Gerile Aodeng"
                    },
                    {
                        "name": "Guozheng Li"
                    },
                    {
                        "name": "Yunshan Feng"
                    },
                    {
                        "name": "Qiyang Chen"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Chi Harold Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chi Harold Liu"
                },
                "author": "Chi Harold Liu",
                "arxiv_comment": "16 pages, 10 figures, accepted at ACM UIST 2025 (to appear)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12349v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12349v4",
                "updated": "2025-08-25T16:24:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    24,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-03-16T04:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    4,
                    10,
                    53,
                    6,
                    75,
                    0
                ],
                "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"
                },
                "summary": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/"
                },
                "authors": [
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Ryan Hsieh"
                    },
                    {
                        "name": "Haisu Zhou"
                    },
                    {
                        "name": "Tianqing Zou"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pramod Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Viswanath"
                },
                "author": "Pramod Viswanath",
                "arxiv_comment": "48 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12349v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17047v2",
                "updated": "2025-08-25T16:17:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    17,
                    48,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-22T22:09:00Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    22,
                    9,
                    0,
                    1,
                    203,
                    0
                ],
                "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Hybrid Captioner for Improved Long-form Video Understanding"
                },
                "summary": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video."
                },
                "authors": [
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Efsun Sarioglu Kayi"
                    },
                    {
                        "name": "Arun Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Arun Reddy"
                },
                "author": "Arun Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18167v1",
                "updated": "2025-08-25T16:16:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    16,
                    42,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T16:16:42Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    16,
                    42,
                    0,
                    237,
                    0
                ],
                "title": "DiscussLLM: Teaching Large Language Models When to Speak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiscussLLM: Teaching Large Language Models When to Speak"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nunderstanding and generating human-like text, yet they largely operate as\nreactive agents, responding only when directly prompted. This passivity creates\nan \"awareness gap,\" limiting their potential as truly collaborative partners in\ndynamic human discussions. We introduce $\\textit{DiscussLLM}$, a framework\ndesigned to bridge this gap by training models to proactively decide not just\n$\\textit{what}$ to say, but critically, $\\textit{when}$ to speak. Our primary\ncontribution is a scalable two-stage data generation pipeline that synthesizes\na large-scale dataset of realistic multi-turn human discussions. Each\ndiscussion is annotated with one of five intervention types (e.g., Factual\nCorrection, Concept Definition) and contains an explicit conversational trigger\nwhere an AI intervention adds value. By training models to predict a special\nsilent token when no intervention is needed, they learn to remain quiet until a\nhelpful contribution can be made. We explore two architectural baselines: an\nintegrated end-to-end model and a decoupled classifier-generator system\noptimized for low-latency inference. We evaluate these models on their ability\nto accurately time interventions and generate helpful responses, paving the way\nfor more situationally aware and proactive conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nunderstanding and generating human-like text, yet they largely operate as\nreactive agents, responding only when directly prompted. This passivity creates\nan \"awareness gap,\" limiting their potential as truly collaborative partners in\ndynamic human discussions. We introduce $\\textit{DiscussLLM}$, a framework\ndesigned to bridge this gap by training models to proactively decide not just\n$\\textit{what}$ to say, but critically, $\\textit{when}$ to speak. Our primary\ncontribution is a scalable two-stage data generation pipeline that synthesizes\na large-scale dataset of realistic multi-turn human discussions. Each\ndiscussion is annotated with one of five intervention types (e.g., Factual\nCorrection, Concept Definition) and contains an explicit conversational trigger\nwhere an AI intervention adds value. By training models to predict a special\nsilent token when no intervention is needed, they learn to remain quiet until a\nhelpful contribution can be made. We explore two architectural baselines: an\nintegrated end-to-end model and a decoupled classifier-generator system\noptimized for low-latency inference. We evaluate these models on their ability\nto accurately time interventions and generate helpful responses, paving the way\nfor more situationally aware and proactive conversational AI."
                },
                "authors": [
                    {
                        "name": "Deep Anil Patel"
                    },
                    {
                        "name": "Iain Melvin"
                    },
                    {
                        "name": "Christopher Malon"
                    },
                    {
                        "name": "Martin Renqiang Min"
                    }
                ],
                "author_detail": {
                    "name": "Martin Renqiang Min"
                },
                "author": "Martin Renqiang Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18166v2",
                "updated": "2025-08-26T09:00:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    0,
                    29,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T16:16:06Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    16,
                    6,
                    0,
                    237,
                    0
                ],
                "title": "PCR-CA: Parallel Codebook Representations with Contrastive Alignment for\n  Multiple-Category App Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PCR-CA: Parallel Codebook Representations with Contrastive Alignment for\n  Multiple-Category App Recommendation"
                },
                "summary": "Modern app store recommender systems struggle with multiple-category apps, as\ntraditional taxonomies fail to capture overlapping semantics, leading to\nsuboptimal personalization. We propose PCR-CA (Parallel Codebook\nRepresentations with Contrastive Alignment), an end-to-end framework for\nimproved CTR prediction. PCR-CA first extracts compact multimodal embeddings\nfrom app text, then introduces a Parallel Codebook VQ-AE module that learns\ndiscrete semantic representations across multiple codebooks in parallel --\nunlike hierarchical residual quantization (RQ-VAE). This design enables\nindependent encoding of diverse aspects (e.g., gameplay, art style), better\nmodeling multiple-category semantics. To bridge semantic and collaborative\nsignals, we employ a contrastive alignment loss at both the user and item\nlevels, enhancing representation learning for long-tail items. Additionally, a\ndual-attention fusion mechanism combines ID-based and semantic features to\ncapture user interests, especially for long-tail apps. Experiments on a\nlarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong\nbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further\nvalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvement\nin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new\nframework has now been fully deployed on the Microsoft Store.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern app store recommender systems struggle with multiple-category apps, as\ntraditional taxonomies fail to capture overlapping semantics, leading to\nsuboptimal personalization. We propose PCR-CA (Parallel Codebook\nRepresentations with Contrastive Alignment), an end-to-end framework for\nimproved CTR prediction. PCR-CA first extracts compact multimodal embeddings\nfrom app text, then introduces a Parallel Codebook VQ-AE module that learns\ndiscrete semantic representations across multiple codebooks in parallel --\nunlike hierarchical residual quantization (RQ-VAE). This design enables\nindependent encoding of diverse aspects (e.g., gameplay, art style), better\nmodeling multiple-category semantics. To bridge semantic and collaborative\nsignals, we employ a contrastive alignment loss at both the user and item\nlevels, enhancing representation learning for long-tail items. Additionally, a\ndual-attention fusion mechanism combines ID-based and semantic features to\ncapture user interests, especially for long-tail apps. Experiments on a\nlarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong\nbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further\nvalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvement\nin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new\nframework has now been fully deployed on the Microsoft Store."
                },
                "authors": [
                    {
                        "name": "Bin Tan"
                    },
                    {
                        "name": "Wangyao Ge"
                    },
                    {
                        "name": "Yidi Wang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Jeff Burtoft"
                    },
                    {
                        "name": "Hao Fan"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "arxiv_comment": "9 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08985v2",
                "updated": "2025-08-25T16:02:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    2,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-12T14:53:54Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    53,
                    54,
                    1,
                    224,
                    0
                ],
                "title": "Low-Regret and Low-Complexity Learning for Hierarchical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Regret and Low-Complexity Learning for Hierarchical Inference"
                },
                "summary": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods."
                },
                "authors": [
                    {
                        "name": "Sameep Chattopadhyay"
                    },
                    {
                        "name": "Vinay Sutar"
                    },
                    {
                        "name": "Jaya Prakash Champati"
                    },
                    {
                        "name": "Sharayu Moharir"
                    }
                ],
                "author_detail": {
                    "name": "Sharayu Moharir"
                },
                "author": "Sharayu Moharir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18148v1",
                "updated": "2025-08-25T15:55:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    55,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:55:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    55,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Few Samples: A Novel Approach for High-Quality Malcode\n  Generation"
                },
                "summary": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats."
                },
                "authors": [
                    {
                        "name": "Haijian Ma"
                    },
                    {
                        "name": "Daizong Liu"
                    },
                    {
                        "name": "Xiaowen Cai"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Yulai Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yulai Xie"
                },
                "author": "Yulai Xie",
                "arxiv_comment": "18pages,5 figures,emnlp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18142v1",
                "updated": "2025-08-25T15:51:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    51,
                    24,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:51:24Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    51,
                    24,
                    0,
                    237,
                    0
                ],
                "title": "Mirroring Users: Towards Building Preference-aligned User Simulator with\n  User Feedback in Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mirroring Users: Towards Building Preference-aligned User Simulator with\n  User Feedback in Recommendation"
                },
                "summary": "User simulation is increasingly vital to develop and evaluate recommender\nsystems (RSs). While Large Language Models (LLMs) offer promising avenues to\nsimulate user behavior, they often struggle with the absence of specific domain\nalignment required for RSs and the efficiency demands of large-scale\nsimulation. A vast yet underutilized resource for enhancing this alignment is\nthe extensive user feedback inherent in RSs. However, directly leveraging such\nfeedback presents two significant challenges. First, user feedback in RSs is\noften ambiguous and noisy, which negatively impacts effective preference\nalignment. Second, the massive volume of feedback largely hinders the\nefficiency of preference alignment, necessitating an efficient filtering\nmechanism to identify more informative samples. To overcome these hurdles, we\nintroduce a novel data construction framework that leverages user feedback in\nRSs with advanced LLM capabilities to generate high-quality simulation data.\nOur framework unfolds in two key phases: (1) employing LLMs to generate\ncognitive decision-making processes on constructed simulation samples, reducing\nambiguity in raw user feedback; (2) data distillation based on uncertainty\nestimation and behavior sampling to filter challenging yet denoised simulation\nsamples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using\nsuch high-quality dataset with corresponding decision-making processes.\nExtensive experiments verify that our framework significantly boosts the\nalignment with human preferences and in-domain reasoning capabilities of\nfine-tuned LLMs, and provides more insightful and interpretable signals when\ninteracting with RSs. We believe our work will advance the RS community and\noffer valuable insights for broader human-centric AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User simulation is increasingly vital to develop and evaluate recommender\nsystems (RSs). While Large Language Models (LLMs) offer promising avenues to\nsimulate user behavior, they often struggle with the absence of specific domain\nalignment required for RSs and the efficiency demands of large-scale\nsimulation. A vast yet underutilized resource for enhancing this alignment is\nthe extensive user feedback inherent in RSs. However, directly leveraging such\nfeedback presents two significant challenges. First, user feedback in RSs is\noften ambiguous and noisy, which negatively impacts effective preference\nalignment. Second, the massive volume of feedback largely hinders the\nefficiency of preference alignment, necessitating an efficient filtering\nmechanism to identify more informative samples. To overcome these hurdles, we\nintroduce a novel data construction framework that leverages user feedback in\nRSs with advanced LLM capabilities to generate high-quality simulation data.\nOur framework unfolds in two key phases: (1) employing LLMs to generate\ncognitive decision-making processes on constructed simulation samples, reducing\nambiguity in raw user feedback; (2) data distillation based on uncertainty\nestimation and behavior sampling to filter challenging yet denoised simulation\nsamples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using\nsuch high-quality dataset with corresponding decision-making processes.\nExtensive experiments verify that our framework significantly boosts the\nalignment with human preferences and in-domain reasoning capabilities of\nfine-tuned LLMs, and provides more insightful and interpretable signals when\ninteracting with RSs. We believe our work will advance the RS community and\noffer valuable insights for broader human-centric AI research."
                },
                "authors": [
                    {
                        "name": "Tianjun Wei"
                    },
                    {
                        "name": "Huizhong Guo"
                    },
                    {
                        "name": "Yingpeng Du"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Dongxia Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "arxiv_comment": "Github: https://github.com/UserMirrorer/UserMirrorer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19134v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19134v4",
                "updated": "2025-08-25T15:50:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    50,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-09-27T20:32:42Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    20,
                    32,
                    42,
                    4,
                    271,
                    0
                ],
                "title": "Confidential Prompting: Privacy-preserving LLM Inference on Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Prompting: Privacy-preserving LLM Inference on Cloud"
                },
                "summary": "This paper introduces a vision of confidential prompting: securing user\nprompts from untrusted, cloud-hosted large language model (LLM) provider while\npreserving model confidentiality, output invariance, and compute efficiency. As\na first step toward this vision, we present Obfuscated Secure Partitioned\nDecoding (OSPD), a system built on two key innovations. First, Secure\nPartitioned Decoding (SPD) isolates user prompts within per-user processes\nresiding in a confidential virtual machine (CVM) on the cloud, which are\ninaccessible for the cloud LLM while allowing it to generate tokens\nefficiently. Second, Prompt Obfuscation (PO) introduces a novel cryptographic\ntechnique that enhances SPD resilience against advanced prompt reconstruction\nattacks. Together, these innovations ensure OSPD protects both prompt and model\nconfidentiality while maintaining service functionality. OSPD enables\npractical, privacy-preserving cloud-hosted LLM inference for sensitive\napplications, such as processing personal data, clinical records, and financial\ndocuments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a vision of confidential prompting: securing user\nprompts from untrusted, cloud-hosted large language model (LLM) provider while\npreserving model confidentiality, output invariance, and compute efficiency. As\na first step toward this vision, we present Obfuscated Secure Partitioned\nDecoding (OSPD), a system built on two key innovations. First, Secure\nPartitioned Decoding (SPD) isolates user prompts within per-user processes\nresiding in a confidential virtual machine (CVM) on the cloud, which are\ninaccessible for the cloud LLM while allowing it to generate tokens\nefficiently. Second, Prompt Obfuscation (PO) introduces a novel cryptographic\ntechnique that enhances SPD resilience against advanced prompt reconstruction\nattacks. Together, these innovations ensure OSPD protects both prompt and model\nconfidentiality while maintaining service functionality. OSPD enables\npractical, privacy-preserving cloud-hosted LLM inference for sensitive\napplications, such as processing personal data, clinical records, and financial\ndocuments."
                },
                "authors": [
                    {
                        "name": "Caihua Li"
                    },
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19134v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19134v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00187v2",
                "updated": "2025-08-25T15:49:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    49,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-28T21:10:03Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    21,
                    10,
                    3,
                    4,
                    59,
                    0
                ],
                "title": "Steering Dialogue Dynamics for Robustness against Multi-turn\n  Jailbreaking Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Dialogue Dynamics for Robustness against Multi-turn\n  Jailbreaking Attacks"
                },
                "summary": "Large language models (LLMs) are shown to be vulnerable to jailbreaking\nattacks where adversarial prompts are designed to elicit harmful responses.\nWhile existing defenses effectively mitigate single-turn attacks by detecting\nand filtering unsafe inputs, they fail against multi-turn jailbreaks that\nexploit contextual drift over multiple interactions, gradually leading LLMs\naway from safe behavior. To address this challenge, we propose a safety\nsteering framework grounded in safe control theory, ensuring invariant safety\nin multi-turn dialogues. Our approach models the dialogue with LLMs using\nstate-space representations and introduces a novel neural barrier function\n(NBF) to detect and filter harmful queries emerging from evolving contexts\nproactively. Our method achieves invariant safety at each turn of dialogue by\nlearning a safety predictor that accounts for adversarial queries, preventing\npotential context drift toward jailbreaks. Extensive experiments under multiple\nLLMs show that our NBF-based safety steering outperforms safety alignment,\nprompt-based steering and lightweight LLM guardrails baselines, offering\nstronger defenses against multi-turn jailbreaks while maintaining a better\ntrade-off among safety, helpfulness and over-refusal. Check out the website\nhere https://sites.google.com/view/llm-nbf/home . Our code is available on\nhttps://github.com/HanjiangHu/NBF-LLM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are shown to be vulnerable to jailbreaking\nattacks where adversarial prompts are designed to elicit harmful responses.\nWhile existing defenses effectively mitigate single-turn attacks by detecting\nand filtering unsafe inputs, they fail against multi-turn jailbreaks that\nexploit contextual drift over multiple interactions, gradually leading LLMs\naway from safe behavior. To address this challenge, we propose a safety\nsteering framework grounded in safe control theory, ensuring invariant safety\nin multi-turn dialogues. Our approach models the dialogue with LLMs using\nstate-space representations and introduces a novel neural barrier function\n(NBF) to detect and filter harmful queries emerging from evolving contexts\nproactively. Our method achieves invariant safety at each turn of dialogue by\nlearning a safety predictor that accounts for adversarial queries, preventing\npotential context drift toward jailbreaks. Extensive experiments under multiple\nLLMs show that our NBF-based safety steering outperforms safety alignment,\nprompt-based steering and lightweight LLM guardrails baselines, offering\nstronger defenses against multi-turn jailbreaks while maintaining a better\ntrade-off among safety, helpfulness and over-refusal. Check out the website\nhere https://sites.google.com/view/llm-nbf/home . Our code is available on\nhttps://github.com/HanjiangHu/NBF-LLM ."
                },
                "authors": [
                    {
                        "name": "Hanjiang Hu"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "arxiv_comment": "23 pages, 10 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18132v1",
                "updated": "2025-08-25T15:38:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    38,
                    56,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:38:56Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    38,
                    56,
                    0,
                    237,
                    0
                ],
                "title": "Test-Time Scaling Strategies for Generative Retrieval in Multimodal\n  Conversational Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling Strategies for Generative Retrieval in Multimodal\n  Conversational Recommendations"
                },
                "summary": "The rapid evolution of e-commerce has exposed the limitations of traditional\nproduct retrieval systems in managing complex, multi-turn user interactions.\nRecent advances in multimodal generative retrieval -- particularly those\nleveraging multimodal large language models (MLLMs) as retrievers -- have shown\npromise. However, most existing methods are tailored to single-turn scenarios\nand struggle to model the evolving intent and iterative nature of multi-turn\ndialogues when applied naively. Concurrently, test-time scaling has emerged as\na powerful paradigm for improving large language model (LLM) performance\nthrough iterative inference-time refinement. Yet, its effectiveness typically\nrelies on two conditions: (1) a well-defined problem space (e.g., mathematical\nreasoning), and (2) the model's ability to self-correct -- conditions that are\nrarely met in conversational product search. In this setting, user queries are\noften ambiguous and evolving, and MLLMs alone have difficulty grounding\nresponses in a fixed product corpus. Motivated by these challenges, we propose\na novel framework that introduces test-time scaling into conversational\nmultimodal product retrieval. Our approach builds on a generative retriever,\nfurther augmented with a test-time reranking (TTR) mechanism that improves\nretrieval accuracy and better aligns results with evolving user intent\nthroughout the dialogue. Experiments across multiple benchmarks show consistent\nimprovements, with average gains of 14.5 points in MRR and 10.6 points in\nnDCG@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of e-commerce has exposed the limitations of traditional\nproduct retrieval systems in managing complex, multi-turn user interactions.\nRecent advances in multimodal generative retrieval -- particularly those\nleveraging multimodal large language models (MLLMs) as retrievers -- have shown\npromise. However, most existing methods are tailored to single-turn scenarios\nand struggle to model the evolving intent and iterative nature of multi-turn\ndialogues when applied naively. Concurrently, test-time scaling has emerged as\na powerful paradigm for improving large language model (LLM) performance\nthrough iterative inference-time refinement. Yet, its effectiveness typically\nrelies on two conditions: (1) a well-defined problem space (e.g., mathematical\nreasoning), and (2) the model's ability to self-correct -- conditions that are\nrarely met in conversational product search. In this setting, user queries are\noften ambiguous and evolving, and MLLMs alone have difficulty grounding\nresponses in a fixed product corpus. Motivated by these challenges, we propose\na novel framework that introduces test-time scaling into conversational\nmultimodal product retrieval. Our approach builds on a generative retriever,\nfurther augmented with a test-time reranking (TTR) mechanism that improves\nretrieval accuracy and better aligns results with evolving user intent\nthroughout the dialogue. Experiments across multiple benchmarks show consistent\nimprovements, with average gains of 14.5 points in MRR and 10.6 points in\nnDCG@1."
                },
                "authors": [
                    {
                        "name": "Hung-Chun Hsu"
                    },
                    {
                        "name": "Yuan-Ching Kuo"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Ming-Feng Tsai"
                    },
                    {
                        "name": "Chuan-Ju Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chuan-Ju Wang"
                },
                "author": "Chuan-Ju Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16638v2",
                "updated": "2025-08-25T15:36:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    36,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-22T13:11:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    11,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "Reconsidering Fairness Through Unawareness From the Perspective of Model\n  Multiplicity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconsidering Fairness Through Unawareness From the Perspective of Model\n  Multiplicity"
                },
                "summary": "Fairness through Unawareness (FtU) describes the idea that discrimination\nagainst demographic groups can be avoided by not considering group membership\nin the decisions or predictions. This idea has long been criticized in the\nmachine learning literature as not being sufficient to ensure fairness. In\naddition, the use of additional features is typically thought to increase the\naccuracy of the predictions for all groups, so that FtU is sometimes thought to\nbe detrimental to all groups. In this paper, we show both theoretically and\nempirically that FtU can reduce algorithmic discrimination without necessarily\nreducing accuracy. We connect this insight with the literature on Model\nMultiplicity, to which we contribute with novel theoretical and empirical\nresults. Furthermore, we illustrate how, in a real-life application, FtU can\ncontribute to the deployment of more equitable policies without losing\nefficacy. Our findings suggest that FtU is worth considering in practical\napplications, particularly in high-risk scenarios, and that the use of\nprotected attributes such as gender in predictive models should be accompanied\nby a clear and well-founded justification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness through Unawareness (FtU) describes the idea that discrimination\nagainst demographic groups can be avoided by not considering group membership\nin the decisions or predictions. This idea has long been criticized in the\nmachine learning literature as not being sufficient to ensure fairness. In\naddition, the use of additional features is typically thought to increase the\naccuracy of the predictions for all groups, so that FtU is sometimes thought to\nbe detrimental to all groups. In this paper, we show both theoretically and\nempirically that FtU can reduce algorithmic discrimination without necessarily\nreducing accuracy. We connect this insight with the literature on Model\nMultiplicity, to which we contribute with novel theoretical and empirical\nresults. Furthermore, we illustrate how, in a real-life application, FtU can\ncontribute to the deployment of more equitable policies without losing\nefficacy. Our findings suggest that FtU is worth considering in practical\napplications, particularly in high-risk scenarios, and that the use of\nprotected attributes such as gender in predictive models should be accompanied\nby a clear and well-founded justification."
                },
                "authors": [
                    {
                        "name": "Benedikt H√∂ltgen"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01710v2",
                "updated": "2025-08-25T15:35:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    35,
                    12,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-03T10:35:05Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    35,
                    5,
                    6,
                    215,
                    0
                ],
                "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for\n  Multilingual Safety Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for\n  Multilingual Safety Applications"
                },
                "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models."
                },
                "authors": [
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Wartikar"
                },
                "author": "Niranjan Wartikar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18124v2",
                "updated": "2025-08-26T04:21:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    4,
                    21,
                    37,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T15:32:22Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    32,
                    22,
                    0,
                    237,
                    0
                ],
                "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics"
                },
                "summary": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench."
                },
                "authors": [
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Dongchen Huang"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Tengchao Yang"
                    },
                    {
                        "name": "Ziyang Zheng"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dong Han"
                    },
                    {
                        "name": "Benteng Chen"
                    },
                    {
                        "name": "Binzhao Luo"
                    },
                    {
                        "name": "Zhiyu Liu"
                    },
                    {
                        "name": "Kunling Liu"
                    },
                    {
                        "name": "Zhiyuan Gao"
                    },
                    {
                        "name": "Shiqi Geng"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Jiaming Su"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Shuchen Pu"
                    },
                    {
                        "name": "Yuhan Shui"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Dongfei Cui"
                    },
                    {
                        "name": "Changyong He"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Zeke Xie"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yunqi Cai"
                    },
                    {
                        "name": "Xi Dai"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jinguang Cheng"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    }
                ],
                "author_detail": {
                    "name": "Hongming Weng"
                },
                "author": "Hongming Weng",
                "arxiv_comment": "29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23163v2",
                "updated": "2025-08-25T15:30:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    30,
                    19,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-30T23:58:37Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    23,
                    58,
                    37,
                    2,
                    211,
                    0
                ],
                "title": "Argumentatively Coherent Judgmental Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argumentatively Coherent Judgmental Forecasting"
                },
                "summary": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions."
                },
                "authors": [
                    {
                        "name": "Deniz Gorur"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "17 pages, 18 figures, ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18118v1",
                "updated": "2025-08-25T15:23:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    23,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:23:21Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    23,
                    21,
                    0,
                    237,
                    0
                ],
                "title": "HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation"
                },
                "summary": "AI-generated content technologies are widely used in content creation.\nHowever, current AIGC systems rely heavily on creators' inspiration, rarely\ngenerating truly user-personalized content. In real-world applications such as\nonline advertising, a single product may have multiple selling points, with\ndifferent users focusing on different features. This underscores the\nsignificant value of personalized, user-centric creative generation. Effective\npersonalized content generation faces two main challenges: (1) accurately\nmodeling user interests and integrating them into the content generation\nprocess while adhering to factual constraints, and (2) ensuring high efficiency\nand scalability to handle the massive user base in industrial scenarios.\nAdditionally, the scarcity of personalized creative data in practice\ncomplicates model training, making data construction another key hurdle. We\npropose HLLM-Creator, a hierarchical LLM framework for efficient user interest\nmodeling and personalized content generation. During inference, a combination\nof user clustering and a user-ad-matching-prediction based pruning strategy is\nemployed to significantly enhance generation efficiency and reduce\ncomputational overhead, making the approach suitable for large-scale\ndeployment. Moreover, we design a data construction pipeline based on\nchain-of-thought reasoning, which generates high-quality, user-specific\ncreative titles and ensures factual consistency despite limited personalized\ndata. This pipeline serves as a critical foundation for the effectiveness of\nour model. Extensive experiments on personalized title generation for Douyin\nSearch Ads show the effectiveness of HLLM-Creator. Online A/B test shows a\n0.476% increase on Adss, paving the way for more effective and efficient\npersonalized generation in industrial scenarios. Codes for academic dataset are\navailable at https://github.com/bytedance/HLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-generated content technologies are widely used in content creation.\nHowever, current AIGC systems rely heavily on creators' inspiration, rarely\ngenerating truly user-personalized content. In real-world applications such as\nonline advertising, a single product may have multiple selling points, with\ndifferent users focusing on different features. This underscores the\nsignificant value of personalized, user-centric creative generation. Effective\npersonalized content generation faces two main challenges: (1) accurately\nmodeling user interests and integrating them into the content generation\nprocess while adhering to factual constraints, and (2) ensuring high efficiency\nand scalability to handle the massive user base in industrial scenarios.\nAdditionally, the scarcity of personalized creative data in practice\ncomplicates model training, making data construction another key hurdle. We\npropose HLLM-Creator, a hierarchical LLM framework for efficient user interest\nmodeling and personalized content generation. During inference, a combination\nof user clustering and a user-ad-matching-prediction based pruning strategy is\nemployed to significantly enhance generation efficiency and reduce\ncomputational overhead, making the approach suitable for large-scale\ndeployment. Moreover, we design a data construction pipeline based on\nchain-of-thought reasoning, which generates high-quality, user-specific\ncreative titles and ensures factual consistency despite limited personalized\ndata. This pipeline serves as a critical foundation for the effectiveness of\nour model. Extensive experiments on personalized title generation for Douyin\nSearch Ads show the effectiveness of HLLM-Creator. Online A/B test shows a\n0.476% increase on Adss, paving the way for more effective and efficient\npersonalized generation in industrial scenarios. Codes for academic dataset are\navailable at https://github.com/bytedance/HLLM."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Lu Chi"
                    },
                    {
                        "name": "Siliang Xu"
                    },
                    {
                        "name": "Shiwei Ran"
                    },
                    {
                        "name": "Bingyue Peng"
                    },
                    {
                        "name": "Zehuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zehuan Yuan"
                },
                "author": "Zehuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02531v2",
                "updated": "2025-08-25T15:23:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    23,
                    8,
                    0,
                    237,
                    0
                ],
                "published": "2025-01-05T13:18:13Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    13,
                    18,
                    13,
                    6,
                    5,
                    0
                ],
                "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially\n  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially\n  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI"
                },
                "summary": "As general-purpose artificial intelligence systems become increasingly\nintegrated into society and are used for information seeking, content\ngeneration, problem solving, textual analysis, coding, and running processes,\nit is crucial to assess their long-term impact on humans. This research\nexplores the sentiment of large language models (LLMs) and humans toward\nartificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared with sentiment data from\nthree independent human sample populations. Temporal variations in sentiment\nwere also evaluated over three consecutive days. The results show a diversity\nin sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4\nrecorded the most positive sentiment toward AGI, while Bard leaned toward a\nneutral sentiment. In contrast, the human samples showed a lower average\nsentiment of 2.97. The analysis outlines potential conflicts of interest and\nbiases in the sentiment formation of LLMs, and indicates that LLMs could subtly\ninfluence societal perceptions. To address the need for regulatory oversight\nand culturally grounded assessments of AI systems, we introduce the Societal AI\nAlignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional\nprompts and empirically validated societal value frameworks to evaluate\nlanguage model outputs across temporal, model, and multilingual axes. This\nbenchmark is designed to guide policymakers and AI agencies, including within\nframeworks such as the EU AI Act, by providing robust, actionable insights into\nAI alignment with human values, public sentiment, and ethical norms at both\nnational and international levels. Future research should further refine the\noperationalization of the SAAS-AI benchmark and systematically evaluate its\neffectiveness through comprehensive empirical testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As general-purpose artificial intelligence systems become increasingly\nintegrated into society and are used for information seeking, content\ngeneration, problem solving, textual analysis, coding, and running processes,\nit is crucial to assess their long-term impact on humans. This research\nexplores the sentiment of large language models (LLMs) and humans toward\nartificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared with sentiment data from\nthree independent human sample populations. Temporal variations in sentiment\nwere also evaluated over three consecutive days. The results show a diversity\nin sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4\nrecorded the most positive sentiment toward AGI, while Bard leaned toward a\nneutral sentiment. In contrast, the human samples showed a lower average\nsentiment of 2.97. The analysis outlines potential conflicts of interest and\nbiases in the sentiment formation of LLMs, and indicates that LLMs could subtly\ninfluence societal perceptions. To address the need for regulatory oversight\nand culturally grounded assessments of AI systems, we introduce the Societal AI\nAlignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional\nprompts and empirically validated societal value frameworks to evaluate\nlanguage model outputs across temporal, model, and multilingual axes. This\nbenchmark is designed to guide policymakers and AI agencies, including within\nframeworks such as the EU AI Act, by providing robust, actionable insights into\nAI alignment with human values, public sentiment, and ethical norms at both\nnational and international levels. Future research should further refine the\noperationalization of the SAAS-AI benchmark and systematically evaluate its\neffectiveness through comprehensive empirical testing."
                },
                "authors": [
                    {
                        "name": "Ljubisa Bojic"
                    },
                    {
                        "name": "Dylan Seychell"
                    },
                    {
                        "name": "Milan Cabarkapa"
                    }
                ],
                "author_detail": {
                    "name": "Milan Cabarkapa"
                },
                "author": "Milan Cabarkapa",
                "arxiv_comment": "20 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18113v1",
                "updated": "2025-08-25T15:21:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    21,
                    49,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:21:49Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    21,
                    49,
                    0,
                    237,
                    0
                ],
                "title": "The AI Data Scientist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Data Scientist"
                },
                "summary": "Imagine decision-makers uploading data and, within minutes, receiving clear,\nactionable insights delivered straight to their fingertips. That is the promise\nof the AI Data Scientist, an autonomous Agent powered by large language models\n(LLMs) that closes the gap between evidence and action. Rather than simply\nwriting code or responding to prompts, it reasons through questions, tests\nideas, and delivers end-to-end insights at a pace far beyond traditional\nworkflows. Guided by the scientific tenet of the hypothesis, this Agent\nuncovers explanatory patterns in data, evaluates their statistical\nsignificance, and uses them to inform predictive modeling. It then translates\nthese results into recommendations that are both rigorous and accessible. At\nthe core of the AI Data Scientist is a team of specialized LLM Subagents, each\nresponsible for a distinct task such as data cleaning, statistical testing,\nvalidation, and plain-language communication. These Subagents write their own\ncode, reason about causality, and identify when additional data is needed to\nsupport sound conclusions. Together, they achieve in minutes what might\notherwise take days or weeks, enabling a new kind of interaction that makes\ndeep data science both accessible and actionable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine decision-makers uploading data and, within minutes, receiving clear,\nactionable insights delivered straight to their fingertips. That is the promise\nof the AI Data Scientist, an autonomous Agent powered by large language models\n(LLMs) that closes the gap between evidence and action. Rather than simply\nwriting code or responding to prompts, it reasons through questions, tests\nideas, and delivers end-to-end insights at a pace far beyond traditional\nworkflows. Guided by the scientific tenet of the hypothesis, this Agent\nuncovers explanatory patterns in data, evaluates their statistical\nsignificance, and uses them to inform predictive modeling. It then translates\nthese results into recommendations that are both rigorous and accessible. At\nthe core of the AI Data Scientist is a team of specialized LLM Subagents, each\nresponsible for a distinct task such as data cleaning, statistical testing,\nvalidation, and plain-language communication. These Subagents write their own\ncode, reason about causality, and identify when additional data is needed to\nsupport sound conclusions. Together, they achieve in minutes what might\notherwise take days or weeks, enabling a new kind of interaction that makes\ndeep data science both accessible and actionable."
                },
                "authors": [
                    {
                        "name": "Farkhad Akimov"
                    },
                    {
                        "name": "Munachiso Samuel Nwadike"
                    },
                    {
                        "name": "Zangir Iklassov"
                    },
                    {
                        "name": "Martin Tak√°ƒç"
                    }
                ],
                "author_detail": {
                    "name": "Martin Tak√°ƒç"
                },
                "author": "Martin Tak√°ƒç",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18106v1",
                "updated": "2025-08-25T15:11:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    11,
                    11,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T15:11:11Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    11,
                    11,
                    0,
                    237,
                    0
                ],
                "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code"
                },
                "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching."
                },
                "authors": [
                    {
                        "name": "Keke Lian"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Libo Chen"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ziming Zhao"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Haotong Duan"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Shuang Liao"
                    },
                    {
                        "name": "Mingda Guo"
                    },
                    {
                        "name": "Jiazheng Quan"
                    },
                    {
                        "name": "Yilu Zhong"
                    },
                    {
                        "name": "Chenhao He"
                    },
                    {
                        "name": "Zichuan Chen"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Zhaoxuan Li"
                    },
                    {
                        "name": "Jiongchi Yu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Dong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Zhang"
                },
                "author": "Dong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06941v2",
                "updated": "2025-08-25T15:03:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    3,
                    0,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-09T11:26:10Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    26,
                    10,
                    5,
                    221,
                    0
                ],
                "title": "CLAP: Coreference-Linked Augmentation for Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAP: Coreference-Linked Augmentation for Passage Retrieval"
                },
                "summary": "Large Language Model (LLM)-based passage expansion has shown promise for\nenhancing first-stage retrieval, but often underperforms with dense retrievers\ndue to semantic drift and misalignment with their pretrained semantic space.\nBeyond this, only a portion of a passage is typically relevant to a query,\nwhile the rest introduces noise--an issue compounded by chunking techniques\nthat break coreference continuity. We propose Coreference-Linked Augmentation\nfor Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that\nsegments passages into coherent chunks, resolves coreference chains, and\ngenerates localized pseudo-queries aligned with dense retriever\nrepresentations. A simple fusion of global topical signals and fine-grained\nsubtopic signals achieves robust performance across domains. CLAP yields\nconsistent gains even as retriever strength increases, enabling dense\nretrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,\nwith up to 20.68% absolute nDCG@10 improvement. These improvements are\nespecially notable in out-of-domain settings, where conventional LLM-based\nexpansion methods relying on domain knowledge often falter. CLAP instead adopts\na logic-centric pipeline that enables robust, domain-agnostic generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based passage expansion has shown promise for\nenhancing first-stage retrieval, but often underperforms with dense retrievers\ndue to semantic drift and misalignment with their pretrained semantic space.\nBeyond this, only a portion of a passage is typically relevant to a query,\nwhile the rest introduces noise--an issue compounded by chunking techniques\nthat break coreference continuity. We propose Coreference-Linked Augmentation\nfor Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that\nsegments passages into coherent chunks, resolves coreference chains, and\ngenerates localized pseudo-queries aligned with dense retriever\nrepresentations. A simple fusion of global topical signals and fine-grained\nsubtopic signals achieves robust performance across domains. CLAP yields\nconsistent gains even as retriever strength increases, enabling dense\nretrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,\nwith up to 20.68% absolute nDCG@10 improvement. These improvements are\nespecially notable in out-of-domain settings, where conventional LLM-based\nexpansion methods relying on domain knowledge often falter. CLAP instead adopts\na logic-centric pipeline that enables robust, domain-agnostic generalization."
                },
                "authors": [
                    {
                        "name": "Huanwei Xu"
                    },
                    {
                        "name": "Lin Xu"
                    },
                    {
                        "name": "Liang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yuan"
                },
                "author": "Liang Yuan",
                "arxiv_doi": "10.1145/3746252.3761113",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761113",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18098v1",
                "updated": "2025-08-25T14:59:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    59,
                    46,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:59:46Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    59,
                    46,
                    0,
                    237,
                    0
                ],
                "title": "Detecting and Characterizing Planning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Characterizing Planning in Language Models"
                },
                "summary": "Modern large language models (LLMs) have demonstrated impressive performance\nacross a wide range of multi-step reasoning tasks. Recent work suggests that\nLLMs may perform planning - selecting a future target token in advance and\ngenerating intermediate tokens that lead towards it - rather than merely\nimprovising one token at a time. However, existing studies assume fixed\nplanning horizons and often focus on single prompts or narrow domains. To\ndistinguish planning from improvisation across models and tasks, we present\nformal and causally grounded criteria for detecting planning and operationalize\nthem as a semi-automated annotation pipeline. We apply this pipeline to both\nbase and instruction-tuned Gemma-2-2B models on the MBPP code generation\nbenchmark and a poem generation task where Claude 3.5 Haiku was previously\nshown to plan. Our findings show that planning is not universal: unlike Haiku,\nGemma-2-2B solves the same poem generation task through improvisation, and on\nMBPP it switches between planning and improvisation across similar tasks and\neven successive token predictions. We further show that instruction tuning\nrefines existing planning behaviors in the base model rather than creating them\nfrom scratch. Together, these studies provide a reproducible and scalable\nfoundation for mechanistic studies of planning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) have demonstrated impressive performance\nacross a wide range of multi-step reasoning tasks. Recent work suggests that\nLLMs may perform planning - selecting a future target token in advance and\ngenerating intermediate tokens that lead towards it - rather than merely\nimprovising one token at a time. However, existing studies assume fixed\nplanning horizons and often focus on single prompts or narrow domains. To\ndistinguish planning from improvisation across models and tasks, we present\nformal and causally grounded criteria for detecting planning and operationalize\nthem as a semi-automated annotation pipeline. We apply this pipeline to both\nbase and instruction-tuned Gemma-2-2B models on the MBPP code generation\nbenchmark and a poem generation task where Claude 3.5 Haiku was previously\nshown to plan. Our findings show that planning is not universal: unlike Haiku,\nGemma-2-2B solves the same poem generation task through improvisation, and on\nMBPP it switches between planning and improvisation across similar tasks and\neven successive token predictions. We further show that instruction tuning\nrefines existing planning behaviors in the base model rather than creating them\nfrom scratch. Together, these studies provide a reproducible and scalable\nfoundation for mechanistic studies of planning in LLMs."
                },
                "authors": [
                    {
                        "name": "Jatin Nainani"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "Connor Watts"
                    },
                    {
                        "name": "Andre N. Assis"
                    },
                    {
                        "name": "Alice Rigg"
                    }
                ],
                "author_detail": {
                    "name": "Alice Rigg"
                },
                "author": "Alice Rigg",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18093v1",
                "updated": "2025-08-25T14:54:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    54,
                    46,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:54:46Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    54,
                    46,
                    0,
                    237,
                    0
                ],
                "title": "Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual\n  Technical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual\n  Technical Question Answering"
                },
                "summary": "We present a case study evaluating large language models (LLMs) with\n128K-token context windows on a technical question answering (QA) task. Our\nbenchmark is built on a user manual for an agricultural machine, available in\nEnglish, French, and German. It simulates a cross-lingual information retrieval\nscenario where questions are posed in English against all three language\nversions of the manual. The evaluation focuses on realistic\n\"needle-in-a-haystack\" challenges and includes unanswerable questions to test\nfor hallucinations. We compare nine long-context LLMs using direct prompting\nagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,\nsemantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this\nspecific manual show that Hybrid RAG consistently outperforms direct\nlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5\n7B achieve high accuracy (over 85%) across all languages with RAG. This paper\ncontributes a detailed analysis of LLM performance in a specialized industrial\ndomain and an open framework for similar evaluations, highlighting practical\ntrade-offs and challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a case study evaluating large language models (LLMs) with\n128K-token context windows on a technical question answering (QA) task. Our\nbenchmark is built on a user manual for an agricultural machine, available in\nEnglish, French, and German. It simulates a cross-lingual information retrieval\nscenario where questions are posed in English against all three language\nversions of the manual. The evaluation focuses on realistic\n\"needle-in-a-haystack\" challenges and includes unanswerable questions to test\nfor hallucinations. We compare nine long-context LLMs using direct prompting\nagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,\nsemantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this\nspecific manual show that Hybrid RAG consistently outperforms direct\nlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5\n7B achieve high accuracy (over 85%) across all languages with RAG. This paper\ncontributes a detailed analysis of LLM performance in a specialized industrial\ndomain and an open framework for similar evaluations, highlighting practical\ntrade-offs and challenges."
                },
                "authors": [
                    {
                        "name": "Julius Gun"
                    },
                    {
                        "name": "Timo Oksanen"
                    }
                ],
                "author_detail": {
                    "name": "Timo Oksanen"
                },
                "author": "Timo Oksanen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18091v1",
                "updated": "2025-08-25T14:52:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    56,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:52:56Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    56,
                    0,
                    237,
                    0
                ],
                "title": "Teaching LLMs to Think Mathematically: A Critical Study of\n  Decision-Making via Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs to Think Mathematically: A Critical Study of\n  Decision-Making via Optimization"
                },
                "summary": "This paper investigates the capabilities of large language models (LLMs) in\nformulating and solving decision-making problems using mathematical\nprogramming. We first conduct a systematic review and meta-analysis of recent\nliterature to assess how well LLMs understand, structure, and solve\noptimization problems across domains. The analysis is guided by critical review\nquestions focusing on learning approaches, dataset designs, evaluation metrics,\nand prompting strategies. Our systematic evidence is complemented by targeted\nexperiments designed to evaluate the performance of state-of-the-art LLMs in\nautomatically generating optimization models for problems in computer networks.\nUsing a newly constructed dataset, we apply three prompting strategies:\nAct-as-expert, chain-of-thought, and self-consistency, and evaluate the\nobtained outputs based on optimality gap, token-level F1 score, and compilation\naccuracy. Results show promising progress in LLMs' ability to parse natural\nlanguage and represent symbolic formulations, but also reveal key limitations\nin accuracy, scalability, and interpretability. These empirical gaps motivate\nseveral future research directions, including structured datasets,\ndomain-specific fine-tuning, hybrid neuro-symbolic approaches, modular\nmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper\ncontributes a structured roadmap for advancing LLM capabilities in mathematical\nprogramming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the capabilities of large language models (LLMs) in\nformulating and solving decision-making problems using mathematical\nprogramming. We first conduct a systematic review and meta-analysis of recent\nliterature to assess how well LLMs understand, structure, and solve\noptimization problems across domains. The analysis is guided by critical review\nquestions focusing on learning approaches, dataset designs, evaluation metrics,\nand prompting strategies. Our systematic evidence is complemented by targeted\nexperiments designed to evaluate the performance of state-of-the-art LLMs in\nautomatically generating optimization models for problems in computer networks.\nUsing a newly constructed dataset, we apply three prompting strategies:\nAct-as-expert, chain-of-thought, and self-consistency, and evaluate the\nobtained outputs based on optimality gap, token-level F1 score, and compilation\naccuracy. Results show promising progress in LLMs' ability to parse natural\nlanguage and represent symbolic formulations, but also reveal key limitations\nin accuracy, scalability, and interpretability. These empirical gaps motivate\nseveral future research directions, including structured datasets,\ndomain-specific fine-tuning, hybrid neuro-symbolic approaches, modular\nmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper\ncontributes a structured roadmap for advancing LLM capabilities in mathematical\nprogramming."
                },
                "authors": [
                    {
                        "name": "Mohammad J. Abdel-Rahman"
                    },
                    {
                        "name": "Yasmeen Alslman"
                    },
                    {
                        "name": "Dania Refai"
                    },
                    {
                        "name": "Amro Saleh"
                    },
                    {
                        "name": "Malik A. Abu Loha"
                    },
                    {
                        "name": "Mohammad Yahya Hamed"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Yahya Hamed"
                },
                "author": "Mohammad Yahya Hamed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18090v1",
                "updated": "2025-08-25T14:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    11,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:52:11Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    52,
                    11,
                    0,
                    237,
                    0
                ],
                "title": "Named Entity Recognition of Historical Text via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition of Historical Text via Large Language Model"
                },
                "summary": "Large language models have demonstrated remarkable versatility across a wide\nrange of natural language processing tasks and domains. One such task is Named\nEntity Recognition (NER), which involves identifying and classifying proper\nnames in text, such as people, organizations, locations, dates, and other\nspecific entities. NER plays a crucial role in extracting information from\nunstructured textual data, enabling downstream applications such as information\nretrieval from unstructured text.\n  Traditionally, NER is addressed using supervised machine learning approaches,\nwhich require large amounts of annotated training data. However, historical\ntexts present a unique challenge, as the annotated datasets are often scarce or\nnonexistent, due to the high cost and expertise required for manual labeling.\nIn addition, the variability and noise inherent in historical language, such as\ninconsistent spelling and archaic vocabulary, further complicate the\ndevelopment of reliable NER systems for these sources.\n  In this study, we explore the feasibility of applying LLMs to NER in\nhistorical documents using zero-shot and few-shot prompting strategies, which\nrequire little to no task-specific training data. Our experiments, conducted on\nthe HIPE-2022 (Identifying Historical People, Places and other Entities)\ndataset, show that LLMs can achieve reasonably strong performance on NER tasks\nin this setting. While their performance falls short of fully supervised models\ntrained on domain-specific annotations, the results are nevertheless promising.\nThese findings suggest that LLMs offer a viable and efficient alternative for\ninformation extraction in low-resource or historically significant corpora,\nwhere traditional supervised methods are infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable versatility across a wide\nrange of natural language processing tasks and domains. One such task is Named\nEntity Recognition (NER), which involves identifying and classifying proper\nnames in text, such as people, organizations, locations, dates, and other\nspecific entities. NER plays a crucial role in extracting information from\nunstructured textual data, enabling downstream applications such as information\nretrieval from unstructured text.\n  Traditionally, NER is addressed using supervised machine learning approaches,\nwhich require large amounts of annotated training data. However, historical\ntexts present a unique challenge, as the annotated datasets are often scarce or\nnonexistent, due to the high cost and expertise required for manual labeling.\nIn addition, the variability and noise inherent in historical language, such as\ninconsistent spelling and archaic vocabulary, further complicate the\ndevelopment of reliable NER systems for these sources.\n  In this study, we explore the feasibility of applying LLMs to NER in\nhistorical documents using zero-shot and few-shot prompting strategies, which\nrequire little to no task-specific training data. Our experiments, conducted on\nthe HIPE-2022 (Identifying Historical People, Places and other Entities)\ndataset, show that LLMs can achieve reasonably strong performance on NER tasks\nin this setting. While their performance falls short of fully supervised models\ntrained on domain-specific annotations, the results are nevertheless promising.\nThese findings suggest that LLMs offer a viable and efficient alternative for\ninformation extraction in low-resource or historically significant corpora,\nwhere traditional supervised methods are infeasible."
                },
                "authors": [
                    {
                        "name": "Shibingfeng Zhang"
                    },
                    {
                        "name": "Giovanni Colavizza"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Colavizza"
                },
                "author": "Giovanni Colavizza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18089v1",
                "updated": "2025-08-25T14:49:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    49,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:49:29Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    49,
                    29,
                    0,
                    237,
                    0
                ],
                "title": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated\n  Software Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated\n  Software Evolution"
                },
                "summary": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals."
                },
                "authors": [
                    {
                        "name": "Karine Even-Mendoza"
                    },
                    {
                        "name": "Alexander Brownlee"
                    },
                    {
                        "name": "Alina Geiger"
                    },
                    {
                        "name": "Carol Hanna"
                    },
                    {
                        "name": "Justyna Petke"
                    },
                    {
                        "name": "Federica Sarro"
                    },
                    {
                        "name": "Dominik Sobania"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Sobania"
                },
                "author": "Dominik Sobania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16175v2",
                "updated": "2025-08-25T14:47:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    47,
                    49,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T07:51:46Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    51,
                    46,
                    4,
                    234,
                    0
                ],
                "title": "Planning for future EV charging infrastructure: A city-scale assessment\n  of demand and capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning for future EV charging infrastructure: A city-scale assessment\n  of demand and capacity"
                },
                "summary": "As the global shift toward transportation electrification has accelerated,\ncapacity planning for electric vehicle (EV) charging infrastructure has become\na critical challenge in the development of low-carbon urban energy systems.\nThis study proposes the first demand-driven, multi-objective planning model for\noptimizing city-scale capacity allocation of EV charging infrastructure. The\nmodel employs a bottom-up approach to estimate charging demand differentiated\nby vehicle type-battery electric vehicles (BEVs), extended-range electric\nvehicles (EREVs), and plug-in hybrid electric vehicles (PHEVs). Chongqing, a\nrapidly expanding EV hub in China with a strong industrial base, supportive\npolicies, and diverse urban morphologies, is selected as the case study. The\nresults show that (1) monthly EV electricity consumption in Chongqing rose from\n18.9 gigawatt-hours (GWh) in June 2022 to 57.5 GWh in December 2024, with\nassociated carbon emissions increasing from 9.9 kilotons of carbon dioxide\n(ktCO2) to 30 ktCO2, driven primarily by BEVs; (2) 181,622 additional charging\npiles were installed between 2022 and 2024, concentrated in densely populated\nareas, reflecting a demand-responsive strategy that prioritizes population\ndensity over geographic coverage; and (3) between 2025 and 2030, EV electricity\ndemand is projected to reach 1940 GWh, with the number of charging piles\nexceeding 1.4 million, and charging demand from EREVs and PHEVs expected to\novertake BEVs later in the period. While Chongqing serves as the pilot area,\nthe proposed planning platform is adaptable for application in cities\nworldwide, enabling cross-regional comparisons under diverse socio-economic,\ngeographic, and policy conditions. Overall, this work offers policymakers a\nversatile tool to support sustainable, cost-effective EV infrastructure\ndeployment aligned with low-carbon electrification targets in the\ntransportation sector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the global shift toward transportation electrification has accelerated,\ncapacity planning for electric vehicle (EV) charging infrastructure has become\na critical challenge in the development of low-carbon urban energy systems.\nThis study proposes the first demand-driven, multi-objective planning model for\noptimizing city-scale capacity allocation of EV charging infrastructure. The\nmodel employs a bottom-up approach to estimate charging demand differentiated\nby vehicle type-battery electric vehicles (BEVs), extended-range electric\nvehicles (EREVs), and plug-in hybrid electric vehicles (PHEVs). Chongqing, a\nrapidly expanding EV hub in China with a strong industrial base, supportive\npolicies, and diverse urban morphologies, is selected as the case study. The\nresults show that (1) monthly EV electricity consumption in Chongqing rose from\n18.9 gigawatt-hours (GWh) in June 2022 to 57.5 GWh in December 2024, with\nassociated carbon emissions increasing from 9.9 kilotons of carbon dioxide\n(ktCO2) to 30 ktCO2, driven primarily by BEVs; (2) 181,622 additional charging\npiles were installed between 2022 and 2024, concentrated in densely populated\nareas, reflecting a demand-responsive strategy that prioritizes population\ndensity over geographic coverage; and (3) between 2025 and 2030, EV electricity\ndemand is projected to reach 1940 GWh, with the number of charging piles\nexceeding 1.4 million, and charging demand from EREVs and PHEVs expected to\novertake BEVs later in the period. While Chongqing serves as the pilot area,\nthe proposed planning platform is adaptable for application in cities\nworldwide, enabling cross-regional comparisons under diverse socio-economic,\ngeographic, and policy conditions. Overall, this work offers policymakers a\nversatile tool to support sustainable, cost-effective EV infrastructure\ndeployment aligned with low-carbon electrification targets in the\ntransportation sector."
                },
                "authors": [
                    {
                        "name": "Hong Yuan"
                    },
                    {
                        "name": "Minda Ma"
                    },
                    {
                        "name": "Nan Zhou"
                    },
                    {
                        "name": "Yanqiao Deng"
                    },
                    {
                        "name": "Junhong Liu"
                    },
                    {
                        "name": "Shufan Zhang"
                    },
                    {
                        "name": "Zhili Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhili Ma"
                },
                "author": "Zhili Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.15022v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.15022v4",
                "updated": "2025-08-25T14:43:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    43,
                    13,
                    0,
                    237,
                    0
                ],
                "published": "2023-08-29T04:59:53Z",
                "published_parsed": [
                    2023,
                    8,
                    29,
                    4,
                    59,
                    53,
                    1,
                    241,
                    0
                ],
                "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models"
                },
                "summary": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts are released."
                },
                "authors": [
                    {
                        "name": "Qingyue Wang"
                    },
                    {
                        "name": "Yanhe Fu"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "arxiv_comment": "This paper has been accepted by Neurocomputing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.15022v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.15022v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18076v1",
                "updated": "2025-08-25T14:43:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    43,
                    10,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:43:10Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    43,
                    10,
                    0,
                    237,
                    0
                ],
                "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges"
                },
                "summary": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG."
                },
                "authors": [
                    {
                        "name": "Khaoula Chehbouni"
                    },
                    {
                        "name": "Mohammed Haddou"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "Prepared for conference submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18060v1",
                "updated": "2025-08-25T14:20:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    20,
                    19,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:20:19Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    20,
                    19,
                    0,
                    237,
                    0
                ],
                "title": "FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated\n  Learning"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across multiple\nclients while preserving data privacy by keeping local datasets on-device. In\nthis work, we address FL settings where clients may behave adversarially,\nexhibiting Byzantine attacks, while the central server is trusted and equipped\nwith a reference dataset. We propose FedGreed, a resilient aggregation strategy\nfor federated learning that does not require any assumptions about the fraction\nof adversarial participants. FedGreed orders clients' local model updates based\non their loss metrics evaluated against a trusted dataset on the server and\ngreedily selects a subset of clients whose models exhibit the minimal\nevaluation loss. Unlike many existing approaches, our method is designed to\noperate reliably under heterogeneous (non-IID) data distributions, which are\nprevalent in real-world deployments. FedGreed exhibits convergence guarantees\nand bounded optimality gaps under strong adversarial behavior. Experimental\nevaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our method\nsignificantly outperforms standard and robust federated learning baselines,\nsuch as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority of\nadversarial scenarios considered, including label flipping and Gaussian noise\ninjection attacks. All experiments were conducted using the Flower federated\nlearning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across multiple\nclients while preserving data privacy by keeping local datasets on-device. In\nthis work, we address FL settings where clients may behave adversarially,\nexhibiting Byzantine attacks, while the central server is trusted and equipped\nwith a reference dataset. We propose FedGreed, a resilient aggregation strategy\nfor federated learning that does not require any assumptions about the fraction\nof adversarial participants. FedGreed orders clients' local model updates based\non their loss metrics evaluated against a trusted dataset on the server and\ngreedily selects a subset of clients whose models exhibit the minimal\nevaluation loss. Unlike many existing approaches, our method is designed to\noperate reliably under heterogeneous (non-IID) data distributions, which are\nprevalent in real-world deployments. FedGreed exhibits convergence guarantees\nand bounded optimality gaps under strong adversarial behavior. Experimental\nevaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our method\nsignificantly outperforms standard and robust federated learning baselines,\nsuch as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority of\nadversarial scenarios considered, including label flipping and Gaussian noise\ninjection attacks. All experiments were conducted using the Flower federated\nlearning framework."
                },
                "authors": [
                    {
                        "name": "Emmanouil Kritharakis"
                    },
                    {
                        "name": "Antonios Makris"
                    },
                    {
                        "name": "Dusan Jakovetic"
                    },
                    {
                        "name": "Konstantinos Tserpes"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Tserpes"
                },
                "author": "Konstantinos Tserpes",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18048v1",
                "updated": "2025-08-25T14:06:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    6,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T14:06:27Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    6,
                    27,
                    0,
                    237,
                    0
                ],
                "title": "HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data"
                },
                "summary": "User queries in real-world recommendation systems often combine structured\nconstraints (e.g., category, attributes) with unstructured preferences (e.g.,\nproduct descriptions or reviews). We introduce HyST (Hybrid retrieval over\nSemi-structured Tabular data), a hybrid retrieval framework that combines\nLLM-powered structured filtering with semantic embedding search to support\ncomplex information needs over semi-structured tabular data. HyST extracts\nattribute-level constraints from natural language using large language models\n(LLMs) and applies them as metadata filters, while processing the remaining\nunstructured query components via embedding-based retrieval. Experiments on a\nsemi-structured benchmark show that HyST consistently outperforms tradtional\nbaselines, highlighting the importance of structured filtering in improving\nretrieval precision, offering a scalable and accurate solution for real-world\nuser queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User queries in real-world recommendation systems often combine structured\nconstraints (e.g., category, attributes) with unstructured preferences (e.g.,\nproduct descriptions or reviews). We introduce HyST (Hybrid retrieval over\nSemi-structured Tabular data), a hybrid retrieval framework that combines\nLLM-powered structured filtering with semantic embedding search to support\ncomplex information needs over semi-structured tabular data. HyST extracts\nattribute-level constraints from natural language using large language models\n(LLMs) and applies them as metadata filters, while processing the remaining\nunstructured query components via embedding-based retrieval. Experiments on a\nsemi-structured benchmark show that HyST consistently outperforms tradtional\nbaselines, highlighting the importance of structured filtering in improving\nretrieval precision, offering a scalable and accurate solution for real-world\nuser queries."
                },
                "authors": [
                    {
                        "name": "Jiyoon Myung"
                    },
                    {
                        "name": "Jihyeon Park"
                    },
                    {
                        "name": "Joohyung Han"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Han"
                },
                "author": "Joohyung Han",
                "arxiv_comment": "Accepted at the 2nd EARL Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models (RecSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03001v3",
                "updated": "2025-08-25T14:06:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    6,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2024-08-06T07:19:51Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    19,
                    51,
                    1,
                    219,
                    0
                ],
                "title": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning"
                },
                "summary": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yu Song"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Jihong Hu"
                    },
                    {
                        "name": "Yen-Wei Chen"
                    },
                    {
                        "name": "Lanfen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lanfen Lin"
                },
                "author": "Lanfen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18596v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18596v4",
                "updated": "2025-08-26T10:08:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    8,
                    51,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-24T08:44:33Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    8,
                    44,
                    33,
                    5,
                    144,
                    0
                ],
                "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models"
                },
                "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication."
                },
                "authors": [
                    {
                        "name": "Chen Han"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xijin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xijin Tang"
                },
                "author": "Xijin Tang",
                "arxiv_comment": "This paper has been accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18596v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05214v3",
                "updated": "2025-08-25T14:03:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    3,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-04-07T16:01:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "Post-Training Language Models for Continual Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Language Models for Continual Relation Extraction"
                },
                "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction."
                },
                "authors": [
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Adrian Paschke"
                    },
                    {
                        "name": "Sonja Schimmler"
                    }
                ],
                "author_detail": {
                    "name": "Sonja Schimmler"
                },
                "author": "Sonja Schimmler",
                "arxiv_comment": "17 pages, Initial Results and Reporting of the work. This work has\n  been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18688v2",
                "updated": "2025-08-25T14:01:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    1,
                    55,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-24T13:19:03Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    19,
                    3,
                    5,
                    144,
                    0
                ],
                "title": "Large Language Models in the Task of Automatic Validation of Text\n  Classifier Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in the Task of Automatic Validation of Text\n  Classifier Predictions"
                },
                "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning."
                },
                "authors": [
                    {
                        "name": "Aleksandr Tsymbalov"
                    },
                    {
                        "name": "Mikhail Khovrichev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Khovrichev"
                },
                "author": "Mikhail Khovrichev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07165v2",
                "updated": "2025-08-25T14:01:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    1,
                    22,
                    0,
                    237,
                    0
                ],
                "published": "2025-01-13T09:51:23Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    51,
                    23,
                    0,
                    13,
                    0
                ],
                "title": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study"
                },
                "summary": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field."
                },
                "authors": [
                    {
                        "name": "Huashan Chen"
                    },
                    {
                        "name": "Zisheng Huang"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Jinfu Chen"
                    },
                    {
                        "name": "Haotang Li"
                    },
                    {
                        "name": "Kebin Peng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Sen He"
                    }
                ],
                "author_detail": {
                    "name": "Sen He"
                },
                "author": "Sen He",
                "arxiv_doi": "10.1007/s10515-025-00536-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10515-025-00536-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18040v1",
                "updated": "2025-08-25T13:57:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    57,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:57:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    57,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and\n  Exploration"
                },
                "summary": "Vision language model (VLM)-based mobile agents show great potential for\nassisting users in performing instruction-driven tasks. However, these agents\ntypically struggle with personalized instructions -- those containing\nambiguous, user-specific context -- a challenge that has been largely\noverlooked in previous research. In this paper, we define personalized\ninstructions and introduce PerInstruct, a novel human-annotated dataset\ncovering diverse personalized instructions across various mobile scenarios.\nFurthermore, given the limited personalization capabilities of existing mobile\nagents, we propose PerPilot, a plug-and-play framework powered by large\nlanguage models (LLMs) that enables mobile agents to autonomously perceive,\nunderstand, and execute personalized user instructions. PerPilot identifies\npersonalized elements and autonomously completes instructions via two\ncomplementary approaches: memory-based retrieval and reasoning-based\nexploration. Experimental results demonstrate that PerPilot effectively handles\npersonalized tasks with minimal user intervention and progressively improves\nits performance with continued use, underscoring the importance of\npersonalization-aware reasoning for next-generation mobile agents. The dataset\nand code are available at: https://github.com/xinwang-nwpu/PerPilot",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language model (VLM)-based mobile agents show great potential for\nassisting users in performing instruction-driven tasks. However, these agents\ntypically struggle with personalized instructions -- those containing\nambiguous, user-specific context -- a challenge that has been largely\noverlooked in previous research. In this paper, we define personalized\ninstructions and introduce PerInstruct, a novel human-annotated dataset\ncovering diverse personalized instructions across various mobile scenarios.\nFurthermore, given the limited personalization capabilities of existing mobile\nagents, we propose PerPilot, a plug-and-play framework powered by large\nlanguage models (LLMs) that enables mobile agents to autonomously perceive,\nunderstand, and execute personalized user instructions. PerPilot identifies\npersonalized elements and autonomously completes instructions via two\ncomplementary approaches: memory-based retrieval and reasoning-based\nexploration. Experimental results demonstrate that PerPilot effectively handles\npersonalized tasks with minimal user intervention and progressively improves\nits performance with continued use, underscoring the importance of\npersonalization-aware reasoning for next-generation mobile agents. The dataset\nand code are available at: https://github.com/xinwang-nwpu/PerPilot"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zhiyao Cui"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Ya Zeng"
                    },
                    {
                        "name": "Chenxu Wang"
                    },
                    {
                        "name": "Ruiqi Song"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Jinzhuo Liu"
                    },
                    {
                        "name": "Siyue Ren"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Wang"
                },
                "author": "Zhen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18025v1",
                "updated": "2025-08-25T13:44:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    44,
                    0,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:44:00Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    44,
                    0,
                    0,
                    237,
                    0
                ],
                "title": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for\n  Autonomous Space Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for\n  Autonomous Space Exploration"
                },
                "summary": "Autonomous planetary exploration missions are critically dependent on\nreal-time, accurate environmental perception for navigation and hazard\navoidance. However, deploying deep learning models on the resource-constrained\ncomputational hardware of planetary exploration platforms remains a significant\nchallenge. This paper introduces the Adaptive Quantized Planetary Crater\nDetection System (AQ-PCDSys), a novel framework specifically engineered for\nreal-time, onboard deployment in the computationally constrained environments\nof space exploration missions. AQ-PCDSys synergistically integrates a Quantized\nNeural Network (QNN) architecture, trained using Quantization-Aware Training\n(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture\nsignificantly optimizes model size and inference latency suitable for real-time\nonboard deployment in space exploration missions, while preserving high\naccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and\nDigital Elevation Models (DEMs) at the feature level, utilizing an Adaptive\nWeighting Mechanism (AWM) to dynamically prioritize the most relevant and\nreliable sensor modality based on planetary ambient conditions. This approach\nenhances detection robustness across diverse planetary landscapes. Paired with\nMulti-Scale Detection Heads specifically designed for robust and efficient\ndetection of craters across a wide range of sizes, AQ-PCDSys provides a\ncomputationally efficient, reliable and accurate solution for planetary crater\ndetection, a critical capability for enabling the next generation of autonomous\nplanetary landing, navigation, and scientific exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous planetary exploration missions are critically dependent on\nreal-time, accurate environmental perception for navigation and hazard\navoidance. However, deploying deep learning models on the resource-constrained\ncomputational hardware of planetary exploration platforms remains a significant\nchallenge. This paper introduces the Adaptive Quantized Planetary Crater\nDetection System (AQ-PCDSys), a novel framework specifically engineered for\nreal-time, onboard deployment in the computationally constrained environments\nof space exploration missions. AQ-PCDSys synergistically integrates a Quantized\nNeural Network (QNN) architecture, trained using Quantization-Aware Training\n(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture\nsignificantly optimizes model size and inference latency suitable for real-time\nonboard deployment in space exploration missions, while preserving high\naccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and\nDigital Elevation Models (DEMs) at the feature level, utilizing an Adaptive\nWeighting Mechanism (AWM) to dynamically prioritize the most relevant and\nreliable sensor modality based on planetary ambient conditions. This approach\nenhances detection robustness across diverse planetary landscapes. Paired with\nMulti-Scale Detection Heads specifically designed for robust and efficient\ndetection of craters across a wide range of sizes, AQ-PCDSys provides a\ncomputationally efficient, reliable and accurate solution for planetary crater\ndetection, a critical capability for enabling the next generation of autonomous\nplanetary landing, navigation, and scientific exploration."
                },
                "authors": [
                    {
                        "name": "Aditri Paul"
                    },
                    {
                        "name": "Archan Paul"
                    }
                ],
                "author_detail": {
                    "name": "Archan Paul"
                },
                "author": "Archan Paul",
                "arxiv_comment": "17 pages, 6 figures. A research paper on a novel deep learning\n  framework for planetary crater detection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07(2020), 68T45(2020), 68T10(2020), 90C90(2020)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.6; I.2.9; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21577v3",
                "updated": "2025-08-25T13:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    40,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-27T08:35:05Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    8,
                    35,
                    5,
                    1,
                    147,
                    0
                ],
                "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub\n  Repositories for Complex Task Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMaster: Autonomous Exploration and Understanding of GitHub\n  Repositories for Complex Task Solving"
                },
                "summary": "The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/QuantaAlpha/RepoMaster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/QuantaAlpha/RepoMaster."
                },
                "authors": [
                    {
                        "name": "Huacan Wang"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Shuo Lu"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyang He"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Pin Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Pin Lyu"
                },
                "author": "Pin Lyu",
                "arxiv_comment": "A novel approach; Very practical",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18020v1",
                "updated": "2025-08-25T13:35:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    35,
                    21,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:35:21Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    35,
                    21,
                    0,
                    237,
                    0
                ],
                "title": "Blade Antenna-SDR System Prototype for the CANTAR Global 21-cm\n  Experiment: Simulations, Measurements, and In-Situ Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blade Antenna-SDR System Prototype for the CANTAR Global 21-cm\n  Experiment: Simulations, Measurements, and In-Situ Results"
                },
                "summary": "We present the design and initial testing of a low-frequency radio telescope\nprototype developed for the CANTAR (Colombian Antarctic Telescope for 21-cm\nAbsorption during Reionization) experiment. Operating from 100 to 200 MHz, the\nsystem integrates a blade dipole antenna inspired by the EDGES high-band design\nwith a software-defined radio (SDR) receiver. We report simulations of antenna\nimpedance and beam chromaticity, along with SDR performance tests (Limenet\nMini, Ettus E310, USRP2920). A dual-stage low-noise amplifier reduces system\ntemperature, enabling foreground-sensitive observations. Radiometric estimates\nsuggest sub-mK sensitivity is achievable with 1000 h of integration. This\nprototype forms part of Colombia's emerging infrastructure for 21-cm cosmology,\nwith deployments planned in low-RFI sites in the Colombian Andes and\nAntarctica.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and initial testing of a low-frequency radio telescope\nprototype developed for the CANTAR (Colombian Antarctic Telescope for 21-cm\nAbsorption during Reionization) experiment. Operating from 100 to 200 MHz, the\nsystem integrates a blade dipole antenna inspired by the EDGES high-band design\nwith a software-defined radio (SDR) receiver. We report simulations of antenna\nimpedance and beam chromaticity, along with SDR performance tests (Limenet\nMini, Ettus E310, USRP2920). A dual-stage low-noise amplifier reduces system\ntemperature, enabling foreground-sensitive observations. Radiometric estimates\nsuggest sub-mK sensitivity is achievable with 1000 h of integration. This\nprototype forms part of Colombia's emerging infrastructure for 21-cm cosmology,\nwith deployments planned in low-RFI sites in the Colombian Andes and\nAntarctica."
                },
                "authors": [
                    {
                        "name": "F. P. Mosquera"
                    },
                    {
                        "name": "J. Rodriguez-Ferreira"
                    },
                    {
                        "name": "E. Acevedo"
                    },
                    {
                        "name": "O. Restrepo"
                    },
                    {
                        "name": "D. Gonzalez"
                    },
                    {
                        "name": "G. Chaparro"
                    }
                ],
                "author_detail": {
                    "name": "G. Chaparro"
                },
                "author": "G. Chaparro",
                "arxiv_comment": "23 pages, 17 figures, Accepted to RMxAA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16153v2",
                "updated": "2025-08-25T13:32:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    32,
                    12,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T07:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    25,
                    30,
                    4,
                    234,
                    0
                ],
                "title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"
                },
                "summary": "In this paper, we introduce a novel learning paradigm for Adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely \\emph{Memento}, which attains top-1\non GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It\nreaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset,\noutperforming the state-of-the-art training-based method, while case-based\nmemory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks.\nOur approach offers a scalable and efficient pathway for developing generalist\nLLM agents capable of continuous, real-time learning without gradient updates,\nadvancing machine learning towards open-ended skill acquisition and deep\nresearch scenarios. The code is available at\nhttps://github.com/Agent-on-the-Fly/Memento.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel learning paradigm for Adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely \\emph{Memento}, which attains top-1\non GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It\nreaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset,\noutperforming the state-of-the-art training-based method, while case-based\nmemory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks.\nOur approach offers a scalable and efficient pathway for developing generalist\nLLM agents capable of continuous, real-time learning without gradient updates,\nadvancing machine learning towards open-ended skill acquisition and deep\nresearch scenarios. The code is available at\nhttps://github.com/Agent-on-the-Fly/Memento."
                },
                "authors": [
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Siyuan Guo"
                    },
                    {
                        "name": "Xue Yan"
                    },
                    {
                        "name": "Kin Hei Lee"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Ka Yiu Lee"
                    },
                    {
                        "name": "Guchun Zhang"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18023v3",
                "updated": "2025-08-25T13:17:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    17,
                    9,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-25T09:32:08Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    32,
                    8,
                    1,
                    56,
                    0
                ],
                "title": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference"
                },
                "summary": "Despite the advancements made in Vision Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tune a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary, based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advancements made in Vision Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tune a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary, based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary"
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Geng"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "EMNLP2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11703v2",
                "updated": "2025-08-25T13:14:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    14,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-13T17:56:59Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    56,
                    59,
                    2,
                    225,
                    0
                ],
                "title": "Data-Driven Discovery of Interpretable Kalman Filter Variants through\n  Large Language Models and Genetic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Discovery of Interpretable Kalman Filter Variants through\n  Large Language Models and Genetic Programming"
                },
                "summary": "Algorithmic discovery has traditionally relied on human ingenuity and\nextensive experimentation. Here we investigate whether a prominent scientific\ncomputing algorithm, the Kalman Filter, can be discovered through an automated,\ndata-driven, evolutionary process that relies on Cartesian Genetic Programming\n(CGP) and Large Language Models (LLM). We evaluate the contributions of both\nmodalities (CGP and LLM) in discovering the Kalman filter under varying\nconditions. Our results demonstrate that our framework of CGP and LLM-assisted\nevolution converges to near-optimal solutions when Kalman optimality\nassumptions hold. When these assumptions are violated, our framework evolves\ninterpretable alternatives that outperform the Kalman filter. These results\ndemonstrate that combining evolutionary algorithms and generative models for\ninterpretable, data-driven synthesis of simple computational modules is a\npotent approach for algorithmic discovery in scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic discovery has traditionally relied on human ingenuity and\nextensive experimentation. Here we investigate whether a prominent scientific\ncomputing algorithm, the Kalman Filter, can be discovered through an automated,\ndata-driven, evolutionary process that relies on Cartesian Genetic Programming\n(CGP) and Large Language Models (LLM). We evaluate the contributions of both\nmodalities (CGP and LLM) in discovering the Kalman filter under varying\nconditions. Our results demonstrate that our framework of CGP and LLM-assisted\nevolution converges to near-optimal solutions when Kalman optimality\nassumptions hold. When these assumptions are violated, our framework evolves\ninterpretable alternatives that outperform the Kalman filter. These results\ndemonstrate that combining evolutionary algorithms and generative models for\ninterpretable, data-driven synthesis of simple computational modules is a\npotent approach for algorithmic discovery in scientific computing."
                },
                "authors": [
                    {
                        "name": "Vasileios Saketos"
                    },
                    {
                        "name": "Sebastian Kaltenbach"
                    },
                    {
                        "name": "Sergey Litvinov"
                    },
                    {
                        "name": "Petros Koumoutsakos"
                    }
                ],
                "author_detail": {
                    "name": "Petros Koumoutsakos"
                },
                "author": "Petros Koumoutsakos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17990v1",
                "updated": "2025-08-25T13:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    0,
                    41,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T13:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    13,
                    0,
                    41,
                    0,
                    237,
                    0
                ],
                "title": "Automating Conflict-Aware ACL Configurations with Natural Language\n  Intents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Conflict-Aware ACL Configurations with Natural Language\n  Intents"
                },
                "summary": "ACL configuration is essential for managing network flow reachability, yet\nits complexity grows significantly with topologies and pre-existing rules. To\ncarry out ACL configuration, the operator needs to (1) understand the new\nconfiguration policies or intents and translate them into concrete ACL rules,\n(2) check and resolve any conflicts between the new and existing rules, and (3)\ndeploy them across the network. Existing systems rely heavily on manual efforts\nfor these tasks, especially for the first two, which are tedious, error-prone,\nand impractical to scale.\n  We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge\nof the target network, Xumi automatically and accurately translates the natural\nlanguage intents into complete ACL rules to reduce operators' manual efforts.\nXumi then detects all potential conflicts between new and existing rules and\ngenerates resolved intents for deployment with operators' guidance, and finally\nidentifies the best deployment plan that minimizes the rule additions while\nsatisfying all intents. Evaluation shows that Xumi accelerates the entire\nconfiguration pipeline by over 10x compared to current practices, addresses\nO(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACL configuration is essential for managing network flow reachability, yet\nits complexity grows significantly with topologies and pre-existing rules. To\ncarry out ACL configuration, the operator needs to (1) understand the new\nconfiguration policies or intents and translate them into concrete ACL rules,\n(2) check and resolve any conflicts between the new and existing rules, and (3)\ndeploy them across the network. Existing systems rely heavily on manual efforts\nfor these tasks, especially for the first two, which are tedious, error-prone,\nand impractical to scale.\n  We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge\nof the target network, Xumi automatically and accurately translates the natural\nlanguage intents into complete ACL rules to reduce operators' manual efforts.\nXumi then detects all potential conflicts between new and existing rules and\ngenerates resolved intents for deployment with operators' guidance, and finally\nidentifies the best deployment plan that minimizes the rule additions while\nsatisfying all intents. Evaluation shows that Xumi accelerates the entire\nconfiguration pipeline by over 10x compared to current practices, addresses\nO(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud\nnetwork."
                },
                "authors": [
                    {
                        "name": "Wenlong Ding"
                    },
                    {
                        "name": "Jianqiang Li"
                    },
                    {
                        "name": "Zhixiong Niu"
                    },
                    {
                        "name": "Huangxun Chen"
                    },
                    {
                        "name": "Yongqiang Xiong"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17973v1",
                "updated": "2025-08-25T12:40:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    40,
                    32,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:40:32Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    40,
                    32,
                    0,
                    237,
                    0
                ],
                "title": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German"
                },
                "summary": "The ability to paraphrase texts across different complexity levels is\nessential for creating accessible texts that can be tailored toward diverse\nreader groups. Thus, we introduce German4All, the first large-scale German\ndataset of aligned readability-controlled, paragraph-level paraphrases. It\nspans five readability levels and comprises over 25,000 samples. The dataset is\nautomatically synthesized using GPT-4 and rigorously evaluated through both\nhuman and LLM-based judgments. Using German4All, we train an open-source,\nreadability-controlled paraphrasing model that achieves state-of-the-art\nperformance in German text simplification, enabling more nuanced and\nreader-specific adaptations. We opensource both the dataset and the model to\nencourage further research on multi-level paraphrasing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to paraphrase texts across different complexity levels is\nessential for creating accessible texts that can be tailored toward diverse\nreader groups. Thus, we introduce German4All, the first large-scale German\ndataset of aligned readability-controlled, paragraph-level paraphrases. It\nspans five readability levels and comprises over 25,000 samples. The dataset is\nautomatically synthesized using GPT-4 and rigorously evaluated through both\nhuman and LLM-based judgments. Using German4All, we train an open-source,\nreadability-controlled paraphrasing model that achieves state-of-the-art\nperformance in German text simplification, enabling more nuanced and\nreader-specific adaptations. We opensource both the dataset and the model to\nencourage further research on multi-level paraphrasing"
                },
                "authors": [
                    {
                        "name": "Miriam Ansch√ºtz"
                    },
                    {
                        "name": "Thanh Mai Pham"
                    },
                    {
                        "name": "Eslam Nasrallah"
                    },
                    {
                        "name": "Maximilian M√ºller"
                    },
                    {
                        "name": "Cristian-George Craciun"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh",
                "arxiv_comment": "Accepted to INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17971v1",
                "updated": "2025-08-25T12:38:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    38,
                    8,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:38:08Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    38,
                    8,
                    0,
                    237,
                    0
                ],
                "title": "Neural Algorithmic Reasoners informed Large Language Model for\n  Multi-Agent Path Finding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Algorithmic Reasoners informed Large Language Model for\n  Multi-Agent Path Finding"
                },
                "summary": "The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems."
                },
                "authors": [
                    {
                        "name": "Pu Feng"
                    },
                    {
                        "name": "Size Wang"
                    },
                    {
                        "name": "Yuhong Cao"
                    },
                    {
                        "name": "Junkang Liang"
                    },
                    {
                        "name": "Rongye Shi"
                    },
                    {
                        "name": "Wenjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Wu"
                },
                "author": "Wenjun Wu",
                "arxiv_comment": "Accepted by IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17960v1",
                "updated": "2025-08-25T12:20:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    20,
                    29,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:20:29Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    20,
                    29,
                    0,
                    237,
                    0
                ],
                "title": "A Unified Transformer Architecture for Low-Latency and Scalable Wireless\n  Signal Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Transformer Architecture for Low-Latency and Scalable Wireless\n  Signal Processing"
                },
                "summary": "We propose a unified Transformer-based architecture for wireless signal\nprocessing tasks, offering a low-latency, task-adaptive alternative to\nconventional receiver pipelines. Unlike traditional modular designs, our model\nintegrates channel estimation, interpolation, and demapping into a single,\ncompact attention-driven architecture designed for real-time deployment. The\nmodel's structure allows dynamic adaptation to diverse output formats by simply\nmodifying the final projection layer, enabling consistent reuse across receiver\nsubsystems. Experimental results demonstrate strong generalization to varying\nuser counts, modulation schemes, and pilot configurations, while satisfying\nlatency constraints imposed by practical systems. The architecture is evaluated\nacross three core use cases: (1) an End-to-End Receiver, which replaces the\nentire baseband processing pipeline from pilot symbols to bit-level decisions;\n(2) Channel Frequency Interpolation, implemented and tested within a\n3GPP-compliant OAI+Aerial system; and (3) Channel Estimation, where the model\ninfers full-band channel responses from sparse pilot observations. In all\ncases, our approach outperforms classical baselines in terms of accuracy,\nrobustness, and computational efficiency. This work presents a deployable,\ndata-driven alternative to hand-engineered PHY-layer blocks, and lays the\nfoundation for intelligent, software-defined signal processing in\nnext-generation wireless communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unified Transformer-based architecture for wireless signal\nprocessing tasks, offering a low-latency, task-adaptive alternative to\nconventional receiver pipelines. Unlike traditional modular designs, our model\nintegrates channel estimation, interpolation, and demapping into a single,\ncompact attention-driven architecture designed for real-time deployment. The\nmodel's structure allows dynamic adaptation to diverse output formats by simply\nmodifying the final projection layer, enabling consistent reuse across receiver\nsubsystems. Experimental results demonstrate strong generalization to varying\nuser counts, modulation schemes, and pilot configurations, while satisfying\nlatency constraints imposed by practical systems. The architecture is evaluated\nacross three core use cases: (1) an End-to-End Receiver, which replaces the\nentire baseband processing pipeline from pilot symbols to bit-level decisions;\n(2) Channel Frequency Interpolation, implemented and tested within a\n3GPP-compliant OAI+Aerial system; and (3) Channel Estimation, where the model\ninfers full-band channel responses from sparse pilot observations. In all\ncases, our approach outperforms classical baselines in terms of accuracy,\nrobustness, and computational efficiency. This work presents a deployable,\ndata-driven alternative to hand-engineered PHY-layer blocks, and lays the\nfoundation for intelligent, software-defined signal processing in\nnext-generation wireless communication systems."
                },
                "authors": [
                    {
                        "name": "Yuto Kawai"
                    },
                    {
                        "name": "Rajeev Koodli"
                    }
                ],
                "author_detail": {
                    "name": "Rajeev Koodli"
                },
                "author": "Rajeev Koodli",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17959v1",
                "updated": "2025-08-25T12:19:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    19,
                    57,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:19:57Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    19,
                    57,
                    0,
                    237,
                    0
                ],
                "title": "Language Models Coupled with Metacognition Can Outperform Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Coupled with Metacognition Can Outperform Reasoning\n  Models"
                },
                "summary": "Large language models (LLMs) excel in speed and adaptability across various\nreasoning tasks, but they often struggle when strict logic or constraint\nenforcement is required. In contrast, Large Reasoning Models (LRMs) are\nspecifically designed for complex, step-by-step reasoning, although they come\nwith significant computational costs and slower inference times. To address\nthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)\ncognitive architecture into SOFAI-LM, which coordinates a fast LLM with a\nslower but more powerful LRM through metacognition. The metacognitive module\nactively monitors the LLM's performance and provides targeted, iterative\nfeedback with relevant examples. This enables the LLM to progressively refine\nits solutions without requiring the need for additional model fine-tuning.\nExtensive experiments on graph coloring and code debugging problems demonstrate\nthat our feedback-driven approach significantly enhances the problem-solving\ncapabilities of the LLM. In many instances, it achieves performance levels that\nmatch or even exceed those of standalone LRMs while requiring considerably less\ntime. Additionally, when the LLM and feedback mechanism alone are insufficient,\nwe engage the LRM by providing appropriate information collected during the\nLLM's feedback loop, tailored to the specific characteristics of the problem\ndomain and leads to improved overall performance. Evaluations on two\ncontrasting domains: graph coloring, requiring globally consistent solutions,\nand code debugging, demanding localized fixes, demonstrate that SOFAI-LM\nenables LLMs to match or outperform standalone LRMs in accuracy while\nmaintaining significantly lower inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in speed and adaptability across various\nreasoning tasks, but they often struggle when strict logic or constraint\nenforcement is required. In contrast, Large Reasoning Models (LRMs) are\nspecifically designed for complex, step-by-step reasoning, although they come\nwith significant computational costs and slower inference times. To address\nthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)\ncognitive architecture into SOFAI-LM, which coordinates a fast LLM with a\nslower but more powerful LRM through metacognition. The metacognitive module\nactively monitors the LLM's performance and provides targeted, iterative\nfeedback with relevant examples. This enables the LLM to progressively refine\nits solutions without requiring the need for additional model fine-tuning.\nExtensive experiments on graph coloring and code debugging problems demonstrate\nthat our feedback-driven approach significantly enhances the problem-solving\ncapabilities of the LLM. In many instances, it achieves performance levels that\nmatch or even exceed those of standalone LRMs while requiring considerably less\ntime. Additionally, when the LLM and feedback mechanism alone are insufficient,\nwe engage the LRM by providing appropriate information collected during the\nLLM's feedback loop, tailored to the specific characteristics of the problem\ndomain and leads to improved overall performance. Evaluations on two\ncontrasting domains: graph coloring, requiring globally consistent solutions,\nand code debugging, demanding localized fixes, demonstrate that SOFAI-LM\nenables LLMs to match or outperform standalone LRMs in accuracy while\nmaintaining significantly lower inference time."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Francesca Rossi"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Murray Campbell"
                    },
                    {
                        "name": "Karthikeyan Natesan Ramamurthy"
                    },
                    {
                        "name": "Lior Horesh"
                    }
                ],
                "author_detail": {
                    "name": "Lior Horesh"
                },
                "author": "Lior Horesh",
                "arxiv_comment": "37 Pages, 95 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17953v1",
                "updated": "2025-08-25T12:16:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    16,
                    56,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:16:56Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    16,
                    56,
                    0,
                    237,
                    0
                ],
                "title": "Understanding Subword Compositionality of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Subword Compositionality of Large Language Models"
                },
                "summary": "Large language models (LLMs) take sequences of subwords as input, requiring\nthem to effective compose subword representations into meaningful word-level\nrepresentations. In this paper, we present a comprehensive set of experiments\nto probe how LLMs compose subword information, focusing on three key aspects:\nstructural similarity, semantic decomposability, and form retention. Our\nanalysis of the experiments suggests that these five LLM families can be\nclassified into three distinct groups, likely reflecting difference in their\nunderlying composition strategies. Specifically, we observe (i) three distinct\npatterns in the evolution of structural similarity between subword compositions\nand whole-word representations across layers; (ii) great performance when\nprobing layer by layer their sensitivity to semantic decompositionality; and\n(iii) three distinct patterns when probing sensitivity to formal features,\ne.g., character sequence length. These findings provide valuable insights into\nthe compositional dynamics of LLMs and highlight different compositional\npattens in how LLMs encode and integrate subword information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) take sequences of subwords as input, requiring\nthem to effective compose subword representations into meaningful word-level\nrepresentations. In this paper, we present a comprehensive set of experiments\nto probe how LLMs compose subword information, focusing on three key aspects:\nstructural similarity, semantic decomposability, and form retention. Our\nanalysis of the experiments suggests that these five LLM families can be\nclassified into three distinct groups, likely reflecting difference in their\nunderlying composition strategies. Specifically, we observe (i) three distinct\npatterns in the evolution of structural similarity between subword compositions\nand whole-word representations across layers; (ii) great performance when\nprobing layer by layer their sensitivity to semantic decompositionality; and\n(iii) three distinct patterns when probing sensitivity to formal features,\ne.g., character sequence length. These findings provide valuable insights into\nthe compositional dynamics of LLMs and highlight different compositional\npattens in how LLMs encode and integrate subword information."
                },
                "authors": [
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Anders S√∏gaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders S√∏gaard"
                },
                "author": "Anders S√∏gaard",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17948v1",
                "updated": "2025-08-25T12:13:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    13,
                    37,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:13:37Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    13,
                    37,
                    0,
                    237,
                    0
                ],
                "title": "Debiasing Multilingual LLMs in Cross-lingual Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Multilingual LLMs in Cross-lingual Latent Space"
                },
                "summary": "Debiasing techniques such as SentDebias aim to reduce bias in large language\nmodels (LLMs). Previous studies have evaluated their cross-lingual\ntransferability by directly applying these methods to LLM representations,\nrevealing their limited effectiveness across languages. In this work, we\ntherefore propose to perform debiasing in a joint latent space rather than\ndirectly on LLM representations. We construct a well-aligned cross-lingual\nlatent space using an autoencoder trained on parallel TED talk scripts. Our\nexperiments with Aya-expanse and two debiasing techniques across four languages\n(English, French, German, Dutch) demonstrate that a) autoencoders effectively\nconstruct a well-aligned cross-lingual latent space, and b) applying debiasing\ntechniques in the learned cross-lingual latent space significantly improves\nboth the overall debiasing performance and cross-lingual transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing techniques such as SentDebias aim to reduce bias in large language\nmodels (LLMs). Previous studies have evaluated their cross-lingual\ntransferability by directly applying these methods to LLM representations,\nrevealing their limited effectiveness across languages. In this work, we\ntherefore propose to perform debiasing in a joint latent space rather than\ndirectly on LLM representations. We construct a well-aligned cross-lingual\nlatent space using an autoencoder trained on parallel TED talk scripts. Our\nexperiments with Aya-expanse and two debiasing techniques across four languages\n(English, French, German, Dutch) demonstrate that a) autoencoders effectively\nconstruct a well-aligned cross-lingual latent space, and b) applying debiasing\ntechniques in the learned cross-lingual latent space significantly improves\nboth the overall debiasing performance and cross-lingual transferability."
                },
                "authors": [
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Anders S√∏gaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders S√∏gaard"
                },
                "author": "Anders S√∏gaard",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17944v1",
                "updated": "2025-08-25T12:09:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    9,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T12:09:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    9,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "A Feminist Account of Intersectional Algorithmic Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Feminist Account of Intersectional Algorithmic Fairness"
                },
                "summary": "Intersectionality has profoundly influenced research and political action by\nrevealing how interconnected systems of privilege and oppression influence\nlived experiences, yet its integration into algorithmic fairness research\nremains limited. Existing approaches often rely on single-axis or formal\nsubgroup frameworks that risk oversimplifying social realities and neglecting\nstructural inequalities. We propose Substantive Intersectional Algorithmic\nFairness, extending Green's (2022) notion of substantive algorithmic fairness\nwith insights from intersectional feminist theory. Building on this foundation,\nwe introduce ten desiderata within the ROOF methodology to guide the design,\nassessment, and deployment of algorithmic systems in ways that address systemic\ninequities while mitigating harms to intersectionally marginalized communities.\nRather than prescribing fixed operationalizations, these desiderata encourage\nreflection on assumptions of neutrality, the use of protected attributes, the\ninclusion of multiply marginalized groups, and enhancing algorithmic systems'\npotential. Our approach emphasizes that fairness cannot be separated from\nsocial context, and that in some cases, principled non-deployment may be\nnecessary. By bridging computational and social science perspectives, we\nprovide actionable guidance for more equitable, inclusive, and\ncontext-sensitive intersectional algorithmic practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intersectionality has profoundly influenced research and political action by\nrevealing how interconnected systems of privilege and oppression influence\nlived experiences, yet its integration into algorithmic fairness research\nremains limited. Existing approaches often rely on single-axis or formal\nsubgroup frameworks that risk oversimplifying social realities and neglecting\nstructural inequalities. We propose Substantive Intersectional Algorithmic\nFairness, extending Green's (2022) notion of substantive algorithmic fairness\nwith insights from intersectional feminist theory. Building on this foundation,\nwe introduce ten desiderata within the ROOF methodology to guide the design,\nassessment, and deployment of algorithmic systems in ways that address systemic\ninequities while mitigating harms to intersectionally marginalized communities.\nRather than prescribing fixed operationalizations, these desiderata encourage\nreflection on assumptions of neutrality, the use of protected attributes, the\ninclusion of multiply marginalized groups, and enhancing algorithmic systems'\npotential. Our approach emphasizes that fairness cannot be separated from\nsocial context, and that in some cases, principled non-deployment may be\nnecessary. By bridging computational and social science perspectives, we\nprovide actionable guidance for more equitable, inclusive, and\ncontext-sensitive intersectional algorithmic practices."
                },
                "authors": [
                    {
                        "name": "Marie Mirsch"
                    },
                    {
                        "name": "Laila Wegner"
                    },
                    {
                        "name": "Jonas Strube"
                    },
                    {
                        "name": "Carmen Leicht-Scholten"
                    }
                ],
                "author_detail": {
                    "name": "Carmen Leicht-Scholten"
                },
                "arxiv_affiliation": "RWTH Aachen University, Germany",
                "author": "Carmen Leicht-Scholten",
                "arxiv_comment": "27 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00026v2",
                "updated": "2025-08-25T12:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    12,
                    0,
                    55,
                    0,
                    237,
                    0
                ],
                "published": "2025-04-26T10:17:48Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    10,
                    17,
                    48,
                    5,
                    116,
                    0
                ],
                "title": "Theory of Mind in Large Language Models: Assessment and Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind in Large Language Models: Assessment and Enhancement"
                },
                "summary": "Theory of Mind (ToM)-the ability to reason about the mental states of oneself\nand others-is a cornerstone of human social intelligence. As Large Language\nModels (LLMs) become increasingly integrated into daily life, understanding\ntheir ability to interpret and respond to human mental states is crucial for\nenabling effective interactions. In this paper, we review LLMs' ToM\ncapabilities by analyzing both evaluation benchmarks and enhancement\nstrategies. For evaluation, we focus on recently proposed and widely used\nstory-based benchmarks. For enhancement, we provide an in-depth analysis of\nrecent methods aimed at improving LLMs' ToM abilities. Furthermore, we outline\npromising directions for future research to further advance these capabilities\nand better adapt LLMs to more realistic and diverse scenarios. Our survey\nserves as a valuable resource for researchers interested in evaluating and\nadvancing LLMs' ToM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM)-the ability to reason about the mental states of oneself\nand others-is a cornerstone of human social intelligence. As Large Language\nModels (LLMs) become increasingly integrated into daily life, understanding\ntheir ability to interpret and respond to human mental states is crucial for\nenabling effective interactions. In this paper, we review LLMs' ToM\ncapabilities by analyzing both evaluation benchmarks and enhancement\nstrategies. For evaluation, we focus on recently proposed and widely used\nstory-based benchmarks. For enhancement, we provide an in-depth analysis of\nrecent methods aimed at improving LLMs' ToM abilities. Furthermore, we outline\npromising directions for future research to further advance these capabilities\nand better adapt LLMs to more realistic and diverse scenarios. Our survey\nserves as a valuable resource for researchers interested in evaluating and\nadvancing LLMs' ToM capabilities."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Cheston Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheston Tan"
                },
                "author": "Cheston Tan",
                "arxiv_comment": "Accepted to ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17913v1",
                "updated": "2025-08-25T11:29:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    29,
                    57,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:29:57Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    29,
                    57,
                    0,
                    237,
                    0
                ],
                "title": "PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol\n  for Secure Digital Twin Binding in Smart Cities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol\n  for Secure Digital Twin Binding in Smart Cities"
                },
                "summary": "Digital twin (DT) technology is rapidly becoming essential for smart city\necosystems, enabling real-time synchronisation and autonomous decision-making\nacross physical and digital domains. However, as DTs take active roles in\ncontrol loops, securely binding them to their physical counterparts in dynamic\nand adversarial environments remains a significant challenge. Existing\nauthentication solutions either rely on static trust models, require\ncentralised authorities, or fail to provide live and verifiable\nphysical-digital binding, making them unsuitable for latency-sensitive and\ndistributed deployments. To address this gap, we introduce PRZK-Bind, a\nlightweight and decentralised authentication protocol that combines\nSchnorr-based zero-knowledge proofs with elliptic curve cryptography to\nestablish secure, real-time correspondence between physical entities and DTs\nwithout relying on pre-shared secrets. Simulation results show that PRZK-Bind\nsignificantly improves performance, offering up to 4.5 times lower latency and\n4 times reduced energy consumption compared to cryptography-heavy baselines,\nwhile maintaining false acceptance rates more than 10 times lower. These\nfindings highlight its suitability for future smart city deployments requiring\nefficient, resilient, and trustworthy DT authentication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twin (DT) technology is rapidly becoming essential for smart city\necosystems, enabling real-time synchronisation and autonomous decision-making\nacross physical and digital domains. However, as DTs take active roles in\ncontrol loops, securely binding them to their physical counterparts in dynamic\nand adversarial environments remains a significant challenge. Existing\nauthentication solutions either rely on static trust models, require\ncentralised authorities, or fail to provide live and verifiable\nphysical-digital binding, making them unsuitable for latency-sensitive and\ndistributed deployments. To address this gap, we introduce PRZK-Bind, a\nlightweight and decentralised authentication protocol that combines\nSchnorr-based zero-knowledge proofs with elliptic curve cryptography to\nestablish secure, real-time correspondence between physical entities and DTs\nwithout relying on pre-shared secrets. Simulation results show that PRZK-Bind\nsignificantly improves performance, offering up to 4.5 times lower latency and\n4 times reduced energy consumption compared to cryptography-heavy baselines,\nwhile maintaining false acceptance rates more than 10 times lower. These\nfindings highlight its suitability for future smart city deployments requiring\nefficient, resilient, and trustworthy DT authentication."
                },
                "authors": [
                    {
                        "name": "Yagmur Yigit"
                    },
                    {
                        "name": "Mehmet Ali Erturk"
                    },
                    {
                        "name": "Kerem Gursu"
                    },
                    {
                        "name": "Berk Canberk"
                    }
                ],
                "author_detail": {
                    "name": "Berk Canberk"
                },
                "author": "Berk Canberk",
                "arxiv_comment": "6 pages, 4 figures, 2 tables, Accepted by IEEE Global Communications\n  Conference (GLOBECOM) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05914v3",
                "updated": "2025-08-25T11:26:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    26,
                    22,
                    0,
                    237,
                    0
                ],
                "published": "2024-06-09T20:56:38Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    20,
                    56,
                    38,
                    6,
                    161,
                    0
                ],
                "title": "Soundscape Captioning using Sound Affective Quality Network and Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soundscape Captioning using Sound Affective Quality Network and Large\n  Language Model"
                },
                "summary": "We live in a rich and varied acoustic world, which is experienced by\nindividuals or communities as a soundscape. Computational auditory scene\nanalysis, disentangling acoustic scenes by detecting and classifying events,\nfocuses on objective attributes of sounds, such as their category and temporal\ncharacteristics, ignoring their effects on people, such as the emotions they\nevoke within a context. To fill this gap, we propose the affective soundscape\ncaptioning (ASSC) task, which enables automated soundscape analysis, thus\navoiding labour-intensive subjective ratings and surveys in conventional\nmethods. With soundscape captioning, context-aware descriptions are generated\nfor soundscape by capturing the acoustic scenes (ASs), audio events (AEs)\ninformation, and the corresponding human affective qualities (AQs). To this\nend, we propose an automatic soundscape captioner (SoundSCaper) system composed\nof an acoustic model, i.e. SoundAQnet, and a large language model (LLM).\nSoundAQnet simultaneously models multi-scale information about ASs, AEs, and\nperceived AQs, while the LLM describes the soundscape with captions by parsing\nthe information captured with SoundAQnet. SoundSCaper is assessed by two juries\nof 32 people. In expert evaluation, the average score of SoundSCaper-generated\ncaptions is slightly lower than that of two soundscape experts on the\nevaluation set D1 and the external mixed dataset D2, but not statistically\nsignificant. In layperson evaluation, SoundSCaper outperforms soundscape\nexperts in several metrics. In addition to human evaluation, compared to other\nautomated audio captioning systems with and without LLM, SoundSCaper performs\nbetter on the ASSC task in several NLP-based metrics. Overall, SoundSCaper\nperforms well in human subjective evaluation and various objective captioning\nmetrics, and the generated captions are comparable to those annotated by\nsoundscape experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a rich and varied acoustic world, which is experienced by\nindividuals or communities as a soundscape. Computational auditory scene\nanalysis, disentangling acoustic scenes by detecting and classifying events,\nfocuses on objective attributes of sounds, such as their category and temporal\ncharacteristics, ignoring their effects on people, such as the emotions they\nevoke within a context. To fill this gap, we propose the affective soundscape\ncaptioning (ASSC) task, which enables automated soundscape analysis, thus\navoiding labour-intensive subjective ratings and surveys in conventional\nmethods. With soundscape captioning, context-aware descriptions are generated\nfor soundscape by capturing the acoustic scenes (ASs), audio events (AEs)\ninformation, and the corresponding human affective qualities (AQs). To this\nend, we propose an automatic soundscape captioner (SoundSCaper) system composed\nof an acoustic model, i.e. SoundAQnet, and a large language model (LLM).\nSoundAQnet simultaneously models multi-scale information about ASs, AEs, and\nperceived AQs, while the LLM describes the soundscape with captions by parsing\nthe information captured with SoundAQnet. SoundSCaper is assessed by two juries\nof 32 people. In expert evaluation, the average score of SoundSCaper-generated\ncaptions is slightly lower than that of two soundscape experts on the\nevaluation set D1 and the external mixed dataset D2, but not statistically\nsignificant. In layperson evaluation, SoundSCaper outperforms soundscape\nexperts in several metrics. In addition to human evaluation, compared to other\nautomated audio captioning systems with and without LLM, SoundSCaper performs\nbetter on the ASSC task in several NLP-based metrics. Overall, SoundSCaper\nperforms well in human subjective evaluation and various objective captioning\nmetrics, and the generated captions are comparable to those annotated by\nsoundscape experts."
                },
                "authors": [
                    {
                        "name": "Yuanbo Hou"
                    },
                    {
                        "name": "Qiaoqiao Ren"
                    },
                    {
                        "name": "Andrew Mitchell"
                    },
                    {
                        "name": "Wenwu Wang"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Tony Belpaeme"
                    },
                    {
                        "name": "Dick Botteldooren"
                    }
                ],
                "author_detail": {
                    "name": "Dick Botteldooren"
                },
                "author": "Dick Botteldooren",
                "arxiv_comment": "IEEE Transactions on Multimedia, Code:\n  https://github.com/Yuanbo2020/SoundSCaper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14119v3",
                "updated": "2025-08-25T11:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    25,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-18T14:24:27Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    14,
                    24,
                    27,
                    0,
                    230,
                    0
                ],
                "title": "Documenting Deployment with Fabric: A Repository of Real-World AI\n  Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Documenting Deployment with Fabric: A Repository of Real-World AI\n  Governance"
                },
                "summary": "Artificial intelligence (AI) is increasingly integrated into society, from\nfinancial services and traffic management to creative writing. Academic\nliterature on the deployment of AI has mostly focused on the risks and harms\nthat result from the use of AI. We introduce Fabric, a publicly available\nrepository of deployed AI use cases to outline their governance mechanisms.\nThrough semi-structured interviews with practitioners, we collect an initial\nset of 20 AI use cases. In addition, we co-design diagrams of the AI workflow\nwith the practitioners. We discuss the oversight mechanisms and guardrails used\nin practice to safeguard AI use. The Fabric repository includes visual diagrams\nof AI use cases and descriptions of the deployed systems. Using the repository,\nwe surface gaps in governance and find common patterns in human oversight of\ndeployed AI systems. We intend for Fabric to serve as an extendable, evolving\ntool for researchers to study the effectiveness of AI governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is increasingly integrated into society, from\nfinancial services and traffic management to creative writing. Academic\nliterature on the deployment of AI has mostly focused on the risks and harms\nthat result from the use of AI. We introduce Fabric, a publicly available\nrepository of deployed AI use cases to outline their governance mechanisms.\nThrough semi-structured interviews with practitioners, we collect an initial\nset of 20 AI use cases. In addition, we co-design diagrams of the AI workflow\nwith the practitioners. We discuss the oversight mechanisms and guardrails used\nin practice to safeguard AI use. The Fabric repository includes visual diagrams\nof AI use cases and descriptions of the deployed systems. Using the repository,\nwe surface gaps in governance and find common patterns in human oversight of\ndeployed AI systems. We intend for Fabric to serve as an extendable, evolving\ntool for researchers to study the effectiveness of AI governance."
                },
                "authors": [
                    {
                        "name": "Mackenzie Jorgensen"
                    },
                    {
                        "name": "Kendall Brogle"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Lujain Ibrahim"
                    },
                    {
                        "name": "Arina Shah"
                    },
                    {
                        "name": "Petra Ivanovic"
                    },
                    {
                        "name": "Noah Broestl"
                    },
                    {
                        "name": "Gabriel Piles"
                    },
                    {
                        "name": "Paul Dongha"
                    },
                    {
                        "name": "Hatim Abdulhussein"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Jillian Powers"
                    },
                    {
                        "name": "Umang Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Umang Bhatt"
                },
                "author": "Umang Bhatt",
                "arxiv_comment": "AIES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17906v1",
                "updated": "2025-08-25T11:24:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    24,
                    55,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:24:55Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    24,
                    55,
                    0,
                    237,
                    0
                ],
                "title": "FinReflectKG: Agentic Construction and Evaluation of Financial Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinReflectKG: Agentic Construction and Evaluation of Financial Knowledge\n  Graphs"
                },
                "summary": "The financial domain poses unique challenges for knowledge graph (KG)\nconstruction at scale due to the complexity and regulatory nature of financial\ndocuments. Despite the critical importance of structured financial knowledge,\nthe field lacks large-scale, open-source datasets capturing rich semantic\nrelationships from corporate disclosures. We introduce an open-source,\nlarge-scale financial knowledge graph dataset built from the latest annual SEC\n10-K filings of all S and P 100 companies - a comprehensive resource designed\nto catalyze research in financial AI. We propose a robust and generalizable\nknowledge graph (KG) construction framework that integrates intelligent\ndocument parsing, table-aware chunking, and schema-guided iterative extraction\nwith a reflection-driven feedback loop. Our system incorporates a comprehensive\nevaluation pipeline, combining rule-based checks, statistical validation, and\nLLM-as-a-Judge assessments to holistically measure extraction quality. We\nsupport three extraction modes - single-pass, multi-pass, and\nreflection-agent-based - allowing flexible trade-offs between efficiency,\naccuracy, and reliability based on user requirements. Empirical evaluations\ndemonstrate that the reflection-agent-based mode consistently achieves the best\nbalance, attaining a 64.8 percent compliance score against all rule-based\npolicies (CheckRules) and outperforming baseline methods (single-pass and\nmulti-pass) across key metrics such as precision, comprehensiveness, and\nrelevance in LLM-guided evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The financial domain poses unique challenges for knowledge graph (KG)\nconstruction at scale due to the complexity and regulatory nature of financial\ndocuments. Despite the critical importance of structured financial knowledge,\nthe field lacks large-scale, open-source datasets capturing rich semantic\nrelationships from corporate disclosures. We introduce an open-source,\nlarge-scale financial knowledge graph dataset built from the latest annual SEC\n10-K filings of all S and P 100 companies - a comprehensive resource designed\nto catalyze research in financial AI. We propose a robust and generalizable\nknowledge graph (KG) construction framework that integrates intelligent\ndocument parsing, table-aware chunking, and schema-guided iterative extraction\nwith a reflection-driven feedback loop. Our system incorporates a comprehensive\nevaluation pipeline, combining rule-based checks, statistical validation, and\nLLM-as-a-Judge assessments to holistically measure extraction quality. We\nsupport three extraction modes - single-pass, multi-pass, and\nreflection-agent-based - allowing flexible trade-offs between efficiency,\naccuracy, and reliability based on user requirements. Empirical evaluations\ndemonstrate that the reflection-agent-based mode consistently achieves the best\nbalance, attaining a 64.8 percent compliance score against all rule-based\npolicies (CheckRules) and outperforming baseline methods (single-pass and\nmulti-pass) across key metrics such as precision, comprehensiveness, and\nrelevance in LLM-guided evaluations."
                },
                "authors": [
                    {
                        "name": "Abhinav Arun"
                    },
                    {
                        "name": "Fabrizio Dimino"
                    },
                    {
                        "name": "Tejas Prakash Agarwal"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17905v1",
                "updated": "2025-08-25T11:22:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    22,
                    58,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:22:58Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    22,
                    58,
                    0,
                    237,
                    0
                ],
                "title": "Pandora: Leveraging Code-driven Knowledge Transfer for Unified\n  Structured Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pandora: Leveraging Code-driven Knowledge Transfer for Unified\n  Structured Knowledge Reasoning"
                },
                "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions by using structured sources such as tables, databases, and knowledge\ngraphs in a unified way. Existing USKR methods rely on task-specific strategies\nor bespoke representations, which hinder their ability to dismantle barriers\nbetween different SKR tasks, thereby constraining their overall performance in\ncross-task scenarios. In this paper, we introduce \\textsc{Pandora}, a novel\nUSKR framework that addresses the limitations of existing methods by leveraging\ntwo key innovations. First, we propose a code-based unified knowledge\nrepresentation using \\textsc{Python}'s \\textsc{Pandas} API, which aligns\nseamlessly with the pre-training of LLMs. This representation facilitates a\ncohesive approach to handling different structured knowledge sources. Building\non this foundation, we employ knowledge transfer to bolster the unified\nreasoning process of LLMs by automatically building cross-task memory. By\nadaptively correcting reasoning using feedback from code execution,\n\\textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive\nexperiments on six widely used benchmarks across three SKR tasks demonstrate\nthat \\textsc{Pandora} outperforms existing unified reasoning frameworks and\ncompetes effectively with task-specific methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions by using structured sources such as tables, databases, and knowledge\ngraphs in a unified way. Existing USKR methods rely on task-specific strategies\nor bespoke representations, which hinder their ability to dismantle barriers\nbetween different SKR tasks, thereby constraining their overall performance in\ncross-task scenarios. In this paper, we introduce \\textsc{Pandora}, a novel\nUSKR framework that addresses the limitations of existing methods by leveraging\ntwo key innovations. First, we propose a code-based unified knowledge\nrepresentation using \\textsc{Python}'s \\textsc{Pandas} API, which aligns\nseamlessly with the pre-training of LLMs. This representation facilitates a\ncohesive approach to handling different structured knowledge sources. Building\non this foundation, we employ knowledge transfer to bolster the unified\nreasoning process of LLMs by automatically building cross-task memory. By\nadaptively correcting reasoning using feedback from code execution,\n\\textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive\nexperiments on six widely used benchmarks across three SKR tasks demonstrate\nthat \\textsc{Pandora} outperforms existing unified reasoning frameworks and\ncompetes effectively with task-specific methods."
                },
                "authors": [
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Junhao He"
                    },
                    {
                        "name": "Linbo Fu"
                    },
                    {
                        "name": "Shenyu Zhang"
                    },
                    {
                        "name": "Rihui Jin"
                    },
                    {
                        "name": "Xinbang Dai"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Dehai Min"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Tongtong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongtong Wu"
                },
                "author": "Tongtong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16390v2",
                "updated": "2025-08-25T11:17:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    17,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T13:48:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    48,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "MedQARo: A Large-Scale Benchmark for Medical Question Answering in\n  Romanian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedQARo: A Large-Scale Benchmark for Medical Question Answering in\n  Romanian"
                },
                "summary": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nMedQARo, the first large-scale medical QA benchmark in Romanian, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. MedQARo is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on MedQARo. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on MedQARo. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/MedQARo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nMedQARo, the first large-scale medical QA benchmark in Romanian, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. MedQARo is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on MedQARo. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on MedQARo. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/MedQARo."
                },
                "authors": [
                    {
                        "name": "Ana-Cristina Rogoz"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Alexandra-Valentina Anghel"
                    },
                    {
                        "name": "Ionut-Lucian Antone-Iordache"
                    },
                    {
                        "name": "Simona Coniac"
                    },
                    {
                        "name": "Andreea Iuliana Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Iuliana Ionescu"
                },
                "author": "Andreea Iuliana Ionescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17901v1",
                "updated": "2025-08-25T11:15:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    15,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:15:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    15,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "Riemannian Optimization for LoRA on the Stiefel Manifold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riemannian Optimization for LoRA on the Stiefel Manifold"
                },
                "summary": "While powerful, large language models (LLMs) present significant fine-tuning\nchallenges due to their size. Parameter-efficient fine-tuning (PEFT) methods\nlike LoRA provide solutions, yet suffer from critical optimizer inefficiencies;\nnotably basis redundancy in LoRA's $B$ matrix when using AdamW, which\nfundamentally limits performance. We address this by optimizing the $B$ matrix\non the Stiefel manifold, imposing explicit orthogonality constraints that\nachieve near-perfect orthogonality and full effective rank. This geometric\napproach dramatically enhances parameter efficiency and representational\ncapacity. Our Stiefel optimizer consistently outperforms AdamW across\nbenchmarks with both LoRA and DoRA, demonstrating that geometric constraints\nare the key to unlocking LoRA's full potential for effective LLM fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While powerful, large language models (LLMs) present significant fine-tuning\nchallenges due to their size. Parameter-efficient fine-tuning (PEFT) methods\nlike LoRA provide solutions, yet suffer from critical optimizer inefficiencies;\nnotably basis redundancy in LoRA's $B$ matrix when using AdamW, which\nfundamentally limits performance. We address this by optimizing the $B$ matrix\non the Stiefel manifold, imposing explicit orthogonality constraints that\nachieve near-perfect orthogonality and full effective rank. This geometric\napproach dramatically enhances parameter efficiency and representational\ncapacity. Our Stiefel optimizer consistently outperforms AdamW across\nbenchmarks with both LoRA and DoRA, demonstrating that geometric constraints\nare the key to unlocking LoRA's full potential for effective LLM fine-tuning."
                },
                "authors": [
                    {
                        "name": "Juneyoung Park"
                    },
                    {
                        "name": "Minjae Kang"
                    },
                    {
                        "name": "Seongbae Lee"
                    },
                    {
                        "name": "Haegang Lee"
                    },
                    {
                        "name": "Seongwan Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17894v1",
                "updated": "2025-08-25T11:04:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    4,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T11:04:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    4,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "Designing Practical Models for Isolated Word Visual Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Practical Models for Isolated Word Visual Speech Recognition"
                },
                "summary": "Visual speech recognition (VSR) systems decode spoken words from an input\nsequence using only the video data. Practical applications of such systems\ninclude medical assistance as well as human-machine interactions. A VSR system\nis typically employed in a complementary role in cases where the audio is\ncorrupt or not available. In order to accurately predict the spoken words,\nthese architectures often rely on deep neural networks in order to extract\nmeaningful representations from the input sequence. While deep architectures\nachieve impressive recognition performance, relying on such models incurs\nsignificant computation costs which translates into increased resource demands\nin terms of hardware requirements and results in limited applicability in\nreal-world scenarios where resources might be constrained. This factor prevents\nwider adoption and deployment of speech recognition systems in more practical\napplications. In this work, we aim to alleviate this issue by developing\narchitectures for VSR that have low hardware costs. Following the standard\ntwo-network design paradigm, where one network handles visual feature\nextraction and another one utilizes the extracted features to classify the\nentire sequence, we develop lightweight end-to-end architectures by first\nbenchmarking efficient models from the image classification literature, and\nthen adopting lightweight block designs in a temporal convolution network\nbackbone. We create several unified models with low resource requirements but\nstrong recognition performance. Experiments on the largest public database for\nEnglish words demonstrate the effectiveness and practicality of our developed\nmodels. Code and trained models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual speech recognition (VSR) systems decode spoken words from an input\nsequence using only the video data. Practical applications of such systems\ninclude medical assistance as well as human-machine interactions. A VSR system\nis typically employed in a complementary role in cases where the audio is\ncorrupt or not available. In order to accurately predict the spoken words,\nthese architectures often rely on deep neural networks in order to extract\nmeaningful representations from the input sequence. While deep architectures\nachieve impressive recognition performance, relying on such models incurs\nsignificant computation costs which translates into increased resource demands\nin terms of hardware requirements and results in limited applicability in\nreal-world scenarios where resources might be constrained. This factor prevents\nwider adoption and deployment of speech recognition systems in more practical\napplications. In this work, we aim to alleviate this issue by developing\narchitectures for VSR that have low hardware costs. Following the standard\ntwo-network design paradigm, where one network handles visual feature\nextraction and another one utilizes the extracted features to classify the\nentire sequence, we develop lightweight end-to-end architectures by first\nbenchmarking efficient models from the image classification literature, and\nthen adopting lightweight block designs in a temporal convolution network\nbackbone. We create several unified models with low resource requirements but\nstrong recognition performance. Experiments on the largest public database for\nEnglish words demonstrate the effectiveness and practicality of our developed\nmodels. Code and trained models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Iason Ioannis Panagos"
                    },
                    {
                        "name": "Giorgos Sfikas"
                    },
                    {
                        "name": "Christophoros Nikou"
                    }
                ],
                "author_detail": {
                    "name": "Christophoros Nikou"
                },
                "author": "Christophoros Nikou",
                "arxiv_comment": "Double-column format, 13 pages with references, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17884v1",
                "updated": "2025-08-25T10:45:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    45,
                    10,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:45:10Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    45,
                    10,
                    0,
                    237,
                    0
                ],
                "title": "PhantomLint: Principled Detection of Hidden LLM Prompts in Structured\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhantomLint: Principled Detection of Hidden LLM Prompts in Structured\n  Documents"
                },
                "summary": "Hidden LLM prompts have appeared in online documents with increasing\nfrequency. Their goal is to trigger indirect prompt injection attacks while\nremaining undetected from human oversight, to manipulate LLM-powered automated\ndocument processing systems, against applications as diverse as r\\'esum\\'e\nscreeners through to academic peer review processes. Detecting hidden LLM\nprompts is therefore important for ensuring trust in AI-assisted human decision\nmaking.\n  This paper presents the first principled approach to hidden LLM prompt\ndetection in structured documents. We implement our approach in a prototype\ntool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402\ndocuments, including both PDF and HTML documents, and covering academic paper\npreprints, CVs, theses and more. We find that our approach is generally\napplicable against a wide range of methods for hiding LLM prompts from visual\ninspection, has a very low false positive rate (approx. 0.092%), is practically\nuseful for detecting hidden LLM prompts in real documents, while achieving\nacceptable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden LLM prompts have appeared in online documents with increasing\nfrequency. Their goal is to trigger indirect prompt injection attacks while\nremaining undetected from human oversight, to manipulate LLM-powered automated\ndocument processing systems, against applications as diverse as r\\'esum\\'e\nscreeners through to academic peer review processes. Detecting hidden LLM\nprompts is therefore important for ensuring trust in AI-assisted human decision\nmaking.\n  This paper presents the first principled approach to hidden LLM prompt\ndetection in structured documents. We implement our approach in a prototype\ntool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402\ndocuments, including both PDF and HTML documents, and covering academic paper\npreprints, CVs, theses and more. We find that our approach is generally\napplicable against a wide range of methods for hiding LLM prompts from visual\ninspection, has a very low false positive rate (approx. 0.092%), is practically\nuseful for detecting hidden LLM prompts in real documents, while achieving\nacceptable performance."
                },
                "authors": [
                    {
                        "name": "Toby Murray"
                    }
                ],
                "author_detail": {
                    "name": "Toby Murray"
                },
                "author": "Toby Murray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17863v1",
                "updated": "2025-08-25T10:16:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    16,
                    7,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:16:07Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    16,
                    7,
                    0,
                    237,
                    0
                ],
                "title": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis\n  for Spoken Language Understanding in SpeechLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis\n  for Spoken Language Understanding in SpeechLLMs"
                },
                "summary": "With the rise of Speech Large Language Models (SpeechLLMs), two dominant\napproaches have emerged for speech processing: discrete tokens and continuous\nfeatures. Each approach has demonstrated strong capabilities in audio-related\nprocessing tasks. However, the performance gap between these two paradigms has\nnot been thoroughly explored. To address this gap, we present a fair comparison\nof self-supervised learning (SSL)-based discrete and continuous features under\nthe same experimental settings. We evaluate their performance across six spoken\nlanguage understanding-related tasks using both small and large-scale LLMs\n(Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including\nefficient comparison, SSL layer analysis, LLM layer analysis, and robustness\ncomparison. Our findings reveal that continuous features generally outperform\ndiscrete tokens in various tasks. Each speech processing method exhibits\ndistinct characteristics and patterns in how it learns and processes speech\ninformation. We hope our results will provide valuable insights to advance\nspoken language understanding in SpeechLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of Speech Large Language Models (SpeechLLMs), two dominant\napproaches have emerged for speech processing: discrete tokens and continuous\nfeatures. Each approach has demonstrated strong capabilities in audio-related\nprocessing tasks. However, the performance gap between these two paradigms has\nnot been thoroughly explored. To address this gap, we present a fair comparison\nof self-supervised learning (SSL)-based discrete and continuous features under\nthe same experimental settings. We evaluate their performance across six spoken\nlanguage understanding-related tasks using both small and large-scale LLMs\n(Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including\nefficient comparison, SSL layer analysis, LLM layer analysis, and robustness\ncomparison. Our findings reveal that continuous features generally outperform\ndiscrete tokens in various tasks. Each speech processing method exhibits\ndistinct characteristics and patterns in how it learns and processes speech\ninformation. We hope our results will provide valuable insights to advance\nspoken language understanding in SpeechLLMs."
                },
                "authors": [
                    {
                        "name": "Dingdong Wang"
                    },
                    {
                        "name": "Junan Li"
                    },
                    {
                        "name": "Mingyu Cui"
                    },
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Xueyuan Chen"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17862v1",
                "updated": "2025-08-25T10:13:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    13,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:13:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    13,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "Retrieval Feedback Memory Enhancement Large Model Retrieval Generation\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Feedback Memory Enhancement Large Model Retrieval Generation\n  Method"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\ndiverse tasks, yet they face inherent limitations such as constrained\nparametric knowledge and high retraining costs. Retrieval-Augmented Generation\n(RAG) augments the generation process by retrieving externally stored knowledge\nabsent from the models internal parameters. However, RAG methods face\nchallenges such as information loss and redundant retrievals during multi-round\nqueries, accompanying the difficulties in precisely characterizing knowledge\ngaps for complex tasks. To address these problems, we propose Retrieval\nFeedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms\nthe stateless retrieval of previous methods into stateful continuous knowledge\nmanagement by constructing a dynamic evidence pool. Specifically, our method\ngenerates refined queries describing the models knowledge gaps using relational\ntriples from questions and evidence from the dynamic evidence pool; Retrieves\ncritical external knowledge to iteratively update this evidence pool; Employs a\nR-Feedback Model to evaluate evidence completeness until convergence. Compared\nto traditional RAG methods, our approach enables persistent storage of\nretrieved passages and effectively distills key information from passages to\nconstruct clearly new queries. Experiments on three public QA benchmarks\ndemonstrate that RFM-RAG outperforms previous methods and improves overall\nsystem accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\ndiverse tasks, yet they face inherent limitations such as constrained\nparametric knowledge and high retraining costs. Retrieval-Augmented Generation\n(RAG) augments the generation process by retrieving externally stored knowledge\nabsent from the models internal parameters. However, RAG methods face\nchallenges such as information loss and redundant retrievals during multi-round\nqueries, accompanying the difficulties in precisely characterizing knowledge\ngaps for complex tasks. To address these problems, we propose Retrieval\nFeedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms\nthe stateless retrieval of previous methods into stateful continuous knowledge\nmanagement by constructing a dynamic evidence pool. Specifically, our method\ngenerates refined queries describing the models knowledge gaps using relational\ntriples from questions and evidence from the dynamic evidence pool; Retrieves\ncritical external knowledge to iteratively update this evidence pool; Employs a\nR-Feedback Model to evaluate evidence completeness until convergence. Compared\nto traditional RAG methods, our approach enables persistent storage of\nretrieved passages and effectively distills key information from passages to\nconstruct clearly new queries. Experiments on three public QA benchmarks\ndemonstrate that RFM-RAG outperforms previous methods and improves overall\nsystem accuracy."
                },
                "authors": [
                    {
                        "name": "Leqian Li"
                    },
                    {
                        "name": "Dianxi Shi"
                    },
                    {
                        "name": "Jialu Zhou"
                    },
                    {
                        "name": "Xinyu Wei"
                    },
                    {
                        "name": "Mingyue Yang"
                    },
                    {
                        "name": "Songchang Jin"
                    },
                    {
                        "name": "Shaowu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shaowu Yang"
                },
                "author": "Shaowu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17858v1",
                "updated": "2025-08-25T10:07:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    7,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:07:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    7,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "LexSemBridge: Fine-Grained Dense Representation Enhancement through\n  Token-Aware Embedding Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexSemBridge: Fine-Grained Dense Representation Enhancement through\n  Token-Aware Embedding Augmentation"
                },
                "summary": "As queries in retrieval-augmented generation (RAG) pipelines powered by large\nlanguage models (LLMs) become increasingly complex and diverse, dense retrieval\nmodels have demonstrated strong performance in semantic matching. Nevertheless,\nthey often struggle with fine-grained retrieval tasks, where precise keyword\nalignment and span-level localization are required, even in cases with high\nlexical overlap that would intuitively suggest easier retrieval. To\nsystematically evaluate this limitation, we introduce two targeted tasks,\nkeyword retrieval and part-of-passage retrieval, designed to simulate practical\nfine-grained scenarios. Motivated by these observations, we propose\nLexSemBridge, a unified framework that enhances dense query representations\nthrough fine-grained, input-aware vector modulation. LexSemBridge constructs\nlatent enhancement vectors from input tokens using three paradigms: Statistical\n(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense\nembeddings via element-wise interaction. Theoretically, we show that this\nmodulation preserves the semantic direction while selectively amplifying\ndiscriminative dimensions. LexSemBridge operates as a plug-in without modifying\nthe backbone encoder and naturally extends to both text and vision modalities.\nExtensive experiments across semantic and fine-grained retrieval tasks validate\nthe effectiveness and generality of our approach. All code and models are\npublicly available at https://github.com/Jasaxion/LexSemBridge/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As queries in retrieval-augmented generation (RAG) pipelines powered by large\nlanguage models (LLMs) become increasingly complex and diverse, dense retrieval\nmodels have demonstrated strong performance in semantic matching. Nevertheless,\nthey often struggle with fine-grained retrieval tasks, where precise keyword\nalignment and span-level localization are required, even in cases with high\nlexical overlap that would intuitively suggest easier retrieval. To\nsystematically evaluate this limitation, we introduce two targeted tasks,\nkeyword retrieval and part-of-passage retrieval, designed to simulate practical\nfine-grained scenarios. Motivated by these observations, we propose\nLexSemBridge, a unified framework that enhances dense query representations\nthrough fine-grained, input-aware vector modulation. LexSemBridge constructs\nlatent enhancement vectors from input tokens using three paradigms: Statistical\n(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense\nembeddings via element-wise interaction. Theoretically, we show that this\nmodulation preserves the semantic direction while selectively amplifying\ndiscriminative dimensions. LexSemBridge operates as a plug-in without modifying\nthe backbone encoder and naturally extends to both text and vision modalities.\nExtensive experiments across semantic and fine-grained retrieval tasks validate\nthe effectiveness and generality of our approach. All code and models are\npublicly available at https://github.com/Jasaxion/LexSemBridge/"
                },
                "authors": [
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Hai Lin"
                    },
                    {
                        "name": "Hongming Tan"
                    },
                    {
                        "name": "Xiaodong Cai"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Ruitong Liu"
                    },
                    {
                        "name": "Hong-Gee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Gee Kim"
                },
                "author": "Hong-Gee Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17856v1",
                "updated": "2025-08-25T10:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    5,
                    44,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    5,
                    44,
                    0,
                    237,
                    0
                ],
                "title": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via\n  LLMs"
                },
                "summary": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development."
                },
                "authors": [
                    {
                        "name": "Tiezhu Sun"
                    },
                    {
                        "name": "Marco Alecci"
                    },
                    {
                        "name": "Aleksandr Pilgun"
                    },
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Xunzhu Tang"
                    },
                    {
                        "name": "Jordan Samhi"
                    },
                    {
                        "name": "Tegawend√© F. Bissyand√©"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "arxiv_comment": "Accepted at ICSME 2025, NIER Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01027v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01027v4",
                "updated": "2025-08-25T10:03:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    3,
                    43,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-03T03:44:35Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    3,
                    44,
                    35,
                    0,
                    34,
                    0
                ],
                "title": "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and\n  Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and\n  Guarantees"
                },
                "summary": "Two-stage Learning-to-Defer (L2D) enables optimal task delegation by\nassigning each input to either a fixed main model or one of several offline\nexperts, supporting reliable decision-making in complex, multi-agent\nenvironments. However, existing L2D frameworks assume clean inputs and are\nvulnerable to adversarial perturbations that can manipulate query\nallocation--causing costly misrouting or expert overload. We present the first\ncomprehensive study of adversarial robustness in two-stage L2D systems. We\nintroduce two novel attack strategie--untargeted and targeted--which\nrespectively disrupt optimal allocations or force queries to specific agents.\nTo defend against such threats, we propose SARD, a convex learning algorithm\nbuilt on a family of surrogate losses that are provably Bayes-consistent and\n$(\\mathcal{R}, \\mathcal{G})$-consistent. These guarantees hold across\nclassification, regression, and multi-task settings. Empirical results\ndemonstrate that SARD significantly improves robustness under adversarial\nattacks while maintaining strong clean performance, marking a critical step\ntoward secure and trustworthy L2D deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage Learning-to-Defer (L2D) enables optimal task delegation by\nassigning each input to either a fixed main model or one of several offline\nexperts, supporting reliable decision-making in complex, multi-agent\nenvironments. However, existing L2D frameworks assume clean inputs and are\nvulnerable to adversarial perturbations that can manipulate query\nallocation--causing costly misrouting or expert overload. We present the first\ncomprehensive study of adversarial robustness in two-stage L2D systems. We\nintroduce two novel attack strategie--untargeted and targeted--which\nrespectively disrupt optimal allocations or force queries to specific agents.\nTo defend against such threats, we propose SARD, a convex learning algorithm\nbuilt on a family of surrogate losses that are provably Bayes-consistent and\n$(\\mathcal{R}, \\mathcal{G})$-consistent. These guarantees hold across\nclassification, regression, and multi-task settings. Empirical results\ndemonstrate that SARD significantly improves robustness under adversarial\nattacks while maintaining strong clean performance, marking a critical step\ntoward secure and trustworthy L2D deployment."
                },
                "authors": [
                    {
                        "name": "Yannis Montreuil"
                    },
                    {
                        "name": "Axel Carlier"
                    },
                    {
                        "name": "Lai Xing Ng"
                    },
                    {
                        "name": "Wei Tsang Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tsang Ooi"
                },
                "author": "Wei Tsang Ooi",
                "arxiv_comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01027v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01027v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17852v1",
                "updated": "2025-08-25T09:59:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    59,
                    32,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T09:59:32Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    59,
                    32,
                    0,
                    237,
                    0
                ],
                "title": "Cross-Domain Lifelong Reinforcement Learning for Wireless Sensor\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Lifelong Reinforcement Learning for Wireless Sensor\n  Networks"
                },
                "summary": "Wireless sensor networks (WSNs) with energy harvesting (EH) are expected to\nplay a vital role in intelligent 6G systems, especially in industrial sensing\nand control, where continuous operation and sustainable energy use are\ncritical. Given limited energy resources, WSNs must operate efficiently to\nensure long-term performance. Their deployment, however, is challenged by\ndynamic environments where EH conditions, network scale, and traffic rates\nchange over time. In this work, we address system dynamics that yield different\nlearning tasks, where decision variables remain fixed but strategies vary, as\nwell as learning domains, where both decision space and strategies evolve. To\nhandle such scenarios, we propose a cross-domain lifelong reinforcement\nlearning (CD-L2RL) framework for energy-efficient WSN design. Our CD-L2RL\nalgorithm leverages prior experience to accelerate adaptation across tasks and\ndomains. Unlike conventional approaches based on Markov decision processes or\nLyapunov optimization, which assume relatively stable environments, our\nsolution achieves rapid policy adaptation by reusing knowledge from past tasks\nand domains to ensure continuous operations. We validate the approach through\nextensive simulations under diverse conditions. Results show that our method\nimproves adaptation speed by up to 35% over standard reinforcement learning and\nup to 70% over Lyapunov-based optimization, while also increasing total\nharvested energy. These findings highlight the strong potential of CD-L2RL for\ndeployment in dynamic 6G WSNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless sensor networks (WSNs) with energy harvesting (EH) are expected to\nplay a vital role in intelligent 6G systems, especially in industrial sensing\nand control, where continuous operation and sustainable energy use are\ncritical. Given limited energy resources, WSNs must operate efficiently to\nensure long-term performance. Their deployment, however, is challenged by\ndynamic environments where EH conditions, network scale, and traffic rates\nchange over time. In this work, we address system dynamics that yield different\nlearning tasks, where decision variables remain fixed but strategies vary, as\nwell as learning domains, where both decision space and strategies evolve. To\nhandle such scenarios, we propose a cross-domain lifelong reinforcement\nlearning (CD-L2RL) framework for energy-efficient WSN design. Our CD-L2RL\nalgorithm leverages prior experience to accelerate adaptation across tasks and\ndomains. Unlike conventional approaches based on Markov decision processes or\nLyapunov optimization, which assume relatively stable environments, our\nsolution achieves rapid policy adaptation by reusing knowledge from past tasks\nand domains to ensure continuous operations. We validate the approach through\nextensive simulations under diverse conditions. Results show that our method\nimproves adaptation speed by up to 35% over standard reinforcement learning and\nup to 70% over Lyapunov-based optimization, while also increasing total\nharvested energy. These findings highlight the strong potential of CD-L2RL for\ndeployment in dynamic 6G WSNs."
                },
                "authors": [
                    {
                        "name": "Hossein Mohammadi Firouzjaei"
                    },
                    {
                        "name": "Rafaela Scaciota"
                    },
                    {
                        "name": "Sumudu Samarakoon"
                    },
                    {
                        "name": "Beatriz Lorenzo"
                    }
                ],
                "author_detail": {
                    "name": "Beatriz Lorenzo"
                },
                "author": "Beatriz Lorenzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17850v1",
                "updated": "2025-08-25T09:57:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    57,
                    35,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T09:57:35Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    57,
                    35,
                    0,
                    237,
                    0
                ],
                "title": "Group Expectation Policy Optimization for Stable Heterogeneous\n  Reinforcement Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Expectation Policy Optimization for Stable Heterogeneous\n  Reinforcement Learning in LLMs"
                },
                "summary": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks."
                },
                "authors": [
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Ruibin Zheng"
                    },
                    {
                        "name": "Zexuan Yi"
                    },
                    {
                        "name": "Hanyang Peng"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Yue Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yu"
                },
                "author": "Yue Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17827v1",
                "updated": "2025-08-25T09:27:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    27,
                    31,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T09:27:31Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    27,
                    31,
                    0,
                    237,
                    0
                ],
                "title": "A Contrastive Learning-Guided Confident Meta-learning for Zero Shot\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Contrastive Learning-Guided Confident Meta-learning for Zero Shot\n  Anomaly Detection"
                },
                "summary": "Industrial and medical anomaly detection faces critical challenges from data\nscarcity and prohibitive annotation costs, particularly in evolving\nmanufacturing and healthcare settings. To address this, we propose CoZAD, a\nnovel zero-shot anomaly detection framework that integrates soft confident\nlearning with meta-learning and contrastive feature representation. Unlike\ntraditional confident learning that discards uncertain samples, our method\nassigns confidence-based weights to all training data, preserving boundary\ninformation while emphasizing prototypical normal patterns. The framework\nquantifies data uncertainty through IQR-based thresholding and model\nuncertainty via covariance based regularization within a Model-Agnostic\nMeta-Learning. Contrastive learning creates discriminative feature spaces where\nnormal patterns form compact clusters, enabling rapid domain adaptation.\nComprehensive evaluation across 10 datasets spanning industrial and medical\ndomains demonstrates state-of-the-art performance, outperforming existing\nmethods on 6 out of 7 industrial benchmarks with notable improvements on\ntexture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) and\npixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminates\ndependence on vision-language alignments or model ensembles, making it valuable\nfor resourceconstrained environments requiring rapid deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial and medical anomaly detection faces critical challenges from data\nscarcity and prohibitive annotation costs, particularly in evolving\nmanufacturing and healthcare settings. To address this, we propose CoZAD, a\nnovel zero-shot anomaly detection framework that integrates soft confident\nlearning with meta-learning and contrastive feature representation. Unlike\ntraditional confident learning that discards uncertain samples, our method\nassigns confidence-based weights to all training data, preserving boundary\ninformation while emphasizing prototypical normal patterns. The framework\nquantifies data uncertainty through IQR-based thresholding and model\nuncertainty via covariance based regularization within a Model-Agnostic\nMeta-Learning. Contrastive learning creates discriminative feature spaces where\nnormal patterns form compact clusters, enabling rapid domain adaptation.\nComprehensive evaluation across 10 datasets spanning industrial and medical\ndomains demonstrates state-of-the-art performance, outperforming existing\nmethods on 6 out of 7 industrial benchmarks with notable improvements on\ntexture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) and\npixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminates\ndependence on vision-language alignments or model ensembles, making it valuable\nfor resourceconstrained environments requiring rapid deployment."
                },
                "authors": [
                    {
                        "name": "Muhammad Aqeel"
                    },
                    {
                        "name": "Danijel Skocaj"
                    },
                    {
                        "name": "Marco Cristani"
                    },
                    {
                        "name": "Francesco Setti"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Setti"
                },
                "author": "Francesco Setti",
                "arxiv_comment": "Accepted to VISION Workshop at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17826v1",
                "updated": "2025-08-25T09:26:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    26,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T09:26:20Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    26,
                    20,
                    0,
                    237,
                    0
                ],
                "title": "LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with\n  Input-Adaptive Control Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with\n  Input-Adaptive Control Flow"
                },
                "summary": "Accurate and fast performance prediction for dataflow-based accelerators is\nvital for efficient hardware design and design space exploration, yet existing\nmethods struggle to generalize across architectures, applications, and\ninput-dependent control flows. We present LLMulator, a progressive numeric\nmodeling framework leveraging the program semantic knowledge of pre-trained\nlarge language models (LLMs) for robust, hardware- and application-aware\nprediction. Our numeric model treats performance values as categorical token\nsequences, enabling range-agnostic estimates and confidence-aware predictions\nfor unseen applications. To handle input-dependent control flows, we introduce\na reinforcement learning-based dynamic calibration method, reducing cycle\nprediction error by 9.7% over static models and converging to 11.2% error after\na few iterations. For cross-hardware generalization, we develop a progressive\ndata augmentation strategy that generates diverse datasets covering multi-level\ndataflow structures, memory parameters, and loop mapping primitives,\nsignificantly boosting prediction accuracy across architectures and\nconfigurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and fast performance prediction for dataflow-based accelerators is\nvital for efficient hardware design and design space exploration, yet existing\nmethods struggle to generalize across architectures, applications, and\ninput-dependent control flows. We present LLMulator, a progressive numeric\nmodeling framework leveraging the program semantic knowledge of pre-trained\nlarge language models (LLMs) for robust, hardware- and application-aware\nprediction. Our numeric model treats performance values as categorical token\nsequences, enabling range-agnostic estimates and confidence-aware predictions\nfor unseen applications. To handle input-dependent control flows, we introduce\na reinforcement learning-based dynamic calibration method, reducing cycle\nprediction error by 9.7% over static models and converging to 11.2% error after\na few iterations. For cross-hardware generalization, we develop a progressive\ndata augmentation strategy that generates diverse datasets covering multi-level\ndataflow structures, memory parameters, and loop mapping primitives,\nsignificantly boosting prediction accuracy across architectures and\nconfigurations."
                },
                "authors": [
                    {
                        "name": "Kaiyan Chang"
                    },
                    {
                        "name": "Wenlong Zhu"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "Accepted by MICRO (IEEE/ACM International Symposium on\n  Microarchitecture) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17825v1",
                "updated": "2025-08-25T09:26:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    26,
                    19,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T09:26:19Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    26,
                    19,
                    0,
                    237,
                    0
                ],
                "title": "FAIRGAMER: Evaluating Biases in the Application of Large Language Models\n  to Video Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIRGAMER: Evaluating Biases in the Application of Large Language Models\n  to Video Games"
                },
                "summary": "Leveraging their advanced capabilities, Large Language Models (LLMs)\ndemonstrate vast application potential in video games--from dynamic scene\ngeneration and intelligent NPC interactions to adaptive opponents--replacing or\nenhancing traditional game mechanics. However, LLMs' trustworthiness in this\napplication has not been sufficiently explored. In this paper, we reveal that\nthe models' inherent social biases can directly damage game balance in\nreal-world gaming environments. To this end, we present FairGamer, the first\nbias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks\nand a novel metrics ${D_lstd}$. It covers three key scenarios in games where\nLLMs' social biases are particularly likely to manifest: Serving as Non-Player\nCharacters, Interacting as Competitive Opponents, and Generating Game Scenes.\nFairGamer utilizes both reality-grounded and fully fictional game content,\ncovering a variety of video game genres. Experiments reveal: (1) Decision\nbiases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$\nscore=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate\nisomorphic social/cultural biases toward both real and virtual world content,\nsuggesting their biases nature may stem from inherent model characteristics.\nThese findings expose critical reliability gaps in LLMs' gaming applications.\nOur code and data are available at anonymous GitHub\nhttps://github.com/Anonymous999-xxx/FairGamer .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging their advanced capabilities, Large Language Models (LLMs)\ndemonstrate vast application potential in video games--from dynamic scene\ngeneration and intelligent NPC interactions to adaptive opponents--replacing or\nenhancing traditional game mechanics. However, LLMs' trustworthiness in this\napplication has not been sufficiently explored. In this paper, we reveal that\nthe models' inherent social biases can directly damage game balance in\nreal-world gaming environments. To this end, we present FairGamer, the first\nbias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks\nand a novel metrics ${D_lstd}$. It covers three key scenarios in games where\nLLMs' social biases are particularly likely to manifest: Serving as Non-Player\nCharacters, Interacting as Competitive Opponents, and Generating Game Scenes.\nFairGamer utilizes both reality-grounded and fully fictional game content,\ncovering a variety of video game genres. Experiments reveal: (1) Decision\nbiases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$\nscore=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate\nisomorphic social/cultural biases toward both real and virtual world content,\nsuggesting their biases nature may stem from inherent model characteristics.\nThese findings expose critical reliability gaps in LLMs' gaming applications.\nOur code and data are available at anonymous GitHub\nhttps://github.com/Anonymous999-xxx/FairGamer ."
                },
                "authors": [
                    {
                        "name": "Bingkang Shi"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Guoyi Li"
                    },
                    {
                        "name": "Xiaodan Zhang"
                    },
                    {
                        "name": "Zhongjiang Yao"
                    }
                ],
                "author_detail": {
                    "name": "Zhongjiang Yao"
                },
                "author": "Zhongjiang Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13832v2",
                "updated": "2025-08-25T09:24:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    24,
                    24,
                    0,
                    237,
                    0
                ],
                "published": "2025-03-18T02:14:17Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    2,
                    14,
                    17,
                    1,
                    77,
                    0
                ],
                "title": "Refined Criteria for QRAM Error Suppression via Efficient Large-Scale\n  QRAM Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refined Criteria for QRAM Error Suppression via Efficient Large-Scale\n  QRAM Simulator"
                },
                "summary": "Quantum random access memory (QRAM) is a critical primitive for quantum\nalgorithms that require data lookup in superposition, but its lack of fault\ntolerance poses a major obstacle to practical deployment. Error filtration (EF)\nhas been proposed as a hardware-efficient alternative to error correction,\ncapable of suppressing incoherent noise without encoding overhead. However, its\nperformance in realistic QRAM systems with moderate fidelity has remained\nunclear, as existing analyses rely on asymptotic approximations and numerical\nsimulations have been limited to small sizes. We address this gap using a new\nsimulator for bucket-brigade (BB) QRAM that combines sparse state encoding with\na noise-aware pruning algorithm. This framework provides full quantum state\naccess and scales efficiently, enabling us to probe EF performance in size and\nnoise regimes far beyond previous studies. Our simulations reveal suppression\nanomalies at high noise levels or large address sizes, where post-selection\nprobability fundamentally constrains EF scaling. Incorporating this effect, we\nrefine EF theory into near-deterministic criteria linking base infidelity to\nachievable suppression, thereby delineating the regime in which EF yields\nprogressive improvement. Beyond refining EF, we quantitatively characterize the\nruntime and memory costs of our noisy BB QRAM simulator, achieving simulations\nof systems with 20 layers using less than 1 GB of memory. This efficiency is\nwhat enables us to probe parameter regimes beyond previous work and to\nestablish the simulator as a practical, ``fine-print'' analysis tool for\nassessing QRAM as a quantum resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum random access memory (QRAM) is a critical primitive for quantum\nalgorithms that require data lookup in superposition, but its lack of fault\ntolerance poses a major obstacle to practical deployment. Error filtration (EF)\nhas been proposed as a hardware-efficient alternative to error correction,\ncapable of suppressing incoherent noise without encoding overhead. However, its\nperformance in realistic QRAM systems with moderate fidelity has remained\nunclear, as existing analyses rely on asymptotic approximations and numerical\nsimulations have been limited to small sizes. We address this gap using a new\nsimulator for bucket-brigade (BB) QRAM that combines sparse state encoding with\na noise-aware pruning algorithm. This framework provides full quantum state\naccess and scales efficiently, enabling us to probe EF performance in size and\nnoise regimes far beyond previous studies. Our simulations reveal suppression\nanomalies at high noise levels or large address sizes, where post-selection\nprobability fundamentally constrains EF scaling. Incorporating this effect, we\nrefine EF theory into near-deterministic criteria linking base infidelity to\nachievable suppression, thereby delineating the regime in which EF yields\nprogressive improvement. Beyond refining EF, we quantitatively characterize the\nruntime and memory costs of our noisy BB QRAM simulator, achieving simulations\nof systems with 20 layers using less than 1 GB of memory. This efficiency is\nwhat enables us to probe parameter regimes beyond previous work and to\nestablish the simulator as a practical, ``fine-print'' analysis tool for\nassessing QRAM as a quantum resource."
                },
                "authors": [
                    {
                        "name": "Yun-Jie Wang"
                    },
                    {
                        "name": "Tai-Ping Sun"
                    },
                    {
                        "name": "Xi-Ning Zhuang"
                    },
                    {
                        "name": "Xiao-Fan Xu"
                    },
                    {
                        "name": "Huan-Yu Liu"
                    },
                    {
                        "name": "Cheng Xue"
                    },
                    {
                        "name": "Yu-Chun Wu"
                    },
                    {
                        "name": "Zhao-Yun Chen"
                    },
                    {
                        "name": "Guo-Ping Guo"
                    }
                ],
                "author_detail": {
                    "name": "Guo-Ping Guo"
                },
                "author": "Guo-Ping Guo",
                "arxiv_comment": "14 pages / 8 figures (main text) and 11 pages / 4 figures\n  (appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08846v3",
                "updated": "2025-08-26T07:56:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    56,
                    10,
                    1,
                    238,
                    0
                ],
                "published": "2024-09-13T14:04:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    4,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint\n  Transfer via Vector Addition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint\n  Transfer via Vector Addition"
                },
                "summary": "Backdoor-based fingerprinting has emerged as an effective technique for\ntracing the ownership of large language models. However, in real-world\ndeployment scenarios, developers often instantiate multiple downstream models\nfrom a shared base model, and applying fingerprinting to each variant\nindividually incurs prohibitive computational overhead. While inheritance-based\napproaches -- where fingerprints are embedded into the base model and expected\nto persist through fine-tuning -- appear attractive, they suffer from three key\nlimitations: late-stage fingerprinting, fingerprint instability, and\ninterference with downstream adaptation. To address these challenges, we\npropose a novel mechanism called the Fingerprint Vector. Our method first\nembeds a fingerprint into the base model via backdoor-based fine-tuning, then\nextracts a task-specific parameter delta as a fingerprint vector by computing\nthe difference between the fingerprinted and clean models. This vector can be\ndirectly added to any structurally compatible downstream model, allowing the\nfingerprint to be transferred post hoc without additional fine-tuning.\nExtensive experiments show that Fingerprint Vector achieves comparable or\nsuperior performance to direct injection across key desiderata. It maintains\nstrong effectiveness across diverse model architectures as well as mainstream\ndownstream variants within the same family. It also preserves harmlessness and\nrobustness in most cases. Even when slight robustness degradation is observed,\nthe impact remains within acceptable bounds and is outweighed by the\nscalability benefits of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor-based fingerprinting has emerged as an effective technique for\ntracing the ownership of large language models. However, in real-world\ndeployment scenarios, developers often instantiate multiple downstream models\nfrom a shared base model, and applying fingerprinting to each variant\nindividually incurs prohibitive computational overhead. While inheritance-based\napproaches -- where fingerprints are embedded into the base model and expected\nto persist through fine-tuning -- appear attractive, they suffer from three key\nlimitations: late-stage fingerprinting, fingerprint instability, and\ninterference with downstream adaptation. To address these challenges, we\npropose a novel mechanism called the Fingerprint Vector. Our method first\nembeds a fingerprint into the base model via backdoor-based fine-tuning, then\nextracts a task-specific parameter delta as a fingerprint vector by computing\nthe difference between the fingerprinted and clean models. This vector can be\ndirectly added to any structurally compatible downstream model, allowing the\nfingerprint to be transferred post hoc without additional fine-tuning.\nExtensive experiments show that Fingerprint Vector achieves comparable or\nsuperior performance to direct injection across key desiderata. It maintains\nstrong effectiveness across diverse model architectures as well as mainstream\ndownstream variants within the same family. It also preserves harmlessness and\nrobustness in most cases. Even when slight robustness degradation is observed,\nthe impact remains within acceptable bounds and is outweighed by the\nscalability benefits of our approach."
                },
                "authors": [
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Qichen Liu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17814v1",
                "updated": "2025-08-25T09:11:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    11,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T09:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    11,
                    27,
                    0,
                    237,
                    0
                ],
                "title": "Scalable Engine and the Performance of Different LLM Models in a SLURM\n  based HPC architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Engine and the Performance of Different LLM Models in a SLURM\n  based HPC architecture"
                },
                "summary": "This work elaborates on a High performance computing (HPC) architecture based\non Simple Linux Utility for Resource Management (SLURM) [1] for deploying\nheterogeneous Large Language Models (LLMs) into a scalable inference engine.\nDynamic resource scheduling and seamless integration of containerized\nmicroservices have been leveraged herein to manage CPU, GPU, and memory\nallocations efficiently in multi-node clusters. Extensive experiments, using\nLlama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe\nthroughput, latency, and concurrency and show that small models can handle up\nto 128 concurrent requests at sub-50 ms latency, while for larger models,\nsaturation happens with as few as two concurrent users, with a latency of more\nthan 2 seconds. This architecture includes Representational State Transfer\nApplication Programming Interfaces (REST APIs) [4] endpoints for single and\nbulk inferences, as well as advanced workflows such as multi-step \"tribunal\"\nrefinement. Experimental results confirm minimal overhead from container and\nscheduling activities and show that the approach scales reliably both for batch\nand interactive settings. We further illustrate real-world scenarios, including\nthe deployment of chatbots with retrievalaugmented generation, which helps to\ndemonstrate the flexibility and robustness of the architecture. The obtained\nresults pave ways for significantly more efficient, responsive, and\nfault-tolerant LLM inference on large-scale HPC infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work elaborates on a High performance computing (HPC) architecture based\non Simple Linux Utility for Resource Management (SLURM) [1] for deploying\nheterogeneous Large Language Models (LLMs) into a scalable inference engine.\nDynamic resource scheduling and seamless integration of containerized\nmicroservices have been leveraged herein to manage CPU, GPU, and memory\nallocations efficiently in multi-node clusters. Extensive experiments, using\nLlama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe\nthroughput, latency, and concurrency and show that small models can handle up\nto 128 concurrent requests at sub-50 ms latency, while for larger models,\nsaturation happens with as few as two concurrent users, with a latency of more\nthan 2 seconds. This architecture includes Representational State Transfer\nApplication Programming Interfaces (REST APIs) [4] endpoints for single and\nbulk inferences, as well as advanced workflows such as multi-step \"tribunal\"\nrefinement. Experimental results confirm minimal overhead from container and\nscheduling activities and show that the approach scales reliably both for batch\nand interactive settings. We further illustrate real-world scenarios, including\nthe deployment of chatbots with retrievalaugmented generation, which helps to\ndemonstrate the flexibility and robustness of the architecture. The obtained\nresults pave ways for significantly more efficient, responsive, and\nfault-tolerant LLM inference on large-scale HPC infrastructures."
                },
                "authors": [
                    {
                        "name": "Anderson de Lima Luiz"
                    },
                    {
                        "name": "Shubham Vijay Kurlekar"
                    },
                    {
                        "name": "Munir Georges"
                    }
                ],
                "author_detail": {
                    "name": "Munir Georges"
                },
                "author": "Munir Georges",
                "arxiv_comment": "Accepted in ESSV 2025 - https://www.essv.de/paper.php?id=1265",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; D.4.7; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14165v2",
                "updated": "2025-08-25T08:54:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    8,
                    54,
                    19,
                    0,
                    237,
                    0
                ],
                "published": "2025-07-08T15:49:46Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    49,
                    46,
                    1,
                    189,
                    0
                ],
                "title": "A Multi-Modal IoT Node for Energy-Efficient Environmental Monitoring\n  with Edge AI Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Modal IoT Node for Energy-Efficient Environmental Monitoring\n  with Edge AI Processing"
                },
                "summary": "The widespread adoption of Internet of Things (IoT) technologies has\nsignificantly advanced environmental monitoring (EM) by enabling cost-effective\nand scalable sensing solutions. Concurrently, machine learning (ML) and\nartificial intelligence (AI) are introducing powerful tools for the efficient\nand accurate analysis of complex environmental data. However, current IoT\nplatforms for environmental sensing are typically limited to a narrow set of\nsensors, preventing a comprehensive assessment of environmental conditions and\nlacking sufficient computational capabilities to support the deployment of\nadvanced ML and AI algorithms on the edge. To overcome these limitations, we\nintroduce a compact (17x38 mm2), multi-modal, MCU-based environmental IoT node\nintegrating 11 sensors, including CO2 concentration, volatile organic compounds\n(VOCs), light intensity, UV radiation, pressure, temperature, humidity, visual\nsensing via an RGB camera, and precise geolocation through a GNSS module. It\nfeatures GAP9, a parallel ultra-low-power system-on-chip, enabling real-time,\nenergy-efficient edge processing of advanced ML models directly on-device. We\nimplemented a YOLOv5-based occupancy detection pipeline (0.3 M parameters, 42\nMOP per inference), demonstrating 42% energy savings over raw data streaming.\nAdditionally, we present a smart indoor air quality (IAQ) monitoring setup that\ncombines occupancy detection with adaptive sample rates, achieving operational\ntimes of up to 143 h on a single compact 600 mAh, 3.7 V battery. Our platform\nlays the groundwork for innovative applications such as predictive indoor IAQ,\nenabling efficient AI-driven on-edge forecasting for energy-efficient and\nautonomous, proactive pollution-mitigation control strategies",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Internet of Things (IoT) technologies has\nsignificantly advanced environmental monitoring (EM) by enabling cost-effective\nand scalable sensing solutions. Concurrently, machine learning (ML) and\nartificial intelligence (AI) are introducing powerful tools for the efficient\nand accurate analysis of complex environmental data. However, current IoT\nplatforms for environmental sensing are typically limited to a narrow set of\nsensors, preventing a comprehensive assessment of environmental conditions and\nlacking sufficient computational capabilities to support the deployment of\nadvanced ML and AI algorithms on the edge. To overcome these limitations, we\nintroduce a compact (17x38 mm2), multi-modal, MCU-based environmental IoT node\nintegrating 11 sensors, including CO2 concentration, volatile organic compounds\n(VOCs), light intensity, UV radiation, pressure, temperature, humidity, visual\nsensing via an RGB camera, and precise geolocation through a GNSS module. It\nfeatures GAP9, a parallel ultra-low-power system-on-chip, enabling real-time,\nenergy-efficient edge processing of advanced ML models directly on-device. We\nimplemented a YOLOv5-based occupancy detection pipeline (0.3 M parameters, 42\nMOP per inference), demonstrating 42% energy savings over raw data streaming.\nAdditionally, we present a smart indoor air quality (IAQ) monitoring setup that\ncombines occupancy detection with adaptive sample rates, achieving operational\ntimes of up to 143 h on a single compact 600 mAh, 3.7 V battery. Our platform\nlays the groundwork for innovative applications such as predictive indoor IAQ,\nenabling efficient AI-driven on-edge forecasting for energy-efficient and\nautonomous, proactive pollution-mitigation control strategies"
                },
                "authors": [
                    {
                        "name": "Philip Wiese"
                    },
                    {
                        "name": "Victor Kartsch"
                    },
                    {
                        "name": "Marco Guermandi"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/COINS65080.2025.11125738",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/COINS65080.2025.11125738",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.14165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 4 figures, 2 tables. This paper has been accepted at 2025\n  IEEE International Conference on Omni-layer Intelligent Systems (COINS)",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02148v2",
                "updated": "2025-08-25T08:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    8,
                    48,
                    25,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-04T07:47:18Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    7,
                    47,
                    18,
                    0,
                    216,
                    0
                ],
                "title": "Large-Scale Model Enabled Semantic Communication Based on Robust\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-Scale Model Enabled Semantic Communication Based on Robust\n  Knowledge Distillation"
                },
                "summary": "Large-scale models (LSMs) can be an effective framework for semantic\nrepresentation and understanding, thereby providing a suitable tool for\ndesigning semantic communication (SC) systems. However, their direct deployment\nis often hindered by high computational complexity and resource requirements.\nIn this paper, a novel robust knowledge distillation based semantic\ncommunication (RKD-SC) framework is proposed to enable efficient and\n\\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses\ntwo key challenges: determining optimal compact model architectures and\neffectively transferring knowledge while maintaining robustness against channel\nnoise. First, a knowledge distillation-based lightweight differentiable\narchitecture search (KDL-DARTS) algorithm is proposed. This algorithm\nintegrates knowledge distillation loss and a complexity penalty into the neural\narchitecture search process to identify high-performance, lightweight semantic\nencoder architectures. Second, a novel two-stage robust knowledge distillation\n(RKD) algorithm is developed to transfer semantic capabilities from an LSM\n(teacher) to a compact encoder (student) and subsequently enhance system\nrobustness. To further improve resilience to channel impairments, a\nchannel-aware transformer (CAT) block is introduced as the channel codec,\ntrained under diverse channel conditions with variable-length outputs.\nExtensive simulations on image classification tasks demonstrate that the RKD-SC\nframework significantly reduces model parameters while preserving a high degree\nof the teacher model's performance and exhibiting superior robustness compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models (LSMs) can be an effective framework for semantic\nrepresentation and understanding, thereby providing a suitable tool for\ndesigning semantic communication (SC) systems. However, their direct deployment\nis often hindered by high computational complexity and resource requirements.\nIn this paper, a novel robust knowledge distillation based semantic\ncommunication (RKD-SC) framework is proposed to enable efficient and\n\\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses\ntwo key challenges: determining optimal compact model architectures and\neffectively transferring knowledge while maintaining robustness against channel\nnoise. First, a knowledge distillation-based lightweight differentiable\narchitecture search (KDL-DARTS) algorithm is proposed. This algorithm\nintegrates knowledge distillation loss and a complexity penalty into the neural\narchitecture search process to identify high-performance, lightweight semantic\nencoder architectures. Second, a novel two-stage robust knowledge distillation\n(RKD) algorithm is developed to transfer semantic capabilities from an LSM\n(teacher) to a compact encoder (student) and subsequently enhance system\nrobustness. To further improve resilience to channel impairments, a\nchannel-aware transformer (CAT) block is introduced as the channel codec,\ntrained under diverse channel conditions with variable-length outputs.\nExtensive simulations on image classification tasks demonstrate that the RKD-SC\nframework significantly reduces model parameters while preserving a high degree\nof the teacher model's performance and exhibiting superior robustness compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Kuiyuan Ding"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhongtian Du"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17803v1",
                "updated": "2025-08-25T08:47:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    8,
                    47,
                    36,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T08:47:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    8,
                    47,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in\n  Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in\n  Reasoning Large Language Models"
                },
                "summary": "Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1,\nhave recently demonstrated remarkable capabilities by performing structured and\nmulti-step reasoning. However, recent studies reveal that RLLMs often suffer\nfrom overthinking, i.e., producing unnecessarily lengthy reasoning chains even\nfor simple questions, leading to excessive token consumption and computational\ninefficiency. Interestingly, we observe that when processing multiple questions\nin batch mode, RLLMs exhibit more resource-efficient behavior by dynamically\ncompressing reasoning steps for easier problems, due to implicit resource\ncompetition. Inspired by this, we propose Dynamic Reasoning Quota Allocation\n(DRQA), a novel method that transfers the benefits of resource competition from\nbatch processing to single-question inference. Specifically, DRQA leverages\nbatch-generated preference data and reinforcement learning to train the model\nto allocate reasoning resources adaptively. By encouraging the model to\ninternalize a preference for responses that are both accurate and concise, DRQA\nenables it to generate concise answers for simple questions while retaining\nsufficient reasoning depth for more challenging ones. Extensive experiments on\na wide range of mathematical and scientific reasoning benchmarks demonstrate\nthat DRQA significantly reduces token usage while maintaining, and in many\ncases improving, answer accuracy. By effectively mitigating the overthinking\nproblem, DRQA offers a promising direction for more efficient and scalable\ndeployment of RLLMs, and we hope it inspires further exploration into\nfine-grained control of reasoning behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1,\nhave recently demonstrated remarkable capabilities by performing structured and\nmulti-step reasoning. However, recent studies reveal that RLLMs often suffer\nfrom overthinking, i.e., producing unnecessarily lengthy reasoning chains even\nfor simple questions, leading to excessive token consumption and computational\ninefficiency. Interestingly, we observe that when processing multiple questions\nin batch mode, RLLMs exhibit more resource-efficient behavior by dynamically\ncompressing reasoning steps for easier problems, due to implicit resource\ncompetition. Inspired by this, we propose Dynamic Reasoning Quota Allocation\n(DRQA), a novel method that transfers the benefits of resource competition from\nbatch processing to single-question inference. Specifically, DRQA leverages\nbatch-generated preference data and reinforcement learning to train the model\nto allocate reasoning resources adaptively. By encouraging the model to\ninternalize a preference for responses that are both accurate and concise, DRQA\nenables it to generate concise answers for simple questions while retaining\nsufficient reasoning depth for more challenging ones. Extensive experiments on\na wide range of mathematical and scientific reasoning benchmarks demonstrate\nthat DRQA significantly reduces token usage while maintaining, and in many\ncases improving, answer accuracy. By effectively mitigating the overthinking\nproblem, DRQA offers a promising direction for more efficient and scalable\ndeployment of RLLMs, and we hope it inspires further exploration into\nfine-grained control of reasoning behaviors."
                },
                "authors": [
                    {
                        "name": "Kaiwen Yan"
                    },
                    {
                        "name": "Xuanqing Shi"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Chengwei Qin"
                    }
                ],
                "author_detail": {
                    "name": "Chengwei Qin"
                },
                "author": "Chengwei Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04352v2",
                "updated": "2025-08-25T08:34:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    8,
                    34,
                    46,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-04T17:16:51Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    16,
                    51,
                    1,
                    35,
                    0
                ],
                "title": "Investigating the Robustness of Deductive Reasoning with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Robustness of Deductive Reasoning with Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have been shown to achieve impressive results\nfor many reasoning-based NLP tasks, suggesting a degree of deductive reasoning\ncapability. However, it remains unclear to which extent LLMs, in both informal\nand autoformalisation methods, are robust on logical deduction tasks. Moreover,\nwhile many LLM-based deduction methods have been proposed, a systematic study\nthat analyses the impact of their design components is lacking. Addressing\nthese two challenges, we propose the first study of the robustness of formal\nand informal LLM-based deductive reasoning methods. We devise a framework with\ntwo families of perturbations: adversarial noise and counterfactual statements,\nwhich jointly generate seven perturbed datasets. We organize the landscape of\nLLM reasoners according to their reasoning format, formalisation syntax, and\nfeedback for error recovery. The results show that adversarial noise affects\nautoformalisation, while counterfactual statements influence all approaches.\nDetailed feedback does not improve overall accuracy despite reducing syntax\nerrors, pointing to the challenge of LLM-based methods to self-correct\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to achieve impressive results\nfor many reasoning-based NLP tasks, suggesting a degree of deductive reasoning\ncapability. However, it remains unclear to which extent LLMs, in both informal\nand autoformalisation methods, are robust on logical deduction tasks. Moreover,\nwhile many LLM-based deduction methods have been proposed, a systematic study\nthat analyses the impact of their design components is lacking. Addressing\nthese two challenges, we propose the first study of the robustness of formal\nand informal LLM-based deductive reasoning methods. We devise a framework with\ntwo families of perturbations: adversarial noise and counterfactual statements,\nwhich jointly generate seven perturbed datasets. We organize the landscape of\nLLM reasoners according to their reasoning format, formalisation syntax, and\nfeedback for error recovery. The results show that adversarial noise affects\nautoformalisation, while counterfactual statements influence all approaches.\nDetailed feedback does not improve overall accuracy despite reducing syntax\nerrors, pointing to the challenge of LLM-based methods to self-correct\neffectively."
                },
                "authors": [
                    {
                        "name": "Fabian Hoppe"
                    },
                    {
                        "name": "Filip Ilievski"
                    },
                    {
                        "name": "Jan-Christoph Kalo"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Christoph Kalo"
                },
                "author": "Jan-Christoph Kalo",
                "arxiv_comment": "to be published in ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05945v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05945v3",
                "updated": "2025-08-25T08:20:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    8,
                    20,
                    8,
                    0,
                    237,
                    0
                ],
                "published": "2025-02-09T16:11:57Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    16,
                    11,
                    57,
                    6,
                    40,
                    0
                ],
                "title": "Head-Specific Intervention Can Induce Misaligned AI Coordination in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Specific Intervention Can Induce Misaligned AI Coordination in\n  Large Language Models"
                },
                "summary": "Robust alignment guardrails for large language models (LLMs) are becoming\nincreasingly important with their widespread application. In contrast to\nprevious studies, we demonstrate that inference-time activation interventions\ncan bypass safety alignments and effectively steer model generations towards\nharmful AI coordination. Our method applies fine-grained interventions at\nspecific attention heads, which we identify by probing each head in a simple\nbinary choice task. We then show that interventions on these heads generalise\nto the open-ended generation setting, effectively circumventing safety\nguardrails. We demonstrate that intervening on a few attention heads is more\neffective than intervening on full layers or supervised fine-tuning. We further\nshow that only a few example completions are needed to compute effective\nsteering directions, which is an advantage over classical fine-tuning. We also\ndemonstrate that applying interventions in the negative direction can prevent a\ncommon jailbreak attack. Our results suggest that, at the attention head level,\nactivations encode fine-grained linearly separable behaviours. Practically, the\napproach offers a straightforward methodology to steer large language model\nbehaviour, which could be extended to diverse domains beyond safety, requiring\nfine-grained control over the model output. The code and datasets for this\nstudy can be found on https://github.com/PaulDrm/targeted_intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust alignment guardrails for large language models (LLMs) are becoming\nincreasingly important with their widespread application. In contrast to\nprevious studies, we demonstrate that inference-time activation interventions\ncan bypass safety alignments and effectively steer model generations towards\nharmful AI coordination. Our method applies fine-grained interventions at\nspecific attention heads, which we identify by probing each head in a simple\nbinary choice task. We then show that interventions on these heads generalise\nto the open-ended generation setting, effectively circumventing safety\nguardrails. We demonstrate that intervening on a few attention heads is more\neffective than intervening on full layers or supervised fine-tuning. We further\nshow that only a few example completions are needed to compute effective\nsteering directions, which is an advantage over classical fine-tuning. We also\ndemonstrate that applying interventions in the negative direction can prevent a\ncommon jailbreak attack. Our results suggest that, at the attention head level,\nactivations encode fine-grained linearly separable behaviours. Practically, the\napproach offers a straightforward methodology to steer large language model\nbehaviour, which could be extended to diverse domains beyond safety, requiring\nfine-grained control over the model output. The code and datasets for this\nstudy can be found on https://github.com/PaulDrm/targeted_intervention."
                },
                "authors": [
                    {
                        "name": "Paul Darm"
                    },
                    {
                        "name": "Annalisa Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Annalisa Riccardi"
                },
                "author": "Annalisa Riccardi",
                "arxiv_comment": "Published at Transaction of Machine Learning Research 08/2025, Large\n  Language Models (LLMs), Interference-time activation shifting, Steerability,\n  Explainability, AI alignment, Interpretability",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05945v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05945v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]